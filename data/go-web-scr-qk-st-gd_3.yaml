- en: Web Scraping Etiquette
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬取礼仪
- en: Before jumping into too much code, there are a few points you will need to keep
    in mind as you begin running a web scraper. It is important to remember that we
    all must be good citizens of the internet in order for everyone to get along.
    Keeping that in mind, there are many tools and best practices to follow in order
    to ensure that you are being fair and respectful when adding a load to an outside
    web server. Stepping outside of these guidelines could put your scraper at risk
    of being blocked by the web server, or in extreme cases, you could find yourself
    in legal trouble.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解太多代码之前，你需要记住一些要点，当你开始运行网络爬虫时。重要的是要记住，为了让每个人都和睦相处，我们都必须成为互联网的良好公民。牢记这一点，有许多工具和最佳实践可供遵循，以确保在向外部网络服务器添加负载时，你是公平和尊重的。违反这些准则可能会使你的网络爬虫面临被网络服务器屏蔽的风险，或者在极端情况下，你可能会陷入法律纠纷。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is a robots.txt file?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是robots.txt文件？
- en: What is a User-Agent string?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是用户代理字符串？
- en: How can you throttle your web scraper?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何限制你的网络爬虫？
- en: How do you use caching?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何使用缓存？
- en: What is a robots.txt file?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是robots.txt文件？
- en: Most of the pages on a website are free to be accessed by web scrapers and bots.
    Some of the reasons for allowing this are in order to be indexed by search engines
    or to allow pages to be discovered by content curators. Googlebot is one of the
    tools that most websites would be more than happy to give access to their content.
    However, there are some sites that may not want everything to show up in a Google
    search result. Imagine if you could google a person and instantly obtain all of
    their social media profiles, complete with contact information and address. This
    would be bad news for the person, and certainly not a good privacy policy for
    the company hosting the site. In order to control access to different parts of
    a website, you would configure a `robots.txt` file.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网站页面都可以被网络爬虫和机器人访问。允许这样做的原因之一是为了被搜索引擎索引，或者允许内容策展人发现页面。Googlebot是大多数网站都很乐意让其访问其内容的工具之一。然而，有些网站可能不希望所有内容都出现在谷歌搜索结果中。想象一下，如果你可以谷歌一个人，立即获得他们所有的社交媒体资料，包括联系信息和地址。这对这个人来说是个坏消息，对于托管网站的公司来说也不是一个好的隐私政策。为了控制网站不同部分的访问权限，你需要配置一个`robots.txt`文件。
- en: 'The `robots.txt` file is typically hosted at the root of the website in the
    `/robots.txt` resource. This file contains definitions of who can access which
    pages in this website. This is done by describing a bot that matches a `User-Agent`
    string, and specifying which paths are allowed and disallowed. Wildcards are also
    supported in the `Allow` and `Disallow` statements. The following is an example
    `robots.txt` file from Twitter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`robots.txt`文件通常托管在网站的根目录下的`/robots.txt`资源中。这个文件包含了谁可以访问网站中的哪些页面的定义。这是通过描述与`User-Agent`字符串匹配的机器人，并指定允许和不允许的路径来完成的。通配符也支持在`Allow`和`Disallow`语句中。以下是Twitter的一个例子`robots.txt`文件：'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is the most restrictive `robots.txt` file you will encounter. It states
    that no web scraper can access any part of [twitter.com](http://twitter.com).
    Violating this will put your scraper at risk of being blacklisted by Twitter''s
    servers. On the other hand, websites like Medium are a little more permissive.
    Here is their `robots.txt` file:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可能会遇到的最严格的`robots.txt`文件。它声明没有网络爬虫可以访问[twitter.com](http://twitter.com)的任何部分。违反这一规定将使你的网络爬虫面临被Twitter服务器列入黑名单的风险。另一方面，像Medium这样的网站则更加宽容。以下是他们的`robots.txt`文件：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Looking into this, you can see that editing profiles is disallowed by the following
    directives:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这些，你可以看到编辑配置文件是被以下指令禁止的：
- en: '`Disallow: /*/edit$`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Disallow: /*/edit$`'
- en: '`Disallow: /*/*/edit$`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Disallow: /*/*/edit$`'
- en: 'The pages that are related to logging in and signing up, which could be used
    for automated account creation, are also disallowed by `Disallow: /m/`.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '与登录和注册相关的页面，这可能被用于自动帐户创建，也被`Disallow: /m/`禁止访问。'
- en: If you value your scraper, do not access these pages. The `Allow` statements
    provide explicit permission to paths in the in `/_/` routes, as well as some `api`
    related resources. Outside of what is defined here, if there is no explicit `Disallow`
    statement, then your scraper has permission to access the information. In the
    case of Medium, this includes all of the publicly available articles, as well
    as public information about the authors and publications. This `robots.txt` file
    also includes a `sitemap`, which is an XML-encoded file listing all of the pages
    available on the website. You can think of this as a giant index, which can come
    in very handy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你重视你的网络爬虫，不要访问这些页面。`Allow`语句明确允许`/_/`路径中的路径，以及一些`api`相关资源。除了这里定义的内容，如果没有明确的`Disallow`语句，那么你的网络爬虫有权限访问这些信息。在Medium的情况下，这包括所有公开可用的文章，以及关于作者和出版物的公开信息。这个`robots.txt`文件还包括一个`sitemap`，这是一个列出网站上所有页面的XML编码文件。你可以把它想象成一个巨大的索引，非常有用。
- en: 'One more example of a `robots.txt` file shows how a site defines rules for
    different `User-Agent` instances. The following `robots.txt` file is from Adidas:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`robots.txt`文件的另一个例子显示了一个网站为不同的`User-Agent`实例定义规则。以下`robots.txt`文件来自Adidas：'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example explicitly disallows access to a few paths for all web scrapers,
    as well as a special note for `bingbot`. The `bingbot` must respect the `Crawl-delay`
    of `1` second, meaning it cannot access any pages more than once per second. `Crawl-delays`
    are very important to take note of, as they will define how quickly you can make
    web requests. Violating this may generate more errors for your web scraper, or
    it may be permanently blocked.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子明确禁止所有网络爬虫访问一些路径，以及对`bingbot`的特殊说明。`bingbot`必须遵守`Crawl-delay`为`1`秒的规定，这意味着它不能每秒访问超过一次的页面。要注意`Crawl-delays`非常重要，因为它们将定义你可以多快地进行网络请求。违反这一规定可能会为你的网络爬虫产生更多的错误，或者它可能会被永久屏蔽。
- en: What is a User-Agent string?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是用户代理字符串？
- en: 'When an HTTP client makes a request to a web server, they identify who they
    are. This holds true for web scrapers and normal browsers alike. Have you ever
    wondered why a website knows that you are a Windows or a Mac user? This information
    is contained inside your `User-Agent` string. Here is an example `User-Agent`
    string for a Firefox browser on a Linux computer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当HTTP客户端向Web服务器发出请求时，它们会标识自己的身份。这对于网络爬虫和普通浏览器都是成立的。你是否曾经想过为什么一个网站知道你是Windows用户还是Mac用户？这些信息包含在你的`User-Agent`字符串中。以下是Linux计算机上Firefox浏览器的`User-Agent`字符串示例：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can see that this string identifies the family, name, and version of the
    web browser, as well as the operating system. This string will be sent with every
    request from this browser inside of a request header, such as the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个字符串标识了Web浏览器的系列、名称和版本，以及操作系统。这个字符串将随着每个来自该浏览器的请求一起发送到请求头，例如以下内容：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Not all `User-Agent` strings contain this much information. HTTP clients that
    are not web browsers are typically much smaller. Here are some examples:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的`User-Agent`字符串都包含这么多信息。非网页浏览器的HTTP客户端通常要小得多。以下是一些示例：
- en: 'cURL: `curl/7.47.0`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'cURL: `curl/7.47.0`'
- en: 'Go: `Go-http-client/1.1`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Go: `Go-http-client/1.1`'
- en: 'Java: `Apache-HttpClient/4.5.2`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Java: `Apache-HttpClient/4.5.2`'
- en: 'Googlebot (for images): `Googlebot-Image/1.0`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Googlebot（用于图像）：`Googlebot-Image/1.0`
- en: '`User-Agent` strings are a good way of introducing your bot and taking responsibility
    for following the rules set in a `robots.txt` file. By using this mechanism, you
    will be held accountable for any violations.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`User-Agent`字符串是介绍你的机器人并对遵循`robots.txt`文件中设置的规则负责的一种好方法。通过使用这种机制，你将对任何违规行为负责。'
- en: Example
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: 'There are open source tools available that help parse `robots.txt` files and
    validate website URLs against them to see if you have access or not. One project
    that I would recommend is available on GitHub called `robotstxt` by user `temoto`.
    In order to download this library, run the following command in your terminal:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些开源工具可用于解析`robots.txt`文件并根据其验证网站URL，以查看你是否有访问权限。我推荐的一个项目在GitHub上叫做`temoto`的`robotstxt`。为了下载这个库，在你的终端中运行以下命令：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `$GOPATH` referred to here is the one you set up during the installation
    of the Go programming language back in [Chapter 1](005f17ec-d83f-4acc-9132-6ee89bc86f1e.xhtml), *Introducing
    Web Scraping and Go*. This is the directory with the `src/ bin/` and `pkg/ directories`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提到的`$GOPATH`是你在Go编程语言安装中设置的路径，见[第1章](005f17ec-d83f-4acc-9132-6ee89bc86f1e.xhtml)，*介绍Web爬虫和Go*。这是带有`src/
    bin/`和`pkg/`目录的目录。
- en: 'This will install the library on your machine at `$GOPATH/src/github/temoto/robotstxt`.
    If you would like, you can read the code to see how it all works. For the sake
    of this book, we will just be using the library in our own project. Inside your
    `$GOPATH/src` folder, create a new folder called `robotsexample`. Create a `main.go`
    file inside the `robotsexample` folder. The following code for `main.go` shows
    you a simple example of how to use the `temoto/robotstxt` package:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的机器上安装库到`$GOPATH/src/github/temoto/robotstxt`。如果愿意，你可以阅读代码以了解其工作原理。为了本书的目的，我们将只在我们自己的项目中使用该库。在你的`$GOPATH/src`文件夹中，创建一个名为`robotsexample`的新文件夹。在`robotsexample`文件夹中创建一个`main.go`文件。以下是`main.go`的代码，展示了如何使用`temoto/robotstxt`包的简单示例：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This example uses Go for each loop using the `range` operator. The `range` operator
    returns two variables, the first is the `index` of the `iteration` (which we ignore
    by assigning it to `_`), and the second is the value at that index.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例使用Go语言的`range`操作符进行每个循环。`range`操作符返回两个变量，第一个是`迭代`的`索引`（我们通过将其分配给`_`来忽略），第二个是该索引处的值。
- en: This code checks six different paths against the `robots.txt` file for [https://www.packtpub.com/](https://www.packtpub.com/),
    using the default `User-Agent` string for the Go HTTP client. If the `User-Agent`
    is allowed to access a page, then the `Test()` method returns `true`. If it returns
    `false`, then your scraper should not access this section of the website.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码对[https://www.packtpub.com/](https://www.packtpub.com/)的`robots.txt`文件检查了六条不同的路径，使用了Go
    HTTP客户端的默认`User-Agent`字符串。如果`User-Agent`被允许访问页面，则`Test()`方法返回`true`。如果返回`false`，则你的爬虫不应该访问网站的这一部分。
- en: How to throttle your scraper
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何限制你的爬虫速度
- en: Part of good web scraping etiquette is making sure you are not putting too much
    load on your target web server. This means limiting the number of requests you
    make within a certain period of time. For smaller servers, this is especially
    true, as they have a much more limited pool of resources. As a good rule of thumb,
    you should only access the same web page as often as you think it will change.
    For example, if you were looking at daily deals, you would probably only need
    to scrape once per day. As for scraping multiple pages from the same website,
    you should first follow the `Crawl-Delay` in a `robots.txt` file. If there is
    no `Crawl-Delay` specified, then you should manually delay your requests by one
    second after every page.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的网络爬取礼仪的一部分是确保你不会对目标Web服务器施加太大的负载。这意味着限制你在一定时间内发出的请求数量。对于较小的服务器，这一点尤为重要，因为它们的资源池更有限。一个很好的经验法则是，你应该只在你认为页面会发生变化的时候访问相同的网页。例如，如果你正在查看每日优惠，你可能只需要每天爬取一次。至于从同一个网站爬取多个页面，你应该首先遵循`robots.txt`文件中的`Crawl-Delay`。如果没有指定`Crawl-Delay`，那么你应该在每个页面后手动延迟一秒钟。
- en: There are many different ways to incorporate delays into your crawler, from
    manually putting your program to sleep to using external queues and worker threads.
    This section will explain a few basic techniques. We will revisit more complicated
    examples when we discuss the Go programming language concurrency model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以将延迟纳入你的爬虫中，从手动让程序休眠到使用外部队列和工作线程。本节将解释一些基本技术。当我们讨论Go编程语言并发模型时，我们将重新讨论更复杂的示例。
- en: 'The simplest way to add throttle to your web scraper is to track the timestamps
    for requests that are made, and ensure that the elapsed time is greater than your
    desired rate. For example, if you were to scrape at a rate of one page per `5`
    seconds, it would look something like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 向您的网络爬虫添加节流的最简单方法是跟踪发出的请求的时间戳，并确保经过的时间大于您期望的速率。例如，如果您要以每5秒一个页面的速率抓取，它可能是这样的：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This example has many instances of `:=` when defining variables. This is a
    shorthand way, in Go, of simultaneously declaring and instantiating variables.
    It replaces the need to say the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例在定义变量时有许多`:=`的实例。这是Go中同时声明和实例化变量的简写方式。它取代了需要说以下内容：
- en: '`var a string`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`var a string`'
- en: '`a = "value"`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: a = "value"
- en: 'Instead, it becomes:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，它变成了：
- en: '`a := "value"`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`a := "value"`'
- en: In this example, we make requests to [http://www.example.com/index.html](http://www.example.com/index.html)
    once every five seconds. We know how long it has been since our last request,
    as we update the `lastRequestTime` variable and check it before we make each request.
    This is all you need to scrape a single website, even if you were scraping multiple
    pages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们每隔五秒向[http://www.example.com/index.html](http://www.example.com/index.html)发出一次请求。我们知道自上次请求以来的时间有多长，因为我们更新`lastRequestTime`变量并在进行每个请求之前检查它。这就是您抓取单个网站所需的全部内容，即使您要抓取多个页面。
- en: 'If you were scraping multiple websites, you would need to separate `lastRequestTime`
    into one variable per website. The simplest way to do this would be with a `map`,
    Go''s key-value structure, where the key would be the host and the value would
    be the timestamp for the last request. This would replace the definition with
    something like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要从多个网站抓取数据，您需要将`lastRequestTime`分成每个网站一个变量。最简单的方法是使用`map`，Go的键值结构，其中键将是主机名，值将是上次请求的时间戳。这将替换定义为以下内容：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our `for` loop would also change slightly and set the value of the map to the
    current scrape time, but only for the website, we are scraping. For example, if
    we were to scrape the pages in an alternating manner, it might look something
    like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`for`循环也会稍微改变，并将地图的值设置为当前抓取时间，但仅适用于我们正在抓取的网站。例如，如果我们要交替抓取页面，可能会是这样的：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, to update the map with the last known request time, we would use a
    similar block:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要更新地图的最后已知请求时间，我们将使用类似的块：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can find the full source code for this example on GitHub.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到此示例的完整源代码。
- en: If you look at the output in the terminal, you will see that the first requests
    to either site have no delay and each sleep period is slightly less than five
    seconds now. This shows that the crawler is respecting each site's rate independently.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看终端中的输出，您将看到对任一站点的第一个请求没有延迟，每个休眠期略少于五秒。这表明爬虫正在独立地尊重每个站点的速率。
- en: How to use caching
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用缓存
- en: One last technique that can benefit your scraper, as well as reducing load on
    the website, is by only requesting new content when it changes. If your scraper
    is downloading the same old content from a web server, then you aren't getting
    any new information and the web server is doing unnecessary work. For this reason,
    most web servers implement techniques to provide the client with instructions
    on caching.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个可以使您的爬虫受益并减少网站负载的技术是仅在内容更改时请求新内容。如果您的爬虫从Web服务器下载相同的旧内容，那么您将不会获得任何新信息，而Web服务器则会做不必要的工作。因此，大多数Web服务器实施技术以向客户端提供有关缓存的指令。
- en: 'A website that supports caching, will give the client information on what it
    can store, and how long to store it. This is done through response headers such
    as `Cache-Control`, `Etag`, `Date`, `Expires`, and `Vary`. Your web scraper should
    be aware of these directives to avoid making unnecessary requests to the web server,
    saving you, and the server, time and computing resources. Let''s take a look at
    our [http://www.example.com/index.html](http://www.example.com/index.html) response
    one more time, given as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 支持缓存的网站将向客户端提供有关可以存储什么以及存储多长时间的信息。这是通过响应头，如`Cache-Control`，`Etag`，`Date`，`Expires`和`Vary`来完成的。您的网络爬虫应该了解这些指令，以避免向网络服务器发出不必要的请求，从而节省您和服务器的时间和计算资源。让我们再次看看我们的[http://www.example.com/index.html](http://www.example.com/index.html)响应，如下所示：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The body of the response is not included in this example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的正文在此示例中未包含。
- en: There are a few response headers used to communicate caching instructions that
    you should follow in order to increase the efficiency of your web scraper. These
    headers will inform you of what information to cache, for how long, and a few
    other helpful pieces of information to make life easier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些响应头用于传达缓存指令，您应该遵循这些指令以增加网络爬虫的效率。这些头部将告诉您应缓存什么信息，以及多长时间，以及其他一些有用的信息，以使生活更轻松。
- en: Cache-Control
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cache-Control
- en: 'The `Cache-Control` header is used to indicate whether this content is cacheable,
    and for how long. Some of the common values for this header are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cache-Control`头用于指示此内容是否可缓存以及缓存多长时间。此头的常见值如下：'
- en: '`no-cache`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no-cache`'
- en: '`no-store`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no-store`'
- en: '`must-revalidated`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`must-revalidated`'
- en: '`max-age=<seconds>`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max-age=<seconds>`'
- en: '`public`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`public`'
- en: Cache directives such as `no-cache`, `no-store`, and `must-revalidate` exist
    to prevent the client from caching the response. Sometimes, the server is aware
    that the content on this page changes frequently, or is dependent on a source
    outside of its control. If none of these directives are sent, you should be able
    to cache the response using the provided `max-age` directive. This defines the
    number of seconds you should consider this content as fresh. After this time,
    the response is said to be stale and a new request should be made to the server.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存指令，如`no-cache`，`no-store`和`must-revalidate`存在是为了防止客户端缓存响应。有时，服务器知道此页面上的内容经常更改，或者依赖于其控制范围之外的来源。如果没有发送这些指令中的任何一个，您应该能够使用提供的`max-age`指令缓存响应。这定义了您应将此内容视为新鲜的秒数。此时间后，响应被认为是陈旧的，并且应向服务器发出新请求。
- en: 'In the response from the previous example, the server sends a `Cache-Control`
    header:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例的响应中，服务器发送了一个`Cache-Control`标头：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This indicates that you should cache this page for up to `604880` seconds (seven
    days).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示您应该将此页面缓存长达`604880`秒（七天）。
- en: Expires
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Expires
- en: The `Expires` header is another way of defining how long to retain cached information.
    This header defines an exact date and time from which the content will be considered
    stale and should be refreshed. This time should coincide with the `max-age` directive
    from the `Cache-Control` header, if one is provided.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Expires`标头是另一种定义保留缓存信息时间的方法。此标头定义了确切的日期和时间，从该日期和时间开始，内容将被视为过时并应该被刷新。如果提供了`Cache-Control`标头的`max-age`指令，这个时间应该与之相符。'
- en: 'In our example, the `Expires` header matches the 7-day expiration based on
    the `Date` header, which defines when the request was received by the server:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`Expires`标头与`Date`标头匹配，根据`Date`标头定义了请求何时被服务器接收，从而定义了7天的到期时间：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Etag
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Etag
- en: 'The `Etag` is also important in keeping cached information. This is a key unique
    to this page, and will only change if the content of the page changes. After the
    cache expires, you can use this tag to check with the server if there is actually
    new content, without downloading a fresh copy. This works by sending an `If-None-Match`
    header containing the `Etag` value. When this happens, the server will check if
    the `Etag` on the current resource matches the `Etag` in the `If-None-Match` header.
    If it does match, then there have been no updates and the server responds with
    a status code of 304 Not Modified, with some headers to extend your cache. The
    following is an example of a `304` response:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`Etag`也是保持缓存信息的重要内容。这是此页面的唯一密钥，只有在页面内容更改时才会更改。在缓存过期后，您可以使用此标记与服务器检查是否实际上有新内容，而无需下载新副本。这通过发送包含`Etag`值的`If-None-Match`标头来实现。当发生这种情况时，服务器将检查当前资源上的`Etag`是否与`If-None-Match`标头中的`Etag`匹配。如果匹配，则表示没有更新，服务器将以304
    Not Modified的状态代码响应，并附加一些标头以扩展您的缓存。以下是`304`响应的示例：'
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The server, in this case, validates the `Etag` and provides a new `Expires`
    time, still matching the `max-age`, from the time this second request was fulfilled.
    This way, you still save time by not needing to read more data over the network.
    You can still use your cached pages to fulfill your needs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，服务器验证`Etag`并提供一个新的`Expires`时间，仍然与`max-age`匹配，从第二次请求被满足的时间开始。这样，您仍然可以节省时间，而无需通过网络读取更多数据。您仍然可以使用缓存的页面来满足您的需求。
- en: Caching content in Go
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Go中缓存内容
- en: The storage and retrieval of cached pages can be implemented by hand using your
    local filesystem, or a database to hold the data and the cached information. There
    are also open source tools available to help simplify this technique. One such
    project is `httpcache` by GitHub user `gregjones`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存页面的存储和检索可以通过手动使用本地文件系统来实现，也可以通过数据库来保存数据和缓存信息。还有一些开源工具可用于简化这种技术。其中一个项目是GitHub用户`gregjones`的`httpcache`。
- en: The `httpcache` follows the caching requirements set by the **Internet Engineering
    Task Force** (**IETF**), the governing body for internet standards. The library
    provides a module that can store and retrieve web pages from your local machine,
    as well as a plugin for the Go HTTP client to automatically handle all cache-related
    HTTP request and response headers. It also provides multiple storage backends
    where you can store the cached information, such as Redis, Memcached, and LevelDB.
    This will allow you to run a web scraper on different machines, but connect to
    the same cached information.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`httpcache`遵循**互联网工程任务组**（**IETF**）制定的缓存要求，这是互联网标准的管理机构。该库提供了一个模块，可以从本地机器存储和检索网页，以及一个用于Go
    HTTP客户端的插件，自动处理所有与缓存相关的HTTP请求和响应标头。它还提供多个存储后端，您可以在其中存储缓存信息，如Redis、Memcached和LevelDB。这将允许您在不同的机器上运行网络爬虫，但连接到相同的缓存信息。'
- en: As your scraper grows in size and you need to design a distributed architecture,
    a feature like this will be critical to ensure that time and resources are not
    wasted on duplicated work. Stable communication between all of your scrapers is
    key!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您的爬虫规模的增长，您需要设计分布式架构，像这样的功能将是至关重要的，以确保时间和资源不会浪费在重复的工作上。所有爬虫之间的稳定通信至关重要！
- en: 'Let''s take a look at an example using `httpcache`. First, install `httpcache`
    by typing the following commands into your terminal, shown as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用`httpcache`的示例。首先，通过在终端中输入以下命令来安装`httpcache`，如下所示：
- en: '`go get github.com/gregjones/httpcache`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`go get github.com/gregjones/httpcache`'
- en: '`go get github.com/peterbourgon/diskv`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`go get github.com/peterbourgon/diskv`'
- en: The `diskv` project is used by `httpcache` to store the web page on your local
    machine.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`diskv`项目被`httpcache`用于在本地机器上存储网页。'
- en: 'Inside your `$GOPATH/src`, create a folder called `cache` with a `main.go`
    inside it. Use the following code for your `main.go` file:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的`$GOPATH/src`中，创建一个名为`cache`的文件夹，并在其中创建一个`main.go`。使用以下代码作为您的`main.go`文件：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This program uses the local disk cache to store the response from [http://www.example.com/index.html](http://www.example.com/index.html).
    Under the hood, it reads all of the cache-related headers to determine if it can
    store the page and includes the expiration date with the data. On the second request,
    `httpcache` checks if the content is expired and returns the cached data instead
    of making another HTTP request. It also adds an extra header, `X-From-Cache`,
    to indicate that this is being read from the cache. If the page had expired, it
    would make the HTTP request with the `If-None-Match` header and handle the response,
    including updating the cache in case of a 304 Not Modified response.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序使用本地磁盘缓存来存储来自[http://www.example.com/index.html](http://www.example.com/index.html)的响应。在底层，它读取所有与缓存相关的标头，以确定是否可以存储页面，并将到期日期与数据一起包括在内。在第二次请求时，`httpcache`检查内容是否已过期，并返回缓存的数据，而不是进行另一个HTTP请求。它还添加了一个额外的标头`X-From-Cache`，以指示这是从缓存中读取的。如果页面已过期，它将使用`If-None-Match`标头进行HTTP请求，并处理响应，包括在304
    Not Modified响应的情况下更新缓存。
- en: Using a client that is automatically set up to handle caching content will make
    your scraper run faster, as well as reducing the likelihood that your web scraper
    will be flagged as a bad citizen. When this is done in combination with respecting
    a website's `robots.txt` file and properly throttling your requests, you can scrape
    confidently, knowing that you are a respectable member of the web community.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动设置好处理缓存内容的客户端将使您的爬虫运行更快，同时减少您的网络爬虫被标记为不良公民的可能性。当这与尊重网站的`robots.txt`文件和适当节流请求结合使用时，您可以自信地进行爬取，知道自己是网络社区中值得尊敬的成员。
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the basic etiquette for respectfully crawling the
    web. You learned what a `robots.txt` file is and why it is important to obey it.
    You also learned how to properly represent yourself using `User-Agent` strings. Controlling
    your scraper via throttling and caching was also covered. With these skills, you
    are one step closer to building a fully functional web scraper.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学会了尊重地爬取网络的基本礼仪。您了解了什么是`robots.txt`文件，以及遵守它的重要性。您还学会了如何使用`User-Agent`字符串来正确表示自己。还介绍了通过节流和缓存来控制您的爬虫。有了这些技能，您离构建一个完全功能的网络爬虫又近了一步。
- en: In [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing HTML*,
    we will look at how to extract information from HTML pages using various techniques.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml)中，*解析HTML*，我们将学习如何使用各种技术从HTML页面中提取信息。
