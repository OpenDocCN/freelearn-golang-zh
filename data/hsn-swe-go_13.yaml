- en: Building, Packaging, and Deploying Software
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建、打包和部署软件
- en: '"Kubernetes is the Linux of distributed systems."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"Kubernetes 是分布式系统的 Linux。"'
- en: – Kelsey Hightower
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – Kelsey Hightower
- en: This chapter will guide you through the steps involved in dockerizing Go programs
    and will iterate the best practices for building the smallest possible container
    image for your applications. Following this, this chapter will focus on Kubernetes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导您完成将 Go 程序 docker 化的步骤，并迭代构建您应用程序可能的最小容器镜像的最佳实践。在此之后，本章将专注于 Kubernetes。
- en: We'll begin our tour of Kubernetes by comparing the different types of nodes
    that comprise a Kubernetes cluster and take a closer look at the function of the
    various services that make up Kubernetes' control plane. Moving forward, we will
    be describing a step-by-step walkthrough for setting up a Kubernetes cluster on
    your local development machine. The last part of this chapter is a practical application
    of everything you have learned so far. We will bring all the components that we
    created in the previous chapters together, join them with a fully functioning
    frontend, and create a monolithic version of Links 'R' Us that we will then deploy
    on Kubernetes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始对 Kubernetes 的探索之旅，通过比较构成 Kubernetes 集群的节点类型，并更深入地了解构成 Kubernetes 控制平面的各种服务的功能。接下来，我们将描述如何在您的本地开发机器上设置
    Kubernetes 集群的逐步指南。本章的最后一部分是对您迄今为止所学内容的实际应用。我们将把之前章节中创建的所有组件集中起来，与一个完全功能的前端连接，并创建一个单一代码库的
    Links 'R' Us 版本，然后将其部署到 Kubernetes 上。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Using intermediate build containers to compile static binaries for your Go applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用中间构建容器为您的 Go 应用程序编译静态二进制文件
- en: Using the correct set of linker flags to ensure that Go binaries compile to
    the smallest possible size
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正确的链接器标志以确保 Go 可执行文件编译成尽可能小的尺寸
- en: The anatomy of the components that comprise a Kubernetes cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构成 Kubernetes 集群的组件的解剖结构
- en: The different types of resource types supported by Kubernetes and their application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 支持的不同类型的资源类型及其应用
- en: Spinning up a Kubernetes cluster on your local workstation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的本地工作站上启动 Kubernetes 集群
- en: Building a monolithic version of the Links 'R' Us application using the components
    we developed in the previous chapters and deploying it on Kubernetes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们在前几章中开发的组件构建 Links 'R' Us 的单一代码库版本，并在 Kubernetes 上部署它
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The full code for the topics that will be discussed in this chapter has been
    published to this book's GitHub repository under the `Chapter10` folder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论的主题的完整代码已发布到本书的 GitHub 仓库中的 `Chapter10` 文件夹下。
- en: You can access this book's GitHub repository, which contains the code and all
    the required resources for the chapters in this book, by pointing your web browser
    to the following URL: [https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将网络浏览器指向以下 URL 来访问本书的 GitHub 仓库，其中包含本书各章节的代码和所有必需的资源：[https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang)。
- en: 'To get you up and running as quickly as possible, each example project includes
    a Makefile that defines the following set of targets:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您尽可能快地上手，每个示例项目都包含一个 Makefile，它定义了以下目标集：
- en: '| **Makefile target** | **Description** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **Makefile 目标** | **描述** |'
- en: '| `deps` | Install any required dependencies. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `deps` | 安装任何必需的依赖项。 |'
- en: '| `test` | Run all tests and report coverage. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `test` | 运行所有测试并报告覆盖率。 |'
- en: '| `lint` | Check for lint errors. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `lint` | 检查 lint 错误。 |'
- en: As with all the other chapters in this book, you will need a fairly recent version
    of Go, which you can download at [https://golang.org/dl](https://golang.org/dl)*.*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的其他所有章节一样，您需要一个相当新的 Go 版本，您可以在 [https://golang.org/dl](https://golang.org/dl)*.*
    下载。
- en: To run some of the code in this chapter, you will need to have a working Docker ^([5]) installation
    on your machine. Furthermore, a subset of the examples have been designed to run
    on Kubernetes ^([8]). If you don't have access to a Kubernetes cluster for testing,
    you can simply follow the instructions laid out in the following sections to set
    up a small cluster on your laptop or workstation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的一些代码，您需要在您的机器上有一个工作的Docker^([5])安装。此外，一些示例已被设计为可以在Kubernetes^([8])上运行。如果您没有访问Kubernetes集群进行测试，您可以简单地遵循以下章节中概述的说明，在您的笔记本电脑或工作站上设置一个小型集群。
- en: Building and packaging Go services using Docker
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker构建和打包Go服务
- en: Over the last few years, more and more software engineers started using systems
    such as Docker to containerize their applications. Containers offer a simple and
    clean way to execute an application without having to worry about the underlying
    hardware or operating system. In other words, the same container image can run
    on your local development machine, a VM on the cloud, or even on a bare-metal
    server located in your company's data center.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，越来越多的软件工程师开始使用Docker等系统来容器化他们的应用程序。容器提供了一个简单且干净的方式来执行应用程序，无需担心底层硬件或操作系统。换句话说，相同的容器镜像可以在您的本地开发机器上运行，在云上的虚拟机上运行，甚至可以在您公司数据中心中的裸机服务器上运行。
- en: Benefits of containerization
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化的好处
- en: Other than portability, containerization offers a few more important benefits,
    both from a software engineering and DevOps perspective. To begin with, containers
    make it easy to deploy a new version of a piece of software and to effortlessly
    roll back the deployment if something goes wrong. Secondly, containerization introduces
    an extra layer of security; every application executes in complete isolation from
    not only other applications but also from the underlying host itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了便携性之外，容器化还提供了从软件工程和DevOps角度出发的几个更重要的好处。首先，容器使得部署软件的新版本变得容易，如果出现问题，可以轻松回滚部署。其次，容器化引入了额外的安全层；每个应用程序不仅与其他应用程序完全隔离，而且与底层主机本身也完全隔离。
- en: Whenever a new container image is being built (for example, as part of a continuous
    integration pipeline), the target application gets packaged with an *immutable* copy
    of all the required dependencies for running it. As a result, when an engineer
    runs a particular container, they are guaranteed to run exactly the same binary
    as their other colleagues, whereas compiling and running the application locally
    could produce different results, depending on what compiler version or system
    libraries were installed on the development machine.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每当构建一个新的容器镜像（例如，作为持续集成管道的一部分）时，目标应用程序都会打包一个运行所需的全部依赖项的**不可变**副本。因此，当工程师运行特定的容器时，他们可以保证运行与他们其他同事完全相同的二进制文件，而本地编译和运行应用程序可能会产生不同的结果，这取决于开发机器上安装的编译器版本或系统库。
- en: To take this a step further, apart from containerizing our applications, we
    can also containerize the tools that are used to build them. This allows us to
    create **hermetic** builds and paves the way for supporting repeatable builds,
    whose benefits we have already enumerated in [Chapter 3](bdd8b231-e9fa-4522-8497-66d77231b7f3.xhtml),
    *Dependency Management*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步，除了容器化我们的应用程序之外，我们还可以容器化用于构建它们的工具。这使我们能够创建**密封**的构建，并为支持可重复的构建铺平道路，这些好处我们在[第3章](bdd8b231-e9fa-4522-8497-66d77231b7f3.xhtml)，*依赖管理*中已经列举过。
- en: When executing a hermetic build, the emitted binary artifact is not affected
    by any of the software or system libraries that are installed on the build machine.
    Instead, the build process uses pinned compiler and dependency versions to ensure
    that compiling the same snapshot (for example, a specific git SHA) of the code
    base will always produce the same, bit-by-bit identical binary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行密封构建时，生成的二进制工件不受构建机器上安装的任何软件或系统库的影响。相反，构建过程使用固定的编译器和依赖项版本，以确保编译相同的代码库快照（例如，特定的git
    SHA）将始终产生相同的、逐位相同的二进制文件。
- en: In the next section, we will delve into the process of building Docker containers
    for your Go applications and explore a set of best practices for producing containers
    that are optimized for size.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入了解为Go应用程序构建Docker容器的过程，并探讨一系列针对优化容器大小的最佳实践。
- en: Best practices for dockerizing Go applications
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker化Go应用程序的最佳实践
- en: Go comes with built-in support for producing standalone, static binaries, making
    it an ideal candidate for containerization! Let's take a look at the best practices
    for building Docker containers for your Go applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Go 语言自带生成独立、静态二进制文件的支持，这使得它成为容器化的理想选择！让我们来看看为您的 Go 应用程序构建 Docker 容器的最佳实践。
- en: Since static Go binaries tend to be quite large, we must take extra steps to
    ensure that the containers we build do not include any of the build tools (for
    example, the Go compiler) that are used at build time. Unless you are using a
    really old version of Docker, your currently installed version will most likely
    support a feature known as *build containers*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于静态 Go 二进制文件通常相当大，我们必须采取额外措施确保我们构建的容器不包含任何在构建时使用的构建工具（例如，Go 编译器）。除非您使用的是非常旧的
    Docker 版本，否则您当前安装的版本很可能支持一个名为 *build containers* 的功能。
- en: 'A build container includes all the tools that are needed for compiling our
    Go application: the Go compiler and the Go standard library, git, tools for compiling
    protocol buffer definitions, and so on. We will be using the build compiler as
    an *intermediate* container for compiling and linking our application. Then, we
    will create a *new* container, copy the compiled binary over, and discard the
    build container.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 构建容器包含编译我们的 Go 应用程序所需的所有工具：Go 编译器和 Go 标准库、git、编译协议缓冲定义的工具等等。我们将使用构建编译器作为一个 *中间*
    容器来编译和链接我们的应用程序。然后，我们将创建一个 *新* 容器，将编译的二进制文件复制过来，并丢弃构建容器。
- en: 'To understand how this process works, let''s examine the Dockerfile for building
    the Links ''R'' Us application that we will be building in the last part of this
    chapter. You can find the Dockerfile in the `Chapter10/linksrus` folder of this
    book''s GitHub repository:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这个过程是如何工作的，让我们检查构建 Links 'R' Us 应用程序的 Dockerfile，我们将在本章的最后部分构建它。您可以在本书 GitHub
    仓库的 `Chapter10/linksrus` 文件夹中找到 Dockerfile：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first line specifies the container that we will be using as the base for
    our build container. We can reference this container within the Dockerfile using
    the `builder` alias. The rest of the commands from the preceding Dockerfile perform
    the following operations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行指定了我们用作构建容器基础的容器。我们可以在 Dockerfile 中使用 `builder` 别名来引用此容器。前一个 Dockerfile 中的其余命令执行以下操作：
- en: The source files for the application are copied from the host into the build
    container. Note that we copy the **entire** book repository into the container
    to ensure that the `make deps` command can resolve all package imports from this
    book's repository and not try to download them from GitHub.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序源文件从主机复制到构建容器中。请注意，我们将整个书籍仓库复制到容器中，以确保 `make deps` 命令可以从这本书的仓库中解析所有包导入，而不是尝试从
    GitHub 下载它们。
- en: The `make deps` command is invoked to fetch any external package dependencies.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make deps` 命令被调用以获取任何外部包依赖项。'
- en: Finally, the Go compiler is invoked to compile the application and place the
    resulting binary in a known location (in this case, `/go/bin/linksrus-monolith`).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，调用 Go 编译器来编译应用程序并将生成的二进制文件放置在已知位置（在这种情况下，`/go/bin/linksrus-monolith`）。
- en: 'Let''s zoom in and explain what actually happens when the `go build` command
    is executed:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大并解释当执行 `go build` 命令时实际上会发生什么：
- en: The `GIT_SHA` environment variable is set to the short git SHA of the current
    commit. The `-X main.appSha=$GIT_SHA` linker flag overrides the value of the placeholder
    variable called `appSha` in the main package with the SHA value that we just calculated.
    We will be outputting the value of the `appSha` variable in the application logs
    to make it easy for operators to figure out which application version is currently
    deployed simply by tailing the logs.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GIT_SHA` 环境变量被设置为当前提交的短 git SHA。`-X main.appSha=$GIT_SHA` 链接器标志覆盖了主包中名为 `appSha`
    的占位符变量的值。我们将输出 `appSha` 变量的值到应用程序日志中，以便操作员可以通过查看日志的尾部来轻松地确定当前部署的应用程序版本。'
- en: The `CGO_ENABLED=0` environment variable notifies the Go compiler that we won't
    be invoking any C code from our program and allows it to optimize away quite a
    bit of code from the final binary.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CGO_ENABLED=0` 环境变量通知 Go 编译器我们不会从我们的程序中调用任何 C 代码，并允许它从最终二进制文件中优化掉相当多的代码。'
- en: The `-static` flag instructs the compiler to produce a static binary.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-static` 标志指示编译器生成一个静态二进制文件。'
- en: Finally, the `-w` and `-s` flags instruct the Go linker to drop debug symbols
    (more specifically, the DWARF section and symbol information) from the final binary.
    This still allows you to get full stack traces in case of a panic but prevents
    you from attaching a debugger (for example, delve) to the binary. On the bright
    side, these flags will significantly reduce the total size of the final binary!
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`-w`和`-s`标志指示Go链接器从最终二进制文件中删除调试符号（更具体地说，是DWARF部分和符号信息）。这仍然允许你在发生panic时获取完整的堆栈跟踪，但防止你将调试器（例如，delve）附加到二进制文件上。从积极的一面来看，这些标志将显著减少最终二进制文件的总大小！
- en: 'The next section of the Dockerfile contains the steps for building the final
    container:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Dockerfile的下一部分包含构建最终容器的步骤：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we know that the Links 'R' Us application will most probably be making
    TLS connections, we need to ensure that the final container image ships with the
    CA certificates for trusted authorities around the world. This is achieved by
    installing the `ca-certificates` package. To complete the build, we *copy* the
    compiled binary from the **build** container into the final container.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道Links 'R' Us应用程序很可能会建立TLS连接，我们需要确保最终的容器镜像包含全球受信任机构的CA证书。这是通过安装`ca-certificates`包来实现的。为了完成构建，我们需要将编译的二进制文件从**构建**容器复制到最终容器中。
- en: Selecting a suitable base container for your application
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择适合您应用程序的基础容器
- en: In the previous example, I chose to use *Alpine* as the base container for the
    application. So, why pick alpine over something more widely known, such as Ubuntu?
    The answer is size!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我选择使用*Alpine*作为应用程序的基础容器。那么，为什么选择alpine而不是更广为人知的，如Ubuntu？答案是大小！
- en: The Alpine Linux ^([1]) container is one of the smallest base containers you
    can find out there. It ships with a small footprint libc implementation (musl)
    and uses busybox as its shell. As a result, the total size of the alpine container
    is only 5 M, thus making it ideal for hosting our Go static binaries. Furthermore,
    it includes its own package manager (apk), which lets you install additional packages
    such as the ca-certificates or network tools while the final container is being
    built.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Alpine Linux^([1])容器是你可以找到的最小的基础容器之一。它包含一个小的libc实现（musl）并使用busybox作为其shell。因此，alpine容器的总大小仅为5
    M，这使得它非常适合托管我们的Go静态二进制文件。此外，它还包括自己的包管理器（apk），允许你在构建最终容器时安装额外的包，如ca-certificates或网络工具。
- en: 'What if we don''t need this extra functionality, though? Is it possible to
    produce an application container that is even smaller? The answer is yes! We can
    use the special **scratch** container as our base container. As the name implies,
    the scratch container is literally empty... It has no root filesystem and only
    includes our application binary. However, it does come with a few caveats:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不需要这个额外的功能，那么能否制作出一个更小的应用程序容器呢？答案是肯定的！我们可以使用特殊的**scratch**容器作为我们的基础容器。正如其名所示，scratch容器实际上是空的...它没有根文件系统，只包含我们的应用程序二进制文件。然而，它也有一些注意事项：
- en: It does not include any CA certificates, nor is there any way to install them
    besides copying them over from an intermediate build container. However, if your
    application or microservice will only communicate with services in a private subnet
    using non-TLS connections, this might not be a problem.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不包含任何CA证书，也没有其他方式可以安装它们，除了从中间构建容器复制它们。然而，如果你的应用程序或微服务将仅使用非TLS连接与私有子网中的服务通信，这可能不是问题。
- en: The container does not include a shell. This makes it impossible to actually
    SSH into a running container for debugging purposes (for example, to check that
    DNS resolution works or to grep through log files).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该容器不包含shell。这使得实际上无法SSH到正在运行的容器进行调试（例如，检查DNS解析是否工作或grep日志文件）。
- en: My recommendation is to always use a tiny container such as alpine or something
    similar instead of the scratch container.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是始终使用像alpine或类似的小型容器，而不是使用scratch容器。
- en: 'At this point, you should be able to apply the best practices we outlined in
    the previous sections and create space-efficient container images for your own
    Go applications. So, what''s next? The next step is, of course, to deploy and
    scale your applications. As you probably suspect, we won''t be doing this manually!
    Instead, we will be leveraging an existing, industrial-grade solution for managing
    containers at scale: Kubernetes.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该能够应用我们在前几节中概述的最佳实践，并为你的Go应用程序创建空间高效的容器镜像。那么，接下来是什么？下一步当然是部署和扩展你的应用程序。正如你可能猜到的，我们不会手动进行！相反，我们将利用现有的、工业级的解决方案来管理大规模容器：Kubernetes。
- en: A gentle introduction to Kubernetes
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes的温和介绍
- en: Kubernetes ^([8]) is an open source platform for managing containerized workloads
    that was built from the start with future extensibility in mind. It was originally
    released by Google back in 2014 and it encompasses both their insights and best
    practices for running large-scale, production-grade applications. Nowadays, it
    has eclipsed the managed container offerings of the most popular cloud providers
    and is en route to becoming the *de facto* standard for deploying applications
    on-premises and in the cloud.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes^([8])是一个开源平台，用于管理容器化工作负载，它从一开始就考虑了未来的可扩展性。它最初由Google在2014年发布，包含了他们在运行大规模、生产级应用程序方面的见解和最佳实践。如今，它已经超越了最受欢迎的云提供商的托管容器服务，并正在成为在本地和云中部署应用程序的*事实标准*。
- en: Describing Kubernetes in detail is not within the scope of this book. Instead,
    the goal of the following sections is to provide you with a brief introduction
    to Kubernetes and distill some of its basic concepts into an easily digestible
    format that conveys enough information to allow you to spin up a test cluster
    and deploy the Links 'R' Us project to it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 详细描述Kubernetes超出了本书的范围。相反，以下章节的目标是为你提供一个Kubernetes的简要介绍，并将一些基本概念提炼成易于消化的格式，以便你能够启动一个测试集群并将“链接即服务”项目部署到其中。
- en: Peeking under the hood
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 漏洞探查
- en: 'Okay, so we have already mentioned that Kubernetes will do the heavy lifting
    and manage different types of containerized workloads for you. But how does this
    work under the hood? The following diagram illustrates the basic components that
    comprise a Kubernetes cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经提到Kubernetes将承担繁重的工作并为你管理不同类型的容器化工作负载。但是，它内部是如何工作的呢？以下图表展示了构成Kubernetes集群的基本组件：
- en: '![](img/985bfbe5-58d9-4726-a105-e945048a2e56.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/985bfbe5-58d9-4726-a105-e945048a2e56.png)'
- en: 'Figure 1: A high-level overview of a Kubernetes cluster'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Kubernetes集群的高级概述
- en: A Kubernetes cluster consists of two types of nodes: **masters** and **workers**.
    These can be either physical or virtual machines. The master nodes implement the
    control plane for your cluster, whereas the worker nodes pool their resources
    together (CPUs, memory, disk, or even GPUs) and execute the workloads that are
    assigned to them by the master.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群由两种类型的节点组成：**master节点**和**worker节点**。这些可以是物理机或虚拟机。master节点实现了集群的控制平面，而worker节点将它们的资源（CPU、内存、磁盘，甚至是GPU）集中起来，并执行由master分配给它们的工作负载。
- en: 'Every master node runs the following processes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每个master节点运行以下进程：
- en: The **kube-api-server**. You can think of this as an API gateway for allowing
    worker nodes and cluster operators to access the control plane for the cluster.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-api-server**。你可以将其视为一个API网关，允许工作节点和集群管理员访问集群的控制平面。'
- en: '**etcd** implements a key-value store where the cluster''s current state is
    persisted. It also provides a convenient API that allows clients to watch a particular
    key or set of keys and receive a notification when their values change.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd** 实现了一个键值存储，其中持久化了集群的当前状态。它还提供了一个方便的API，允许客户端监视特定的键或一组键，并在它们的值发生变化时接收通知。'
- en: The **scheduler** monitors the cluster state for incoming workloads and makes
    sure that every workload is assigned to one of the available worker nodes. If
    the workload requirements cannot be met by any of the worker nodes, the scheduler
    might opt to **reschedule** an existing workload to a different worker so as to
    make room for the incoming workload.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scheduler** 监控集群状态以处理即将到来的工作负载，并确保每个工作负载都被分配给可用的某个工作节点。如果工作负载需求无法由任何工作节点满足，调度器可能会选择将现有工作负载**重新调度**到不同的工作节点，以便为即将到来的工作负载腾出空间。'
- en: The **cloud controller manager** handles all the necessary interactions with
    the underlying cloud substrate that hosts the cluster. Examples of such interactions
    include provisioning *cloud-specific* services, such as storage or load balancers,
    and creating or manipulating resources such as routing tables and DNS records.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云控制器管理器**处理与集群宿主云底层的所有必要交互。此类交互的例子包括提供**云特定**服务，如存储或负载均衡器，以及创建或操作诸如路由表和 DNS
    记录等资源。'
- en: A production-grade Kubernetes cluster will typically be configured with multiple
    master nodes; the control plane manages the cluster state, so it is quite important
    for it to be *highly available*. In such a scenario, data will be automatically
    replicated across the master nodes and DNS-based load balancing will be used to
    access the kube-api-server gateway.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生产级别的 Kubernetes 集群通常配置有多个主节点；控制平面管理集群状态，因此它必须**高度可用**。在这种情况下，数据将在主节点之间自动复制，并使用基于
    DNS 的负载均衡来访问 kube-api-server 网关。
- en: Now, let's take a look at the internals of a worker node. Given that Kubernetes
    manages containers, a key requirement is that each worker node provides a suitable
    container runtime. As you've probably guessed, the most commonly used runtime
    is Docker; however, Kubernetes will happily work with other types of container
    runtime interfaces, such as containerd ^([4]) or rkt ^([12]).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看工作节点的内部结构。鉴于 Kubernetes 管理容器，每个工作节点提供合适的容器运行时是一个关键要求。正如你可能猜到的，最常用的运行时是
    Docker；然而，Kubernetes 也会高兴地与其他类型的容器运行时接口一起工作，例如 containerd ^([4]) 或 rkt ^([12])。
- en: Each and every workload that is scheduled on a particular worker node is executed
    in isolation within its container runtime. The minimum unit of work in Kubernetes
    is referred to as a **pod**. Pods consist of one or *more* container images that
    are executed on the same worker instance. While single-container pods are the
    most typical, multi-container pods are also quite useful. For instance, we could
    deploy a pod that includes nginx and a sidecar container that monitors an external
    configuration source and regenerates the nginx configuration as needed. An application
    can be horizontally scaled by creating additional pod instances.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每个在特定工作节点上安排的工作负载都在其容器运行时中独立执行。在 Kubernetes 中，最小的作业单位被称为**pod**。Pod 包含一个或多个在同一工作实例上执行的容器镜像。虽然单容器
    pod 是最常见的，但多容器 pod 也非常有用。例如，我们可以部署一个包含 nginx 和一个侧车容器的 pod，该容器监控外部配置源并在需要时重新生成
    nginx 配置。通过创建额外的 pod 实例，可以水平扩展应用程序。
- en: 'The worker nodes also run the following processes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点还运行以下进程：
- en: The **kubelet** agent connects to the master's **api-server** and watches for
    workload assignments to the worker node it is running on. It ensures that the
    required containers are always up and running by automatically restarting them
    if they suddenly die.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubelet**代理连接到主机的**api-server**，并监视分配给其运行的工作节点的作业。它通过在容器意外死亡时自动重启它们来确保所需的容器始终处于运行状态。'
- en: The **kube-proxy** works like a network proxy. It maintains a set of rules that
    control the routing of internal (cluster) or external traffic to the pods that
    are currently executing on the worker.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**像网络代理一样工作。它维护一组规则，控制内部（集群）或外部流量路由到当前在工作节点上执行的任务的 pod。'
- en: Summarizing the most common Kubernetes resource types
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结最常用的 Kubernetes 资源类型
- en: Operators interact with the Kubernetes cluster by creating, deleting, or otherwise
    manipulating different types of resources via a CRUD-like interface. Let's take
    a brief look at some of the most common Kubernetes resource types.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 运营商通过创建、删除或以类似 CRUD 的接口操纵不同类型的资源来与 Kubernetes 集群交互。让我们简要地看看一些最常用的 Kubernetes
    资源类型。
- en: It is quite rare to come across an application that doesn't require any sort
    of configuration. While we could definitely hardcode the configuration settings
    when we create our pods, this is generally considered to be bad practice and frankly
    becomes a major source of frustration when we need to change a configuration setting
    (for example, the endpoint for a database) that is shared between multiple applications.
    To alleviate this problem, Kubernetes offers the *config map* resource. Config
    maps are collections of key-value pairs that can be injected into pods either
    as environment variables or mounted as plain text files. This approach allows
    us to manage configuration settings at a single location and avoid hardcoding
    them when creating pods for our applications. Kubernetes also provides the **secret** resource,
    which works in a similar fashion to a config map but is meant to be used for sharing
    sensitive information such as certificate keys and service credentials between
    pods.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很少会遇到不需要任何配置的应用程序。虽然我们可以在创建我们的Pod时直接硬编码配置设置，但这通常被认为是不良的做法，而且坦白说，当我们需要更改共享多个应用程序的配置设置（例如，数据库的端点）时，这会变成一个主要的挫折来源。为了缓解这个问题，Kubernetes提供了*配置映射*资源。配置映射是一系列键值对的集合，可以注入到Pod中作为环境变量，或者以纯文本文件的形式挂载。这种方法允许我们在单个位置管理配置设置，并在创建应用程序的Pod时避免硬编码它们。Kubernetes还提供了**秘密**资源，它的工作方式与配置映射类似，但旨在用于在Pod之间共享敏感信息，如证书密钥和服务凭证。
- en: A **namespace** resource works as a virtual container for logically grouping
    other Kubernetes resources and controlling access to them. This is a very handy
    feature if multiple teams are using the same cluster for their deployments. In
    such a scenario, each team is typically assigned full access to their own namespace
    so that they cannot interfere with the resources that are deployed by other teams
    unless they are granted explicit access.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名空间**资源作为一个虚拟容器，用于逻辑上分组其他Kubernetes资源并控制对它们的访问。如果多个团队使用同一个集群进行部署，这是一个非常实用的功能。在这种情况下，每个团队通常被分配对其自己的命名空间的完全访问权限，这样他们就不能干扰其他团队部署的资源，除非他们被授予明确的访问权限。'
- en: Once a pod dies, any data stored within any of its containers will be lost.
    To support use cases where we want to persist data across pod restarts or we simply
    want to share the same set of data (for example, the pages served by a web server)
    across multiple pod instances, Kubernetes provides the **persistent volume** (**PV**)
    and **persistent volume claim** (**PVC**) resources. A persistent volume is nothing
    more than a piece of block storage that is made available to the cluster. Depending
    on the substrate, it can either be manually provisioned by the cluster administrators
    or dynamically allocated on-demand by the underlying substrate (for example, an
    EBS volume when running on AWS). On the other hand, a persistent volume claim
    represents an operator's request for a block of storage with a particular set
    of attributes (for example, size, IOPS, and spinning disk or SSD). The Kubernetes
    control plane attempts to match the available volumes with the operator-specified
    claims and mount the volumes to the pods that reference each claim.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Pod死亡，其任何容器中存储的数据都将丢失。为了支持我们希望在Pod重启之间持久化数据或我们希望在多个Pod实例之间共享相同数据集（例如，由Web服务器提供的页面）的使用案例，Kubernetes提供了**持久卷**（**PV**）和**持久卷声明**（**PVC**）资源。持久卷不过是一块可供集群使用的块存储。根据底层基础结构，它可以是集群管理员手动配置的，也可以是由底层基础结构按需动态分配的（例如，在AWS上运行时，可以是EBS卷）。另一方面，持久卷声明代表操作员对具有特定属性（例如，大小、IOPS和旋转磁盘或SSD）的存储块的需求。Kubernetes控制平面试图将可用的卷与操作员指定的声明相匹配，并将卷挂载到引用每个声明的Pod上。
- en: To deploy a stateless application on Kubernetes, the recommended approach is
    to create a **deployment** resource. A deployment resource specifies a **template** for
    instantiating a single pod of the application and the desired number of replicas.
    Kubernetes continuously monitors the state of each deployment and attempts to
    synchronize the cluster state with the desired state by either creating new pods
    (using the template) or deleting existing pods when the number of active pods
    exceeds the requested number of replicas. Each pod in a deployment gets assigned
    a random hostname by Kubernetes and shares the *same PVC* with every other pod.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Kubernetes上部署无状态应用程序，推荐的方法是创建一个 **deployment** 资源。deployment资源指定了一个用于实例化应用程序单个Pod的模板和所需的副本数量。Kubernetes持续监控每个deployment的状态，并通过创建新的Pod（使用模板）或删除超出请求副本数量的现有Pod来尝试将集群状态与所需状态同步。在deployment中的每个Pod都会被Kubernetes分配一个随机主机名，并且与每个其他Pod共享 *相同的PVC*。
- en: Many types of workloads, such as databases or message queues, require a stateful
    kind of deployment where pods are assigned stable and predictable hostnames and
    each individual pod gets its own PVC. What's more, these applications usually
    operate in a clustered configuration and expect nodes to be deployed, upgraded,
    and scaled in a particular sequence. In Kubernetes, this type of deployment is
    accomplished by creating a **StatefulSet**. Similar to a deployment resource,
    a StatefulSet also defines a pod template and a number of replicas. Each replica
    is assigned a hostname, which is constructed by concatenating the name of the
    StatefulSet and the index of each pod in the set (for example, web-0 and web-1).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 许多类型的工作负载，如数据库或消息队列，需要一种有状态的部署方式，其中Pod被分配稳定且可预测的主机名，并且每个单独的Pod都有自己的PVC。更重要的是，这些应用程序通常以集群配置运行，并期望节点以特定的顺序部署、升级和扩展。在Kubernetes中，这种类型的部署是通过创建一个 **StatefulSet** 来实现的。类似于deployment资源，StatefulSet也定义了一个Pod模板和副本数量。每个副本都会被分配一个主机名，该主机名是通过连接StatefulSet的名称和集合中每个Pod的索引来构建的（例如，web-0和web-1）。
- en: 'Being able to scale the number of deployed pods up and down is a great feature
    to have but not that useful unless other resources in the cluster can connect
    to them! To this end, Kubernetes supports another type of resource, called a **service**.
    Services come in two flavors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 能够根据需要上下调整已部署Pod的数量是一个很好的特性，但如果没有其他集群资源能够连接到它们，那么这个特性就不是很实用！为此，Kubernetes支持另一种类型的资源，称为 **服务**。服务有两种类型：
- en: A service can sit in front of a group of pods and act as a **load balancer**.
    In this scenario, the service is automatically assigned both an IP address and
    a DNS record to aid its discovery by clients. In case you are wondering, this
    functionality is implemented by the **kube-proxy** component that runs on each
    worker node.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个服务可以位于一组Pod的前面，充当一个 **负载均衡器**。在这种情况下，该服务会自动分配一个IP地址和DNS记录，以帮助客户端发现它。如果你在疑惑，这种功能是通过在每个工作节点上运行的 **kube-proxy** 组件实现的。
- en: A **headless** service allows you to implement a custom service discovery mechanism.
    These services are not assigned a cluster IP address and they are totally ignored
    by kube-proxy. However, these services create DNS records for the service and
    resolve to the address of every single pod behind the service.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **无头服务**允许你实现自定义的服务发现机制。这些服务不会被分配集群IP地址，并且完全被kube-proxy忽略。然而，这些服务为服务创建DNS记录，并解析到服务背后每个Pod的地址。
- en: The last Kubernetes resource that we will be examining is **ingresses**. Depending
    on its configuration, an ingress exposes HTTP or HTTPS endpoints for routing traffic
    from outside the cluster to particular services within the cluster. The common
    set of features that are supported by the majority of ingress controller implementations
    include TLS termination, name-based virtual hosts, and URL rewriting for incoming
    requests.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要检查的最后一个Kubernetes资源是 **ingress**。根据其配置，ingress会公开HTTP或HTTPS端点，用于将集群外部的流量路由到集群内部特定的服务。大多数ingress控制器实现支持的常见功能包括TLS终止、基于名称的虚拟主机和入站请求的URL重写。
- en: This concludes our overview of the most common Kubernetes resource types. Keep
    in mind that this is only the tip of the iceberg! Kubernetes supports many other
    resource types (for example, cron jobs) and even provides APIs that allow operators
    to define their own custom resources. If you want to learn more about Kubernetes
    resources, I would strongly recommend browsing the quite extensive set of Kubernetes
    documentation that is available online ^([8]).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对最常见的 Kubernetes 资源类型的概述。请记住，这仅仅是冰山一角！Kubernetes 支持许多其他资源类型（例如，cron 作业），甚至提供了允许操作员定义他们自己的自定义资源的
    API。如果您想了解更多关于 Kubernetes 资源的信息，我强烈建议您浏览在线上可用的相当广泛的 Kubernetes 文档集 [8]。
- en: Next, you will learn how to easily set up your very own Kubernetes cluster on
    your laptop or workstation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何轻松地在您的笔记本电脑或工作站上设置自己的 Kubernetes 集群。
- en: Running a Kubernetes cluster on your laptop!
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的笔记本电脑上运行 Kubernetes 集群！
- en: A few years ago, experimenting with Kubernetes was more or less restricted to
    engineers who were either granted access to a test or dev cluster or they had
    the resources and knowledge that was required to bootstrap and operate their own
    cluster on the cloud. Nowadays, things are much simpler... In fact, you can even
    spin up a fully operational Kubernetes cluster on your laptop in just a couple
    of minutes!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，对 Kubernetes 的实验基本上仅限于那些被授予访问测试或开发集群权限的工程师，或者他们拥有在云上启动和运行自己集群所需资源和知识。如今，事情要简单得多...
    事实上，您甚至可以在几分钟内就在您的笔记本电脑上启动一个完全可操作的 Kubernetes 集群！
- en: 'Let''s take a look at some of the most popular, dev-friendly Kubernetes distributions
    that you can deploy on your development machine:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些最流行的、对开发者友好的 Kubernetes 发行版，您可以在您的开发机器上部署它们：
- en: K3S ^([7]) is a tiny (it's literally a 50 M binary!) distribution that allows
    you to run Kubernetes on resource-constrained devices. It provides binaries for
    multiple architectures, including ARM64/ARMv7\. This makes it a great candidate
    for running Kubernetes on Raspberry Pi.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K3S [7] 是一个微型（实际上是一个 50 M 的二进制文件！）发行版，允许您在资源受限的设备上运行 Kubernetes。它为多个架构提供了二进制文件，包括
    ARM64/ARMv7。这使得它成为在 Raspberry Pi 上运行 Kubernetes 的理想选择。
- en: Microk8s ^([9]) is a project by Canonical that promises zero-ops Kubernetes
    cluster setups. Getting a Kubernetes cluster up and running on Linux is as simple
    as running `snap install microk8s`. On other platforms, the recommended approach
    for installing microk8s is to use an application such as Multipass ^([11]) to
    spin up a VM and run the aforementioned command inside it.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microk8s [9] 是 Canonical 的一个项目，承诺零操作 Kubernetes 集群设置。在 Linux 上将 Kubernetes 集群启动并运行就像运行
    `snap install microk8s` 一样简单。在其他平台上，安装 microk8s 的推荐方法是使用 Multipass [11] 这样的应用程序启动一个虚拟机，并在其中运行上述命令。
- en: Minikube ^([10]) is yet another distribution, this time by the Kubernetes authors.
    It can work with different types of hypervisors (for example, VirtualBox, Hyperkit,
    Parallels, VMware Fusion, or Hyper-V) and can even be deployed on bare metal (Linux
    only).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minikube [10] 是另一个发行版，这次是由 Kubernetes 作者提供的。它可以与不同类型的虚拟机管理程序一起工作（例如，VirtualBox、Hyperkit、Parallels、VMware
    Fusion 或 Hyper-V），甚至可以在裸金属（仅限 Linux）上部署。
- en: To make it as easy as possible for you to set up your own Kubernetes cluster
    on your favorite OS and run the examples shown in the upcoming sections, we will
    be working exclusively with Minikube and use VirtualBox as our hypervisor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能简化您在您喜欢的操作系统上设置自己的 Kubernetes 集群并运行即将在下一节中展示的示例，我们将仅使用 Minikube，并使用 VirtualBox
    作为我们的虚拟机管理程序。
- en: 'Before we begin, make sure that you have downloaded and installed the following
    software:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请确保您已下载并安装以下软件：
- en: Docker ^([5]).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker [5]。
- en: VirtualBox ^([13]).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VirtualBox [13]。
- en: The kubectl binary for your platform.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您平台上的 kubectl 二进制文件。
- en: The helm [6] binary for your platform. Helm is a package manager for Kubernetes
    and we will be using it to deploy the CockroachDB and Elasticsearch instances
    for the Links 'R' Us project.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您平台上的 Helm [6] 二进制文件。Helm 是 Kubernetes 的包管理器，我们将使用它来部署 Links 'R' Us 项目的 CockroachDB
    和 Elasticsearch 实例。
- en: The latest Minikube version for your platform.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您平台上的最新 Minikube 版本。
- en: 'With all the preceding dependencies in place, we are ready to bootstrap our
    Kubernetes cluster using the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有前置依赖都准备就绪的情况下，我们可以使用以下代码启动我们的 Kubernetes 集群：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This command will create a virtual machine with 4 GB of RAM and deploy Kubernetes
    1.15.3 to it. It will also update the local configuration for kubectl so that
    it automatically connects to the cluster we have just provisioned. What's more,
    it will enable the **Container Networking Interface** (**CNI**) plugin for the
    cluster. In the next chapter, we will leverage this functionality to install a
    network security solution such as Calico ^([2]) or Cilium ^([3]) and define fine-grained
    network policies to lock down our cluster.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将在一个拥有 4 GB 内存的虚拟机上创建 Kubernetes 1.15.3，并部署到该虚拟机上。它还将更新本地 kubectl 配置，以便自动连接到我们刚刚配置的集群。更重要的是，它将为集群启用
    **容器网络接口**（**CNI**）插件。在下一章中，我们将利用这一功能安装像 Calico ^([2]) 或 Cilium ^([3]) 这样的网络安全解决方案，并定义细粒度的网络策略来锁定我们的集群。
- en: As our deployed services will be running inside Minikube's virtual machine,
    the only way to access them from the **host** machine is by provisioning an ingress
    resource. Luckily for us, Minikube provides a suitable ingress implementation
    as an add-on that we can activate by running `minikube addons enable ingress`. What's
    more, for our tests, we want to use a private Docker registry for pushing the
    Docker images that we will be building. Minikube ships with a private registry
    add-on that we can enable by running `minikube addons enable registry`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们部署的服务将运行在 Minikube 的虚拟机中，从 **主机** 访问它们的唯一方法是通过配置一个 ingress 资源。幸运的是，Minikube
    提供了一个合适的 ingress 实现作为插件，我们可以通过运行 `minikube addons enable ingress` 来激活它。更重要的是，为了我们的测试，我们希望使用一个私有
    Docker 仓库来推送我们将构建的 Docker 镜像。Minikube 随带一个私有仓库插件，我们可以通过运行 `minikube addons enable
    registry` 来启用它。
- en: However, by default, Minikube's private registry runs in insecure mode. When
    using insecure registries, we need to explicitly configure our local Docker daemon
    to allow connections to them; otherwise, we won't be able to push our images.
    The registry is exposed on port `5000` at the IP used by Minikube.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认情况下，Minikube 的私有仓库以不安全模式运行。当使用不安全仓库时，我们需要明确配置我们的本地 Docker 守护进程以允许连接到它们；否则，我们无法推送我们的镜像。该仓库在
    Minikube 使用的 IP 地址上暴露在端口 `5000`。
- en: You can find Minikube's IP address by running `minikube ip`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行 `minikube ip` 来找到 Minikube 的 IP 地址。
- en: 'On Linux, you can edit `/etc/docker/daemon.json`, merge in the following JSON
    block (replacing `$MINIKUBE_IP` with the IP we obtained with the `minikube ip` command),
    and then *restart the Docker daemon*, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 上，您可以编辑 `/etc/docker/daemon.json`，合并以下 JSON 块（将 `$MINIKUBE_IP` 替换为使用
    `minikube ip` 命令获得的 IP），然后按照以下方式重启 Docker 守护进程：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On OS X and Windows, you can simply right-click on the Docker for desktop, select
    preferences, and then click on the Daemon tab to access the list of trusted insecure
    registries.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OS X 和 Windows 上，您只需在 Docker for desktop 上右键单击，选择首选项，然后点击“守护进程”选项卡，即可访问受信任的不安全仓库列表。
- en: The last thing we need to do is install the required cluster resources so that
    we can use the helm package manager. We can do this by running `helm init`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的最后一件事是安装所需的集群资源，以便我们可以使用 Helm 包管理器。我们可以通过运行 `helm init` 来完成这项任务。
- en: To save you some time, I have encoded all the preceding steps into a Makefile,
    which you can find in the `Chapter10/k8s` folder of this book's GitHub repository.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省您的时间，我已经将所有前面的步骤编码到一个 Makefile 中，您可以在本书 GitHub 仓库的 `Chapter10/k8s` 文件夹中找到它。
- en: To bootstrap the cluster, install all the required add-ons, and configure helm,
    you can simply type `make bootstrap-minikube`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动集群，安装所有必需的插件，并配置 Helm，你只需输入 `make bootstrap-minikube`。
- en: That's it! We have a fully functioning Kubernetes cluster at our disposal. Now,
    we are ready to build and deploy a monolithic version of the Links 'R' Us project.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单！我们现在已经拥有了一个完全可用的 Kubernetes 集群。现在，我们准备构建和部署 Links 'R' Us 项目的单体版本。
- en: Building and deploying a monolithic version of Links 'R' Us
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 和部署 Links 'R' Us 的单体版本
- en: This is the moment of truth! In the following sections, we will leverage everything
    we have learned in this chapter to assemble all the Links 'R' Us components that
    we developed in the previous chapters into a monolithic application that we will
    then proceed to deploy on Kubernetes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检验的时刻！在接下来的章节中，我们将利用本章学到的所有知识，将我们在前几章中开发的 Links 'R' Us 组件组装成一个单体应用程序，然后我们将继续在
    Kubernetes 上部署它。
- en: 'Based on the user stories from [Chapter 5](6e4047ad-1fc1-4c3e-b90a-f27a62d06f17.xhtml),
    *The Links ''R'' Us Project*, in order for our application to satisfy our design
    goals, it should provide the following services:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [第 5 章](6e4047ad-1fc1-4c3e-b90a-f27a62d06f17.xhtml) 中 *Links 'R' Us 项目* 的用户故事，为了使我们的应用程序满足我们的设计目标，它应该提供以下服务：
- en: A periodically running, multi-pass crawler for scanning the link graph, retrieving
    links for indexing, and augmenting the graph with newly discovered links to be
    crawled during a future pass
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个定期运行的多遍爬虫，用于扫描链接图，检索索引链接，并在未来的遍历中增加新发现的链接以进行爬取
- en: Another periodically running service to recalculate and persist PageRank scores
    for the continuously expanding link graph
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个定期运行的服务，用于重新计算并持久化不断扩展的链接图的 PageRank 分数
- en: A frontend for our end users to perform search queries and to submit website
    URLs for indexing
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我们的最终用户提供前端，以便执行搜索查询并提交网站 URL 以进行索引
- en: So far, we haven't really discussed the frontend. Don't worry; we will be building
    a fully fledged frontend for our application in one of the following sections.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有真正讨论前端。别担心；我们将在接下来的某个部分为我们的应用程序构建一个完整的前端。
- en: As you've probably guessed, due to the number of components involved, the final
    application will undoubtedly include quite a bit of boilerplate code. Given that
    it is not feasible to include the full source code in this chapter, we will only
    focus on the most interesting parts. Nevertheless, you can find the documented
    source code for the entire application in the `Chapter10/linksrus` folder of this
    book's GitHub repository.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你可能猜到的，由于涉及的组件数量，最终的应用程序无疑将包含相当多的样板代码。鉴于在本章中不可能包含完整的源代码，我们只会关注最有趣的部分。尽管如此，你可以在本书
    GitHub 仓库的 `Chapter10/linksrus` 文件夹中找到整个应用程序的文档化源代码。
- en: Distributing computation across application instances
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在应用程序实例之间分配计算
- en: In anticipation of the Links 'R' Us project becoming an overnight success and
    attracting a lot of traffic, especially after posting a link on sites such as
    Hacker News and Slashdot, we need to come up with a reasonable plan for scaling.
    Even though we are currently dealing with a monolithic application, we can always
    scale horizontally by spinning up additional instances. Moreover, as our link
    graph size grows, we will undoubtedly need additional compute resources for both
    our web crawlers and our PageRank calculator.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 预计 Links 'R' Us 项目一夜之间取得成功并吸引大量流量，尤其是在在 Hacker News 和 Slashdot 等网站上发布链接之后，我们需要制定一个合理的扩展计划。尽管我们目前正在处理一个单体应用程序，但我们总是可以通过启动额外的实例来水平扩展。此外，随着我们的链接图规模的增长，我们无疑需要为我们的网络爬虫和
    PageRank 计算器提供额外的计算资源。
- en: One of the key benefits of using a container orchestration platform such as
    Kubernetes is that we can effortlessly scale up (or down) any deployed application.
    As we saw at the beginning of this chapter, a `Service` resource connected to
    an `Ingress` can act as a load balancer and distribute *incoming* traffic to our
    application. This transparently takes care of our frontend scaling issues with
    no additional development effort on our end.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 Kubernetes 这样的容器编排平台的一个关键好处是我们可以轻松地扩展（或缩减）任何已部署的应用程序。正如我们在本章开头所看到的，连接到 `Ingress`
    的 `Service` 资源可以充当负载均衡器，并将 *传入* 流量分配给我们的应用程序。这透明地处理了我们的前端扩展问题，而无需我们进行额外的开发工作。
- en: On the other hand, making sure that *each* application instance crawls a specific *subset* of
    the graph isn't straightforward as it requires application instances to coordinate
    with each other. This implies that we need to establish a communication channel
    between the individual instances. Or does it?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，确保 *每个* 应用程序实例爬取图的具体 *子集* 并不简单，因为这需要应用程序实例之间进行协调。这意味着我们需要在各个实例之间建立一个通信通道。或者，这是否意味着我们不需要这样做？
- en: Carving the UUID space into non-overlapping partitions
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 UUID 空间划分为不重叠的分区
- en: In [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml), *Building a Persistence
    Layer*, we mentioned that the caller of the `Links` and `Edges` methods that are
    exposed by the link-graph component is responsible for implementing a suitable
    partitioning scheme and providing the appropriate UUID ranges as arguments to
    these methods. So, how can we go about implementing such a partitioning scheme?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6 章](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml) 中，我们提到了 *构建持久层*，我们提到，由链接图组件公开的
    `Links` 和 `Edges` 方法的调用者负责实现一个合适的分区方案，并将适当的 UUID 范围作为参数提供给这些方法。那么，我们该如何实现这样的分区方案呢？
- en: Our approach exploits the observation that the link (and edge) IDs are, in fact,
    V4 (random) UUIDs and are therefore expected to be more or less evenly distributed
    in the massive (2^(128)) UUID space. Let's assume that the total number of workers
    (that is, the number of partitions) available to us is *N*. For the time being,
    we will treat the number of workers as being fixed and a priority known. In the
    following section, we will learn how to leverage the Kubernetes infrastructure
    to automatically discover this information.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法利用了观察到的链接（和边）ID实际上是V4（随机）UUID，并且因此预计在庞大的（2^(128)）UUID空间中分布得或多或少均匀。让我们假设我们可用的总工作器数量（即分区数量）是*N*。目前，我们将工作器数量视为固定且已知优先级。在下一节中，我们将学习如何利用Kubernetes基础设施自动发现这些信息。
- en: 'To figure out the range of UUIDs that the *M[th]* worker (where 0 <= M < N)
    needs to provide as arguments to the `Links` and `Edges` methods of the graph,
    we need to perform some calculations. First, we need to subdivide the 128-bit
    UUID space into *N* equally sized sections; in essence, each section will contain *C
    = 2^(128) / N* UUIDs. Consequently, to calculate the *M[th]* worker''s UUID range,
    we can use the following formula:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定*M[th]*（其中0 <= M < N）工作器需要提供给图`Links`和`Edges`方法的UUID范围，我们需要进行一些计算。首先，我们需要将128位UUID空间细分为*N*个大小相等的部分；本质上，每个部分将包含*C
    = 2^(128) / N*个UUID。因此，为了计算*M[th]*工作器的UUID范围，我们可以使用以下公式：
- en: '![](img/a40a2bfe-c178-4e4a-8af1-34cffdf18d39.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a40a2bfe-c178-4e4a-8af1-34cffdf18d39.png)'
- en: 'If the number of workers (*N*) is *odd*, then we will not be able to divide
    the UUID space evenly; therefore, the **last** (N-1) section is treated in a special
    manner: it always extends to the **end** of the UUID space (the UUID value `ffffffff-ffff-ffff-ffff-ffffffffffff`).
    This ensures that we always cover the entire UUID space, regardless of whether *N* is
    odd or even!'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果工作器数量（*N*）是*奇数*，那么我们将无法均匀地分割UUID空间；因此，**最后**的(N-1)部分将以特殊方式处理：它始终扩展到UUID空间的**末尾**（UUID值`ffffffff-ffff-ffff-ffff-ffffffffffff`）。这确保了我们始终覆盖整个UUID空间，无论*N*是奇数还是偶数！
- en: 'The rationale behind this type of split is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分割方式的理由如下：
- en: Most modern database systems tend to cache the primary key index in memory
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数现代数据库系统倾向于在内存中缓存主键索引
- en: They contain special optimized code paths for performing *range scans* on primary
    key ranges
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们包含用于在主键范围内执行*范围扫描*的特殊优化代码路径
- en: The combination of the preceding two properties makes this solution quite attractive
    for the read-heavy workloads that are performed by both the crawler and the PageRank
    calculator components. One small nuisance is that UUIDs are 128-bit values and
    Go does not provide scalar types for performing 128-bit arithmetic. Fortunately,
    the standard library provides the `math/big` package, which can perform arbitrary-precision
    arithmetic operations!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个特性的组合使得这种解决方案对于爬虫和PageRank计算组件执行的读密集型工作负载非常有吸引力。一个小麻烦是UUID是128位值，Go没有提供用于执行128位算术的标量类型。幸运的是，标准库提供了`math/big`包，它可以执行任意精度的算术运算！
- en: 'Let''s go ahead and create a helper that will take care of all these calculations
    for us. The `Range` helper implementation will live in a file called `range.go`,
    which is part of the `Chapter10/linksrus/partition` package (see this book''s
    GitHub repository). Its type definition is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续创建一个辅助函数，它将为我们处理所有这些计算。`Range`辅助函数的实现将位于一个名为`range.go`的文件中，它是`Chapter10/linksrus/partition`包的一部分（参见本书的GitHub仓库）。其类型定义如下：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For our particular application, we will provide two constructors for creating
    ranges. The first constructor creates a `Range` that spans the full UUID space
    and splits it into `numPartitions`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的特定应用，我们将提供两个构造函数来创建范围。第一个构造函数创建一个覆盖整个UUID空间的`Range`，并将其分割成`numPartitions`：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see, the constructor delegates the creation of the range to the `NewRange` helper,
    whose implementation has been broken down into smaller snippets:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，构造函数将范围的创建委托给`NewRange`辅助函数，其实现已被分解成更小的片段：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Before we proceed, the code verifies that the provided UUID range is valid by
    making sure that the start UUID is smaller than the end UUID. To achieve this,
    we use the handy `bytes.Compare` function, which compares two byte slices and
    returns a value greater than or equal to zero if the two byte slices are either
    equal or the first byte slice is greater than the second. One caveat here is that
    the UUID type is defined as `[16]byte`, whereas the `bytes.Compare` function expects
    byte slices. However, we can easily convert each UUID into a byte slice using
    the convenience operator, `[:]`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，代码会验证提供的UUID范围是否有效，确保起始UUID小于结束UUID。为此，我们使用方便的`bytes.Compare`函数，该函数比较两个字节切片，如果两个字节切片相等或第一个字节切片大于第二个字节切片，则返回大于或等于零的值。这里的一个注意事项是，UUID类型定义为`[16]byte`，而`bytes.Compare`函数期望字节切片。然而，我们可以通过使用便利操作符`[:]`轻松地将每个UUID转换为字节切片。
- en: After the preliminary argument validation, we create an empty `big.Integer` value
    and use the cumbersome API of the `math/big` package to load it with the result
    of the `(end - start) + 1` expression. Once the value has been loaded, we divide
    it by the number of partitions that the caller provided as an argument to the
    function. This yields the `C` value from the formula we saw in the previous section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步的参数验证之后，我们创建一个空的`big.Integer`值，并使用`math/big`包的繁琐API通过`(end - start) + 1`表达式加载结果。一旦值被加载，我们就将其除以调用者作为函数参数提供的分区数量。这得到了我们在上一节中看到的公式中的`C`值。
- en: 'The following block of code uses a `for` loop to calculate and store the **end** UUID
    for each partition that is part of the range we are creating:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块使用一个`for`循环来计算并存储我们正在创建的范围内每个分区所对应的**结束**UUID：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we mentioned in the previous section, the end UUID for the last partition
    is always the maximum possible UUID value. For all the other partitions, we calculate
    the end by multiplying the size of each partition by the partition number, plus
    one. Once all the calculations have been completed, a new `Range` object is allocated
    and returned to the caller. In addition to the calculated end ranges, we also
    keep track of the start UUID for the range.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中提到的，最后一个分区的结束UUID总是最大的可能UUID值。对于所有其他分区，我们通过将每个分区的尺寸乘以分区号再加一来计算结束值。一旦所有计算完成，就为调用者分配并返回一个新的`Range`对象。除了计算出的结束范围之外，我们还跟踪范围的起始UUID。
- en: 'Now, to make the `Range` type easier to use from within the crawler service
    code, let''s define two auxiliary methods:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使`Range`类型更容易在爬虫服务代码中使用，让我们定义两个辅助方法：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `Extends` method returns the start (inclusive) and end (exclusive) UUID
    value for the *entire* range. On the other hand, the `PartitionExtents` function returns
    the start and end UUID values for a specific *partition* within the range.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Extends`方法返回整个范围的起始（包含）和结束（排除）UUID值。另一方面，`PartitionExtents`函数返回范围内特定**分区**的起始和结束UUID值。'
- en: Assigning a partition range to each pod
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为每个Pod分配分区范围
- en: 'With the help of the `Range` type from the previous section, we now have the
    means to query the UUID range that''s assigned to every single partition. For
    our particular use case, the number of partitions is equal to the number of pods
    that we launch. However, one crucial bit of information that we are lacking is
    the partition number that''s assigned to each individual launched pod! Consequently,
    we now have two problems that we need to solve:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中介绍的`Range`类型帮助下，我们现在有了查询分配给每个单独分区的UUID范围的方法。对于我们的特定用例，分区的数量等于我们启动的Pod数量。然而，我们缺少的一个重要信息是分配给每个已启动Pod的分区号！因此，我们现在有两个问题需要解决：
- en: What is the partition number of an individual pod?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个Pod的分区号是多少？
- en: What is the total number of pods?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共有多少个Pod？
- en: If we deploy our application as a StatefulSet with *N* replicas, every pod in
    the set will be assigned a hostname that follows the pattern `SET_NAME-INDEX`,
    where `INDEX` is a number from *0* to *N-1* that indicates the index of the pod
    in the set. All we need to do is read the pod's hostname from our application,
    parse the numeric suffix, and use that as the partition number.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将我们的应用程序作为具有*N*个副本的StatefulSet部署，该集中每个Pod都会被分配一个遵循模式`SET_NAME-INDEX`的主机名，其中`INDEX`是从*0*到*N-1*的数字，表示Pod在集中的索引。我们只需要从我们的应用程序中读取Pod的主机名，解析数字后缀，并将其用作分区号。
- en: One approach to answering the second question would be to query the Kubernetes
    server API. However, this requires additional effort to set up (for example, create
    service accounts, RBAC records) – not to mention that it effectively locks us
    into Kubernetes! Fortunately, there is an easier way...
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 回答第二个问题的方法之一是查询Kubernetes服务器API。然而，这需要额外的努力来设置（例如，创建服务帐户、RBAC记录）——更不用说这实际上将我们锁定在Kubernetes上了！幸运的是，有一个更简单的方法...
- en: 'If we were to create a **headless** service for our application, it would automatically
    generate a set of SRV records that we can query and obtain the host for each individual
    pod that belongs to the service. The following diagram shows the results of running
    an SRV query from within a pod in the Kubernetes cluster:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要为我们的应用程序创建一个**无头**服务，它将自动生成一组SRV记录，我们可以查询并获取属于该服务的每个单独Pod的主机。以下图显示了在Kubernetes集群中的一个Pod内部运行SRV查询的结果：
- en: '![](img/b560f6db-9058-4603-85ec-01efca0ad74e.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b560f6db-9058-4603-85ec-01efca0ad74e.png)'
- en: Figure 2: The linksrus-headless service is associated with four pods whose hostnames
    are visible on the right-hand side
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：`linksrus-headless`服务与四个Pod相关联，其主机名在右侧可见
- en: 'Based on the information displayed in the preceding screenshot, we could write
    a helper for figuring out the partition number and the total number of partitions
    for a running application instance, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面截图显示的信息，我们可以编写一个辅助程序来找出运行应用程序实例的分区数和总分区数，如下所示：
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To get the hostname, we invoke the `Hostname` function provided by the `os` package.
    Then, we split on the dash separator, extract the right-most part of the hostname,
    and use `ParseInt` to convert it into a number.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取主机名，我们调用由`os`包提供的`Hostname`函数。然后，我们在破折号分隔符上分割，提取主机名的最右边部分，并使用`ParseInt`将其转换为数字。
- en: Next, to get the SRV records, we use the `LookupSRV` function from the `net` package
    and pass the service name as the last argument. Then, we count the number of results
    to figure out the total number of pods in the set. One important thing to be aware
    of is that SRV record creation is not instantaneous! When the StatefulSet is initially
    deployed, it will take a bit of time for the SRV records to become available.
    To this end, if the SRV lookup does not yield any results, the code will return
    a typed error to let the caller know that they should try again later.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了获取SRV记录，我们使用`net`包中的`LookupSRV`函数，并将服务名作为最后一个参数传递。然后，我们计算结果的数量以确定集合中的总Pod数量。需要注意的一个重要事项是，SRV记录的创建不是瞬时的！当StatefulSet最初部署时，SRV记录变得可用需要一些时间。为此，如果SRV查找没有返回任何结果，代码将返回一个类型错误，让调用者知道他们应该稍后再试。
- en: Building wrappers for the application services
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为应用程序服务构建包装器
- en: So far, we intentionally designed the various Link 'R' Us components so that
    they are more or less decoupled from their input sources. For example, the crawler
    component from [Chapter 7](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml), *Data-Processing
    Pipelines*, expects an iterator that yields the set of links to be crawled, while
    the PageRank calculator component from [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    Graph*-Based Data Processing*, only provides convenience methods for creating
    the nodes and edges of the graph that are used by the PageRank algorithm.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们故意设计了各种Link 'R' Us组件，使它们与其输入源或多或少解耦。例如，来自[第7章](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml)，*数据处理管道*的爬虫组件期望一个迭代器，该迭代器产生要爬取的链接集合，而来自[第8章](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml)，*基于图的数据处理*的PageRank计算器组件只为创建PageRank算法使用的图节点和边提供便利方法。
- en: 'To integrate these components into a larger application, we need to provide
    a thin layer that implements two key functions:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些组件集成到更大的应用程序中，我们需要提供一个薄层来实现两个关键功能：
- en: It connects each component with a suitable link graph and the text indexer data
    store implementation from [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml),
    *Building a Persistence Layer*
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过一个合适的链接图将每个组件与来自[第6章](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml)，*构建持久层*的文本索引器数据存储实现连接起来
- en: It manages the *refresh cycle* for each component (for example, triggering a
    new crawler pass or recalculating PageRank scores)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它管理每个组件的**刷新周期**（例如，触发新的爬虫遍历或重新计算PageRank分数）
- en: 'Each service will be started from the main package of the Links ''R'' Us application
    and execute independently of other services. If any of the services exit due to
    an error, we want our application to cleanly shut down, log the error, and exit
    with the appropriate status code. This necessitates the introduction of a supervisor
    mechanism that will manage the execution of each service. Before we get to that,
    let''s start by defining an interface that each of our application services needs
    to implement:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务都将从“链接‘R’我们”应用程序的主包中启动，并独立于其他服务执行。如果任何服务由于错误而退出，我们希望我们的应用程序能够干净地关闭，记录错误，并以适当的退出状态码退出。这需要引入一个管理每个服务执行的监督机制。在我们到达那里之前，让我们首先定义一个接口，我们的应用程序中的每个服务都需要实现：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: No surprise there... The `Name` method returns the name of the service, which
    we can use for logging purposes. As you've probably guessed, the `Run` method
    implements the business logic for the service. Calls to `Run` are expected to
    block until either the provided context expires or an error occurs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不令人惊讶... `Name` 方法返回服务的名称，我们可以用它来记录日志。正如你可能猜到的，`Run` 方法实现了服务的业务逻辑。对 `Run`
    的调用预期会阻塞，直到提供的上下文过期或发生错误。
- en: The crawler service
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬虫服务
- en: 'The business logic for the crawler service is quite straightforward. The service
    uses a timer to sleep until the next update interval is due and then executes
    the following steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫服务的业务逻辑相当简单。服务使用计时器休眠，直到下一个更新间隔到期，然后执行以下步骤：
- en: First, it queries the most recent information about partition assignments. This
    includes the partition number for the pod and the total number of partitions (pod
    count).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它查询关于分区分配的最新信息。这包括pod的分区号和分区总数（pod计数）。
- en: Using the partition count information from the previous step, a new full `Range` is
    created and the extents (UUID range) for the currently assigned partition number
    are calculated.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步骤中的分区计数信息，创建一个新的完整`Range`，并计算当前分配的分区号的范围（UUID范围）。
- en: Finally, the service obtains a link iterator for the calculated UUID range and
    uses it as a data source to drive the crawler component that we built in [Chapter
    7](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml), *Data-Processing Pipelines*.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，该服务获取了计算出的UUID范围的链接迭代器，并将其用作数据源来驱动我们在[第7章](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml)，“数据处理管道”中构建的爬虫组件。
- en: 'The service constructor expects a configuration object that includes not only
    the required configuration options but also a set of interfaces that the service
    depends on. This approach allows us to test the service in total isolation by
    injecting mock objects that satisfy these interfaces. Here''s what the `Config` type
    for the crawler service looks like:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 服务构造函数期望一个配置对象，它不仅包括所需的配置选项，还包括服务所依赖的一组接口。这种方法允许我们通过注入满足这些接口的模拟对象来完全隔离地测试服务。以下是爬虫服务的`Config`类型：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You might be wondering why I chose to redefine the `GraphAPI` and `IndexAPI` interfaces inside this
    package instead of simply importing and using the original interfaces from the `graph` or
    `index` packages. This is, in fact, an application of the interface segregation
    principle! The original interfaces contain more methods than what we actually
    need for this service. For example, the following is the set of methods that the
    crawler requires to access the link graph and indexing documents:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我选择在这个包内重新定义`GraphAPI`和`IndexAPI`接口，而不是简单地导入并使用来自`graph`或`index`包的原始接口。这实际上是对接口分离原则的应用！原始接口包含比我们实际需要的更多方法。例如，以下是为访问链接图和索引文档而需要的爬虫所需的方法集：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A very handy side effect of using the smallest possible interface definitions
    for the graph and index APIs is that these minimal interfaces also happen to be
    compatible with the gRPC clients that we created in the previous chapter. We will
    be exploiting this observation in the next chapter so that we can split our monolithic
    application into microservices! Now, let''s take a look at the rest of the configuration
    fields:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用尽可能小的图和索引API接口定义的一个非常实用的副作用是，这些最小接口也恰好与我们在上一章中创建的gRPC客户端兼容。我们将在下一章利用这一观察结果，以便将我们的单体应用程序拆分为微服务！现在，让我们看看其他配置字段：
- en: '`PartitionDetector` will be queried by the service to obtain its partition
    information. When running in Kubernetes, the detector will use the code from the
    previous section to discover the available partitions. Alternatively, a partition
    detector that always reports a single partition can be injected to allow us to
    run the application as a standalone binary on our development machine.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PartitionDetector`将由服务查询以获取其分区信息。当在Kubernetes上运行时，检测器将使用上一节中的代码来发现可用的分区。或者，可以注入一个总是报告单个分区的分区检测器，以便我们可以在我们的开发机器上以独立二进制文件的形式运行应用程序。'
- en: '`Clock` allows us to inject a fake clock instance for our tests. Just as we
    did in [Chapter 4](d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml), *The Art of Testing*,
    we will be using the `juju/clock` package to mock time-related operations within
    our tests.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Clock`允许我们在测试中注入一个假的时钟实例。正如我们在[第4章](d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml)，*测试的艺术*中所做的那样，我们将使用`juju/clock`包来模拟测试中的时间相关操作。'
- en: '`Fand so onhWorkers` controls the number of workers that are used by the crawler
    component to retrieve links.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fand so onhWorkers`控制爬虫组件用于检索链接的工作线程数量。'
- en: '`UpdateInterval` specifies how often the crawler should perform a new pass.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UpdateInterval`指定爬虫应该多久执行一次新的遍历。'
- en: '`ReIndexThreshold` is used as a filter when selecting the set of links to be
    crawled in the next crawler pass. A link will be considered for crawling when
    its *last retrieval time* is *older* than `time.Now() - ReIndexThreshold`.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReIndexThreshold`在下一轮爬虫遍历中选择要爬取的链接集合时用作过滤器。当链接的*最后检索时间*比`time.Now() - ReIndexThreshold`更早时，将考虑该链接进行爬取。'
- en: '`Logger` specifies an optional logger instance to use for log messages. We
    will talk more about structured logging in the next chapter.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Logger`指定用于日志消息的可选日志实例。我们将在下一章中更多地讨论结构化日志。'
- en: The PageRank calculator service
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PageRank计算器服务
- en: In a similar fashion to the crawler service, the PageRank service also wakes
    up periodically to recalculate the PageRank scores for every link in the graph.
    Under the hood, it uses the PageRank calculator component that we built in [Chapter
    8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml), *Graph-Based Data Processing*,
    to execute a complete pass of the PageRank algorithm. The service layer is responsible
    for populating the internal graph representation that's used by the calculator
    component, invoking it to calculate the updated PageRank scores, and updating
    the PageRank scores for every indexed document.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与爬虫服务类似，PageRank服务也会定期唤醒以重新计算图中每个链接的PageRank分数。在底层，它使用我们在[第8章](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml)，*基于图的数据处理*中构建的PageRank计算器组件来执行完整的PageRank算法遍历。服务层负责填充计算器组件使用的内部图表示，调用它来计算更新的PageRank分数，并更新每个索引文档的PageRank分数。
- en: 'The service constructor also accepts a `Config` object that looks like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 服务构造函数还接受一个类似这样的`Config`对象：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `pagerank` service package defines its own version of the `GraphAPI` and `IndexAPI` types.
    As shown in the following code, the method list for these interfaces is different
    from the one we used for the crawler service in the previous section:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`pagerank`服务包定义了自己的`GraphAPI`和`IndexAPI`类型版本。如下面的代码所示，这些接口的方法列表与我们之前在爬虫服务中使用的不同：'
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `ComputeWorkers` parameter is passed through to the PageRank calculator
    component and controls the number of workers that are used to execute the PageRank
    algorithm. On the other hand, the `UpdateInterval` parameter controls the score
    refresh frequency.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`ComputeWorkers`参数传递给PageRank计算器组件，并控制用于执行PageRank算法的工作线程数量。另一方面，`UpdateInterval`参数控制分数刷新频率。'
- en: Unfortunately, it is not *currently* feasible to run the PageRank service in
    partitioned mode. As we saw in [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    *Graph-Based Data Processing*, the calculator implementation operates under the
    assumption that every node in the graph can send messages to *every* other node
    in the graph and that all the vertices have access to the shared global state
    (aggregators). For the time being, we will use the detected partition information
    as a constraint to execute the service on a **single** pod (more specifically,
    the one assigned to partition 0). No need to worry, though! In [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph-Processing Systems*, we will revisit this implementation
    and rectify all the aforementioned issues.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前无法以分区模式运行PageRank服务。正如我们在第8章[图基于数据处理](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml)中看到的，计算器实现是在以下假设下进行的：图中的每个节点都可以向图中的**每个**其他节点发送消息，并且所有顶点都可以访问共享的全局状态（聚合器）。目前，我们将使用检测到的分区信息作为约束，在**单个**Pod上执行服务（更具体地说，分配给分区0的那个）。不过，不用担心！在第12章[构建分布式图处理系统](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml)中，我们将重新审视这个实现并纠正所有上述问题。
- en: Serving a fully functioning frontend to users
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为用户提供一个完全功能的用户界面
- en: Of course, our little project can't really be complete without a proper frontend
    for our users! To build one, we will leverage the Go standard library's support
    for HTML templates (the `text/template` and `html/template` packages) to design
    a fully functioning static website for Links 'R' Us. For simplicity, all the HTML
    templates will be embedded into our application as *strings* and parsed into `text.Template` when
    the application starts. In terms of functionality, our frontend must implement
    a number of features.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的这个小项目如果没有为用户提供适当的用户界面是无法完成的！为了构建一个，我们将利用Go标准库对HTML模板的支持（`text/template`和`html/template`包）来为Links
    'R' Us设计一个完全功能的静态网站。为了简单起见，所有的HTML模板都将作为字符串嵌入到我们的应用程序中，并在应用程序启动时解析为`text.Template`。在功能方面，我们的前端必须实现许多功能。
- en: 'First, it must implement an index/landing page where the user can enter a search
    query. Queries can either be keyword- or phrase-based. The index page should also
    include a link that can navigate users to another page where they can submit a
    website for indexing. The following screenshot shows a rendering of the index
    page template:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它必须实现一个索引/着陆页，用户可以在其中输入搜索查询。查询可以是基于关键字或短语的。索引页面还应包括一个链接，可以导航用户到另一个页面，他们可以在那里提交网站进行索引。以下截图显示了索引页面模板的渲染：
- en: '![](img/066ea421-a3a2-4d39-862d-4f6372496d63.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/066ea421-a3a2-4d39-862d-4f6372496d63.png)'
- en: 'Figure 3: The landing page for Links ''R'' Us'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Links 'R' Us的着陆页
- en: 'Next, we need a page where webmasters can manually submit their websites for
    indexing. As we mentioned previously, the index/landing page will include a link
    to the site submission page. The following screenshot shows what the rendered
    site submission page for indexing will look like:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个页面，网站管理员可以手动提交他们的网站进行索引。正如我们之前提到的，索引/着陆页将包括一个链接到网站提交页面。以下截图显示了索引网站提交页面的渲染效果：
- en: '![](img/416b6e86-061e-41af-967d-05be5b1dcec3.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/416b6e86-061e-41af-967d-05be5b1dcec3.png)'
- en: 'Figure 4: A form for manually submitting sites for indexing'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：手动提交网站进行索引的表单
- en: 'The final, and obviously most important, page in our entire application is
    the search results page. As shown in the following screenshot, the results page
    renders a **paginated** list of websites matching the user''s search query. The
    header of the page includes a search text box that displays the currently searched
    term and allows users to change their search terms without leaving the page:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们整个应用程序中最后，并且显然是最重要的页面是搜索结果页面。如图所示，结果页面显示与用户搜索查询匹配的网站的分页列表。页面标题包括一个搜索文本框，显示当前搜索的术语，并允许用户在不离开页面的情况下更改搜索术语：
- en: '![](img/8f8aab16-317e-4958-a397-50aefedf6ed1.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f8aab16-317e-4958-a397-50aefedf6ed1.png)'
- en: 'Figure 5: The paginated list of search results'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：搜索结果分页列表
- en: 'The template for rendering the individual search result blocks, as portrayed
    in the preceding screenshot, consists of three sections:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 生成单个搜索结果块模板的模板，如前一个截图所示，包括三个部分：
- en: A link to the web page. The link text will either display the title of the matched
    page or the link to the page itself, depending on whether the crawler was able
    to extract its title.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个指向网页的链接。链接文本将显示匹配页面的标题或页面本身的链接，具体取决于爬虫是否能够提取其标题。
- en: The URL to the matched web page in a smaller font.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以较小字体显示匹配网页的 URL。
- en: A summary of the page's contents where the matched keywords are highlighted.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含突出显示匹配关键词的页面内容摘要。
- en: Now that we have defined all the necessary templates for rendering the pages
    that compose the Links 'R' Us frontend, we need to register a series of HTTP routes
    to allow our end users to access our service.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为渲染构成 Links 'R' Us 前端页面的所有必要模板定义了所有必要的模板，我们需要注册一系列 HTTP 路由，以便我们的最终用户可以访问我们的服务。
- en: Specifying the endpoints for the frontend application
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定前端应用的端点
- en: 'The following table lists the HTTP request types and endpoints that our frontend
    service needs to handle in order to implement all the features we described previously:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了我们前端服务需要处理的 HTTP 请求类型和端点，以便实现我们之前描述的所有功能：
- en: '| **Request Type** | **Path** | **Description** |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **请求类型** | **路径** | **描述** |'
- en: '| GET | `/` | Displays the index page |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| GET | `/` | 显示索引页面 |'
- en: '| GET | `/search?q=TERM` | Displays the first page of results for TERM |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| GET | `/search?q=TERM` | 显示 TERM 的第一页结果 |'
- en: '| GET | `/search?q=TERM&offset=X` | Displays the results for TERM, starting
    from a particular offset |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| GET | `/search?q=TERM&offset=X` | 显示从特定偏移量开始的 TERM 的结果 |'
- en: '| GET | `/submit/site` | Displays the site submission form |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| GET | `/submit/site` | 显示网站提交表单 |'
- en: '| POST | `/submit/site` | Handles a site submission |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| POST | `/submit/site` | 处理网站提交 |'
- en: '| ANY | Any other path | Displays a 404 page |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ANY | 任何其他路径 | 显示 404 页面 |'
- en: 'To make our life easier, we will be using `gorilla/mux` as our preferred router.
    Creating the router and registering the endpoint handlers is as simple as using
    the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的工作更简单，我们将使用 `gorilla/mux` 作为首选的路由器。创建路由器并注册端点处理程序就像使用以下代码一样简单：
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To make the frontend service easier to test, the `Service` type stores a reference
    to the router. This way, we can use the `httptest` package primitives to perform
    HTTP requests directly at the mux without having to spin up any servers.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使前端服务更容易测试，`Service` 类型存储了对路由器的引用。这样，我们可以使用 `httptest` 包的原始程序直接在 mux 上执行 HTTP
    请求，而无需启动任何服务器。
- en: Performing searches and paginating results
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行搜索和分页结果
- en: Searching and paginating results is more or less a straightforward task for
    the frontend service. All our service needs to do is parse the search terms, offset
    from the request's query string, and invoke the `Query` method of the text indexer
    store that was passed as a configuration option when the service was instantiated.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索和分页结果对于前端服务来说基本上是一个直接的任务。我们服务需要做的只是解析搜索词、请求查询字符串中的偏移量，并调用在服务实例化时作为配置选项传递的文本索引存储库的
    `Query` 方法。
- en: 'Then, the service consumes the result iterator until it has either processed
    enough results to populate the results page or the iterator reaches the end of
    the result set. Consider the following code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，服务消费结果迭代器，直到它已经处理了足够的结果以填充结果页面或迭代器达到结果集的末尾。考虑以下代码：
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The service creates a decorated model for each result that provides some convenience
    methods that will be called by the Go code blocks within the template. In addition,
    the `matchedDoc` type includes a `summary` field, which is populated with a short
    excerpt of the matched page's contents, with the search terms highlighted.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 服务为每个结果创建了一个装饰模型，该模型提供了一些方便的方法，这些方法将由模板内的 Go 代码块调用。此外，`matchedDoc` 类型包含一个 `summary`
    字段，其中包含匹配页面内容的简短摘录，并突出显示搜索词。
- en: To highlight search terms in the text summary, the keyword highlighter will
    wrap each term in a `<em>` tag. However, this approach requires the result page
    template to render summaries as **raw HTML**. Consequently, we must be very careful
    not to allow any other HTML tags in our result summaries as this would make our
    application vulnerable to **cross-site scripting** (**XSS**) attacks. While the
    crawler component strips all the HTML tags from crawled pages, it doesn't hurt
    to be a little paranoid and escape any HTML characters from the generated summaries
    before passing them through to our keyword highlighter.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在文本摘要中突出显示搜索词，关键词突出显示器将每个术语包裹在一个`<em>`标签中。然而，这种方法要求结果页面模板以**原始HTML**格式渲染摘要。因此，我们必须非常小心，不要允许我们的结果摘要中包含任何其他HTML标签，因为这会使我们的应用程序容易受到**跨站脚本**（**XSS**）攻击。虽然爬虫组件从爬取的页面中删除了所有HTML标签，但在将生成的摘要传递给关键词突出显示器之前，逃避任何HTML字符也不无裨益。
- en: 'To be able to render the navigation header and footer, we need to provide the
    page template with information about the current pagination state. The following
    code shows how the `paginationDetails` type is populated with the required bits
    of information:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够渲染导航标题和页脚，我们需要向页面模板提供有关当前分页状态的信息。以下代码显示了如何用所需的片段信息填充`paginationDetails`类型：
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A *previous* result page link will always be rendered when the current result
    offset is greater than 0\. Unless we are moving back to the *first* result page,
    the link will always include an offset parameter. Similarly, the *next* result
    page link will be rendered as long as we haven't reached the end of the result
    set.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当当前结果偏移量大于0时，将始终渲染一个**上一个**结果页面链接。除非我们正在返回到**第一个**结果页面，否则链接将始终包含一个偏移量参数。同样，只要我们没有达到结果集的末尾，就会渲染**下一个**结果页面链接。
- en: Generating convincing summaries for search results
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成搜索结果的令人信服的摘要
- en: Generating a descriptive short summary that conveys enough information to the
    user about the contents of a web page that matched their query is quite a hard
    problem to solve. As a matter of fact, automatic summarization is an active research
    field for natural language processing and machine learning.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个描述性的简短摘要，向用户传达足够的信息，关于匹配他们查询的网页内容，这是一个相当难以解决的问题。事实上，自动摘要自然语言处理和机器学习的一个活跃的研究领域。
- en: 'Arguably, building such a system is outside the scope of this book. Instead,
    we will be implementing a much simpler algorithm that yields plausible summaries
    that should be good enough for our particular use case. Here is an outline of
    the algorithm''s steps:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 建立这样的系统可能超出了本书的范围。相反，我们将实现一个更简单的算法，该算法生成的摘要应该是足够好的，适用于我们的特定用例。以下是算法步骤的概述：
- en: Split the matched page's content into sentences.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将匹配页面的内容拆分成句子。
- en: For each sentence, calculate the ratio of matched keywords to the total number
    of words. This will serve as our quality metric for selecting and prioritizing
    the set of sentences to be included in the summary.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个句子，计算匹配关键词与总词数的比例。这将作为我们选择和优先考虑要包含在摘要中的句子集的质量指标。
- en: Skip any sentences where the ratio is zero; that is, they don't contain any
    of the searched keywords. These sentences are not really useful for our summary.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过任何匹配率为零的句子；也就是说，它们不包含任何搜索关键词。这些句子对我们摘要的实际用途并没有真正的作用。
- en: Add the remaining sentences to a list where each entry is a tuple of `{ordinal,
    text, matchRatio}`. The ordinal value refers to the location of the sentence in
    the text.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将剩余的句子添加到一个列表中，其中每个条目都是一个包含`{序号，文本，匹配率}`的元组。序号值指的是句子在文本中的位置。
- en: Sort the list by *match ratio* in *descending* order.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照匹配率**降序**对列表进行排序。
- en: Initialize a second list for the sentence fragments that will be used for the
    summary and a variable that keeps track of the remaining characters for the summary.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个用于摘要的句子片段的第二个列表和一个变量，用于跟踪摘要中剩余的字符数。
- en: Iterate the sorted list; for each entry, do the following: If its length is *less* than
    the remaining summary characters, append the entry as is to the second list and
    subtract its length from the remaining characters, variable. If its length is *more* than
    the remaining summary character, truncate the sentence text to the remaining summary
    characters, append it to the second list, and *terminate* the iteration.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历排序后的列表；对于每个条目，执行以下操作：如果其长度小于剩余的摘要字符数，则将条目原样追加到第二个列表中，并从剩余字符变量中减去其长度。如果其长度大于剩余的摘要字符数，则截断句子文本到剩余的摘要字符数，将其追加到第二个列表中，并**终止**迭代。
- en: Sort the summary fragment list by *ordinal* in *ascending* order. This ensures
    that sentence fragments appear in the same order as the text.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照升序顺序对摘要片段列表进行排序。这确保了句子片段以与文本相同的顺序出现。
- en: 'Iterate the sorted fragment list and concatenate the entries as follows:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式遍历排序后的片段列表并连接条目：
- en: If the ordinal of the current sentence is one more than the previous sentence's
    ordinal, they should be joined with a single period, just like they were connected
    together in the original text.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前句子的序号比上一句的序号多一个，它们应该用单个句号连接，就像它们在原始文本中连接在一起一样。
- en: Otherwise, the sentences should be joined with an ellipsis since they belong
    to different parts of the text.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，句子应该用省略号连接，因为它们属于文本的不同部分。
- en: The complete Go implementation of the preceding algorithm is too long to list
    here, but if you're curious, you can take a look at it by visiting this book's
    GitHub repository and browsing the contents of the `summarizer.go` file, which
    you can find under the `Chapter10/linksrus/service/frontend` folder.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 上述算法的完整Go实现太长，无法在此列出，但如果您感兴趣，可以通过访问本书的GitHub仓库并浏览`Chapter10/linksrus/service/frontend`文件夹下的`summarizer.go`文件来查看它。
- en: Highlighting search keywords
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 突出显示搜索关键词
- en: Once we have generated a summary for a matched document, we need to identify
    and highlight all the search keywords that are present within it. For this task,
    we will create a helper type named `matchHighlighter` that constructs a set of
    regular expressions for matching each search keyword and wrap it with a special
    HTML tag that our frontend template renders using a highlighted style.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为匹配的文档生成了摘要，我们需要识别并突出显示其中存在的所有搜索关键词。为此任务，我们将创建一个名为`matchHighlighter`的辅助类型，它构建一组正则表达式以匹配每个搜索关键词，并用一个特殊的HTML标签将其包裹起来，我们的前端模板使用突出显示样式渲染。
- en: 'The frontend creates a single `matchHighlighter` instance for the entire set
    of results by invoking the `newMatchHighlighter` function, which is listed in
    the following code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 前端通过调用`newMatchHighlighter`函数创建一个用于整个结果集的单一`matchHighlighter`实例，该函数在以下代码中列出：
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The constructor receives the user's search terms as input and splits them into
    a list of words. Note that the search term will be enclosed in quotes if the user
    is searching for an exact phrase. Therefore, before passing the term string to `strings.Fields`, we
    need to trim any quotes at the beginning and end of the input string.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接收用户的搜索词作为输入，并将它们分割成一个单词列表。请注意，如果用户正在搜索一个精确短语，搜索词将被引号包围。因此，在将术语字符串传递给`strings.Fields`之前，我们需要删除输入字符串开头和结尾的任何引号。
- en: 'Then, for each individual term, we compile a *case-insensitive* regular expression,
    which will be used by the `Highlight` method. This is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个单独的术语，我们编译一个**不区分大小写**的正则表达式，该表达式将由`Highlight`方法使用。具体如下：
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `Highlight` method simply iterates the list of regular expressions and wraps
    each match in a `<em>` tag that our result page template can style using CSS rules.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`Highlight`方法简单地遍历正则表达式列表，并将每个匹配项包裹在一个`<em>`标签中，我们的结果页面模板可以使用CSS规则来样式化。'
- en: Orchestrating the execution of individual services
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协调单个服务的执行
- en: 'So far, we have created three services for our monolith that all implement
    the `Service` interface. Now, we need to introduce a supervisor for coordinating
    their execution and making sure that they all cleanly terminate if any of them
    reports an error. Let''s define a new type so that we can model a group of services
    and add a helper `Run` method to manage their execution life cycle:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经为我们的单体应用创建了三个服务，它们都实现了`Service`接口。现在，我们需要引入一个协调它们执行并确保在它们中的任何一个报告错误时都能干净地终止的监督器。让我们定义一个新的类型，以便我们可以模拟一组服务，并添加一个辅助`Run`方法来管理它们的执行生命周期：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s break down the `Run` method''s implementation into smaller chunks
    and go through each one:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 `Run` 方法的实现分解成更小的部分，并逐一分析：
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, first, we create a new cancelable context that wraps the one
    that was externally provided to us by the `Run` method caller. The wrapped context
    will be provided as an argument to the `Run` method of each individual service,
    thus ensuring that *all* the services can be canceled in one of two ways:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，首先，我们创建了一个新的可取消上下文，该上下文包装了由 `Run` 方法调用者外部提供给我们的一个上下文。包装上下文将作为参数传递给每个单独服务的
    `Run` 方法，从而确保所有服务都可以通过两种方式之一取消：
- en: By the caller if, for instance, the provided context is canceled or expires
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果调用者，例如，提供的上下文被取消或过期
- en: By the supervisor, if any of the services raise an error
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何服务引发错误，则由管理器处理
- en: 'Next, we will spin up a goroutine for each service in the group and execute
    its `Run` method, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为组中的每个服务启动一个 goroutine 并执行其 `Run` 方法，如下所示：
- en: '[PRE22]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If an error occurs, the goroutine will annotate it with the service name and
    write it to a buffered error channel before invoking the cancel function for the
    wrapped context. As a result, if any service fails, all the other services will
    be automatically instructed to shut down.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生错误，goroutine 将使用服务名称对其进行注释，并将其写入缓冲错误通道，然后在调用包装上下文的取消函数之前。因此，如果任何服务失败，所有其他服务将自动被指示关闭。
- en: 'A `sync.WaitGroup` helps us keep track of the currently running goroutines.
    As we mentioned previously, we are working with long-running services whose `Run` method
    only returns if the context is canceled or an error occurs. In either case, the
    wrapped context will expire so that we can have our service runner wait for this
    event to occur and then call the wait group''s `Wait` method to ensure that all
    the spawned goroutines have terminated before proceeding. The following code demonstrates
    how this is achieved:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`sync.WaitGroup` 帮助我们跟踪当前运行的 goroutine。如前所述，我们正在处理长时间运行的服务，其 `Run` 方法仅在上下文被取消或发生错误时返回。在任何情况下，包装上下文都将过期，这样我们的服务运行器就可以等待此事件发生，然后调用等待组的
    `Wait` 方法，以确保在继续之前所有派生的 goroutine 都已终止。以下代码演示了如何实现这一点：'
- en: '[PRE23]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Before returning, we must check for the presence of errors. To this end, we
    close the error channel so that we can iterate it using a `range` statement. Closing
    the channel is safe since all the goroutines that could potentially write to it
    have already terminated. Consider the following code:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在返回之前，我们必须检查错误的存在。为此，我们关闭错误通道，以便可以使用 `range` 语句迭代它。关闭通道是安全的，因为所有可能写入它的 goroutine
    都已经终止。考虑以下代码：
- en: '[PRE24]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As shown in the preceding snippet, after closing the channel, the code dequeues
    and aggregates any reported errors and returns them to the caller. Note that a
    nil error value will be returned if no error has occurred.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文代码片段所示，在关闭通道后，代码将取消队列并汇总任何报告的错误，并将它们返回给调用者。注意，如果没有发生错误，将返回一个 nil 错误值。
- en: Putting everything together
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'The main package serves as the entry point for our application. It exposes
    the configuration options for the various services as command-line flags and takes
    care of the following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 主包作为我们应用程序的入口点。它将各种服务的配置选项作为命令行标志暴露出来，并负责以下事项：
- en: Instantiating the appropriate data store implementations for the link graph
    (memory or CockroachDB) and text indexer (memory or Elasticsearch)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为链接图（内存或CockroachDB）和文本索引器（内存或Elasticsearch）实例化适当的数据存储实现
- en: Instantiating the various application services with the correct configuration
    options
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正确的配置选项实例化各种应用程序服务
- en: 'The `runMain` method implements the main loop of the application:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`runMain` 方法实现了应用程序的主循环：'
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As shown in the preceding code, the first line instantiates all the required
    services and adds them to a `Group`. Then, a new cancelable context is created
    and is used to invoke the group's (blocking) `Run` method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文代码所示，第一行实例化了所有必需的服务并将它们添加到 `Group` 中。然后，创建了一个新的可取消上下文，并用于调用组的（阻塞）`Run` 方法。
- en: Terminating the application in a clean way
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以干净的方式终止应用程序
- en: 'At this point, you are probably wondering: how does the application terminate?
    The answer is by receiving a signal from the operating system. The `signal` package
    in the Go standard library comes with a `Notify` function that allows an application
    to register for, and receive, notifications when the application receives a particular
    signal type. Common signal types include the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道：应用程序是如何终止的？答案是应用程序从操作系统接收信号。Go标准库中的`signal`包提供了一个`Notify`函数，允许应用程序注册并接收当应用程序接收特定信号类型时的通知。常见的信号类型包括以下：
- en: '`SIGINT`, which is normally sent to a foreground application when the user
    presses* Ctrl *+ *C*.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SIGINT`，当用户按下*Ctrl* + *C*时通常发送到前台应用程序。'
- en: '`SIGHUP`, which many applications (for example, HTTP servers) hook and use
    as a trigger to reload their configuration.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SIGHUP`，许多应用程序（例如，HTTP服务器）挂钩并用作重新加载其配置的触发器。'
- en: '`SIGKILL`, which is sent to an application before the operating system kills
    it. This particular signal cannot be caught.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SIGKILL`，在操作系统杀死应用程序之前发送给应用程序。这个特定的信号无法捕获。'
- en: '`SIGQUIT`, which is sent to a foreground application when the user presses
    *Ctrl*+ *_*. The Go runtime hooks this signal so that it can print the stacks
    for every running goroutine before terminating the application.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SIGQUIT`，当用户按下 *Ctrl*+ *_* 时发送到前台应用程序。Go运行时挂钩此信号，以便在终止应用程序之前打印每个正在运行的goroutine的堆栈。 '
- en: 'Since our application will be running as a Docker container, we are only interested
    in handling `SIGINT` (sent by Kubernetes when the pod is about to shut down) and `SIGHUP` (for
    debug purposes). Since the preceding code blocks on the group''s `Run` method,
    we need to use a goroutine to watch for incoming signals:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的应用程序将以Docker容器的形式运行，我们只对处理`SIGINT`（由Kubernetes在Pod即将关闭时发送）和`SIGHUP`（用于调试目的）感兴趣。由于前面的代码块在组的`Run`方法上，我们需要使用goroutine来监视传入的信号：
- en: '[PRE26]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Upon receiving one of the specified signals, we immediately invoke the cancellation
    function for the context and return. This action will cause all the services in
    the group to cleanly shut down and for the `svcGroup.Run` call to return, thus
    allowing `runMain` to also return and for the application to terminate.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在收到指定的信号之一后，我们立即调用上下文的取消函数并返回。这一动作将导致组中的所有服务干净地关闭，并且`svcGroup.Run`调用返回，从而允许`runMain`也返回，并使应用程序终止。
- en: Dockerizing and starting a single instance of the monolith
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将单体应用Docker化并启动单个实例
- en: The `Chapter10/linksrus` package comes with a Dockerfile that includes the necessary
    steps for building a dockerized version of the monolithic application that you
    can then run either locally or deploy to Kubernetes using the guide in the following
    section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter10/linksrus`包附带一个Dockerfile，其中包含了构建单体应用的Docker化版本所需的步骤，然后你可以运行它，或者根据下一节中的指南将其部署到Kubernetes。'
- en: To create a Docker image for testing purposes, you can simply type `make dockerize` into
    the package directory. Alternatively, if you wish to build and push the generated
    images to a Docker registry, you can type `make dockerize-and-push`. The Makefile
    target assumes that you are running Minikube and have enabled the private registry
    add-on according to the instructions from the previous sections.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建用于测试目的的Docker镜像，你只需在包目录中输入`make dockerize`。或者，如果你希望构建并将生成的镜像推送到Docker注册表，你可以输入`make
    dockerize-and-push`。Makefile目标假设你正在运行Minikube，并且已经根据前几节的说明启用了私有注册表插件。
- en: 'The tags for all the Docker images that are created by this Makefile will include
    the private registry URL as a prefix. For example, if the IP currently in use
    by Minikube is `192.168.99.100`, the generated image will be tagged as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由这个Makefile创建的所有Docker镜像的标签都将包括私有注册表的URL作为前缀。例如，如果当前Minikube使用的IP是`192.168.99.100`，生成的镜像将被标记如下：
- en: '`192.168.99.100/linksrus-monolith:latest`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.99.100/linksrus-monolith:latest`'
- en: '`192.168.99.100/linksrus-monolith:$GIT_SHA`'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.99.100/linksrus-monolith:$GIT_SHA`'
- en: If you want to use a different private registry (for example, `localhost:32000`,
    if you're using microk8s), you can run `make PRIVATE_REGISTRY=localhost:32000
    dockerize-and-push` instead.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用不同的私有注册表（例如，如果你使用microk8s，则`localhost:32000`），你可以运行`make PRIVATE_REGISTRY=localhost:32000
    dockerize-and-push`代替。
- en: On the other hand, if you want to push the images to the **public** Docker registry,
    you can invoke the command with an **empty** `PRIVATE_REGISTRY` environment variable
    with `make PRIVATE_REGISTRY= dockerize-and-push`.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您想将镜像推送到**公共**Docker注册库，您可以通过使用带有`make PRIVATE_REGISTRY=dockerize-and-push`的**空**`PRIVATE_REGISTRY`环境变量来调用命令。
- en: 'To make it easier for those of you who don''t want to spin up a Kubernetes
    cluster to test-drive the monolithic application, the application default command-line
    values will start the application in standalone mode:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让那些不想启动Kubernetes集群来测试单体应用程序的人更容易，应用程序默认的命令行值将以独立模式启动应用程序：
- en: An in-memory store will be used for both the link graph and the text indexer
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将使用内存存储来存储链接图和文本索引器
- en: A new crawler pass will be triggered every 5 minutes and a PageRank recalculation
    will occur every hour
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每隔5分钟将触发一个新的爬虫遍历，每小时将进行一次PageRank重新计算。
- en: The frontend is exposed on port `8080`
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端暴露在端口`8080`
- en: The receding default settings make it easy to start the application either locally
    by running a command such as `go run main.go` or inside a Docker container by
    running `docker run -it --rm -p 8080:8080 $(minikube ip):5000/linksrus-monolith:latest`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的递减设置使得通过运行命令（如`go run main.go`）在本地启动应用程序或通过运行`docker run -it --rm -p 8080:8080
    $(minikube ip):5000/linksrus-monolith:latest`在Docker容器内启动应用程序变得容易。
- en: Deploying and scaling the monolith on Kubernetes
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署和扩展单体
- en: In the last part of this chapter, we will deploy the Links 'R' Us monolithic
    application on Kubernetes and put the partitioning logic to the test by scaling
    our deployment horizontally.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将部署Links 'R' Us单体应用程序到Kubernetes上，并通过水平扩展我们的部署来测试分区逻辑。
- en: 'The following diagram illustrates what our final setup will look like. As you
    can see, we will be using Kubernetes namespaces to logically split the various
    components for our deployment:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了我们的最终设置将是什么样子。正如您所看到的，我们将使用Kubernetes命名空间来逻辑上分割部署的各种组件：
- en: '![](img/b93c4d8a-8468-4f80-aa46-e849cbb671cc.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b93c4d8a-8468-4f80-aa46-e849cbb671cc.png)'
- en: Figure 6: Deploying a monolithic version of Links 'R' Us on Kubernetes
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在Kubernetes上部署Links 'R' Us的单体版本
- en: From the preceding diagram, we can see that the `linksrus-data` namespace will
    host our data stores, which will be configured in highly available mode. The CockroachDB
    cluster consists of multiple nodes that are hidden behind a Kubernetes service
    resource called `cdb-cockroachdb-public`. Our application can access the DB cluster
    via the service's DNS entry, `cdb-cockroachdb-public.linksrus-data`. The Elasticsearch
    cluster follows exactly the same pattern; it also exposes a service that we can
    use to reach the master nodes by connecting to `elasticsearch-master.linksrus-data:9200`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到`linksrus-data`命名空间将托管我们的数据存储，这些存储将以高可用模式配置。CockroachDB集群由多个节点组成，这些节点隐藏在名为`cdb-cockroachdb-public`的Kubernetes服务资源后面。我们的应用程序可以通过服务的DNS条目`cdb-cockroachdb-public.linksrus-data`访问DB集群。Elasticsearch集群遵循完全相同的模式；它也提供了一个服务，我们可以通过连接到`elasticsearch-master.linksrus-data:9200`来访问主节点。
- en: On the other hand, the `linksrus` namespace is where our application will be
    deployed as a StatefulSet consisting of four replicas. The choice of the number
    of replicas is arbitrary and can be easily adjusted upward or downward at any
    point in time by reconfiguring the StatefulSet.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`linksrus`命名空间是我们将应用程序作为由四个副本组成的StatefulSet部署的地方。副本数量的选择是任意的，并且可以在任何时间点通过重新配置StatefulSet轻松向上或向下调整。
- en: To be able to query the SRV records for all the pods in the StatefulSet, we
    will create a **headless** Kubernetes service. This service makes it possible
    for us to use the partition discovery code that we described in *The crawler service* section.
    Before we can expose our frontend to the outside world, we need to create yet
    another Kubernetes service that will act as a load balancer for distributing incoming
    traffic to the pods in our StatefulSet.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询所有Pod在StatefulSet中的SRV记录，我们将创建一个**无头**Kubernetes服务。这个服务使我们能够使用我们在*爬虫服务*部分描述的分区发现代码。在我们将前端暴露给外部世界之前，我们需要创建另一个Kubernetes服务，该服务将作为负载均衡器，将传入流量分发到我们StatefulSet中的Pod。
- en: The final ingredient in our deploy recipe is an Ingress resource, which will
    allow our end users to access the frontend service over the internet.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署配方中的最后一个成分是一个Ingress资源，它将允许我们的最终用户通过互联网访问前端服务。
- en: 'Every Kubernetes manifest that we will be working with in the following sections
    is available in the `Chapter10/k8s` folder of this book''s GitHub repository.
    Inside the same folder, you can find a Makefile with the following handy targets:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将要处理的每个Kubernetes清单都可在本书GitHub存储库的`Chapter10/k8s`文件夹中找到。在同一个文件夹中，你可以找到一个包含以下便捷目标的Makefile：
- en: '`bootstrap-minikube`: Bootstraps a Kubernetes cluster using Minikube and installs
    all the required add-ons for deploying Links ''R'' Us'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap-minikube`: 使用Minikube引导Kubernetes集群并安装部署“Links ''R'' Us”所需的所有附加组件'
- en: '`deploy`: Deploys all the components for the Links ''R'' Us project, including
    the data stores'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deploy`: 部署“Links ''R'' Us”项目的所有组件，包括数据存储'
- en: '`purge`: Removes all the components that have been installed via `make deploy`'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`purge`: 删除通过`make deploy`安装的所有组件'
- en: '`dockerize-and-push`: Builds and pushes **all** the required container images
    for the Links ''R'' Us project'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dockerize-and-push`: 为“Links ''R'' Us”项目构建并推送所有必需的容器镜像'
- en: Setting up the required namespaces
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置所需的命名空间
- en: 'To create the required namespaces for the deployment, you need to switch to
    the `Chapter10/k8s` folder and apply the `01-namespaces.yaml` manifest by running
    the following command:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建部署所需的命名空间，你需要切换到`Chapter10/k8s`文件夹，并运行以下命令来应用`01-namespaces.yaml`清单：
- en: '[PRE27]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After applying the manifest, the new namespaces should show up when you run `kubectl
    get namespaces`. The following screenshot shows a list of the Kubernetes cluster
    namespaces:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 应用清单后，当你运行`kubectl get namespaces`时，应该会显示新的命名空间。以下截图显示了Kubernetes集群命名空间列表：
- en: '![](img/75c397ac-2ad3-4e25-b8f8-44f0f52f42da.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75c397ac-2ad3-4e25-b8f8-44f0f52f42da.png)'
- en: 'Figure 7: Listing the Kubernetes cluster namespaces'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：列出Kubernetes集群命名空间
- en: The following steps entail the deployment of our database services, followed
    by the deployment of the monolithic Links 'R' Us application.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤包括部署我们的数据库服务，然后是部署单体“Links 'R' Us”应用程序。
- en: Deploying CockroachDB and Elasticsearch using Helm
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Helm部署CockroachDB和Elasticsearch
- en: Setting up the CockroachDB and Elasticsearch cluster is quite tedious and involves
    applying quite a few manifests. Instead of doing this manually, we will actually
    cheat and deploy both data stores using the `helm` tool!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 设置CockroachDB和Elasticsearch集群相当繁琐，涉及到应用相当多的清单。我们实际上会采取作弊的方式，使用`helm`工具来部署这两个数据存储！
- en: 'For CockroachDB, we can run the following command to deploy a three-node cluster:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CockroachDB，我们可以运行以下命令来部署一个三节点集群：
- en: '[PRE28]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `cdb-settings.yaml` file that's referenced by the preceding command contains
    overrides for the default chart values, which restrict the spawned database instance
    to using 512 M of RAM and 100 M of disk space.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令引用的`cdb-settings.yaml`文件包含对默认图表值的覆盖，限制生成的数据库实例使用512 M的RAM和100 M的磁盘空间。
- en: 'The `helm` charts for Elasticsearch are currently maintained in an external
    repository that must be registered with `helm` before we can proceed with the
    installation. Similar to CockroachDB, a settings override file is also provided
    that restricts the Elasticsearch master nodes to using 512 M of RAM and 300 M
    of disk space. The following command will take care of the Elasticsearch deployment:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch的`helm`图表目前维护在外部存储库中，在我们可以继续安装之前，必须使用`helm`注册。类似于CockroachDB，还提供了一个设置覆盖文件，限制Elasticsearch主节点使用512
    M的RAM和300 M的磁盘空间。以下命令将负责Elasticsearch的部署：
- en: '[PRE29]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After running all of the preceding commands, you should be able to type `kubectl
    -n linksrus-data get pods` and see an output similar to the following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行所有前面的命令后，你应该能够输入`kubectl -n linksrus-data get pods`并看到以下类似的输出：
- en: '![](img/eea6e389-2e3e-4817-866a-52f5b4298485.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eea6e389-2e3e-4817-866a-52f5b4298485.png)'
- en: 'Figure 8: Listing the pods in the linksrus-data namespace'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：列出linksrus-data命名空间中的Pod
- en: Once all the data store pods show up as *running*, we can deploy Links 'R' Us!
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有数据存储Pod都显示为*运行中*，我们就可以部署“Links 'R' Us”了！
- en: Deploying Links 'R' Us
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署“Links 'R' Us”
- en: 'Before we can create the Links ''R'' Us StatefulSet, there is one more aspect
    that we need to take care of: the CockroachDB instance is not aware of the schema
    for the link graph. Nothing to worry about... We can remedy this issue by spawning
    a one-off container that will create the database for the link graph and apply
    the schema migrations from [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml),
    *Building a Persistence Layer*.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够创建Links 'R' Us StatefulSet之前，还有一个方面需要我们注意：CockroachDB实例并不知道链接图的架构。不用担心……我们可以通过启动一个一次性容器来解决这个问题，该容器将为链接图创建数据库并应用来自[第6章](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml)，*构建持久层*的架构迁移。
- en: 'You can find the source code and Dockerfile for this container in the `Chapter10/cdb-schema` folder.
    Assuming that you are currently using Minikube for your cluster, you can run the
    following command in the preceding folder to create the Docker image and push
    it to the private registry exposed by Minikube:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Chapter10/cdb-schema`文件夹中找到这个容器的源代码和Dockerfile。假设你目前使用Minikube作为你的集群，你可以在上一个文件夹中运行以下命令来创建Docker镜像并将其推送到Minikube公开的私有仓库：
- en: '[PRE30]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Moving back to the manifests inside the `Chapter10/k8s` folder, you can apply
    the `02-cdb-schema.yaml` manifest to create a one-off Kubernetes `Job` that waits
    for the DB cluster to become available, ensures that the link-graph database and
    schema are up to date, and then exits. Here''s what the content of this YAML file
    looks like:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`Chapter10/k8s`文件夹中的清单，你可以应用`02-cdb-schema.yaml`清单来创建一个一次性Kubernetes `Job`，该Job等待DB集群变得可用，确保链接图数据库和架构是最新的，然后退出。以下是这个YAML文件的内容：
- en: '[PRE31]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, we can deploy the remaining Links 'R' Us resources by applying the `03-linksrus-monolith.yaml` manifest.
    If you haven't done so already, make sure that you run `make dockerize-and-push` in
    the `Chapter10/linksrus` folder prior to applying the manifest to make sure that
    Kubernetes can find the referenced container images.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过应用`03-linksrus-monolith.yaml`清单来部署剩余的Links 'R' Us资源。如果你还没有这样做，确保在应用清单之前，在`Chapter10/linksrus`文件夹中运行`make
    dockerize-and-push`，以确保Kubernetes可以找到引用的容器镜像。
- en: The Makefile in the `k8s` folder also defines a `dockerize-and-push` target
    that can build and push **all** the required container images for running the
    Links 'R' Us demo from this section with a single command.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`k8s`文件夹中的Makefile还定义了一个`dockerize-and-push`目标，它可以构建并推送运行Links ''R'' Us演示所需的**所有**容器镜像，只需一个命令即可。'
- en: 'After a few seconds, you can type `kubectl -n linksrus get pods,statefulsets,services,ingresses` to
    get a list of all the resources we just deployed. The following screenshot shows
    the expected output of this command:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，你可以输入`kubectl -n linksrus get pods,statefulsets,services,ingresses`来获取我们刚刚部署的所有资源的列表。以下截图显示了此命令的预期输出：
- en: '![](img/28e1cb08-0975-483f-98a1-78e90f99850c.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/28e1cb08-0975-483f-98a1-78e90f99850c.png)'
- en: 'Figure 9: Listing all the resources in the linksrus namespace'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：列出linksrus命名空间中的所有资源
- en: Success! Our monolithic application has been deployed and connected to the data
    stores in the `linksrus-data` namespace. You can access the frontend service by
    pointing your browser to the IP address of your ingress. In the preceding output,
    I was using Minikube inside a VM and therefore the displayed ingress address is
    not accessible from the host. However, you can easily find out the public IP that
    was used by Minikube by running `minikube ip` and pointing your browser to it.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们的单体应用已经部署并连接到了`linksrus-data`命名空间中的数据存储。你可以通过将浏览器指向你的ingress IP地址来访问前端服务。在上面的输出中，我正在VM中使用Minikube，因此显示的ingress地址无法从主机访问。然而，你可以通过运行`minikube
    ip`并指向它来轻松地找到Minikube使用的公共IP。
- en: You can tail the logs of each individual pod in the StatefulSet using the `kubectl
    -n linksrus logs linksrus-monolith-instance-X -f` command, where *X* is a pod
    number from the set.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl -n linksrus logs linksrus-monolith-instance-X -f`命令跟踪StatefulSet中每个单独pods的日志，其中*X*是集合中的pods编号。
- en: Moreover, you can also tail the logs from *all* the pods in the set using the `kubectl
    -n linksrus logs -lapp=linksrus-monolith-instance -f` command.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以使用`kubectl -n linksrus logs -lapp=linksrus-monolith-instance -f`命令来跟踪集中所有pods的日志。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to dockerize our Go applications in a way that
    yields container images with the smallest possible size. Then, we talked about
    the design philosophy and general architecture behind Kubernetes and elaborated
    on the different types of resources that you can create and manage on a Kubernetes
    cluster. In the last part of this chapter, we pieced together the first fully
    functioning version of the Links 'R' Us project and deployed it as a single monolithic
    application on Kubernetes.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何以产生尽可能小尺寸容器镜像的方式对Go应用程序进行docker化。然后，我们讨论了Kubernetes背后的设计哲学和总体架构，并详细说明了您可以在Kubernetes集群上创建和管理的不同类型资源。在本章的最后部分，我们将Links
    'R' Us项目的第一个完全功能版本拼凑起来，并将其作为单个单体应用程序部署到Kubernetes上。
- en: In the next chapter, we will discuss the challenges and potential caveats involved
    when switching to a microservice architecture.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论在切换到微服务架构时可能遇到的挑战和潜在问题。
- en: Questions
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Name some benefits of containerization.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列举一些容器化的好处。
- en: What is the difference between a master and a worker node in a Kubernetes cluster?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes集群中主节点和工作节点之间的区别是什么？
- en: What is the difference between a regular service and a headless service?
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正常服务和无头服务之间的区别是什么？
- en: What kind of Kubernetes resource would you use to share your OAuth2 client ID
    and secret with your frontend?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会使用哪种Kubernetes资源来与您的前端共享OAuth2客户端ID和密钥？
- en: Explain the difference between a deployment and a StatefulSet.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释部署和StatefulSet之间的区别。
- en: Further reading
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Alpine Linux: A security-oriented, lightweight Linux distribution based on
    musl libc and busybox. [https://alpinelinux.org](https://alpinelinux.org).'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alpine Linux：基于musl libc和busybox的安全导向、轻量级Linux发行版。[https://alpinelinux.org](https://alpinelinux.org).
- en: 'Calico: Secure networking for the cloud-native era. [https://www.projectcalico.org](https://www.projectcalico.org).'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calico：云原生时代的网络安全。[https://www.projectcalico.org](https://www.projectcalico.org).
- en: 'Cilium: API-aware networking and security. [https://cilium.io](https://cilium.io).'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cilium：API感知的网络和安全。[https://cilium.io](https://cilium.io).
- en: 'Containerd: An industry-standard container runtime with an emphasis on simplicity,
    robustness, and portability. [https://containerd.io](https://containerd.io).'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Containerd：一个以简洁、健壮和可移植性为重点的行业标准容器运行时。[https://containerd.io](https://containerd.io).
- en: 'Docker: Enterprise container platform. [https://www.docker.com](https://www.docker.com).'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker：企业级容器平台。[https://www.docker.com](https://www.docker.com).
- en: 'Helm: The package manager for Kubernetes. [https://helm.sh](https://helm.sh).'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Helm：Kubernetes的包管理器。[https://helm.sh](https://helm.sh).
- en: 'K3S: Lightweight Kubernetes. [https://k3s.io/](https://k3s.io/).'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K3S：轻量级Kubernetes。[https://k3s.io/](https://k3s.io/).
- en: 'Kubernetes: Production-grade container orchestration. [https://www.kubernetes.io](https://www.kubernetes.io).'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes：生产级容器编排。[https://www.kubernetes.io](https://www.kubernetes.io).
- en: 'Microk8s: Zero-ops Kubernetes for workstations and edge / IoT. [https://microk8s.io](https://microk8s.io).'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Microk8s：工作站和边缘/物联网的零操作Kubernetes。[https://microk8s.io](https://microk8s.io).
- en: 'Minikube: Local Kubernetes, focused on application development and education. [https://minikube.sigs.k8s.io](https://minikube.sigs.k8s.io).'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Minikube：专注于应用程序开发和教育的本地Kubernetes。[https://minikube.sigs.k8s.io](https://minikube.sigs.k8s.io).
- en: 'Multipass: Orchestrates virtual Ubuntu instances. [https://multipass.run/](https://multipass.run/).'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Multipass：编排虚拟Ubuntu实例。[https://multipass.run/](https://multipass.run/).
- en: 'rkt: A security-minded, standards-based container engine. [https://coreos.com/rkt](https://coreos.com/rkt).'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: rkt：一个以安全性和标准为基础的容器引擎。[https://coreos.com/rkt](https://coreos.com/rkt).
- en: 'VirtualBox: A powerful x86 and AMD64/Intel64 virtualization product for enterprise
    as well as home use*.* [https://www.virtualbox.org](https://www.virtualbox.org).'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VirtualBox：适用于企业及家庭使用的强大x86和AMD64/Intel64虚拟化产品。[https://www.virtualbox.org](https://www.virtualbox.org).
