- en: Splitting Monoliths into Microservices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将单体分解为微服务
- en: '"If the components do not compose cleanly (when migrating to microservices),
    then all you are doing is shifting the complexity from inside a component to the
    connections between components. This does not just move complexity around; it
    moves it to a place that''s less explicit and harder to control."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “如果组件不能干净地组合（在迁移到微服务时），那么你所做的只是将复杂性从组件内部转移到组件之间的连接。这不仅只是移动复杂性；它将复杂性移动到一个不太明确且更难控制的地方。”
- en: – Martin Fowler and James Lewis
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – 马丁·福勒和詹姆斯·刘易斯
- en: This chapter introduces the concept of **Service-Oriented Architecture** (**SOA**)
    and compares it with the traditional monolithic design pattern. This will help
    us discuss the various challenges of microservices such as logging, tracing, and
    service introspection, and provides advice for reducing the pain points from moving
    to an SOA.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**面向服务架构**（**SOA**）的概念，并将其与传统单体设计模式进行比较。这将帮助我们讨论微服务面临的各项挑战，如日志记录、跟踪和服务自省，并提供减少迁移到SOA时的痛点的建议。
- en: Toward the end of this chapter, we will be breaking down the monolithic Links
    'R' Us implementation from the previous chapter into several microservices and
    deploying them to Kubernetes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的末尾，我们将把上一章中的单体“链接之我行”实现分解为几个微服务，并将它们部署到Kubernetes。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: When is a good time to switch from monolithic design to a microservice-based
    architecture?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时从单体设计切换到基于微服务的架构是合适的？
- en: Common anti-patterns for microservice implementations and how to work around
    them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微服务实现的常见反模式和如何规避它们
- en: Tracing requests through distributed systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分布式系统跟踪请求
- en: Best practices for logging and pitfalls to avoid
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录的最佳实践和要避免的陷阱
- en: Introspection of live Go services
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对实时Go服务的自省
- en: Breaking down the Links 'R' Us monolith into microservices and deploying them
    to Kubernetes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将“链接之我行”单体分解为微服务并将它们部署到Kubernetes
- en: Locking down access to microservices using Kubernetes network policies
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes网络策略锁定对微服务的访问
- en: By leveraging the knowledge you will have obtained in this chapter, you will
    be able to horizontally scale your own projects so as to better handle spikes
    in incoming traffic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用本章所获得的知识，你将能够水平扩展自己的项目，以更好地处理 incoming traffic 的峰值。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The full code for the topics that will be discussed in this chapter has been
    published to this book's GitHub repository under the `Chapter11` folder.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将要讨论的主题的完整代码已发布到本书的GitHub仓库中的`Chapter11`文件夹下。
- en: You can access this book's GitHub repository, which contains the code and all
    the required resources for each chapter in this book, by pointing your web browser
    to the following URL: [https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将网络浏览器指向以下URL来访问本书的GitHub仓库，该仓库包含本书每个章节的代码和所有必需的资源：[https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang)。
- en: 'To get you up and running as quickly as possible, each example project includes
    a Makefile that defines the following set of targets:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你尽快开始，每个示例项目都包含一个Makefile，它定义了以下目标集：
- en: '| **Makefile target** | **Description** |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **Makefile目标** | **描述** |'
- en: '| `deps` | Installs any required dependencies |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `deps` | 安装任何必需的依赖项 |'
- en: '| `test` | Runs all tests and report coverage |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `test` | 运行所有测试并报告覆盖率 |'
- en: '| `lint` | Checks for lint errors |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `lint` | 检查lint错误 |'
- en: As with all the other chapters in this book, you will need a fairly recent version
    of Go, which you can download at [https://golang.org/dl/](https://golang.org/dl/)*.*
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的所有其他章节一样，你需要一个相当新的Go版本，你可以在[https://golang.org/dl/](https://golang.org/dl/)下载它。
- en: To run some of the code examples in this chapter, you will need to have a working
    Docker ^([4]) installation on your machine.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章的一些代码示例，你需要在你的机器上有一个工作的Docker安装。
- en: Furthermore, a subset of the examples are designed to run on Kubernetes. If
    you don't have access to a Kubernetes cluster for testing, you can simply follow
    the instructions laid out in the following sections to set up a small cluster
    on your laptop or workstation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些示例被设计为在Kubernetes上运行。如果你没有访问Kubernetes集群进行测试，你可以简单地遵循以下章节中的说明，在你的笔记本电脑或工作站上设置一个小型集群。
- en: Monoliths versus service-oriented architectures
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单体架构与面向服务的架构
- en: In the last couple of years, more and more organizations, especially in the
    start up scene, have been actively embracing the SOA paradigm either for building
    new systems or for modernizing existing legacy systems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，越来越多的组织，尤其是在初创企业领域，已经开始积极采用SOA范式，无论是构建新系统还是对现有遗留系统进行现代化改造。
- en: SOA is an architectural approach to creating systems that have been built from
    autonomous services that may be written in different programming languages and
    communicate with each other over a network link.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: SOA是一种创建系统的架构方法，这些系统由可能用不同编程语言编写的自主服务组成，并通过网络链接进行通信。
- en: In the following sections, we will examine this architectural pattern in more
    detail and highlight some best practices for migrating from a monolithic application
    to microservices. At the same time, we will explore some common anti-patterns
    that can impede the transition to a microservice-based architecture.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地研究这种架构模式，并突出一些从单体应用程序迁移到微服务的最佳实践。同时，我们将探讨一些可能阻碍向基于微服务的架构过渡的常见反模式。
- en: Is there something inherently wrong with monoliths?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单体架构本身有什么固有的问题吗？
- en: 'Before you decide to take the plunge and convert your monolithic application
    into an SOA, you should take a small pause and ask yourself: is a microservice-based
    design the right model for my application *at this point in time*?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在您决定跳入水并转换您的单体应用程序为SOA之前，您应该稍作停顿，问问自己：基于微服务的架构设计是否是当前这个时间点适合我的应用程序的正确模型？
- en: Try not to be influenced by the hype surrounding microservices! Just because
    this kind of model works at a massive scale for companies such as Google, Netflix,
    or Twitter, it doesn't mean that it also will for your particular use case.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被围绕微服务的炒作所影响！尽管这种模型在谷歌、Netflix或Twitter等公司的大规模应用中效果显著，但这并不意味着它也适用于您的特定用例。
- en: 'Monolithic system designs have been around for much longer and have proven
    themselves time and time again when it comes to supporting business-critical systems.
    As the saying goes: if it''s good enough for banks and airlines, it''s probably
    adequate for your next start up idea!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 单体系统设计已经存在很长时间，并且在支持业务关键系统方面一次又一次地证明了自己的价值。正如俗话所说：如果它对银行和航空公司足够好，那么它可能也适合您的下一个创业想法！
- en: In many cases, the decision to transition to a microservice-based architecture
    is driven purely by necessity; scaling large, monolithic systems to deal with
    irregular spikes in traffic can prove to be quite costly and can oftentimes lead
    to underutilization of the resources available at our disposal. This is a great
    example where switching to microservices would most probably have both an observable
    and measurable effect.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，转向基于微服务的架构的决定纯粹是由必要性驱动的；将大型单体系统扩展以应对流量不规则激增可能会非常昂贵，并且往往会导致我们可利用的资源利用率低下。这是一个很好的例子，说明转向微服务可能会产生可观察和可衡量的效果。
- en: On the other hand, if you are building a new product or a **minimum viable product**
    (**MVP**), it is always much easier to begin with a monolithic design and introduce
    the right abstractions from the start to facilitate an easier transition path
    to microservices, if and when that is required.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您正在构建一个新产品或**最小可行产品**（MVP），从单体设计开始并从一开始引入正确的抽象，以便更容易地过渡到微服务，如果需要的话，这总是更容易。
- en: 'Lots of new start-ups get trapped in the mentality that microservices are the
    next best thing since sliced bread and forget about the hidden cost of such an
    architecture: increased complexity, which directly translates to increased demand
    for DevOps. As a result, engineering teams tend to spend a significant chunk of
    their development time debugging communication issues or setting up elaborate
    schemes for monitoring microservices instead of focusing their efforts on building
    and developing their core product.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多新成立的初创公司陷入了这样的思维定式，认为微服务是继切片面包之后的最佳选择，而忘记了这种架构的潜在成本：复杂性增加，这直接转化为对DevOps需求的增加。因此，工程团队往往会在调试通信问题或设置复杂的监控微服务方案上花费大量开发时间，而不是将精力集中在构建和开发核心产品上。
- en: Microservice anti-patterns and how to deal with them
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务反模式及其处理方法
- en: Now, let's take a look at some anti-patterns that you might encounter when working
    with microservice-based projects and explore alternative ways of dealing with
    them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在处理基于微服务的项目时可能会遇到的反模式，并探讨处理它们的替代方法。
- en: '*Sharing a database* is probably the biggest mistake that engineers new to
    the microservice pattern make when they attempt to split a monolith into microservices
    for the first time. As a rule of thumb, each microservice must be provisioned
    with its own, private data store (assuming it needs one) and expose an API so
    that other microservices can access it. This pattern provides us with the flexibility
    to select the most suitable technology (for example, NoSQL, relational) for the
    needs of each particular microservice.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*共享数据库*可能是工程师在第一次尝试将单体应用拆分为微服务时犯的最大错误。一般来说，每个微服务都必须配备其自己的、私有的数据存储（假设它需要的话），并公开一个API，以便其他微服务可以访问它。这种模式为我们提供了灵活性，可以根据每个特定微服务的需求选择最合适的技术（例如，NoSQL、关系型）。'
- en: Communication between microservices might fail for a variety of reasons (for
    example, a service crash, network partition, or lost packets). A correct microservice
    implementation should operate under the assumption that outbound calls can fail
    at any time. Instead of immediately bailing out with an error when things go wrong,
    microservices should always implement some sort of *retry logic*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务之间的通信可能会因为各种原因（例如，服务崩溃、网络分区或数据包丢失）而失败。正确的微服务实现应该基于这样的假设：出站调用可能在任何时候失败。当事情出错时，微服务不应该立即因为错误而退出，而应该始终实现某种形式的*重试逻辑*。
- en: A corollary to the preceding statement is that when a connection to a remote
    microservice drops before receiving a reply, the client cannot be sure whether
    the remote server actually managed to process the request. Based on the preceding
    recommendation, the client will typically retry the call. Consequently, every
    microservice that exposes an API must be written in such a way so that requests
    are always *idempotent*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前述陈述的一个推论是，当连接到远程微服务在收到回复之前断开时，客户端无法确定远程服务器是否实际上成功处理了请求。根据前面的建议，客户端通常会重试调用。因此，每个公开API的微服务都必须以这种方式编写，使得请求始终是*幂等的*。
- en: Another common anti-pattern is to allow a service to become a *single point
    of failure* for the entire system. Imagine a scenario where you have three services
    that all depend on a piece of data that's been exposed by a fourth, downstream
    service. If the latter service is underprovisioned, a sudden request in traffic
    to the three upstream services might cause requests to the downstream service
    to time out. The upstream services would then retry their requests, increasing
    the load on the downstream service even further, up to the point where it becomes
    unresponsive or crashes. As a result, the upstream services now begin experiencing
    elevated error rates that affect calls that are made to them by other upstream
    services, and so on and so forth.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的反模式是允许一个服务成为整个系统的*单点故障*。想象一下这样一个场景：你有三个服务，它们都依赖于由第四个下游服务公开的数据。如果后者服务配置不足，对三个上游服务的突发流量请求可能会导致对下游服务的请求超时。然后，上游服务会重试它们的请求，进一步增加下游服务的负载，直到它变得无响应或崩溃。结果，上游服务现在开始经历更高的错误率，这影响了其他上游服务对它们的调用，等等。
- en: 'To avoid situations like this, microservices can implement the circuit breaker
    pattern: when the number of errors from a particular downstream service exceeds
    a particular threshold, the circuit breaker is tripped and all future requests
    automatically fail with an error. Periodically, the circuit breaker lets some
    requests go through and after a number of successful responses, the circuit breaker
    switches back to the open position, allowing all requests to go through.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，微服务可以实现断路器模式：当特定下游服务的错误数量超过特定阈值时，断路器被触发，所有未来的请求都会自动以错误失败。定期地，断路器会允许一些请求通过，并在收到一定数量的成功响应后，断路器切换回开启位置，允许所有请求通过。
- en: By implementing this pattern into your microservices, we allow downstream services
    to recover from load spikes or crashes. Moreover, some services might be able
    to respond with cached data when downstream services are not available, thus ensuring
    that the system remains functional, even in the presence of problems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将此模式应用于您的微服务，我们允许下游服务从负载峰值或崩溃中恢复。此外，当下游服务不可用时，一些服务可能能够使用缓存的数据进行响应，从而确保系统即使在出现问题时也能保持功能正常。
- en: As we have already explained, microservice-based architectures are inherently
    complex as they consist of a large number of moving parts. The biggest mistake
    that we can make is switching to this kind of architecture before laying down
    the necessary infrastructure for collecting the log output of each microservice
    and monitoring its health. Without this infrastructure in place, we are effectively
    flying blind. In the next section, we will explore a few different approaches
    to microservice instrumentation and monitoring.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经解释过的，基于微服务的架构本质上很复杂，因为它们由大量移动部件组成。我们可能犯的最大错误是在没有为收集每个微服务的日志输出和监控其健康状态建立必要的基础设施之前就切换到这种架构。如果没有这种基础设施，我们实际上是在盲目飞行。在下一节中，我们将探讨几种不同的微服务仪表化和监控方法。
- en: Monitoring the state of your microservices
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控您的微服务状态
- en: 'In the following sections, we will be analyzing an array of different approaches
    for monitoring the state of a microservice deployment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将分析一系列用于监控微服务部署状态的不同的方法：
- en: Request tracing
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求跟踪
- en: Log collection and aggregation
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志收集和聚合
- en: Introspection of live Go services with the help of `pprof`
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pprof`对实时Go服务进行内省
- en: Tracing requests through distributed systems
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过分布式系统跟踪请求
- en: In a world where you might have distributed systems with hundreds or thousands
    of microservices running, request tracing is an invaluable tool for figuring out
    bottlenecks, understanding the dependencies between individual services, and figuring
    out the root cause of issues that affect production systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可能拥有数百或数千个微服务运行的分布式系统中，请求跟踪是确定瓶颈、理解各个服务之间的依赖关系以及找出影响生产系统问题的根本原因的无价工具。
- en: The idea behind tracing is to tag an incoming (usually external) request with
    a unique identifier and keep track of it as it propagates through the system,
    hopping from one microservice to the next until it eventually exits the system.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪背后的想法是为传入的（通常是外部的）请求添加一个唯一的标识符，并在它通过系统传播时跟踪它，从一个微服务跳到另一个微服务，直到它最终退出系统。
- en: 'The concept of a distributed tracing system is definitely not new. In fact,
    systems such as Google''s Dapper ^([17]) and Twitter''s Zipkin ^([16]) have been
    around for almost a decade. So, why isn''t everyone jumping on the wagon and implementing
    it for their code bases? The reason is simple: up until now, updating your entire
    code base to support request tracing used to be a daunting task.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式跟踪系统的概念绝对不是新的。事实上，像Google的Dapper^([17])和Twitter的Zipkin^([16])这样的系统已经存在了近十年。那么，为什么不是每个人都跳上这辆马车，为自己的代码库实现它呢？原因很简单：直到现在，更新整个代码库以支持请求跟踪曾经是一项艰巨的任务。
- en: Imagine a system where the components communicate with each other via different
    types of transports, that is, some microservices use REST, others use gRPC, and
    others perhaps exchange events over WebSockets. Ensuring that request IDs get
    injected into all outgoing requests and unmarshaled on the receiving end requires
    quite a bit of effort to implement across all microservices. What's more, if you
    were to go down this route, you would be expected to do a bit of research, select
    a tracing *vendor* to use, and finally integrate with their (typically proprietary)
    API, which would effectively lock you into their offering.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个系统，其中组件通过不同类型的传输相互通信，也就是说，一些微服务使用REST，其他使用gRPC，还有一些可能通过WebSockets交换事件。确保请求ID被注入到所有发出的请求中，并在接收端反序列化，需要在所有微服务中实施相当多的努力。更重要的是，如果您选择走这条路，您将需要做一些研究，选择一个要使用的跟踪*供应商*，并最终将其（通常是专有的）API集成，这将有效地将您锁定在其产品中。
- en: There's got to be a better way to implement request tracing!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要有更好的方法来实现请求跟踪！
- en: The OpenTracing project
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenTracing项目
- en: The OpenTracing ^([18]) project was created to solve exactly the set of problems
    that we outlined in the previous section. It provides a standardized, vendor-neutral
    API that software engineers can use to instrument their code base to enable support
    for request tracing. Moreover, OpenTracing not only dictates the appropriate encoding
    for transferring trace contexts *across service boundaries*, but also provides
    APIs to facilitate the exchange of tracing context over REST and gRPC transports.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: OpenTracing ^([18]) 项目是为了解决我们在上一节中概述的 exactly set of problems 而创建的。它提供了一个标准化的、供应商中立的
    API，软件工程师可以使用它来对其代码库进行配置，以启用对请求跟踪的支持。此外，OpenTracing 不仅规定了跨服务边界传输跟踪上下文的适当编码，而且还提供了
    API 以促进通过 REST 和 gRPC 传输交换跟踪上下文。
- en: Before we continue, let's spend some time explaining a term that we will be
    using quite a lot in the following sections. A request trace is comprised of a
    sequence of **spans**. A span represents a timed unit of work that executes inside
    a microservice. In a typical scenario, a new span begins when the service receives
    a request and ends when the service returns a response.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们花些时间解释一下我们将在以下部分中大量使用的术语。请求跟踪由一系列 **跨度** 组成。跨度表示在微服务内部执行的时间单位的工作。在典型场景中，当服务收到请求时开始一个新的跨度，当服务返回响应时结束。
- en: Furthermore, spans can also be nested. If service *A* needs to contact downstream
    services *B* and *C* for additional data before it can send back a response, then
    the spans from *B* and *C* can be added as children of *A*'s span. Consequently,
    a request trace can be thought of as a *tree of spans* whose root is the service
    that received the initial request.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，跨度也可以嵌套。如果服务 *A* 在发送响应之前需要联系下游服务 *B* 和 *C* 以获取额外的数据，那么 *B* 和 *C* 的跨度可以作为
    *A* 的跨度的子跨度添加。因此，请求跟踪可以被视为一个 *跨度树*，其根是接收初始请求的服务。
- en: Stepping through a distributed tracing example
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示分布式跟踪示例
- en: To understand how distributed tracing works, let's build a small demo application
    that simulates a system for collecting price quotes for a particular SKU from
    a variety of vendors. You can find the full source code for this demo in the `Chapter11/tracing` folder
    of this book's GitHub repository.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解分布式跟踪是如何工作的，让我们构建一个小型演示应用程序，该应用程序模拟从多个供应商收集特定 SKU 的价格报价的系统。您可以在本书 GitHub
    存储库的 `Chapter11/tracing` 文件夹中找到此演示的完整源代码。
- en: 'Our system will feature three types of services, all of which will be built
    on top of gRPC:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统将具有三种类型的服务，所有这些服务都将建立在 gRPC 之上：
- en: The **provider** service returns price quotes for a single vendor. For our example
    scenario, we will be spinning up multiple provider instances to simulate different
    vendor systems.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**provider** 服务返回单个供应商的价格报价。在我们的示例场景中，我们将启动多个 provider 实例来模拟不同的供应商系统。'
- en: An **aggregator** service that sends incoming queries to a list of downstream
    services (providers or other aggregators) collects the responses and returns the
    aggregated results.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **aggregator** 服务，它将传入的查询发送到一系列下游服务（提供者或其他聚合器），收集响应并返回聚合结果。
- en: An **API gateway** service, which will serve as the root of the captured request
    traces. In the real world, the API gateway would handle requests from a frontend
    application running on the users' browser.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **API 网关** 服务，它将作为捕获的请求跟踪的根。在现实世界中，API 网关将处理来自用户浏览器上运行的前端应用程序的请求。
- en: 'Let''s begin by listing the protocol buffer and RPC definitions for the services:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先列出服务的协议缓冲区和 RPC 定义：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, we define a single RPC named `GetQuote` that receives a `QuotesRequest` and
    returns a `QuotesResponse`. The response is simply a collection to `Quote` objects,
    with each one consisting of a `vendor` and a `price` field.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们定义了一个名为 `GetQuote` 的单一 RPC，它接收一个 `QuotesRequest` 并返回一个 `QuotesResponse`。响应只是一个包含
    `Quote` 对象的集合，每个对象都包含一个 `vendor` 字段和一个 `price` 字段。
- en: The provider service
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供者服务
- en: 'The first and easiest service to implement is `Provider`. The following is
    the definition for the `Provider` type and its constructor:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先也是最简单的服务是 `Provider`。以下是对 `Provider` 类型及其构造函数的定义：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will implement the `GetQuote` method, as specified in the preceding
    protocol buffer definitions. To keep our example as simple as possible, we will
    provide a dummy implementation that returns a single quote with a random price
    value and the `vendorID` value that was passed as an argument to the `NewProvider` constructor:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `GetQuote` 方法，如前述协议缓冲定义中指定。为了使我们的示例尽可能简单，我们将提供一个模拟实现，它返回一个具有随机价格值和作为
    `NewProvider` 构造函数参数传递的 `vendorID` 值的单个报价：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To simulate a microservice architecture, our main file will start multiple
    instances of this service. Each service instance will create its own gRPC server
    and bind it to a random port. Let''s implement this functionality for the `Provider` type:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟微服务架构，我们的主文件将启动多个此服务的实例。每个服务实例将创建自己的 gRPC 服务器并将其绑定到一个随机端口。让我们为 `Provider`
    类型实现此功能：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `tracer` package encapsulates the required logic for creating tracer instances
    that satisfy the `opentracing.Tracer` interface. The obtained tracer will be used
    for each of the services that we will be creating so that it can collect and report
    spans. In the following sections, we will explore the implementation of this package
    when we select a suitable tracing provider for our example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`tracer` 包封装了创建满足 `opentracing.Tracer` 接口的跟踪实例所需的逻辑。获得的跟踪器将用于我们创建的每个服务，以便它可以收集和报告跨度。在接下来的章节中，我们将探讨在为我们的示例选择合适的跟踪提供者时，该包的实现。'
- en: 'After obtaining a tracer, the `Serve` method calls out to `doServe`, whose
    task is to expose a gRPC server to a random available port and return its listen
    address. The `doServe` code, which is listed in the following code block, has
    been intentionally extracted since we will be using this to implement the aggregator
    service:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取跟踪器后，`Serve` 方法调用 `doServe`，其任务是向一个随机可用的端口公开 gRPC 服务器并返回其监听地址。以下代码块中列出的 `doServe`
    代码已被有意提取，因为我们将会使用它来实现聚合服务：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first lines in the preceding function ask the `net` package to listen to
    a random free port by passing `:0` as the listen address. The next line is where
    the real magic happens! The `grpc-opentracing` ^([10]) package provides gRPC interceptors
    that decode tracing-related information from incoming gRPC requests and *embed* them
    into the request context that is passed to the RPC method implementations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，前几行代码请求 `net` 包通过传递 `:0` 作为监听地址来监听一个随机空闲端口。下一行是真正发生魔法的地方！`grpc-opentracing`
    ^([10]) 包提供了 gRPC 拦截器，可以从传入的 gRPC 请求中解码跟踪相关信息并将它们 *嵌入* 到传递给 RPC 方法实现的请求上下文中。
- en: A gRPC interceptor is a kind of middleware that wraps an RPC call and provides
    additional functionality. Depending on the type of call that is being wrapped,
    interceptors are classified as unary or streaming.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 拦截器是一种中间件，它包装 RPC 调用并提供额外的功能。根据被包装的调用类型，拦截器被分类为单一或流式。
- en: Moreover, interceptors can be applied on the server- or client-side. On the
    server-side, interceptors are typically used to implement features such as authentication,
    logging, and metrics collection. Client-side interceptors can be used to implement
    patterns such as circuit breakers or retries.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拦截器可以在服务器端或客户端应用。在服务器端，拦截器通常用于实现诸如身份验证、日志记录和指标收集等功能。客户端拦截器可以用于实现如断路器或重试等模式。
- en: Since our service only defines a unary RPC, we need to create a unary interceptor
    and pass it to the `grpc.NewServer` function. Then, we register the RPC implementation
    with the server and spin up a goroutine so that we can start serving requests
    until the provided context expires. While the goroutine is running, the function
    returns with the address of the server listener.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的服务仅定义了单一 RPC，我们需要创建一个单一拦截器并将其传递给 `grpc.NewServer` 函数。然后，我们将 RPC 实现注册到服务器并启动一个
    goroutine，以便我们可以开始服务请求直到提供的上下文过期。在 goroutine 运行期间，函数返回服务器监听器的地址。
- en: The aggregator service
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合服务
- en: 'The next service that we will be implementing is the `Aggregator` type. As
    shown in the following code snippet, it stores a vendor ID, a list of provider
    addresses to query, and a list of gRPC clients for those addresses:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要实现的服务是 `Aggregator` 类型。如下代码片段所示，它存储了一个供应商 ID、要查询的提供者地址列表以及这些地址的 gRPC 客户端列表：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The gRPC clients are lazily created when the `Serve` method is invoked:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用 `Serve` 方法时，gRPC 客户端是延迟创建的：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This time, we create a **client** unary interceptor and pass it as an option
    to each client connection that we dial. Then, we invoke the `doServe` helper that
    we examined in the previous section so that we can start our server. The use of
    interceptors for both the server **and** the client ensures that the trace context
    information that we receive from incoming requests gets **automatically** injected
    into any outgoing gRPC request without us having to do anything.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们创建了一个**客户端**单例拦截器，并将其作为选项传递给每个我们拨打的客户端连接。然后，我们调用之前章节中检查的`doServe`辅助函数，以便我们可以启动我们的服务器。服务器和客户端都使用拦截器确保我们从传入请求中接收到的跟踪上下文信息会**自动**注入到任何出去的gRPC请求中，而无需我们做任何事情。
- en: 'Finally, let''s examine how the `GetQuote` method is implemented for the `Aggregator` type:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查`Aggregator`类型的`GetQuote`方法是如何实现的：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This method is quite straightforward. All it does is allocate a new `QuotesResponse`,
    invoke the `sendRequests` helper, flatten the results into a list, and return
    it to the caller. The `sendRequests` method queries the downstream providers in
    parallel and returns a channel where the quotes are posted:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法相当直接。它所做的只是分配一个新的`QuotesResponse`，调用`sendRequests`辅助函数，将结果展平到一个列表中，并将其返回给调用者。`sendRequests`方法并行查询下游提供者，并返回一个发布报价的通道：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice how the request context argument from `GetQuote` is passed along to the `client.GetQuote` calls.
    This is all we need to do to associate the span from this service with the spans
    of the downstream services. Easy, right?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意从`GetQuote`传递到`client.GetQuote`调用中的请求上下文参数。这就是我们将此服务的跨度与下游服务的跨度关联所需做的全部工作。简单，对吧？
- en: The gateway
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网关
- en: 'The gateway service is nothing more than a wrapper on top of a gRPC client.
    The interesting bit of its implementation is the `CollectQuotes` method, which
    is what our main package will invoke to *begin a new trace*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 网关服务不过是在gRPC客户端之上的包装。其实现中有趣的部分是`CollectQuotes`方法，这是我们的主包将调用来*开始一个新的跟踪*：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we use `StartSpanFromContext` to create a new *named* span and embed its
    trace details into a new context that wraps the one that was provided as an argument
    to the method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用`StartSpanFromContext`来创建一个新的*命名*跨度，并将其跟踪详细信息嵌入到一个新的上下文中，该上下文包装了作为方法参数提供的上下文。
- en: 'The rest of the code is pretty self-explanatory: we invoke the `GetQuote` method
    on the embedded client instance, collect the responses, and place them in a map
    that we then return to the caller.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分相当直观：我们在嵌入的客户端实例上调用`GetQuote`方法，收集响应，并将它们放入一个映射中，然后将其返回给调用者。
- en: Putting it all together
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: The main file prepares a microservice deployment environment via a call to the `deployServices` helper
    function. The idea here is to string together the services in such a manner so
    that tracing a request through the system will yield an interesting trace graph.
    Let's see how this is done.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 主要文件通过调用`deployServices`辅助函数来准备一个微服务部署环境。这里的想法是将服务以这种方式连接起来，以便通过系统跟踪请求将产生一个有趣的跟踪图。让我们看看这是如何实现的。
- en: 'First, the helper starts three `Provider` instances and keeps track of their
    addresses:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，辅助函数启动三个`Provider`实例，并跟踪它们的地址：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, it starts an `Aggregator` instance and sets it up to connect to providers *1* and *2* from
    the preceding list:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它启动一个`Aggregator`实例，并将其设置为连接到前面列表中的提供者*1*和*2*：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following that, it instantiates yet another `Aggregator` type and connects
    it to provider *0* and the aggregator we just created:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，它实例化另一个`Aggregator`类型，并将其连接到提供者*0*和刚刚创建的聚合器：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, a `Gateway` instance is created with the preceding aggregator as its
    target and returned to the caller:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建一个以先前聚合器为目标并返回给调用者的`Gateway`实例：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `Gateway` instance that''s returned by the `deployServices` function is
    used by `runMain` to trigger the execution of a quote query that marks the beginning
    of a new request trace:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由`deployServices`函数返回的`Gateway`实例被`runMain`用于触发一个标记新请求跟踪开始的报价查询的执行：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the following section, we will be hooking up a tracer implementation to our
    code so that we can capture and visualize the request traces that our code generates.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把一个跟踪器实现连接到我们的代码中，以便我们可以捕获和可视化我们的代码生成的请求跟踪。
- en: Capturing and visualizing traces using Jaeger
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Jaeger捕获和可视化跟踪
- en: In the previous sections, we saw how OpenTracing allows us to create and propagate
    span information across microservice boundaries. But, *how *and, more importantly, *where* is
    this information collected and processed? After all, having this information available
    without the means to slice and dice it greatly diminishes its value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了 OpenTracing 如何允许我们在微服务边界之间创建和传播跨度信息。但是，*如何* 以及更重要的是，*在哪里* 收集和处理这些信息？毕竟，如果没有切片和切块的手段，仅仅有这些信息将大大降低其价值。
- en: As we mentioned previously, one of the key design goals of the OpenTracing framework
    is to avoid vendor lock-in. To this end, when it comes to span collection and
    visualization, you can either select an open source solution such as Uber's Jaeger ^([11]) or
    Elastic's APM ^([5]), which you host yourself. Alternatively, you can use one
    of the several available **Software as a Service** (**SaaS**) solutions ^([19]).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，OpenTracing 框架的一个关键设计目标是避免供应商锁定。为此，在跨度收集和可视化的方面，您可以选择一个开源解决方案，如 Uber
    的 Jaeger ^([11]) 或 Elastic 的 APM ^([5])，您自己托管。或者，您可以使用几种可用的 **软件即服务** (**SaaS**)
    解决方案 ^([19])。
- en: 'As far as our open tracing example is concerned, we will be using Jaeger ^([11]) as
    our tracer implementation. Jaeger is simple to install and integrate with the
    code we have already written so far. It is written in Go and can also be used
    as a drop-in replacement for Zipkin ^([16]). A Jaeger deployment typically consists
    of two components:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们的开放跟踪示例而言，我们将使用 Jaeger ^([11]) 作为我们的跟踪器实现。Jaeger 安装简单，易于与我们迄今为止编写的代码集成。它是用
    Go 编写的，也可以用作 Zipkin ^([16]) 的直接替换。Jaeger 部署通常由两个组件组成：
- en: A local span collection agent is normally deployed as a sidecar container alongside
    your application. It collects spans that are published by the application over
    UDP, applies *configurable* probabilistic sampling so that it can select a subset
    of the spans to be sent upstream, and transmits them to the Jaeger collector service.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地跨度收集代理通常作为侧车容器与您的应用程序一起部署。它收集应用程序通过 UDP 发布的跨度，应用 `*可配置*` 的概率采样，以便它可以选择要发送到上游的跨度子集，并将它们传输到
    Jaeger 收集器服务。
- en: The collector service aggregates the spans that are transmitted by the various
    Jaeger agent instances and persists them to a data store. Depending on the rate
    at which new spans are produced, collectors can be configured to work in either *direct-to-storage* mode,
    where they interface directly with the DB, or in *streaming* mode, where a Kafka
    instance is used as a buffer between the collectors and another process that ingests,
    indexes, and stores the data in the DB.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集器服务聚合由各种 Jaeger 代理实例传输的跨度，并将它们持久化到数据存储中。根据新产生的跨度速率，收集器可以配置为在 `*直接到存储*` 模式下工作，其中它们直接与数据库接口，或者在
    `*流式*` 模式下工作，其中 Kafka 实例用作收集器和另一个进程之间的缓冲，该进程消费、索引并将数据存储在数据库中。
- en: 'For our testing purposes, we will use the official all-in-one Docker image,
    which includes an agent and collector (backed by an in-memory store) instance,
    as well as the Jaeger UI. We can start the container using the following command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的测试目的，我们将使用官方的集成 Docker 镜像，该镜像包括一个代理和收集器实例（由内存存储支持），以及 Jaeger UI。我们可以使用以下命令启动容器：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Port `6831` is where the Jaeger agent listens for spans that our instrumented
    services will publish over UDP. On the other hand, port `16686` exposes the Jaeger
    UI where we can browse, search, and visualize captured request traces.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 端口 `6831` 是 Jaeger 代理监听我们的仪器化服务通过 UDP 发布的跨度的地方。另一方面，端口 `16686` 提供了 Jaeger UI，我们可以在这里浏览、搜索和可视化捕获的请求跟踪。
- en: As we mentioned in the previous sections, the `tracer` package will encapsulate
    the logic for instantiating new Jaeger tracer instances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中提到的，`tracer` 包将封装实例化新的 Jaeger 跟踪器实例的逻辑。
- en: 'Let''s take a look at the `GetTracer` function''s implementation. The `MustGetTracer` function
    that our services invoke calls `GetTracer` and panics in case of an error, as
    shown here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 `GetTracer` 函数的实现。我们的服务调用的 `MustGetTracer` 函数调用 `GetTracer` 并在出错时崩溃，如下所示：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The Go client for Jaeger provides several convenience helpers for creating new
    tracers. The approach we chose here was to instantiate a tracer from a configuration
    object that we can obtain via the `FromEnv` helper. `FromEnv` initializes a configuration
    object with a set of sane defaults and then examines the environment for the presence
    of Jaeger-specific variables that override the default values. For instance, `JAEGER_AGENT_HOST`
    and `JAEGER_AGENT_PORT` can be used to specify the address where the Jaeger agent
    is listening for incoming spans. By default, the agent is expected to listen at `localhost:6831`, which
    matches the port that was exposed by the Docker container we just launched.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger 的 Go 客户端提供了创建新跟踪器的一些便利助手。我们在这里选择的方法是从配置对象实例化一个跟踪器，我们可以通过 `FromEnv` 助手获得这个配置对象。`FromEnv`
    使用一组合理的默认值初始化配置对象，然后检查环境以查找覆盖默认值的 Jaeger 特定变量。例如，可以使用 `JAEGER_AGENT_HOST` 和 `JAEGER_AGENT_PORT`
    来指定 Jaeger 代理正在监听传入跟踪的地址。默认情况下，代理预计将在 `localhost:6831` 上监听，这与我们刚刚启动的 Docker 容器公开的端口相匹配。
- en: 'Next, we need to configure a sampling strategy for the tracer. It stands to
    reason that if we were operating a deployment with very high throughput, we wouldn''t
    necessarily want to trace every single request as that would generate an enormous
    amount of data that would need to be stored and indexed. To this end, Jaeger allows
    us to configure different sampling strategies, depending on our particular application
    requirements:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为跟踪器配置采样策略。如果我们正在运行一个具有非常高吞吐量的部署，那么我们不一定希望跟踪每个请求，因为这会产生大量需要存储和索引的数据。为此，Jaeger
    允许我们根据特定的应用程序需求配置不同的采样策略：
- en: A **constant** sampler always makes the same decision for each trace. This is
    the strategy we are using for our example to ensure that traces are always persisted
    each time we run our demo.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**恒定**采样器对每个跟踪总是做出相同的决定。这是我们用于示例的策略，以确保每次运行我们的演示时跟踪总是被持久化。'
- en: A **probabilistic** sampler retains traces with a specific probability (for
    example, 10% of traces).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率**采样器以特定的概率（例如，10%的跟踪）保留跟踪。'
- en: The **rate-limiting** sampler ensures that traces are sampled at a particular
    rate (for example, 10 traces per second).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率限制**采样器确保以特定的速率（例如，每秒10个跟踪）采样跟踪。'
- en: 'The following screenshot shows a detailed view of a captured trace that was
    generated by running the example application that we just built:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了由运行我们刚刚构建的示例应用程序生成的捕获跟踪的详细视图：
- en: '![](img/5b467669-f51f-4cd5-814b-a858b23ca7ab.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b467669-f51f-4cd5-814b-a858b23ca7ab.png)'
- en: 'Figure 1: Visualizing a request trace in Jaeger''s UI'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在 Jaeger 的 UI 中可视化请求跟踪
- en: 'The first row represents the *total* time spent in the `api-gateway` waiting
    for a response to the outgoing quote request. The rows beneath it contain the
    nested spans that correspond to other requests that were executing in parallel.
    Here is a brief overview of the events that occurred:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行表示在 `api-gateway` 中等待对出站报价请求的响应所花费的**总时间**。其下方的行包含对应于并行执行的其他请求的嵌套时间段。以下是发生事件的简要概述：
- en: The gateway makes a request to aggr-0 and blocks waiting for a response.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网关向 aggr-0 发出请求并阻塞等待响应。
- en: 'aggr-0 makes two requests in parallel: one to vendor-0 and one to aggr-1\.
    Then, it blocks waiting for the downstream responses before returning a response
    to the gateway.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: aggr-0 并行发出两个请求：一个发送给 vendor-0，另一个发送给 aggr-1。然后，它在返回给网关响应之前阻塞等待下游响应。
- en: 'aggr-1 makes two requests in parallel: one to vendor-1 and one to vendor-2\.
    It blocks waiting for the downstream responses before returning a response to aggr-0.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: aggr-1 并行发出两个请求：一个发送给 vendor-1，另一个发送给 vendor-2。它在返回给 aggr-0 响应之前阻塞等待下游响应。
- en: 'One other very cool feature of the Jaeger UI is that it can display the dependencies
    between services as a **directed acyclic graph** (**DAG**). The following screenshot
    shows the DAG for our example microservice deployment, which matches the preceding
    event sequence:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger UI 的另一个非常酷的功能是它可以以**有向无环图**（DAG）的形式显示服务之间的依赖关系。以下截图显示了我们的示例微服务部署的 DAG，它与前面的事件序列相匹配：
- en: '![](img/7d15b395-ac88-4aa6-b842-875f68077f81.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d15b395-ac88-4aa6-b842-875f68077f81.png)'
- en: 'Figure 2: Visualizing the dependencies between services'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：可视化服务之间的依赖关系
- en: In conclusion, request tracing is a great tool for gaining a deeper insight
    into the internals of modern, complex microservice-based systems. I would strongly
    recommend considering it for your next large-scale project.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，请求跟踪是深入了解现代复杂基于微服务的系统内部结构的强大工具。我强烈建议您在您的下一个大型项目中考虑使用它。
- en: Making logging your trusted ally
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将日志作为您的可靠盟友
- en: Time and time again, logging always proves to be an invaluable resource for
    investigating the root cause of a problem in contemporary computer software. However,
    in the context of a microservice-based architecture, where requests cross service
    boundaries, being able to collect, correlate, and search the log entries that
    are emitted by each individual service is of paramount importance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一次又一次地，日志始终证明是调查当代计算机软件问题根本原因的无价资源。然而，在基于微服务的架构中，由于请求跨越服务边界，能够收集、关联和搜索每个单独服务发出的日志条目至关重要。
- en: In the following sections, we will focus on the best practices for writing succinct
    log entries that are easily searchable and highlight some common pitfalls that
    you definitely want to avoid. Furthermore, we will discuss solutions for collecting
    and shipping the logs from your Kubernetes pods (or dockerized services) to a
    central location where they can be indexed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将关注编写简洁、易于搜索的日志条目的最佳实践，并强调一些您绝对想要避免的常见陷阱。此外，我们将讨论从您的Kubernetes pods（或docker化服务）收集和传输日志到中央位置以进行索引的解决方案。
- en: Logging best practices
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志最佳实践
- en: 'The first item on our best practice checklist is **leveled logging**. When
    using leveled logging, there are two aspects you need to consider:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最佳实践清单上的第一项是 **分级日志**。当使用分级日志时，您需要考虑两个方面：  '
- en: '**Selecting the appropriate log level to use for each message**: The majority
    of logging packages for Go applications support at least the *DEBUG*, *INFO, *and *ERROR* levels.
    However, your preferred logging solution might also support more granular log
    levels, such as *TRACE*, *DEBUG*, and *WARNING*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择为每条消息使用适当的日志级别**：Go应用程序的大多数日志包至少支持 *DEBUG*、*INFO* 和 *ERROR* 级别。然而，您首选的日志解决方案也可能支持更细粒度的日志级别，例如
    *TRACE*、*DEBUG* 和 *WARNING*。'
- en: '**Deciding which log levels to actually output**: For instance, perhaps you
    want your application to only output messages at the *INFO* and *ERROR* levels
    to reduce the volume of produced logs.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决定实际输出哪些日志级别**：例如，您可能希望应用程序仅在 *INFO* 和 *ERROR* 级别输出消息，以减少产生的日志量。'
- en: 'When debugging an application, it makes sense to also include *DEBUG* or *TRACE* messages
    in the logs so that you can get a better understanding of what''s going on. It
    stands to reason that you shouldn''t have to recompile and redeploy an application
    just to change its log level! To this end, it''s good practice to implement some
    sort of hook to allow you to dynamically change the active log level of an application
    while it is executing. Here are a few suggestions you can try:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试应用程序时，在日志中包含 *DEBUG* 或 *TRACE* 消息也是有意义的，这样您可以更好地了解正在发生的事情。从逻辑上讲，您不应该需要重新编译和重新部署应用程序，只是为了更改其日志级别！为此，实现某种类型的钩子以允许您在应用程序执行时动态更改其活动日志级别是一种良好的做法。以下是一些您可以尝试的建议：
- en: Toggle between the configured log level and the *DEBUG* level when the application
    receives a particular signal (for example, SIGHUP). If your application reads
    the initial log level from a config file, you could use the same signal-based
    approach to force a reload of the config file.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用程序接收到特定信号（例如，SIGHUP）时，在配置的日志级别和 *DEBUG* 级别之间切换。如果您的应用程序从配置文件中读取初始日志级别，您可以使用基于信号的相同方法强制重新加载配置文件。
- en: Expose an HTTP endpoint to change the log level.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露一个HTTP端点来更改日志级别。
- en: Store the per-application log level setting in a distributed key-value store
    such as etcd ^([6]), which allows clients to watch a key for changes. If you are
    running multiple instances of your application, this is an effective way to change
    the log level for all the instances in a single step.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个应用程序的日志级别设置存储在分布式键值存储中，如etcd ^([6])，这允许客户端监视键的变化。如果您正在运行应用程序的多个实例，这是一种有效的方法，可以在单步中更改所有实例的日志级别。
- en: If you haven't already done so, you can step up your logging game simply by
    switching to **structured** logging. While the good old way of extracting timestamps
    and messages from logs using regular expressions certainly does work, updating
    your applications to output logs in a format that is easily parsed by the service
    that indexes them goes a long way toward increasing the volume of logs that can
    be ingested per unit of time. Consequently, application logs become searchable
    in real time or near real-time fashions, allowing you to diagnose problems much
    quicker. There are quite a few Go packages out there that implement structured
    loggers. If we had to single out some, our list would definitely include `sirupsen/logrus` ^([14]), `uber-go/zap` ^([20]), `rs/zerolog` ^([21]), and `gokit/log` ^([9]).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，你可以通过切换到**结构化**日志来提升你的日志记录水平。虽然使用正则表达式从日志中提取时间戳和消息的老方法确实有效，但将你的应用程序更新为输出服务可以轻松解析的日志格式，对于增加每单位时间内可以处理的日志量大有裨益。因此，应用程序日志可以实时或接近实时地搜索，让你能够更快地诊断问题。现在有很多Go包实现了结构化日志记录器。如果我们必须挑选一些，我们的列表中肯定会包括`sirupsen/logrus`^([14])、`uber-go/zap`^([20])、`rs/zerolog`^([21])和`gokit/log`^([9])。
- en: 'So, what does a structured log entry look like? The following screenshot demonstrates
    how the `sirupsen/logrus` package formats and prints the same set of logs using
    two of its built-in text formatters. The Terminal at the top uses a text-based
    formatter that is more suited for running applications on your development machine,
    while the Terminal at the bottom displays the same output as JSON. As you can
    see, each log entry consists, as a minimum, of a level, a timestamp, and a message.
    In addition, log entries can contain a variable number of key-value pairs:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，结构化日志条目是什么样的呢？以下截图展示了`sirupsen/logrus`包如何使用其内置的两种文本格式器格式和打印同一组日志。顶部的终端使用的是更适合在开发机器上运行应用程序的基于文本的格式化器，而底部的终端显示的是相同的JSON输出。正如你所见，每个日志条目至少包含一个级别、一个时间戳和一个消息。此外，日志条目可以包含可变数量的键值对：
- en: '![](img/3c81a9c9-55fc-4aaf-a356-489e3f2f0cb9.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c81a9c9-55fc-4aaf-a356-489e3f2f0cb9.png)'
- en: Figure 3: Example log output produced by logrus
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：由logrus生成的示例日志输出
- en: When the log ingestion platform consumes such a log entry, it will also index
    the key-value pairs and make them available for searching. Consequently, you can
    compose highly targeted queries by slicing and dicing the log entries by multiple
    attributes (for example, a customer ID, service name, and data center location).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当日志摄入平台消耗这样的日志条目时，它也会索引键值对并将它们提供给搜索。因此，你可以通过多个属性（例如，客户ID、服务名称和数据中心位置）对日志条目进行切片和切块来编写高度针对性的查询。
- en: 'Always make sure that the *message* portion of your log entries never includes *variable* parts.
    To explain why not adhering to this advice could lead to problems, let''s take
    a look at two equivalent error messages that are produced by a service whose task
    is to redirect users:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要确保你的日志条目的*消息*部分从不包括*变量*部分。为了解释为什么不遵循这条建议可能会导致问题，让我们看看一个服务产生的两个等效错误消息，该服务的任务是重定向用户：
- en: '`level=error message="cannot connect to server: dial tcp4: lookup invalid.host:
    no such host" service=redirector customer-id=42`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level=error message="无法连接到服务器: dial tcp4: lookup invalid.host: no such host"
    service=redirector customer-id=42`'
- en: '`level=error message="cannot connect to server" err="dial tcp4: lookup invalid.host:
    no such host" host=invalid.host service=redirector customer-id=42`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level=error message="无法连接到服务器" err="dial tcp4: lookup invalid.host: no such
    host" host=invalid.host service=redirector customer-id=42`'
- en: The first message embeds the error that's returned by the Go dialer into the
    log message. Given that the *no such host* error will most probably change for
    each request, adding it to the log message introduces a variable component that
    makes searching harder. What if we want to find the logs for *all* failed connection
    attempts? The only way to do that would be to use a regular expression, which
    would be quite slow since the log search engine would need to perform a full table
    scan and apply the regular expression to each entry.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条消息将Go dialer返回的错误嵌入到日志消息中。鉴于*没有这样的主机*错误很可能会随着每个请求而变化，将其添加到日志消息中引入了一个可变组件，这使得搜索变得更加困难。如果我们想找到所有失败的连接尝试的日志怎么办？唯一的方法是使用正则表达式，这将相当慢，因为日志搜索引擎需要对整个表进行全表扫描并将正则表达式应用于每个条目。
- en: 'On the other hand, the second message uses a constant message for errors of
    a particular *class* and includes the error details and hostname as key-value
    pairs. Searching for this type of error is much easier: the log search engine
    can probably answer this type of query quickly and efficiently using an index.
    What''s more, we can keep slicing the data further, for example, count failed
    attempts by the host. Answering this type of query for the first message would
    be nearly impossible!'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，第二条消息使用特定**类别**的错误常量消息，并包含错误详情和主机名作为键值对。搜索此类错误要容易得多：日志搜索引擎可能能够快速有效地使用索引来回答此类查询。更重要的是，我们可以进一步切片数据，例如，按主机计数失败的尝试。对于第一条消息，回答此类查询几乎是不可能的！
- en: 'We have already argued about the usefulness of structured logging. At this
    point, you might be wondering whether there''s a list of fields that should always
    be included in your log messages. I would definitely recommend including at least
    the following bits of information:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过结构化日志记录的有用性。到目前为止，您可能想知道是否有一个应该始终包含在日志消息中的字段列表。我肯定会推荐至少包括以下信息：
- en: 'The application/service **name**. Having this value present allows you to answer
    one of the most common queries out there: "display the logs for application *foo*".'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序/服务的**名称**。有这个值可以回答最常见的问题之一：“显示应用程序*foo*的日志”。
- en: The **hostname** where the application is executing. When your log storage is
    centralized, having this field available is quite handy if you need to figure
    out which machine (or container, if you're using Kubernetes) produced the log.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序正在执行时的**主机名**。当您的日志存储是集中式时，如果需要确定哪个机器（或如果您使用Kubernetes，则容器）产生了日志，这个字段非常有用。
- en: The **SHA** of the git (or your preferred VCS) branch that's used to compile
    the binary for the application. If your organization is a fan of the *ship frequently* mantra,
    adding the SHA to your logs makes it easy to link an error to a particular snapshot
    of the code base.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于编译应用程序二进制的git（或您首选的VCS）分支的**SHA**。如果您的组织是“频繁发布”格言的粉丝，将SHA添加到日志中可以轻松地将错误链接到代码库的特定快照。
- en: Imagine that, against your better judgment, you decided to ignore the *never
    deploy on a Friday* rule and push a set of seemingly innocent changes to some
    of the microservices in production; after all, the code was thoroughly reviewed
    and all the tests passed. What could possibly go wrong, right? You come back to
    work on Monday and your mailbox is full of tickets that have been opened by the
    support team. According to the tickets, several users experienced issues adding
    products to their carts. To assist you with tracking down the problem, the support
    team has included both the affected user IDs and the approximate time when they
    were accessing the service in the tickets.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，违背您的判断，您决定忽略“**周五永不部署**”的规则，并将一系列看似无害的更改推送到生产中的某些微服务；毕竟，代码经过了彻底的审查，所有测试都通过了。可能出什么问题呢，对吧？您周一回到工作岗位，您的邮箱里充满了由支持团队打开的工单。根据工单，几位用户在将产品添加到购物车时遇到了问题。为了帮助您追踪问题，支持团队在工单中包含了受影响用户ID以及他们访问服务的近似时间。
- en: You fire up your log search tool, plug in the timestamp and user details, and
    receive a list of logs for the API gateway, which is the first microservice that
    users hit when they request something from their web browser. The gateway service
    makes several calls to downstream services that, unfortunately, *do not* have
    access to the user ID and therefore don't show up in the logs... Good luck tracking
    down the cause of the problem!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您启动日志搜索工具，输入时间戳和用户详情，然后收到API网关的日志列表，这是用户从他们的网络浏览器请求某物时遇到的第一个微服务。网关服务调用下游服务，不幸的是，这些服务**没有**访问用户ID，因此不会出现在日志中...祝您好运，找到问题的原因！
- en: To avoid hairy situations like this, it's good practice to also include a **correlation
    ID** in your log entries. In this particular scenario, the API gateway would generate
    a unique correlation ID for incoming requests and inject it into requests to downstream
    services (which then include it in their own log entries), and so on and so forth.
    This approach is quite similar to request tracing, but instead of tracking spans
    and time-related request details, it allows us to correlate logs across service
    boundaries.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种棘手的情况，在日志条目中也包含一个**关联ID**是一个好的做法。在这个特定的场景中，API网关会为传入的请求生成一个唯一的关联ID，并将其注入到对下游服务的请求中（然后它们将其包含在自己的日志条目中），等等。这种方法与请求跟踪非常相似，但不同的是，它不是跟踪跨度和时间相关的请求细节，而是允许我们在服务边界之间关联日志。
- en: The devil is in the (logging) details
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 魔鬼在于（日志的）细节
- en: 'When using structured logging, it is very easy to get carried away and try
    to stuff as much information as possible into the key-value pairs. Unfortunately,
    this can often prove to be dangerous security-wise! Take a look at the following
    code snippet, which retrieves a user''s data from a URL they have provided to
    us:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用结构化日志时，很容易陷入一个误区，试图将尽可能多的信息塞入键值对中。不幸的是，这往往在安全方面证明是危险的！看看以下代码片段，它从我们提供的URL中检索用户的数据：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Whenever we succeed in fetching the data, we log an *INFO* message with the
    URL and the time it took to retrieve it. This code looks pretty innocent, right?
    Wrong! The following screenshot shows the log output from this function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们成功获取数据时，我们都会记录一个包含URL和检索所需时间的*INFO*消息。这段代码看起来很无辜，对吧？错了！以下截图显示了该函数的日志输出：
- en: '![](img/5eed49c9-147a-4af6-85fb-01f742041256.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5eed49c9-147a-4af6-85fb-01f742041256.png)'
- en: Figure 4: Forgetting to properly sanitize log output can lead to credential
    leaks
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：忘记正确清理日志输出可能导致凭证泄露
- en: Yikes! We just splatted the user's credentials all over our logs... We have
    to be very careful not to leak any credentials or any other bits of sensitive
    information (for example, credit card, bank account, or SSN numbers) to the logs.
    But how can we achieve this without having to audit every single log line in our
    code base? Most logging frameworks allow you to provide an `io.Writer` instance
    for receiving the logger output. You can leverage this capability to implement
    a filtering mechanism that uses a set of regular expressions to either mask or
    strip away sensitive information that adheres to specific patterns.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！我们不小心把用户的凭证散布到了日志中... 我们必须非常小心，不要将任何凭证或其他敏感信息（例如，信用卡、银行账户或SSN号码）泄露到日志中。但是，我们如何在不需要审计代码库中的每一行日志的情况下实现这一点？大多数日志框架允许你提供一个`io.Writer`实例来接收日志输出。你可以利用这个功能来实现一个过滤机制，该机制使用一组正则表达式来屏蔽或删除符合特定模式的敏感信息。
- en: Another pitfall that you must be aware of is **synchronous** logging. The golden
    rule here is that logging should always be considered as an auxiliary function
    and should never interfere with the normal operation of your service. If you are
    using a synchronous logger and the output stream blocks (that is, it cannot keep
    up with the volume of generated logs), your service would also block and cause
    noticeable delays for upstream services depending on it. Whenever possible, try
    to use an **asynchronous** logger implementation – ideally, one that uses a leaky
    bucket abstraction to drop messages when it cannot keep up with the load.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须注意的另一个陷阱是**同步**日志。这里的黄金法则是将日志始终视为一个辅助功能，并且永远不应该干扰你服务的正常操作。如果你使用的是同步日志记录器，并且输出流阻塞（即，它无法跟上生成的日志量），你的服务也会阻塞，并导致上游服务出现明显的延迟。尽可能使用**异步**日志记录器实现
    - 理想情况下，使用漏桶抽象来在无法跟上负载时丢弃消息。
- en: Shipping and indexing logs inside Kubernetes
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中传输和索引日志
- en: If you have been following the guidelines from the previous sections, your applications
    will now emit clean and succinct logs in a format that makes them suitable for
    ingestion by a log aggregation system. The only missing piece of the puzzle is
    how we can collect the individual logs from each application instance running
    on a Kubernetes cluster and ship them either to a self-hosted or SaaS log indexing
    solution.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在遵循前几节的指南，你的应用程序现在将输出干净简洁的日志，这种格式适合被日志聚合系统摄取。唯一缺失的拼图是如何收集在Kubernetes集群上运行的每个应用程序实例的单独日志并将它们发送到自托管或SaaS日志索引解决方案。
- en: Running a log collector on each Kubernetes node
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在每个Kubernetes节点上运行日志收集器
- en: This option uses a Kubernetes *DaemonSet* to install a log collection daemon
    on each node of the Kubernetes cluster. Besides being relatively easy to implement,
    the main benefit of this particular approach is that it is totally transparent
    to running applications.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项使用Kubernetes *DaemonSet*在每个Kubernetes集群的每个节点上安装日志收集守护进程。除了相对容易实现之外，这种特定方法的优点是它对运行中的应用程序是完全透明的。
- en: 'A DaemonSet is a special type of Kubernetes resource that ensures that all
    cluster nodes run a copy of a particular pod. Using daemon sets is a quite common
    pattern for the following reasons:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet是一种特殊的Kubernetes资源，确保所有集群节点都运行特定Pod的一个副本。使用daemon sets是一种非常常见的模式，原因如下：
- en: Running cluster storage daemons (for example, ceph or glusterd)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行集群存储守护进程（例如，ceph或glusterd）
- en: Collecting and shipping logs
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集和发送日志
- en: Monitoring nodes and transmitting node-specific metrics (for example, load,
    memory, or disk space usage)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控节点并传输节点特定的指标（例如，负载、内存或磁盘空间使用情况）
- en: When a pod executes, Kubernetes will capture its standard output and error streams
    and redirect them to a pair of log files. When you run the `kubectl logs` command,
    the `kubelet` instance running on the worker node streams the logs by reading
    off those files.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod执行时，Kubernetes将捕获其标准输出和错误流并将它们重定向到一对日志文件。当你运行`kubectl logs`命令时，运行在工作节点上的`kubelet`实例通过读取这些文件来流式传输日志。
- en: 'The log collector pod that''s shown in the following diagram digests the captured
    log files for each executing pod, transforms them (if necessary) into the format
    expected by the log indexing service, and optionally augments them with additional
    information such as the Kubernetes namespace and container hostname. Depending
    on the log ingesting solution in use, logs can be either directly uploaded for
    ingestion or written to a message queue such as Kafka:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图中显示的日志收集器Pod将每个正在执行的Pod捕获的日志文件进行消化，如果需要，将它们转换为日志索引服务期望的格式，并可选择地添加额外的信息，例如Kubernetes命名空间和容器主机名。根据使用的日志摄取解决方案，日志可以直接上传进行摄取或写入消息队列，例如Kafka：
- en: '![](img/92219a31-af22-4046-82da-5417d9a16c2a.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92219a31-af22-4046-82da-5417d9a16c2a.png)'
- en: Figure 5: Running a log collector using a DaemonSet
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：使用DaemonSet运行日志收集器
- en: The two most popular log collection daemons out there are Fluent Bit ^([8]), which
    is written in C, and Logstash ^([15]), which is written in Ruby. Quite often,
    logs will be shipped to an Elasticsearch cluster for indexing, and a frontend
    such as Kibana ^([12]) will be used to browse and search the logs. You will hear
    this type of setup commonly referred to as an EFK or ELK stack, depending on which
    log collection daemon is being used.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最流行的两个日志收集守护进程是Fluent Bit^([8])，它用C编写，以及Logstash^([15])，它用Ruby编写。通常，日志会被发送到Elasticsearch集群进行索引，并使用Kibana^([12])这样的前端来浏览和搜索日志。这种类型的设置通常被称为EFK或ELK堆栈，具体取决于使用的日志收集守护进程。
- en: Using a sidecar container to collect logs
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用边车容器收集日志
- en: 'The second option when it comes to collecting logs is to run a sidecar container
    for each application, as shown in the following diagram:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 收集日志的第二种选择是为每个应用程序运行一个边车容器，如下面的图所示：
- en: '![](img/d0e6746c-fe4f-41c6-8c54-0f05ec18eb97.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0e6746c-fe4f-41c6-8c54-0f05ec18eb97.png)'
- en: Figure 6: Running a log collector as a sidecar container
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：将日志收集器作为边车容器运行
- en: 'While this approach might feel a bit more cumbersome compared to using the
    infrastructure already built into Kubernetes, it can work quite nicely in cases
    where the following happens:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用Kubernetes已内置的基础设施相比，这种方法可能感觉有点繁琐，但在以下情况发生时可以工作得相当好：
- en: The application writes to multiple log files. For example, a web server such
    as Apache or Nginx might be configured to write error logs to a different location
    than access logs. In such a case, you would add a sidecar container for each log
    file that you want to scrape.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序写入多个日志文件。例如，一个像Apache或Nginx这样的Web服务器可能被配置为将错误日志写入与访问日志不同的位置。在这种情况下，您将为每个要抓取的日志文件添加一个边车容器。
- en: Applications use non-standard log formats that need to be transformed on a **per-application** basis.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序使用非标准日志格式，需要根据每个应用程序进行转换。
- en: Shipping logs directly from the application
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接从应用程序发送日志
- en: 'The last log shipping strategy that we will be examining is to actually embed
    the log shipping logic into each application. As shown in the following diagram,
    the application is sending its logs directly to a log repository:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要检查的最后一种日志传输策略是将日志传输逻辑实际嵌入到每个应用程序中。如图所示，应用程序正在直接将其日志发送到日志存储库：
- en: '![](img/9b319765-f4d7-485f-8e9c-671bc457d949.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b319765-f4d7-485f-8e9c-671bc457d949.png)'
- en: Figure 7: Shipping logs directly from within the application
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：从应用程序内部直接传输日志
- en: An interesting use case for this strategy is to integrate with an external,
    third-party SaaS offering that requires applications to import and use a vendor-specific
    software development kit.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的一个有趣用例是与需要应用程序导入和使用特定供应商软件开发套件的外部第三方 SaaS 产品集成。
- en: Introspecting live Go services
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查实时 Go 服务
- en: 'After a long journey transitioning from a monolithic application into one based
    on microservices, you have reached a point where all your new and shiny services
    are happily running in production. At this stage, you will probably start becoming
    more and more curious about their operation in the long run:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在从单体应用程序过渡到基于微服务的应用程序的漫长旅程之后，你已经达到了一个点，即所有你新而闪亮的服务都在生产中愉快地运行。在这个阶段，你可能会越来越好奇它们长期运行的状况：
- en: How much memory are they using?
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们使用了多少内存？
- en: How many goroutines are currently running?
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前有多少 goroutines 正在运行？
- en: Are there any leaks (memory or goroutines) that can eventually force the service
    to crash?
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有任何泄漏（内存或 goroutines）最终会迫使服务崩溃？
- en: How often does the Go garbage collector run and how much time does each run
    actually take?
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go 垃圾收集器多久运行一次，每次运行实际上需要多少时间？
- en: If you are using a container orchestration framework such as Kubernetes, crashing
    services are not normally a big concern; just bump the number of instances and
    Kubernetes will take care of restarting them when they crash. However, if the
    root cause of the crash is a memory leak or an unbounded explosion in the number
    of running goroutines, crashes will become more and more frequent as the traffic
    to your system increases. This pattern will inevitably lead to service disruption.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用容器编排框架，如 Kubernetes，崩溃的服务通常不是一个大问题；只需增加实例数量，Kubernetes 就会在它们崩溃时负责重启它们。然而，如果崩溃的根本原因是内存泄漏或运行中
    goroutine 数量的无界增长，随着系统流量的增加，崩溃将变得越来越频繁。这种模式不可避免地会导致服务中断。
- en: Fortunately, the Go runtime exposes a heap of information that we can use to
    answer these questions. All we need to do is extract, export, and aggregate this
    information. In [Chapter 13](56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml), *Metrics
    Collection and Visualization*, where we will be discussing the SRE aspects of
    operating microservice architectures, we will explore metrics collection systems
    such as Prometheus, which let us not only collect this information but also act
    on it by creating alerts that can eventually turn into page calls for the SRE
    team.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Go 运行时公开了大量信息，我们可以使用这些信息来回答这些问题。我们只需要提取、导出和汇总这些信息。在[第 13 章](56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml)“指标收集和可视化”中，我们将讨论操作微服务架构的
    SRE 方面，我们将探讨如 Prometheus 这样的指标收集系统，它不仅允许我们收集这些信息，还可以通过创建警报来采取行动，这些警报最终可能变成 SRE
    团队的页面调用。
- en: 'In some cases, however, we might want to dig a bit deeper... Here are a few
    interesting examples:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，我们可能想要深入挖掘...以下是一些有趣的例子：
- en: A service suddenly became unresponsive but the Go deadlock detector is not complaining
    about any deadlocks. The service still accepts requests but never sends out any
    reply and we need to find out why.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个服务突然变得无响应，但 Go 死锁检测器并没有抱怨任何死锁。该服务仍然接受请求，但从未发送任何回复，我们需要找出原因。
- en: Our metrics dashboard indicates that instance *X* of service *Y* is using quite
    a lot of heap memory. But how can we inspect the heap's contents and see what
    is taking up all that memory? Perhaps we are leaking file handles (for example, forgot
    to close the response body off from the HTTP calls we are making) or maintain
    unneeded references to objects that prevent the Go garbage collector from freeing
    them.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的指标仪表板显示，服务 *Y* 的实例 *X* 正在使用相当多的堆内存。但我们如何检查堆的内容，看看是什么占用了所有这些内存？也许我们泄漏了文件句柄（例如，忘记关闭我们正在进行的
    HTTP 调用的响应体）或者维护了对对象的未必要引用，这阻止了 Go 垃圾收集器释放它们。
- en: A data ingestion service unexpectedly pegs the CPU, but only when running in
    production! So far, you have been unable to replicate this issue on your local
    development machine.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据摄取服务意外地 peg 了 CPU，但仅在运行在生产环境中时才会发生！到目前为止，你还没有能在本地开发机器上重现这个问题。
- en: To debug issues such as the ones described in the first example, we can SSH
    into the container that the service executes in and send it a *SIGQUIT* signal.
    This will force the Go runtime to dump a stack trace for each running goroutine
    and exit. Then, we can examine the log stream and figure out where exactly the
    service got stuck.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要调试如第一个示例中描述的问题，我们可以 SSH 进入服务执行所在的容器，并发送一个 *SIGQUIT* 信号。这将强制 Go 运行时为每个正在运行的
    goroutine 输出堆栈跟踪并退出。然后，我们可以检查日志流，找出服务确切卡在何处。
- en: If your Go application, which seemingly appears to be stuck, is running on the
    foreground, instead of looking for its pid so that you can send it a *SIGQUIT* signal,
    you can force it to dump the stack trace for every executing goroutine by pressing
    *CTRL+\* .
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 Go 应用程序似乎卡住了，正在前台运行，而不是寻找它的 pid 以发送一个 *SIGQUIT* 信号，你可以通过按 *CTRL+\* 来强制它为每个正在执行的
    goroutine 输出堆栈跟踪。
- en: Note that, behind the scenes, this key combination actually sends a *SIGQUIT* to
    the running process and will cause it to exit.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在幕后，这个键组合实际上向运行中的进程发送了一个 *SIGQUIT*，并会导致它退出。
- en: However, the obvious caveat of this trick is that our application or service
    will effectively crash. What's more, it doesn't really allow us to introspect
    its internal state, which is more or less required for dealing with situations
    such as the ones from the other examples.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个技巧的明显缺点是，我们的应用程序或服务实际上会崩溃。更重要的是，它并不真正允许我们检查其内部状态，这对于处理诸如其他示例中提到的情况是或多或少必需的。
- en: 'Fortunately, Go allows us to embed the `pprof` package into our services and
    expose a frontend to it over HTTP. You might be worried that shipping what is
    effectively a sampling profiler with your code will undoubtedly make it run slower.
    In principle, this is not really the case as you can ask `pprof` to capture various
    kinds of profiles on demand, thus allowing it to stay out of the way until you
    need it. As the following code snippet shows, all you need to do is import the `net/http/pprof` package
    and launch an HTTP server:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Go 允许我们将 `pprof` 包嵌入到我们的服务中，并通过 HTTP 暴露其前端。你可能担心，将一个采样分析器与你的代码一起分发无疑会使它运行得更慢。原则上，这并不是真的，因为你可以要求
    `pprof` 根据需要捕获各种类型的配置文件，从而使其在需要时才介入。如下代码片段所示，你所需要做的只是导入 `net/http/pprof` 包并启动一个
    HTTP 服务器：
- en: '[PRE18]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `net/http/pprof` function defines an `init` function, which registers various
    pprof-related route handlers to the default `http.ServeMux`. Therefore, the package
    is typically imported only for its side effects. After spinning up an HTTP server,
    you can simply point your web browser to `http://localhost:6060/debug/pprof` and
    access a minimalistic frontend for capturing `pprof` profiles and make them available
    for download so that they can be processed offline via the `pprof` tool.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`net/http/pprof` 函数定义了一个 `init` 函数，该函数将各种与 pprof 相关的路由处理程序注册到默认的 `http.ServeMux`。因此，通常只为了副作用而导入该包。启动
    HTTP 服务器后，你只需将你的网络浏览器指向 `http://localhost:6060/debug/pprof`，就可以访问一个用于捕获 `pprof`
    配置文件并使其可供下载的简约前端，以便可以通过 `pprof` 工具离线处理。'
- en: 'The following screenshot is exposing a `pprof` frontend to introspect running
    applications:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了暴露 `pprof` 前端以检查运行中的应用程序：
- en: '![](img/a4ce5dc0-34b4-474a-9792-702bdf5c084f.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4ce5dc0-34b4-474a-9792-702bdf5c084f.png)'
- en: Figure 8: Exposing a pprof frontend to introspect running applications
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：暴露 pprof 前端以检查运行中的应用程序
- en: 'As shown in the preceding screenshot, the `pprof` UI allows you to capture
    the following profile types on-demand:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述截图所示，`pprof` 用户界面允许你按需捕获以下配置文件类型：
- en: '**allocs**: A sampling of all past memory allocations'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**allocs**：所有过去内存分配的样本'
- en: '**block**: Stack traces that led to blocking on synchronization primitives'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**block**：导致在同步原语上阻塞的堆栈跟踪'
- en: '**cmdline**: The command-line invocation of the current program'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cmdline**：当前程序的命令行调用'
- en: '**goroutine**: Stack traces of all current goroutines'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**goroutine**：所有当前 goroutine 的堆栈跟踪'
- en: '**heap**: A sample of the memory allocations of live objects'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**heap**：活动对象的内存分配样本'
- en: '**mutex**: Stack traces of holders of contended mutexes'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mutex**：竞争互斥锁持有者的堆栈跟踪'
- en: '**profile**: CPU profile'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**profile**：CPU 配置文件'
- en: '**threadcreate**: Stack traces that led to the creation of new OS threads'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**threadcreate**：导致创建新操作系统线程的堆栈跟踪'
- en: '**trace**: A trace of the execution of the current program'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**trace**：当前程序执行的跟踪'
- en: If your application/service already spins up its own HTTP server that can potentially
    be exposed to the outside world (for example, via a Kubernetes ingress), make
    sure you bind its routes to a **new mux** instance and expose the `pprof` routes
    using the **default mux** via a second HTTP server running on a different (internal)
    port.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用程序/服务已经启动了自己的HTTP服务器，该服务器可能被暴露给外界（例如，通过Kubernetes ingress），请确保将其路由绑定到新的**mux**实例，并通过运行在不同（内部）端口上的第二个HTTP服务器使用**默认mux**来公开`pprof`路由。
- en: This way, you don't run the risk of allowing unauthorized access to your introspection
    endpoints.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做，您就不会有允许未经授权访问您的内省端点的风险。
- en: I would strongly recommend enabling `pprof` support for your production services
    using the approaches we discussed in this section. It only requires a little bit
    of effort on your end to set up but will prove to be a great asset if you ever
    need to debug a live application.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您使用本节中讨论的方法为您的生产服务启用`pprof`支持。这只需要您稍微努力一点来设置，但如果您需要调试实时应用程序，这将证明是一个巨大的资产。
- en: Building a microservice-based version of Links 'R' Us
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建“Links 'R' Us”的基于微服务的版本
- en: 'In the last part of this chapter, we will take the monolithic Links ''R'' Us
    application that we built and deployed in the previous chapter and apply *everything*
    we have learned so far to break it down into a bunch of microservices. The following
    diagram illustrates the expected state of our cluster after we''ve made all the
    necessary changes:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将使用我们在上一章中构建和部署的单体应用“Links 'R' Us”，并将我们迄今为止所学的一切应用到将其分解为一组微服务。以下图表展示了我们在做出所有必要的更改后集群的预期状态：
- en: '![](img/5ecc62e1-e8b4-4f04-87ce-f626803d6ab0.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ecc62e1-e8b4-4f04-87ce-f626803d6ab0.png)'
- en: Figure 9: Breaking down the Links 'R' Us monolith into microservices
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：将“Links 'R' Us”单体分解为微服务
- en: The Kubernetes manifest files that we will be using for the microservice-based
    version of Links 'R' Us are available under the `Chapter11/k8s` folder of this
    book's GitHub repository.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为“Links 'R' Us”的基于微服务的版本使用Kubernetes manifest文件，这些文件位于本书GitHub仓库的`Chapter11/k8s`文件夹下。
- en: If you haven't already set up a Minikube cluster and whitelisted its private
    registry, you can either take a quick break and manually follow the step-by-step
    instructions from [Chapter 10](bd9d530b-f50e-4b81-a6c1-95b31e79b8c6.xhtml), *Building,
    Packaging, and Deploying Software*, or simply run `make bootstrap-minikube`, which
    will take care of everything for you. On the other hand, if you have already deployed
    the monolithic version of Links 'R' Us from the previous chapter, make sure to
    run `kubectl delete namespace linksrus` before proceeding. By deleting the **linksrus** namespace,
    Kubernetes will get rid of all pods, services, and ingresses for Links 'R' Us
    but leave the data stores (which live in the **linksrus-data** namespace) intact.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有设置Minikube集群并将其私有仓库列入白名单，您可以选择短暂休息并手动按照[第10章](bd9d530b-f50e-4b81-a6c1-95b31e79b8c6.xhtml)“构建、打包和部署软件”中的逐步说明进行操作，或者简单地运行`make
    bootstrap-minikube`，这将为您处理一切。另一方面，如果您已经从上一章部署了“Links 'R' Us”的单体版本，在继续之前请确保运行`kubectl
    delete namespace linksrus`。通过删除`**linksrus**`命名空间，Kubernetes将移除所有针对“Links 'R'
    Us”的pods、services和ingresses，但保留数据存储（位于`**linksrus-data**`命名空间中）。
- en: 'To deploy the various Links ''R'' Us components that we will be defining in
    the following sections, you will need to build and push a handful of Docker images.
    To save you some time, the Makefile in the `Chapter11/k8s` folder provides two
    handy build targets to get you up and running as quickly as possible:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了部署我们在以下章节中定义的“Links 'R' Us”的各种组件，您需要构建和推送一些Docker镜像。为了节省您的时间，`Chapter11/k8s`文件夹中的Makefile提供了两个方便的构建目标，以尽可能快地让您开始运行：
- en: '`make dockerize-and-push` will build all required Docker images and push them
    to Minikube''s private registry'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make dockerize-and-push`将构建所有必需的Docker镜像并将它们推送到Minikube的私有仓库'
- en: '`make deploy` will ensure that all the necessary data stores have been provisioned
    and apply all the manifests for deploying the microservice-based version of Links
    ''R'' Us in one go'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make deploy`将确保所有必要的数据存储都已配置，并一次性应用部署基于微服务的“Links ''R'' Us”版本的manifests'
- en: 'Before we can start breaking down our monolith into microservices, there is
    one small task we need to take care of first: removing the coupling between our
    service and the underlying data stores. The next section explores how this can
    be achieved using the knowledge we acquired in [Chapter 9](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml), *Communicating
    with the Outside World*.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始将单体应用拆分为微服务之前，我们首先需要处理一个小任务：移除我们的服务与底层数据存储之间的耦合。下一节将探讨如何利用我们在[第9章](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml)，《与外界通信》中获得的知识来实现这一点。
- en: Decoupling access to the data stores
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解耦对数据存储的访问
- en: One fundamental issue with the monolithic implementation from the previous chapter
    is that our application was talking directly to the Elasticsearch and CockroachDB
    clusters. We have effectively introduced a tight coupling between the application
    and the data store implementations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章单体实现的一个基本问题是我们的应用程序直接与Elasticsearch和CockroachDB集群通信。我们实际上在应用程序和数据存储实现之间引入了紧密耦合。
- en: Now that it's time to create the microservice-based version of Links 'R' Us,
    we need to take a few steps to rectify this problem. To this end, the first two
    services that we will be creating as part of our refactoring work will serve as
    a kind of proxy for facilitating access to the underlying data stores. The **text-indexer** and **link-graph** services
    will be deployed in the **linksrus-data** namespace and allow other services to
    interact with the data stores through the gRPC-based APIs that we defined in [Chapter
    9](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml), *Communicating with the Outside
    World*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候创建Links 'R' Us的基于微服务版本了，我们需要采取一些步骤来纠正这个问题。为此，我们将创建的前两个服务将作为重构工作中的一种代理，以促进对底层数据存储的访问。**文本索引器**和**链接图**服务将在**linksrus-data**命名空间中部署，并允许其他服务通过我们在[第9章](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml)，《与外界通信》中定义的基于gRPC的API与数据存储进行交互。
- en: An important benefit of introducing an indirection layer between the services
    and the data stores is that we gain the ability to change the data store implementation *at
    any moment* without having to change, update, or otherwise reconfigure any of
    the other Links 'R' Us services.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务和数据存储之间引入间接层的一个重要好处是，我们能够随时更改数据存储实现，而无需更改、更新或以其他方式重新配置任何其他Links 'R' Us服务。
- en: In terms of service implementation, things are surprisingly simple. Each service
    binary receives the URI of the data store to connect to as an argument. Then,
    it creates a gRPC server on port `8080` and exposes the `pprof` debug endpoints
    on port 6060\. All the boilerplate code fits nicely into a single main file, which
    you can find in the `Chapter11/linksrus/linkgraph` and `Chapter11/linksrus/textindexer` folders
    of this book's GitHub repository.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务实现方面，事情出奇地简单。每个服务二进制文件接收作为参数的数据存储连接URI。然后，它在端口`8080`上创建一个gRPC服务器，并在端口6060上暴露`pprof`调试端点。所有样板代码都完美地整合到一个主文件中，您可以在本书GitHub仓库的`Chapter11/linksrus/linkgraph`和`Chapter11/linksrus/textindexer`文件夹中找到。
- en: Breaking down the monolith into distinct services
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将单体应用拆分为独立的服务
- en: In this section, we will extract the individual services from the Links 'R'
    Us monolith and build a standalone service binary for each one. This is also the
    point where you will probably realize that our clean, interface-based design that
    we have been preaching about since the beginning of this book finally begins to
    pay off.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从Links 'R' Us单体中提取单个服务，并为每个服务构建一个独立的服务二进制文件。这也是您可能会意识到我们自本书开始以来一直在宣扬的干净、基于接口的设计最终开始产生效益的时刻。
- en: 'As it turns out, we can take the service-specific code from [Chapter 10](bd9d530b-f50e-4b81-a6c1-95b31e79b8c6.xhtml),
    *Building, Packaging, and Deploying Software*, and use it as is with a few minor
    changes. For each service, we will create a `main` package that performs the following
    set of tasks:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们可以从[第10章](bd9d530b-f50e-4b81-a6c1-95b31e79b8c6.xhtml)，《构建、打包和部署软件》中提取特定服务的代码，稍作修改后即可直接使用。对于每个服务，我们将创建一个执行以下一系列任务的`main`包：
- en: Creates a logger instance for each service
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个服务创建一个日志实例
- en: Exposes the `pprof` debug endpoints on a configurable port
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可配置端口上暴露`pprof`调试端点
- en: Instantiates the gRPC clients for accessing the link-graph and text-indexer
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化用于访问链接图和文本索引器的gRPC客户端
- en: Populates the configuration object for each service with the appropriate settings
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个服务填充适当的配置对象
- en: Runs the service main loop and cleanly shuts down the application upon receiving
    a signal
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行服务主循环，并在收到信号时干净地关闭应用程序
- en: All of this boilerplate code is more or less the same for each service, so we
    will omit it for brevity. However, if you want to, you can extract the common
    parts into a separate package and make the `main.go` files for each service a
    bit leaner.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些样板代码对于每个服务来说都大致相同，因此为了简洁起见，我们将省略它。然而，如果您愿意，可以将公共部分提取到单独的包中，并使每个服务的`main.go`文件更加精简。
- en: 'The configuration of each service is one of the places where we will be deviating
    slightly compared to the monolithic implementation from the previous chapter.
    We want our services to be configured either via command-line flags or via environment
    variables. The latter offers us the flexibility to define all the configuration
    options in a shared Kubernetes ConfigMap and inject them into our service manifests.
    Since the built-in `flags` package does not support this kind of functionality,
    we will be switching to the `urfave/cli` ^([1]) package for our flag parsing needs.
    This package supports an elegant way of defining typed flags, which also allows
    us to (optionally) specify the name of an environment variable that can be set
    to override each flag value:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务的配置与上一章中单体实现的配置略有不同。我们希望我们的服务通过命令行标志或环境变量进行配置。后者为我们提供了在共享的Kubernetes ConfigMap中定义所有配置选项的灵活性，并将它们注入到我们的服务清单中。由于内置的`flags`包不支持此类功能，我们将切换到`urfave/cli`^([1])包来满足我们的标志解析需求。此包支持优雅地定义类型化标志，这还允许我们（可选地）指定可以设置为覆盖每个标志值的变量名：
- en: '[PRE19]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Each of our new services will need to access both the link-graph and the text-indexer
    data stores. Both of these stores are exposed over gRPC via the two services we
    described in the previous section. The following code snippet shows how to obtain
    a high-level (see [Chapter 9](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml), *Communicating
    with the Outside World*) client instance for each of the two services:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有新服务都需要访问链接图和文本索引器数据存储。这两个存储都通过我们之前章节中描述的两个服务通过gRPC暴露。以下代码片段显示了如何为两个服务中的每一个获取一个高级（见第9章，*与外界通信*）客户端实例：
- en: '[PRE20]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you may recall, back in [Chapter 9](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml),
    *Communicating with the Outside* *World*, we meticulously designed the high-level
    clients so that they implement a subset of the `graph.Graph` and `index.Indexer` interfaces.
    This makes it possible to use the clients as a *drop-in replacement* for the concrete
    graph and indexer store implementations that are used in the monolithic Links
    'R' Us version. This is a testament to the benefits of applying the SOLID design
    principles to make our code more modular and easier to interface with.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所忆，在[第9章](b3edd7bf-fd1d-4203-bd96-9113cdbb2422.xhtml)，*与外界通信*中，我们精心设计了高级客户端，以便它们实现`graph.Graph`和`index.Indexer`接口的子集。这使得可以使用客户端作为*直接替换*用于单体“链接之用”版本中使用的具体图和索引存储实现。这是将SOLID设计原则应用于使我们的代码更加模块化和易于接口的益处的证明。
- en: Deploying the microservices that comprise the Links 'R' Us project
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署构成“链接之用”项目的微服务
- en: Now that we have built standalone binaries for each of the new Links 'R' Us
    services, it's time to deploy them on Kubernetes! Let's take a quick look at the
    required Kubernetes resources for deploying each individual service.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每个新的“链接之用”服务构建了独立的二进制文件，是时候将它们部署到Kubernetes上了！让我们快速了解一下部署每个单独服务所需的Kubernetes资源。
- en: Deploying the link-graph and text-indexer API services
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署链接图和文本索引器API服务
- en: For the link-graph and text-indexer API services, we will be using a Kubernetes
    deployment resource to spin up two replicas for each service in the **linksrus-data** namespace.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于链接图和文本索引器API服务，我们将在**linksrus-data**命名空间中为每个服务启动两个副本，我们将使用Kubernetes部署资源。
- en: To allow clients from the **linksrus** namespace to access the API, we will
    be creating a Kubernetes service to load balance traffic to the pods that we will
    be spinning up.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许来自**linksrus**命名空间的客户端访问API，我们将创建一个Kubernetes服务来负载均衡到我们将启动的Pods的流量。
- en: 'Clients can then access the data stores by connecting their gRPC clients to
    the following endpoints:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以通过将他们的gRPC客户端连接到以下端点来访问数据存储：
- en: '`linksrus-textindexer.linksrus-data:8080`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linksrus-textindexer.linksrus-data:8080`'
- en: '`linksrus-linkgraph.linksrus-data:8080`'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linksrus-linkgraph.linksrus-data:8080`'
- en: Deploying the web crawler
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署网络爬虫
- en: The crawler service deployment will use the same partition detection logic that
    the monolithic implementation from the previous chapter did. Consequently, we
    will be deploying two instances of the crawler service as a Kubernetes Stateful
    set, which guarantees that each pod will be assigned a predictable hostname that
    includes the pod's ordinal in the set.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫服务部署将使用与上一章中单体实现相同的分区检测逻辑。因此，我们将作为 Kubernetes 状态集部署两个爬虫服务实例，这保证了每个 pod 都会被分配一个包含集合中
    pod 序号的可预测主机名。
- en: In addition, we will be creating a **headless** Kubernetes service that will
    populate the SRV records for the crawler pods and allow the partition detection
    code to query the total number of available pods.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将创建一个**无头**的 Kubernetes 服务，该服务将为爬虫 pod 的 SRV 记录填充数据，并允许分区检测代码查询可用的 pod
    总数。
- en: Deploying the PageRank service
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 PageRank 服务
- en: The PageRank service is still subject to the same constraints and limitations
    that we discussed in the previous chapter. As a result, we will only run a *single* instance
    of the service by creating a Kubernetes deployment with the replica count set
    to *one*.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank 服务仍然受到我们在上一章中讨论的相同约束和限制。因此，我们将通过创建一个副本计数设置为**一个**的 Kubernetes 部署来运行服务的**单个**实例。
- en: Deploying the frontend service
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署前端服务
- en: The last service that we will be deploying is the frontend. As with most other
    services in our cluster, we will create a Kubernetes deployment with the required
    number of replicas for the frontend.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要部署的最后一个服务是前端。正如我们集群中的大多数其他服务一样，我们将为前端创建一个具有所需副本数的 Kubernetes 部署。
- en: Just as we did in the previous chapter, we will define a Kubernetes service
    to load balance traffic to the frontend pods and then expose it outside the cluster
    with the help of a Kubernetes ingress resource.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中所做的那样，我们将定义一个 Kubernetes 服务来负载均衡流量到前端 pod，然后借助 Kubernetes ingress 资源将其暴露在集群外部。
- en: Locking down access to our Kubernetes cluster using network policies
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网络策略锁定对我们的 Kubernetes 集群的访问
- en: As the number of microservices begins to increase, it is probably a good time
    to start thinking more actively about security. Do we really want each and every
    pod in our cluster to be able to access every other pod across all namespaces?
    Truth be told, for our current deployment, it is not that important. However,
    for larger projects, that's definitely a question that you need to answer.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微服务数量的增加，可能是一个开始更积极地考虑安全性的好时机。我们真的希望集群中的每个 pod 都能访问所有命名空间中的其他 pod 吗？说实话，对于我们的当前部署来说，这并不那么重要。然而，对于更大的项目来说，这确实是一个你需要回答的问题。
- en: Kubernetes offers a special type of resource called **NetworkPolicy** to assist
    us with the creation of fine-grained rules for governing access to namespaces
    and pods. A prerequisite to creating and enforcing network policies is for your
    cluster to run with the *cni* network plugin enabled and to use a network provider
    implementation that is compliant with the **Container Networking Interface** (**CNI**).
    Examples of such providers include Calico ^([2]), Cilium ^([3]), and Flannel ^([7]).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了一种特殊类型的资源，称为**网络策略**，以帮助我们创建细粒度的规则来管理对命名空间和 pod 的访问。创建和实施网络策略的先决条件是您的集群运行时启用了**cni**网络插件，并使用符合**容器网络接口**（**CNI**）的网络提供者实现。此类提供者的例子包括
    Calico ^([2])、Cilium ^([3]) 和 Flannel ^([7])。
- en: If you have bootstrapped a Minikube cluster using the `make bootstrap-minikube`
    target from the Makefile in either the `Chapter10/k8s` or the `Chapter11/k8s`
    folder, Calico has already been installed for you.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经使用 Makefile 中的 `make bootstrap-minikube` 目标在 `Chapter10/k8s` 或 `Chapter11/k8s`
    文件夹中初始化了一个 Minikube 集群，Calico 已经为你安装好了。
- en: 'Alternatively, you can manually install Calico to your test cluster by running
    the following command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过运行以下命令手动将 Calico 安装到您的测试集群中：
- en: '`kubectl apply -f \ https://docs.projectcalico.org/v3.10/manifests/calico.yaml`The
    installation might take a few moments. You can monitor the status of the deployment
    by running `kubectl -n kube-system get pods -lk8s-app=calico-node -w` and waiting
    for the pod status to show up as *running*.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl apply -f https://docs.projectcalico.org/v3.10/manifests/calico.yaml`
    安装可能需要一些时间。你可以通过运行 `kubectl -n kube-system get pods -lk8s-app=calico-node -w`
    来监控部署状态，等待 pod 状态显示为**运行**。'
- en: What would be a good example of a network policy for our Links 'R' Us deployment?
    Since the various pods in the **linksrus** namespace are now expected to access
    the data stores over gRPC, it would be good practice to specify a network policy
    that would block access to any other pod in the **linksrus-data** namespace from
    other namespaces.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 Links 'R' Us 部署，一个好的网络策略示例会是什么？由于现在期望 **linksrus** 命名空间中的各种 pod 通过 gRPC
    访问数据存储，因此指定一个阻止来自其他命名空间的 pod 访问 **linksrus-data** 命名空间中任何其他 pod 的网络策略将是良好的实践。
- en: 'A really cool thing about Kubernetes network policies is that we can combine
    multiple policies to construct more elaborate policies. For our use case, we will
    start with a **DENY ALL** policy:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络策略的一个真正酷的特性是我们可以将多个策略组合起来构建更复杂的策略。对于我们的用例，我们将从一个 **DENY ALL** 策略开始：
- en: '[PRE21]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Each policy has two sections:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 每个策略有两个部分：
- en: The destination is where we specify the set of pods that we want to control
    access to. In this example, we are using a `podSelector` block with an empty `matchLabels` selector
    to match *all* pods in the namespace.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是我们指定想要控制访问的 pod 集合。在这个例子中，我们使用一个 `podSelector` 块和一个空的 `matchLabels` 选择器来匹配命名空间中的所有
    pod。
- en: The traffic origin, which is where we specify the set of pods that are subject
    to the policy when attempting to access a pod in the destination list.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通源，即我们在尝试访问目标列表中的 pod 时，指定受策略影响的 pod 集合的位置。
- en: 'So, the preceding policy can be interpreted as ***deny** access to any pod
    in the **linksrus-data** namespace from any pod in another namespace*. Moving
    on, we will define a second network policy that will explicitly whitelist the
    pods that we want to grant access to:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的策略可以解释为**拒绝**来自另一个命名空间中任何 pod 对 **linksrus-data** 命名空间中任何 pod 的访问。继续前进，我们将定义第二个网络策略，该策略将明确地将我们想要授予访问权限的
    pod 列为白名单：
- en: '[PRE22]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The pods for the two gRPC services have been tagged with a `role: data-api` label,
    which the preceding policy uses to explicitly target the pods we are interested
    in. On the other hand, the pods running in the **linksrus** namespace have been
    tagged with a `role: linksrus-components` label, which allows us to specify them
    as part of the ingress selector. This rule is interpreted as ***allow** access
    from **all** pods with a **linksrus-components** role to pods with a **data-api** role
    in the **linksrus-data** namespace*.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '两个 gRPC 服务的 pod 已经被标记为 `role: data-api` 标签，该标签被前面的策略用来明确地针对我们感兴趣的 pod。另一方面，在
    **linksrus** 命名空间中运行的 pod 已经被标记为 `role: linksrus-components` 标签，这允许我们将它们指定为入口选择器的一部分。此规则被解释为**允许**所有带有
    **linksrus-components** 角色的 pod 从 **linksrus-data** 命名空间中的 pod 访问具有 **data-api**
    角色的 pod*。'
- en: 'Let''s apply these rules by running `kubectl apply -f 08-net-policy.yaml` and
    verify that they work as expected by connecting to the pods in both namespaces
    and running the *nc* command to check whether we are allowed to connect to the
    pods protected by the two network policies. The following screenshot shows us
    how to verify that our network policies work as expected:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行 `kubectl apply -f 08-net-policy.yaml` 来应用这些规则，并通过连接到两个命名空间中的 pod 并运行
    *nc* 命令来检查我们是否被允许连接到由两个网络策略保护的 pod 来验证它们是否按预期工作。以下屏幕截图显示了如何验证我们的网络策略按预期工作：
- en: '![](img/cf41a1ea-2f1a-4850-ba36-6494e660adea.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf41a1ea-2f1a-4850-ba36-6494e660adea.png)'
- en: Figure 10: Verifying that our network policies work as expected
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：验证我们的网络策略按预期工作
- en: Success! As shown in the preceding screenshot, attempting to connect from one
    of the crawler pods in the **linksrus** namespace to the CockroachDB cluster times
    out, while attempts to connect to the text indexer API succeed. On the other hand,
    the connection attempt to CockroachDB succeeds when the same command is executed
    inside a pod running in the **linksrus-data** namespace.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！如前一个屏幕截图所示，尝试从 **linksrus** 命名空间中的一个爬虫 pod 连接到 CockroachDB 集群时超时，而尝试连接到文本索引器
    API 的尝试则成功。另一方面，当在 **linksrus-data** 命名空间中运行的 pod 内执行相同命令时，连接到 CockroachDB 的尝试成功。
- en: Of course, this brief introduction to Kubernetes network policies barely scratches
    the surface. The Kubernetes documentation contains several other match rules that
    you can use to formulate the appropriate network policies for your particular
    use case. For those of you who are interested in exploring the different types
    of policies that you can implement further, I would strongly recommend taking
    a look at the *Kubernetes network policy recipes* ^([13]) GitHub repository.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对Kubernetes网络策略的简要介绍只是触及了表面。Kubernetes文档中包含其他一些匹配规则，您可以使用这些规则为您的特定用例制定适当的网络策略。对于那些有兴趣进一步探索可以实施的不同类型策略的人，我强烈建议查看*Kubernetes网络策略食谱*^([13])GitHub仓库。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on the process of splitting a monolithic application
    into a series of microservices. We identified some common anti-patterns for building
    microservices and elaborated on ways for working around them.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于将单体应用程序拆分为一系列微服务的过程。我们确定了构建微服务的常见反模式，并详细阐述了绕过它们的方法。
- en: In the second part of this chapter, we examined some interesting approaches
    for tracing requests through distributed systems, as well as collecting, aggregating,
    and searching logs. In the last part of this chapter, we split the monolithic
    Links 'R' Us project from the previous chapter into a series of microservices
    and deployed them to our test Kubernetes cluster.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们探讨了通过分布式系统跟踪请求的一些有趣方法，以及收集、聚合和搜索日志。在本章的最后部分，我们将上一章中的单体Links 'R' Us项目拆分为一系列微服务，并将它们部署到我们的测试Kubernetes集群中。
- en: In the next chapter, we will discuss building fault-tolerant systems and build
    a distributed version of the PageRank calculator using the master/slave pattern.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论构建容错系统，并使用主/从模式构建PageRank计算器的分布式版本。
- en: Questions
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Explain why using the microservices pattern for an MVP or **proof of concept**
    (**PoC**) project is often considered to be a bad idea.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么使用微服务模式进行MVP或**概念验证**（**PoC**）项目通常被认为是一个糟糕的想法。
- en: Describe how the circuit breaker pattern works.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述断路器模式是如何工作的。
- en: List some of the benefits of being able to trace requests as they travel through
    the system.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出能够跟踪请求穿越系统时的某些好处。
- en: Why is it important to sanitize log output?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么清理日志输出很重要？
- en: Briefly describe the three strategies for collecting logs from the pods running
    inside a Kubernetes cluster.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简要描述从运行在Kubernetes集群内部的Pod中收集日志的三个策略。
- en: Further reading
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: A simple, fast, and fun package for building command-line apps in Go: [https://github.com/urfave/cli](https://github.com/urfave/cli)
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个简单、快速且有趣的Go语言命令行应用程序构建包：[https://github.com/urfave/cli](https://github.com/urfave/cli)
- en: '**Calico**: Secure networking for the cloud-native era: [https://www.projectcalico.org](https://www.projectcalico.org)'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Calico**：云原生时代的安全网络：[https://www.projectcalico.org](https://www.projectcalico.org)'
- en: '**Cilium**: API-aware networking and security: [https://cilium.io](https://cilium.io)'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Cilium**：API感知网络和安全：[https://cilium.io](https://cilium.io)'
- en: '**Docker**: Enterprise container platform: [https://www.docker.com](https://www.docker.com)'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Docker**：企业级容器平台：[https://www.docker.com](https://www.docker.com)'
- en: '**Elastic APM**: Open source application performance monitoring: [https://www.elastic.co/products/apm](https://www.elastic.co/products/apm)'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Elastic APM**：开源应用程序性能监控：[https://www.elastic.co/products/apm](https://www.elastic.co/products/apm)'
- en: '**etcd**: A distributed, reliable key-value store for the most critical data
    of a distributed system: [https://etcd.io](https://etcd.io)'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**etcd**：分布式、可靠的键值存储，用于分布式系统中最关键的数据：[https://etcd.io](https://etcd.io)'
- en: '**Flannel**: A network fabric for containers, designed for Kubernetes: [https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flannel**：为Kubernetes设计的容器网络布线：[https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
- en: '**Fluent Bit**: A cloud-native log forwarder: [https://fluentbit.io](https://fluentbit.io)'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Fluent Bit**：云原生日志转发器：[https://fluentbit.io](https://fluentbit.io)'
- en: '**gokit/log**: A minimal interface for structured logging in services: [https://github.com/go-kit/kit/tree/master/log](https://github.com/go-kit/kit/tree/master/log)'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**gokit/log**：服务中结构化日志的最小接口：[https://github.com/go-kit/kit/tree/master/log](https://github.com/go-kit/kit/tree/master/log)'
- en: '**grpc-opentracing**: A package for enabling distributed tracing in gRPC clients
    via the OpenTracing project: [https://github.com/grpc-ecosystem/grpc-opentracing](https://github.com/grpc-ecosystem/grpc-opentracing)'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**grpc-opentracing**: 通过 OpenTracing 项目在 gRPC 客户端启用分布式跟踪的包：[https://github.com/grpc-ecosystem/grpc-opentracing](https://github.com/grpc-ecosystem/grpc-opentracing)'
- en: '**Jaeger**: For open source, end-to-end distributed tracing: [https://jaegertracing.io](https://jaegertracing.io)'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Jaeger**: 为开源，端到端分布式跟踪：[https://jaegertracing.io](https://jaegertracing.io)'
- en: '**kibana**: Your window into the Elastic Stack: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**kibana**: 您进入 Elastic Stack 的窗口：[https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
- en: Kubernetes Network Policy Recipes: [https://github.com/ahmetb/kubernetes-network-policy-recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 网络策略菜谱：[https://github.com/ahmetb/kubernetes-network-policy-recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)
- en: '**logrus**: Structured, pluggable logging for Go: [https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus)'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**logrus**: 为 Go 提供结构化、可插拔的日志记录：[https://github.com/sirupsen/logrus](https://github.com/sirupsen/logrus)'
- en: '**Logstash**: Centralize, transform, and stash your data: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Logstash**: 集中、转换和存储您的数据：[https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
- en: '**OpenZipkin**: A distributed tracing system: [https://zipkin.io](https://zipkin.io)'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**OpenZipkin**: 分布式跟踪系统：[https://zipkin.io](https://zipkin.io)'
- en: Sigelman, Benjamin H.; Barroso, Luiz André; Burrows, Mike; Stephenson, Pat; Plakal,
    Manoj; Beaver, Donald; Jaspan, Saul; Shanbhag, Chandan: *Dapper, a Large-Scale
    Distributed Systems Tracing Infrastructure*. Google, Inc., 2010.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sigelman, Benjamin H.; Barroso, Luiz André; Burrows, Mike; Stephenson, Pat;
    Plakal, Manoj; Beaver, Donald; Jaspan, Saul; Shanbhag, Chandan: *Dapper, a Large-Scale
    Distributed Systems Tracing Infrastructure*. Google, Inc., 2010.'
- en: '**The OpenTracing project**: [https://opentracing.io](https://opentracing.io)'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**The OpenTracing project**: [https://opentracing.io](https://opentracing.io)'
- en: '**The OpenTracing project: Supported tracers**: [https://opentracing.io/docs/supported-tracers](https://opentracing.io/docs/supported-tracers)'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**The OpenTracing project: Supported tracers**: [https://opentracing.io/docs/supported-tracers](https://opentracing.io/docs/supported-tracers)'
- en: '**zap**: Blazing fast, structured, leveled logging in Go: [https://github.com/uber-go/zap](https://github.com/uber-go/zap)'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**zap**: Go 中的闪电般快速、结构化、分层日志记录：[https://github.com/uber-go/zap](https://github.com/uber-go/zap)'
- en: '**zerolog**: Zero allocation JSON logger: [https://github.com/rs/zerolog](https://github.com/rs/zerolog)'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**zerolog**: 零分配 JSON 记录器：[https://github.com/rs/zerolog](https://github.com/rs/zerolog)'
