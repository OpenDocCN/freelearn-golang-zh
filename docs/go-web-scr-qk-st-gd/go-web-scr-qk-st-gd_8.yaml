- en: Scraping at 100x
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 100倍速抓取
- en: 'By now, you should have a very broad understanding of how to build a solid
    web scraper. Up to this point, you have learned how to collect information from
    the internet efficiently, safely, and respectfully. The tools that you have at
    your disposal are enough to build web scrapers on a small to medium scale, which
    may be just what you need to accomplish your goals. However, there may come a
    day when you need to upscale your application to handle large and production-sized
    projects. You may be lucky enough to make a living out of offering services, and,
    as that business grows, you will need an architecture that is robust and manageable.
    In this chapter, we will review the architectural components that make a good
    web scraping system, and look at example projects from the open source community.
    Here are the topics we will discuss:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该对如何构建一个稳固的网页抓取程序有了非常广泛的了解。到目前为止，您已经学会了如何高效、安全和尊重地从互联网收集信息。您手头的工具足以构建小到中等规模的网页抓取程序，这可能正是您需要实现目标的。然而，可能有一天您需要将应用程序升级以处理大型和生产规模的项目。您可能很幸运地能够通过提供服务谋生，随着业务的增长，您将需要一个稳健且可管理的架构。在本章中，我们将回顾构建良好的网页抓取系统的架构组件，并查看开源社区的示例项目。以下是我们将讨论的主题：
- en: Components of a web scraping system
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页抓取系统的组件
- en: Scraping HTML pages with colly
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用colly抓取HTML页面
- en: Scraping JavaScript pages with chrome-protocol
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用chrome-protocol抓取JavaScript页面
- en: Distributed scraping with dataflowkit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用dataflowkit进行分布式抓取
- en: Components of a web scraping system
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网页抓取系统的组件
- en: In [Chapter 7](1f846c4e-a180-4341-bced-ebf1ed109211.xhtml), *Scraping with Concurrency*,
    about concurrency, we saw how defining a clear separation of roles between the
    worker goroutines and the main goroutine helped mitigate issues in the program.
    By clearly giving the main goroutine the responsibility of maintaining the state
    of the target URLs, and allowing the scraper threads to focus on scraping, we
    laid the groundwork for making a modular system which can easily scale components
    independently. This separation of concerns is the foundation for building large-scale
    systems of any kind.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](1f846c4e-a180-4341-bced-ebf1ed109211.xhtml)中，*并发抓取*，关于并发性，我们看到如何在程序中定义工作goroutine和主goroutine之间的明确角色分离有助于减轻问题。通过明确地将主goroutine赋予维护目标URL状态的责任，并允许爬虫线程专注于抓取，我们为构建一个可以轻松扩展各个组件的模块化系统奠定了基础。这种关注点的分离是构建任何大型系统的基础。
- en: There are a few main components that make up a web scraper. Each of these components
    should be able to scale without affecting other parts of the system, if they are
    properly decoupled. You will know if this decoupling is solid if you can break
    this system into its own package and reuse it for other projects. You might even
    want to release it to the open source community! Let's take a look at some of
    these components.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取系统由一些主要组件组成。如果这些组件得到适当解耦，每个组件都应该能够独立扩展而不影响系统的其他部分。如果您可以将此系统分解为自己的包并在其他项目中重用它，那么您就会知道这种解耦是牢固的。您甚至可能希望将其发布到开源社区！让我们来看看其中一些组件。
- en: Queue
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 队列
- en: Before a web scraper can start collecting information, it needs to know where
    to go. It also needs to know where it has been. A proper queuing system will accomplish
    both of these goals. Queues can be set up in many different ways. In many of the
    previous examples, we used a `[]string` or a `map[string]string` to hold the target
    URLs the scraper should pursue. This works for smaller scale web scrapers where
    the work is being pushed to the workers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在网页抓取程序开始收集信息之前，它需要知道去哪里。它还需要知道它已经去过哪里。一个合适的排队系统将实现这两个目标。队列可以以许多不同的方式设置。在许多先前的示例中，我们使用`[]string`或`map[string]string`来保存爬虫应该追踪的目标URL。这适用于规模较小的网页抓取程序，其中工作被推送到工作线程。
- en: In larger applications, a work-stealing queue would be preferred. In a work-stealing
    queue, the worker threads would take the first available job out of the queue
    as fast as they can accomplish the task. This way, if you need your system to
    increase throughput, you can simply add more worker threads. In this system, the
    queue does not need to concern itself with the status of the workers, and focuses
    only on the status of the jobs. This is beneficial to systems that push to the
    workers because it must be aware of how many workers there are, which workers
    are busy or free, and handles workers coming on and offline.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在较大的应用程序中，工作窃取队列将是首选。在工作窃取队列中，工作线程将尽快从队列中获取第一个可用的任务。这样，如果您需要系统增加吞吐量，您只需简单地添加更多的工作线程。在这个系统中，队列不需要关心工作线程的状态，只关注作业的状态。这对于推送给工作线程的系统是有益的，因为它必须知道有多少工作线程，哪些工作线程正在忙碌或空闲，并处理工作线程的上线和下线。
- en: Queuing systems are not always a part of the main scraping application. There
    are many suitable solutions for external queues, such as databases, or streaming
    platforms, such as Redis and Kafka. These tools will support your queuing system
    to the limits of your own imagination.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 排队系统并不总是主要抓取应用程序的一部分。有许多适合外部队列的解决方案，例如数据库，或者流平台，例如Redis和Kafka。这些工具将支持您的排队系统达到您自己想象的极限。
- en: Cache
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存
- en: As we have seen in [Chapter 3](16487efd-3a75-4823-ad19-627a83752cd4.xhtml), *Web
    Scraping Etiquette*, caching web pages is an essential part of an efficient web
    scraper. With a cache, we are able to avoid requesting content from a website
    if we know nothing has changed. In our previous examples, we used a local cache
    which saves the content into a folder on the local machine. In larger web scrapers
    with multiple machines, this causes problems, as each machine would need to maintain
    its own cache. Having a shared caching solution would solve this problem and increase
    the efficiency of your web scraper.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](16487efd-3a75-4823-ad19-627a83752cd4.xhtml)中所看到的，*网络爬虫礼仪*，缓存网页是高效网络爬虫的重要组成部分。有了缓存，我们可以避免从网站请求内容，如果我们知道没有发生变化。在我们之前的例子中，我们使用了一个本地缓存，它将内容保存到本地机器上的一个文件夹中。在具有多台机器的大型网络爬虫中，这会导致问题，因为每台机器都需要维护自己的缓存。拥有一个共享的缓存解决方案将解决这个问题，并提高您的网络爬虫的效率。
- en: There are many different ways to approach this problem. Much like the queuing
    system, a database can help store a cache of your information. Most databases
    support storage of binary objects, so whether you are storing HTML pages, images,
    or any other content, it is possible to put it into a database. You can also include
    a lot of metadata about a file, such as a date it was recovered, the date it expires,
    the size, the Etag, and so on. Another caching solution you can use is a form
    of cloud object storage, such as Amazon S3, Google Cloud Store, and Microsoft
    object storage. These services typically offer low-cost storage solutions that
    mimic a file system and require a specific SDK, or use of their APIs. A third
    solution you could use is a **Network File System** (**NFS**) where each node
    would connect. Writing to cache on an NFS would be the same as if it were on the
    local file system, as far as your scraper code is concerned. There can be challenges
    in configuring your worker machines to connect to an NFS. Each of these approaches
    has its own unique set of pros and cons, depending on your own setup.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题有许多不同的方法。就像排队系统一样，数据库可以帮助存储您的信息的缓存。大多数数据库支持存储二进制对象，因此无论您是存储HTML页面、图像还是其他任何内容，都可以将其放入数据库中。您还可以包含有关文件的大量元数据，例如恢复日期、过期日期、大小、Etag等。您可以使用的另一个缓存解决方案是一种云对象存储，例如Amazon
    S3、Google Cloud Store和Microsoft对象存储。这些服务通常提供低成本的存储解决方案，模拟文件系统并需要特定的SDK或使用它们的API。您可以使用的第三种解决方案是**网络文件系统**（**NFS**），其中每个节点都会连接。在NFS上写入缓存与在本地文件系统上写入缓存对于您的爬虫代码来说是一样的。配置工作机器连接到NFS可能会有挑战。每种方法都有自己独特的一套优缺点，具体取决于您自己的设置。
- en: Storage
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储
- en: In most cases, when you are scraping the web, you will be looking for very specific
    information. This is probably going to be a very small amount of data relative
    to the size of the web page itself. Because of the cache stores the entire contents
    of the web page, you will need some other storage system to store the parsed information.
    The storage component of a web scraper could be as simple as a text file, or as
    large as a distributed database.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，当您爬取网络时，您将寻找非常具体的信息。这可能相对于网页本身的大小来说是非常少量的数据。由于缓存存储了整个网页的内容，您将需要一些其他存储系统来存储解析后的信息。网络爬虫的存储组件可以是一个简单的文本文件，也可以是一个分布式数据库。
- en: These days, there are many database solutions available to satisfy different
    needs. If you have data that has many intricate relationships, then an SQL database
    might be a good fit for you. If you have data that has more of a nested structure,
    then you may want to look at NoSQL databases. There are also solutions that offer
    full-text indexing to make searching for documents easier, and time-series databases
    if you need to relate your data to some chronological order. Because there is
    no one-size-fits-all solution, the Go standard library only offers a package to
    handle the most common family of databases through the `sql` package.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，有许多数据库解决方案可满足不同的需求。如果您的数据有许多复杂的关系，那么SQL数据库可能适合您。如果您的数据具有更多的嵌套结构，那么您可能需要查看NoSQL数据库。还有一些解决方案提供全文索引，以使搜索文档更容易，并且如果您需要将数据与某种时间顺序相关联，则需要时间序列数据库。由于没有一种大小适合所有的解决方案，Go标准库只通过`sql`包提供了处理最常见数据库系列的包。
- en: The `sql` package was built to provide a common set of functions used to communicate
    with SQL databases such as MySQL, PostgreSQL, and Couchbase. For each of these
    databases, a separate driver has been written to fit into the framework defined
    by the `sql` package. These drivers, along with various others, can be found on
    GitHub and easily integrated with your project. The core of the `sql` package
    provides methods for opening and closing database connections, querying the database,
    iterating through rows of results, and performing inserts and modifications to
    the data. By mandating a standard interface for drivers, Go allows you to swap
    out your database for another SQL database with less effort.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`sql`包旨在提供一组用于与MySQL、PostgreSQL和Couchbase等SQL数据库通信的常用函数。为每个这些数据库，都编写了一个单独的驱动程序，以适应`sql`包定义的框架。这些驱动程序以及其他各种驱动程序可以在GitHub上找到，并轻松集成到您的项目中。`sql`包的核心提供了用于打开和关闭数据库连接、查询数据库、迭代结果行以及对数据进行插入和修改的方法。通过强制驱动程序的标准接口，Go允许您更轻松地将数据库替换为另一个SQL数据库。'
- en: Logs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志
- en: One system that is often overlooked during the design of a scraping system is
    the logging system. It is important, first and foremost, to have clear log statements
    without logging too many unnecessary items. These statements should be informing
    the operator of the current status of scraping and any errors, or successes, the
    scraper encounters. This helps you get a picture of the overall health of your
    web scraper.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计爬取系统时经常被忽视的一个系统是日志系统。首先，重要的是要有清晰的日志声明，而不记录太多不必要的项目。这些声明应该通知操作员有关爬取的当前状态以及爬虫遇到的任何错误或成功。这有助于您了解您的网络爬虫的整体健康状况。
- en: The simplest logging that can be done is printing messages to the terminal with
    `println()` or `fmt.Println()` type statements. This works well enough for a single
    node, but, as your scraper grows into a distributed architecture, it causes problems.
    In order to check how things are running in your system an operator would need
    to log into each machine to look at the logs. If there is an actual problem in
    the system, it may be difficult to diagnose by trying to piece together logs from
    multiple sources. A logging system built for distributed computing would be ideal
    at this point.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的日志记录方法是使用`println()`或`fmt.Println()`类型的语句将消息打印到终端。这对于单个节点来说已经足够好了，但是当你的爬虫发展成分布式架构时，就会出现问题。为了检查系统的运行情况，操作员需要登录每台机器查看日志。如果系统出现实际问题，试图从多个来源拼凑日志可能会很难诊断。这时建立一个适用于分布式计算的日志记录系统将是理想的。
- en: There are many logging solutions available in the open source world. One of
    the more popular choices is Graylog. Setting up a Graylog server is a simple process,
    requiring a MongoDB database and an Elasticsearch database to support it. Graylog
    defines a JSON format called GELF for sending log data to its servers, and accepts
    a very flexible set of keys. Graylog servers can accept log streams from multiple
    sources and you can define post-processing actions as well, such as reformatting
    data and sending alerts based on user-defined rules. There are many other similar
    systems, as well as paid services, that offer very similar features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源世界中有许多日志记录解决方案。其中比较流行的选择之一是Graylog。设置Graylog服务器是一个简单的过程，需要一个MongoDB数据库和一个Elasticsearch数据库来支持它。Graylog定义了一个名为GELF的JSON格式，用于将日志数据发送到其服务器，并接受非常灵活的键集。Graylog服务器可以接受来自多个来源的日志流，您还可以定义后处理操作，比如根据用户定义的规则重新格式化数据和发送警报。还有许多其他类似的系统，以及付费服务，提供非常相似的功能。
- en: As there are various logging solutions, the open source community has built
    a library that eases the burden of integrating with different systems. The `logrus`
    package by GitHub user `sirupsen` provides a standard utility for writing log
    statements, as well as a plugin architecture for log formatters. Many people have
    built formatters for logging statements, including one for GELF statements to
    be sent to a Graylog server. If you decide to change your logging server during
    the development of your scraper, you need only to change the formatter instead
    of replacing all of your log statements.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有各种各样的日志记录解决方案，开源社区构建了一个库，简化了与不同系统集成的负担。GitHub用户`sirupsen`的`logrus`包提供了一个标准的实用程序来编写日志语句，以及一个用于日志格式化的插件架构。许多人已经为日志语句构建了格式化程序，包括一个用于将GELF语句发送到Graylog服务器的格式化程序。如果您在开发爬虫时决定更改日志服务器，您只需要更改格式化程序，而不是替换所有的日志语句。
- en: Scraping HTML pages with colly
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用colly抓取HTML页面
- en: '`colly` is one of the available projects on GitHub that covers most of the
    systems discussed earlier. This project is built to run on a single machine, due
    to its reliance on a local cache and queuing system.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`colly`是GitHub上可用的项目之一，涵盖了前面讨论过的大部分系统。由于它依赖于本地缓存和排队系统，这个项目是为在单台机器上运行而构建的。'
- en: '![](img/3d2669cc-87d2-4eb3-979a-7a70d075a46c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d2669cc-87d2-4eb3-979a-7a70d075a46c.png)'
- en: The main worker object in `colly`, the `Collector`, is built to run in its own
    goroutine, allowing you to run multiple `Collectors` simultaneously. This design
    offers you the ability to scrape from multiple sites at the same time with different
    parameters, such as crawl delays, white and blacklists, and proxies.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`colly`中的主要工作对象`Collector`是为了在自己的goroutine中运行而构建的，这使您可以同时运行多个`Collector`。这种设计为您提供了同时从多个站点抓取的能力，具有不同的参数，比如爬行延迟、白名单和黑名单以及代理。'
- en: '`colly` is built to only process HTML and XML files. It does not offer support
    for JavaScript execution. However, you would be surprised at how much information
    you can collect with pure HTML. The following example is adapted from the GitHub
    `README`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`colly`只能处理HTML和XML文件。它不支持JavaScript执行。然而，你会惊讶地发现，纯HTML可以收集到多少信息。下面的例子是从GitHub的`README`中改编的：'
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Before running this example, download `colly` via:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此示例之前，请通过以下方式下载`colly`：
- en: '`go get github.com/gocolly/colly/...`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`go get github.com/gocolly/colly/...`'
- en: In this example, a `Collector` is created and defines a whitelist for `go-colly.org`,
    and a callback using the `OnHTML()` function. In this function, it performs a
    CSS query for `<a>` tags containing the `href` attribute. The callback specifies
    that the collector should navigate to the endpoint contained in that link. For
    each new page it visits, it repeats the process of visiting each link. Another
    callback is added to the collector using the `OnRequest()` function. This callback
    prints the name of the URL of each site it visits. As you can see, the `Collector`
    performs a depth-first crawl of the website because it follows each link as deep
    as it can go, before checking the other links on the same page.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，创建了一个`Collector`，并为`go-colly.org`定义了一个白名单，并使用`OnHTML()`函数定义了一个回调。在这个函数中，它对包含`href`属性的`<a>`标签执行了CSS查询。回调指定收集器应该导航到该链接中包含的端点。对于它访问的每个新页面，它重复访问每个链接的过程。使用`OnRequest()`函数向收集器添加了另一个回调。这个回调打印了它访问的每个站点的URL名称。正如你所看到的，`Collector`对网站进行了深度优先爬取，因为它会在检查同一页上的其他链接之前尽可能深入地跟踪每个链接。
- en: '`colly` provides many other features, such as respecting `robots.txt`, an extendable
    storage system for the queue, and various callbacks for different events in the
    system. This project is a great starting point for any web scraper that only requires
    HTML pages. It does not need much to set up and have a flexible system for parsing
    HTML pages.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`colly`提供了许多其他功能，比如遵守`robots.txt`、可扩展的队列存储系统，以及系统中不同事件的各种回调。这个项目是任何只需要HTML页面的网络爬虫的绝佳起点。它不需要太多设置，就可以拥有一个灵活的HTML页面解析系统。'
- en: Scraping JavaScript pages with chrome-protocol
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用chrome-protocol抓取JavaScript页面
- en: In [Chapter 5](a4a15c5d-908d-4a1c-bce4-0bf2181c80e3.xhtml), *Web Scraping Navigation*,
    we looked at navigating websites that require JavaScript using `selenium` and
    the WebDriver protocol. There is another protocol that has been developed recently
    that offers many more features you can take advantage of to drive a web browser.
    The Chrome DevTools Protocol was started for use on Chrome browsers, but it has
    been adopted by the *W3C's Web Platform Incubator Community Group* as a project.
    The major web browsers work together to develop a standard protocol called the
    DevTools Protocol to adopt for all of their browsers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](a4a15c5d-908d-4a1c-bce4-0bf2181c80e3.xhtml)中，*网页抓取导航*，我们看了一下使用`selenium`和WebDriver协议导航需要JavaScript的网站。最近开发了另一个协议，它提供了许多更多功能，您可以利用它来驱动Web浏览器。Chrome
    DevTools Protocol最初是用于Chrome浏览器的，但它已被*W3C的Web平台孵化器社区组*采纳为一个项目。主要的Web浏览器共同开发了一个称为DevTools
    Protocol的标准协议，以适用于所有他们的浏览器。
- en: The DevTools Protocol allows external programs to connect to a web browser and
    send commands to run JavaScript, and collect information from the browser. Most
    importantly, the protocol allows the program to collect the HTML on demand. This
    way, if you were scraping a web page in which search results were loaded via JavaScript,
    you could wait for the results to display, request the HTML page, and continue
    parsing the information needed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DevTools Protocol允许外部程序连接到Web浏览器并发送命令来运行JavaScript，并从浏览器中收集信息。最重要的是，该协议允许程序按需收集HTML。这样，如果您正在抓取一个通过JavaScript加载搜索结果的网页，您可以等待结果显示，请求HTML页面，并继续解析所需的信息。
- en: 'The `chrome-protocol` project on GitHub, developed by the GitHub user `4ydx`,
    provides access to use the DevTools Protocol to drive compatible web browsers.
    Because these browsers expose a port, much like a web server does, you can run
    browsers on multiple machines. Using the `chrome-protocol` package, you would
    connect to the browser through a port and start building a series of tasks such
    as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub上的`chrome-protocol`项目由GitHub用户`4ydx`开发，提供了使用DevTools Protocol驱动兼容的Web浏览器的访问权限。因为这些浏览器像Web服务器一样公开了一个端口，所以您可以在多台机器上运行浏览器。使用`chrome-protocol`包，您可以通过端口连接到浏览器，并开始构建一系列任务，例如：
- en: '`Navigate`: Opens a web page'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Navigate`：打开一个网页'
- en: '`FindAll`: Searches for elements by CSS query'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FindAll`：通过CSS查询搜索元素'
- en: '`Click`: Sends click events to a specific element'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Click`：向特定元素发送点击事件'
- en: There are many more actions that you can send to a browser, and, by building
    your own custom script, you can navigate through JavaScript websites and collect
    the data that you need.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以向浏览器发送许多其他操作，通过构建自己的自定义脚本，您可以浏览JavaScript网站并收集所需的数据。
- en: Example – Amazon Daily Deals
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例-亚马逊每日优惠
- en: 'In the following example, we will use `chrome-protocol` and `goquery` to retrieve
    the Daily Deals from [amazon.com](http://amazon.com). This example is a bit complex
    so the program has been broken into smaller chunks, which we will go through piece
    by piece. Let''s begin with the package and `import` statements, as shown in the
    following code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将使用`chrome-protocol`和`goquery`来检索[amazon.com](http://amazon.com)的每日优惠。这个示例有点复杂，所以程序已经被分成了更小的块，我们将逐个讨论。让我们从包和`import`语句开始，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This block of code imports the necessary packages to run the rest of the program.
    Some new packages that we have not seen before are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码块导入了运行程序的其余部分所需的包。我们之前没有见过的一些新包是：
- en: '`encoding/json`: Go standard library for handling JSON data'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding/json`：处理JSON数据的Go标准库'
- en: '`github.com/4ydx/chrome-protocol`: Open source library for using the DevTools
    Protocol'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/4ydx/chrome-protocol`：使用DevTools Protocol的开源库'
- en: '`github.com/4ydx/chrome-protocol/actions`: Open source library defining the
    DevTools Protocol actions'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/4ydx/chrome-protocol/actions`：定义DevTools Protocol操作的开源库'
- en: '`github.com/4ydx/cdp/protocol/dom`: Open source library for handling DOM nodes
    with `chrome-protocol`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/4ydx/cdp/protocol/dom`：用于处理DOM节点的开源库，与`chrome-protocol`一起使用'
- en: 'The rest of the imported libraries should be familiar to you as we have used
    them in previous chapters. Next, we will define two functions: one function for
    retrieving the HTML page from Amazon, and the second to parse the results with
    `goquery`. The following code shows the function for retrieving the HTML data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的其余库应该对您来说很熟悉，因为我们在之前的章节中使用过它们。接下来，我们将定义两个函数：一个用于从亚马逊检索HTML页面，另一个用于使用`goquery`解析结果。以下代码显示了检索HTML数据的函数：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The function begins by opening a new instance of a Google Chrome browser and
    obtaining a handle for it for future commands. We use the `actions.EnableAll()`
    function to ensure that all of the events happening in the Chrome browser are
    sent back to our program so we do not miss anything. Next, we navigate to [https://www.amazon.com/gp/goldbox](https://www.amazon.com/gp/goldbox),
    which is Amazon's Daily Deals web page.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先打开一个新的Google Chrome浏览器实例，并获取其句柄以供将来使用。我们使用`actions.EnableAll()`函数来确保Chrome浏览器中发生的所有事件都会被发送回我们的程序，以便我们不会错过任何内容。接下来，我们导航到[https://www.amazon.com/gp/goldbox](https://www.amazon.com/gp/goldbox)，这是亚马逊的每日优惠网页。
- en: If you were to retrieve this page with a simple `GET` command, you would get
    a fairly empty shell of HTML code with a lot of JavaScript files waiting to be
    run. Making the request in the browser automatically runs the JavaScript that
    populates the remaining content.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用简单的`GET`命令检索此页面，您将得到一个相当空的HTML代码外壳，其中有许多等待运行的JavaScript文件。在浏览器中进行请求会自动运行JavaScript，从而填充剩余的内容。
- en: The function then enters a `for` loop which checks for our HTML element that
    contains the daily deals data to populate in the page. The `for` loop will check
    every second for 5 seconds (as defined by the retries variable) before it either
    finds results or gives up. If there are no results, we exit the program. Next,
    the function sends a request to the browser to retrieve the `<body>` element via
    a JavaScript command. The processing of the results is a bit tricky as the value
    of the reply needs to be processed as a JSON string in order to return the raw
    HTML content. Once the content is parsed out, the function returns it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该函数进入一个`for`循环，检查包含每日交易数据的HTML元素以填充页面。`for`循环将每秒检查5秒（由重试变量定义），然后找到结果或放弃。如果没有结果，我们将退出程序。接下来，该函数向浏览器发送请求，通过JavaScript命令检索`<body>`元素。结果的处理有点棘手，因为需要将回复的值处理为JSON字符串，以便返回原始HTML内容。一旦内容被解析出来，函数就会返回它。
- en: 'The second function, responsible for parsing the HTML content is as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 负责解析HTML内容的第二个功能如下：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Much like the example, we saw in [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing
    HTML*, we use `goquery` to first look for the HTML element that contains the results.
    Within that container, we iterate through the details for each daily deal item,
    pulling out the title and the price for each item. We then append each product's
    title and price string to an array and return that array.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[第4章](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml)中看到的示例*解析HTML*一样，我们使用`goquery`首先查找包含结果的HTML元素。在该容器内，我们遍历每个每日交易项目的详细信息，提取每个项目的标题和价格。然后，我们将每个产品的标题和价格字符串附加到数组中并返回该数组。
- en: 'The `main` function ties these two functions together, first retrieving the
    body of the HTML page, then passing that on to parse the results. The `main` function
    then prints the title and price of each of the daily deals. The `main` function
    is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数将这两个功能联系在一起，首先检索HTML页面的主体，然后将其传递给解析结果。`main`函数然后打印每个每日交易的标题和价格。`main`函数如下：'
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, driving a web browser can be more difficult than scraping with
    just simple HTTP requests, but it can be done.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，驱动网络浏览器可能比仅使用简单的HTTP请求进行抓取更困难，但这是可以做到的。
- en: Distributed scraping with dataflowkit
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dataflowkit进行分布式抓取
- en: Now that you have seen the progression of building fully featured web scrapers,
    I would like to introduce you to the most complete web scraping project in Go
    that has been built today. `dataflowkit`, by GitHub user `slotix`, is a fully
    featured web scraper that is modular and extensible for building scalable, large-scale
    distributed applications. It allows for multiple backends for storage of cached
    and computed information and is capable of both simple HTTP requests as well as
    driving browsers through the DevTools Protocol. Going above and beyond, `dataflowkit`
    has both a command-line interface and a JSON format to declare web scraping scripts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了构建完整功能的网络爬虫的进展，我想向您介绍今天已经构建的最完整的Go网络爬虫项目。`dataflowkit`，由GitHub用户`slotix`创建，是一个功能齐全的网络爬虫，可用于构建可扩展的大规模分布式应用程序。它允许多个后端存储缓存和计算信息，并且能够进行简单的HTTP请求以及通过DevTools协议驱动浏览器。除此之外，`dataflowkit`还具有命令行界面和JSON格式来声明网络爬取脚本。
- en: 'The architecture of `dataflowkit` is separated into two distinct parts: fetching
    and parsing. Both Fetch and Parse phases of the system are built as separate binaries
    to be run on different machines. They communicate over HTTP via an API, as would
    you if you need to send or receive any information. By running these as separate
    entities, fetching operations and parsing operations can scale independently as
    the system grows. Depending on what type of sites you scrape, you may need more
    fetchers than scrapers, as JavaScript sites tend to require more resources. Once
    the page has been received, parsing the page often provides little overhead.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataflowkit`的架构分为两个不同的部分：获取和解析。系统的获取和解析阶段都构建为单独的二进制文件，可以在不同的机器上运行。它们通过API通过HTTP进行通信，就像您需要发送或接收任何信息一样。通过将它们作为单独的实体运行，获取操作和解析操作可以在系统增长时独立扩展。根据您抓取的网站类型，您可能需要更多的获取器而不是解析器，因为JavaScript网站往往需要更多的资源。一旦页面被接收，解析页面通常提供很少的开销。'
- en: 'To get started using `dataflowkit`, you can either clone it from GitHub using
    the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用`dataflowkit`，您可以使用以下代码从GitHub克隆它：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'or via `go get`, using the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过`go get`，使用以下代码：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The Fetch service
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取服务
- en: The Fetch service is responsible for retrieving HTML data either via simple
    HTTP requests, or driving a web browser such as Google Chrome. To get started
    using the Fetch service, first, navigate to your local repository and run `go
    build` from the `cmd/fetch.d` directory. Once the build completes, you can start
    the service via `./fetch.d`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 获取服务负责通过简单的HTTP请求或驱动Google Chrome等网络浏览器来检索HTML数据。要开始使用获取服务，首先导航到您的本地存储库，然后从`cmd/fetch.d`目录运行`go
    build`。构建完成后，您可以通过`./fetch.d`启动服务。
- en: An instance of Google Chrome browser must be started prior to starting the Fetch
    service. This instance must be started with the `--remote-debugging-port` option
    set (usually to 9222). You may use the `--headless` flag as well to run without
    displaying any content.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动获取服务之前，必须先启动一个Google Chrome浏览器实例。此实例必须使用`--remote-debugging-port`选项设置（通常为9222）。您也可以使用`--headless`标志来在不显示任何内容的情况下运行。
- en: 'The Fetch service is now ready to accept commands. You should now open a second
    terminal window and navigate to the `cmd/fetch.cli` directory and run `go build`.
    This builds the CLI tool that you can use to send commands to the Fetch service.
    Using the CLI, you can tell the Fetch service to retrieve a web page on your behalf,
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 获取服务现在已准备好接受命令。您现在应该打开第二个终端窗口，然后导航到`cmd/fetch.cli`目录并运行`go build`。这将构建CLI工具，您可以使用它向获取服务发送命令。使用CLI，您可以告诉获取服务代表您检索网页，如下所示：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This can also be done with a simple JSON `POST` request to `/fetch` of the
    Fetch service. In Go, you would write something like the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过向Fetch服务的`/fetch`发送简单的JSON `POST`请求来完成。在Go中，您可以编写类似以下代码：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `fetch.Request` object is a convenient way of structuring our `POST` request
    data, and the `json` library makes it easy to attach as the request body. Most
    of the rest of the code you have already seen in earlier chapters. In this example,
    we use the basic type of fetcher which only uses HTTP requests. If we needed to
    drive a browser instead, we would be able to send actions to the browser in our
    request.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetch.Request`对象是构造我们的`POST`请求数据的便捷方式，`json`库使其易于附加为请求主体。您已经在早期章节中看到了大部分其他代码。在此示例中，我们使用的是基本类型的抓取器，它仅使用HTTP请求。如果我们需要驱动浏览器，我们将能够在我们的请求中向浏览器发送操作。'
- en: 'Actions are sent as an array of JSON objects representing a small subset of
    commands. As of right now, only the click and paginate commands are supported.
    If you want to send a `click` command to the browser, your Fetch request would
    look similar to the following example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 操作作为表示一小部分命令的JSON对象数组发送。目前，只支持点击和分页命令。如果要向浏览器发送`click`命令，您的Fetch请求将类似于以下示例：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By communicating with the external Fetch service, you can easily control the
    switch back and forth between HTTP requests and driving web browsers. Combined
    with the power of remote execution, you can make sure that you size the right
    machines for the right jobs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与外部Fetch服务通信，您可以轻松控制HTTP请求和驱动Web浏览器之间的切换。结合远程执行的强大功能，您可以确保为正确的工作选择正确的机器。
- en: The Parse service
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parse服务
- en: 'The Parse service is responsible for parsing data out of an HTML page and returning
    it in an easily usable format, such as CSV, XML, or JSON. The Parse service relies
    on the Fetch service to retrieve the page, and does not function on its own. To
    get started using the Parse service, first navigate to your local repository and
    run `go build` from the `cmd/parse.d` directory. Once the build completes, you
    can start the service via `./parse.d`. There are many options you can set when
    configuring the Parse service that will determine the backend it uses to cache
    the results: how to handle pagination, the location of the Fetch service, and
    so on. For now, we will use the standard defaults.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Parse服务负责从HTML页面中解析数据并以易于使用的格式返回，例如CSV、XML或JSON。Parse服务依赖于Fetch服务来检索页面，并且不能独立运行。要开始使用Parse服务，首先导航到本地存储库，然后从`cmd/parse.d`目录运行`go
    build`。构建完成后，可以通过`./parse.d`启动服务。在配置Parse服务时，可以设置许多选项，这些选项将确定它用于缓存结果的后端：如何处理分页、Fetch服务的位置等。现在，我们将使用标准默认值。
- en: 'To send commands to the Parse service, you use `POST` requests to the `/parse`
    endpoint. The body of the request contains information on what site to open, how
    to map HTML elements to fields and, and how to format the returned data.  Let''s
    look at the daily deals example from [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing
    HTML*, and build a request for the Parse service. First, we will look at the `package`
    and `import` statements, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 向Parse服务发送命令，您可以使用`POST`请求到`/parse`端点。请求的主体包含有关要打开的站点、如何将HTML元素映射到字段以及如何格式化返回的数据的信息。让我们看一下来自[第4章](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml)的每日交易示例，*解析HTML*，并为Parse服务构建一个请求。首先，我们将查看`package`和`import`语句，如下所示：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, you can see where we import the necessary `dataflowkit` packages. The
    `fetch` package is used in this example to build the request for the Parse service
    to send to the Fetch service. You can see it in the `main` function, as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到我们导入了必要的`dataflowkit`包。`fetch`包在此示例中用于构建要发送到Fetch服务的Parse服务请求。您可以在`main`函数中看到它，如下所示：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This `scrape.Payload` object is what we use to communicate with the Parse service.
    It defines the request to make to the Fetch service, as well as how to collect
    and format our data. In our case, we want to collect rows of two fields: the title
    and the price. We use CSS selectors to define where to find the fields, and where
    to extract the data from. The `Extractor` that this program will use is the text
    extractor which will copy all of the inner text for the matching element.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`scrape.Payload`对象是我们用来与Parse服务通信的对象。它定义了要发送到Fetch服务的请求，以及如何收集和格式化我们的数据。在我们的例子中，我们想收集两个字段的行：标题和价格。我们使用CSS选择器来定义字段的位置以及从哪里提取数据。此程序将使用的`Extractor`是文本提取器，它将复制匹配元素的所有内部文本。
- en: 'Finally, we send the request to the Parse service and wait for the result,
    as shown in the following example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们向Parse服务发送请求并等待结果，如下例所示：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The Parse service replies with a JSON object summarizing the whole process,
    including where we can find the file containing the results, as shown in the following
    example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Parse服务以JSON对象的形式回复，总结了整个过程，包括我们可以在其中找到包含结果的文件，如下例所示：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The convenience that the Parse service offers, allows you as a user to be even
    more creative by building on top of it. With systems that are open source, and
    composable, you can start with a solid foundation and apply your best skills towards
    making a complete system. You are armed with enough knowledge and enough tools
    to build efficient and powerful systems, but I hope your learning does not stop
    here!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Parse服务提供的便利性使您作为用户可以更有创意地构建。通过开源和可组合的系统，您可以从坚实的基础开始，并运用您最好的技能来构建完整的系统。您已经掌握了足够的知识和工具来构建高效和强大的系统，但我希望您的学习不会止步于此！
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked under the hood at the components that make a solid
    web scraping system. We used `colly` to scrape HTML pages that did not require
    JavaScript. We used `chrome-protocol` to drive web browsers to scrape sites that
    do require JavaScript. Finally, we examined `dataflowkit` and saw how its architecture
    opens the door for building distributed web crawlers. There is more to learn and
    do when it comes to building distributed systems in Go, but this is where the
    scope of this book ends. I hope you check out some other publications on building
    applications in Go and continue to hone your skills!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了构建稳健网络抓取系统的组件。我们使用`colly`来抓取不需要JavaScript的HTML页面。我们使用`chrome-protocol`来驱动网络浏览器抓取需要JavaScript的网站。最后，我们研究了`dataflowkit`，看到了它的架构如何为构建分布式网络爬虫打开了大门。在使用Go构建分布式系统方面还有更多的知识和工作要学习和做，但这就是本书的范围所在。我希望你能阅读一些其他关于在Go中构建应用程序的出版物，并继续磨练你的技能！
