

# 第十章：可靠性概述

我们已经走过了本书所有前面的章节，完成了关于微服务开发基础的章节。到目前为止，您已经学习了如何启动微服务、编写测试、设置服务发现、在微服务之间使用同步和异步通信，以及使用不同的格式在它们之间序列化数据，以及如何部署服务并验证它们的 API 是否正常工作。

本章开始本书的第三部分，这部分内容专注于微服务开发的更高级概念，包括可靠性、可观察性、可维护性和可伸缩性。在本章中，我们将探讨微服务开发的一些实用方面，这些方面对于确保您的服务能够在多种条件下良好运行至关重要，包括故障场景、网络流量变化和意外服务关闭。

在本章中，我们将介绍各种技术和流程，可以帮助您提高服务的可靠性。我们将涵盖以下主题：

+   可靠性基础

+   通过自动化实现可靠性

+   通过开发流程和文化实现可靠性

让我们继续本章的第一部分，这将帮助您更好地理解服务可靠性概念。

# 技术要求

要完成本章，您需要 Go 1.11 或更高版本。

您可以在此处找到本章的 GitHub 代码：

[`github.com/PacktPublishing/microservices-with-go/tree/main/Chapter`](https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter10)10

# 可靠性基础

在实现新应用程序、服务或功能时，工程师通常会首先关注满足各种系统要求，例如实现特定的应用程序功能。这种工作的初步结果通常是某些可以正确执行其任务的代码，例如处理某些数据处理任务或作为 API 端点处理网络请求。我们可以这样说，这样的代码最初在独立运行时表现良好——实现的代码为我们提供的输入产生预期的输出。

当我们向系统中添加更多组件时，事情通常会变得更加复杂。让我们以*第二章*中的电影服务为例，并假设其 API 被某个拥有数百万用户的第三方服务使用。我们的服务可以完美实现并针对各种测试输入产生正确的结果。然而，一旦我们从外部服务收到请求，我们可能会注意到各种问题。其中之一被称为**拒绝服务**（**DoS**）——外部服务可以通过请求处理过多的请求来超载我们的服务，以至于我们的服务停止处理新的请求。这种问题的后果可能从轻微的系统性能下降到由于达到 CPU、文件或内存限制而导致的服务崩溃。

DoS 只是微服务环境中可能出现问题的例子之一。假设您实施了一个限制传入服务请求数量的修复，但这个修复破坏了调用您的 API 的服务，因为它们没有预料到请求会突然遭受 DoS 攻击。另一种情况是服务 API 的变化引入了**向后不兼容的更改**。这种更改与您的服务 API 的一个或多个先前发布的版本不兼容。结果，调用您的 API 的服务可能会经历各种负面效果，甚至无法处理任何请求。

让我们定义一种服务在面对意外故障时仍能保持弹性的质量为**可靠性**——即按照预期运行并具有明确定义的限制。在我们对可靠性的定义中，最后一句话对它的含义产生了重大影响——仅仅做好某个功能是不够的。同样重要的是要明确服务的限制以及当这些限制被违反时会发生什么。

在我们的电影服务示例中，我们需要明确多个方面，如下所示：

+   **系统吞吐量**：服务可以处理多少请求（例如，每秒最大请求量）

+   **拥堵策略**：当我们的服务过载时我们将如何处理场景

例如，如果我们的服务每个实例无法处理超过 100 个并发请求，我们可以在 API 文档中明确指出这一点，并通过返回特殊错误代码（如`HTTP 429 Too Many Requests`）来拒绝所有额外的传入请求。这种对系统限制的指示和明确沟通拥堵问题将是一个巨大的步骤，有助于通过使行为更加确定性和可靠性来提高整体系统可靠性。

通常情况下，实现高度的可靠性是一个持续的过程，需要不断在以下三个类别中持续改进：

+   **预防**：在可能的情况下防止可能出现的问题

+   **检测**：尽可能早地发现可能出现的问题

+   **缓解**：尽可能早地减轻任何问题

通过执行两种类型的操作可以改进预防、检测和缓解：

+   自动化服务对各种类型故障的响应

+   改变和改进服务开发流程

我们将把本章的其余部分分为两个部分，描述这两种类型的操作。让我们先进行第一部分，涵盖与自动化相关的可靠性工作。

# 通过自动化实现可靠性

在本节中，我们将讨论各种自动化技术，这些技术可以帮助您提高服务的可靠性。

首先，让我们回到通信错误处理，这是我们之前在*第五章*中简要介绍过的。拥有正确的通信错误处理逻辑是实现服务更高可靠性的第一步，因此我们将关注在微服务开发中同样重要的错误处理的多个方面。

## 通信错误处理

正如我们在本书*第五章*中讨论的那样，当两个组件（例如客户端和服务器）相互通信时，有三种可能的结果场景：

+   **成功响应**：服务器接收并成功处理了一个请求。

+   **客户端错误**：发生错误，并非由服务器引起（例如，客户端发送无效请求）。

+   **服务器错误**：发生错误，是由服务器引起的（例如，由于应用程序崩溃或服务器端意外错误）。

从客户端的角度来看，存在两类不同的错误：

+   **可重试错误**：客户端可以重试原始请求（例如，当服务器暂时不可用）。

+   **非可重试错误**：客户端不应重试请求（例如，当请求本身由于验证失败而不正确）。

区分可重试和非可重试错误是客户端的责任。然而，在可能的情况下明确指出这一点是一个好习惯。例如，服务器可以返回特定的代码，指示错误类型（例如`HTTP 404 Not Found`），以便客户端可以识别可重试错误并执行重试。区分客户端和服务器错误还有助于确保不会对非可重试错误进行重试。从服务器的角度来看，这很重要，因为处理重复的、无效的请求会增加其负载。

让我们通过实现客户端请求重试来展示如何处理可重试的通信错误。设置对潜在问题的自动响应，例如通信错误，有助于使系统对瞬时故障更具弹性，从而为系统中的所有组件提供更好的体验。

### 实现请求重试

让我们通过在微服务代码中实现请求重试来展示如何操作。为此，让我们回顾一下我们在*第五章*中实现的元数据 gRPC 网关代码。`Get`函数包括对`metadata`服务的实际调用：

```go
    resp, err := client.GetMetadata(ctx, &gen.GetMetadataRequest{MovieId: id})
    if err != nil {
        return nil, err
    }
```

现在，让我们看看在元数据服务 gRPC 处理器中实现`GetMetadata`端点的实现。`GetMetadata`函数包括以下代码：

```go
func (h *Handler) GetMetadata(ctx context.Context, req *gen.GetMetadataRequest) (*gen.GetMetadataResponse, error) {
    if req == nil || req.MovieId == "" {
        return nil, status.Errorf(codes.InvalidArgument, "nil req or empty id")
    }
    m, err := h.ctrl.Get(ctx, req.MovieId)
    if err != nil && errors.Is(err, metadata.ErrNotFound) {
        return nil, status.Errorf(codes.NotFound, err.Error())
    } else if err != nil {
        return nil, status.Errorf(codes.Internal, err.Error())
    }
    return &gen.GetMetadataResponse{Metadata: model.MetadataToProto(m)}, nil
}
```

如我们所见，`GetMetadata`端点的实现包括三个错误情况，每个都有自己的 gRPC 错误代码：

+   `InvalidArgument`：传入的请求未通过验证。

+   `NotFound`：未找到带有提供标识符的记录。

+   `Internal`：内部服务器错误。

`InvalidArgument`和`NotFound`错误是不可重试的——重试验证失败的请求或尝试检索未找到的记录是没有意义的。"Internal"错误可能表明一系列问题，例如服务代码中的错误，因此我们无法肯定地说你应该对它们进行重试。

然而，还有一些其他类型的 gRPC 错误代码表示可能可重试的错误。让我们列出其中一些：

+   `DeadlineExceeded`：表示在配置的时间间隔内处理请求存在问题。

+   `ResourceExhausted`：处理请求的服务已耗尽。这可能表明可用资源不足的问题（例如，CPU、内存或磁盘达到其限制）或客户端达到访问服务的配额（例如，当服务不允许超过一定数量的并行请求时）。

+   `Unavailable`：服务当前不可用。

让我们首先在元数据 gRPC 网关内部实现一些简单的重试逻辑，通过替换`Get`函数为以下代码：

```go
// Get returns movie metadata by a movie id.
func (g *Gateway) Get(ctx context.Context, id string) (*model.Metadata, error) {
    conn, err := grpcutil.ServiceConnection(ctx, "metadata", g.registry)
    if err != nil {
        return nil, err
    }
    defer conn.Close()
    client := gen.NewMetadataServiceClient(conn)
    var resp *model.Metadata
    const maxRetries = 5
    for i := 0; i < maxRetries; i++ {
        resp, err = client.GetMetadata(ctx, &gen.GetMetadataRequest{MovieId: id})
        if err != nil {
            if shouldRetry(err) {
                continue
            }
            return nil, err
        }
        return model.MetadataFromProto(resp.Metadata), nil
    }
    return nil, err
}
```

添加一个函数，帮助我们检查通信错误是否可重试：

```go
func shouldRetry(err error) bool {
    e, ok := status.FromError(err)
    if !ok {
        return false
    }
    return e.Code() == codes.DeadlineExceeded || e.Code() == codes.ResourceExhausted || e.Code() == codes.Unavailable
}
```

注意，我们还需要导入两个额外的包来检查特定的 gRPC 错误代码——`google.golang.org/grpc/codes`用于访问错误代码列表，`google.golang.org/grpc/status`用于检查通信错误是否为有效的 gRPC 错误。

现在，我们的元数据 gRPC 网关可以对元数据服务的请求进行最多五次的重试。我们刚刚添加的重试逻辑应该有助于我们最小化偶尔的错误影响，例如临时服务器不可用（例如，在意外中断或临时网络问题期间）。然而，这也引入了一些额外的挑战：

+   `Get`函数，元数据服务 gRPC 网关现在对于可重试的错误进行最多五次调用，而不是一次。

+   **请求突发**：元数据 gRPC 网关在出现错误时立即重试，这将生成对服务器的请求突发。

后者场景可能对服务器特别具有挑战性，因为负载分布不均。想象一下，你正在做一些工作，同时接到一些带有额外任务的电话。如果你回应这样的电话并说你在忙，你不想立即再次被叫去执行同样的任务——相反，你希望呼叫者在一段时间后再回电。同样，立即重试对于经历拥塞问题的服务器来说也是不理想的，因此我们需要对我们的重试逻辑进行额外的修改，在重试之间引入额外的延迟，以便我们的服务器不会因为立即重试而超载。

在客户端请求重试之间添加额外延迟的技术称为**退避**。通过在重试请求之间使用不同的延迟间隔来实现不同的退避类型：

+   **恒定退避**：每次重试都在一个恒定的延迟之后执行。

+   **指数退避**：每次重试都是在比前一次指数级更高的延迟后进行的。

指数退避的一个例子是一系列调用，其中第一次重试会在 100 毫秒的延迟后进行，第二次将需要 400 毫秒的等待，第三次重试延迟将是 900 毫秒。指数退避通常比恒定退避更好，因为它使下一次重试比前一次慢得多，允许服务器在过载的情况下恢复。一个流行的 Go 库[`github.com/cenkalti/backoff`](https://github.com/cenkalti/backoff)提供了指数退避和其他类型退避算法的实现。

通过引入对持续时间的小随机变化，退避延迟也可以被修改。例如，每一步的重试延迟值可以增加或减少多达 10%，以更好地分散服务器上的负载。这种优化称为**抖动**。为了说明抖动的有用性，假设多个客户端同时开始调用服务器。如果每个客户端的重试都使用相同的延迟，它们将同时不断调用服务器，生成服务器请求的突发。向重试延迟间隔添加伪随机偏移有助于更均匀地分配服务器上的负载，防止请求重试可能产生的流量突发。

### 截止日期和超时

现在我们来谈谈与时间相关的另一类通信问题。当客户端向服务器发起请求时，多种可能的失败可能导致客户端或服务器接收到的数据不足以认为请求成功。可能的失败场景包括以下几种：

+   客户端请求因网络问题未能到达服务器。

+   服务器过载，响应客户端需要更长的时间。

+   服务器处理了请求，但由于网络问题，响应未能到达客户端。

这些失败可能导致客户端的等待时间更长。想象一下，你正在给你的亲戚写信，但没有收到回复。没有额外的信息，你会继续等待，不知道信是否在某个步骤中丢失，或者亲戚只是还没有回复。

对于同步请求，有一种方法可以通过设置**请求超时**来提高客户端体验——即在未收到成功响应的情况下，经过一定时间间隔后，请求被视为失败。由于多个原因，设置请求超时是一种良好的实践：

+   **消除意外等待**：如果请求花费了意外长的时间，客户端可以提前停止它并执行可选的重试。

+   **估计最大请求处理时间的能力**：当使用显式超时执行请求时，更容易计算操作返回响应或错误给调用者需要多长时间。

+   **能够为长时间运行的操作设置更长的超时时间**：用于执行网络调用的库通常设置默认请求超时时间（例如，30 秒）。有时客户端希望设置一个更高的值，因为他们知道请求可能需要更长的时间才能完成（例如，当将大文件上传到服务器时）。显式地设置一个更高的超时时间有助于防止请求因超过默认超时而被取消的情况。

在 Go 中，超时通常通过`context.Context`对象传播。正如我们在*第一章*中提到的，每个 I/O 操作，如网络调用，都接受`context`对象作为参数，我们可以通过调用`context.WithTimeout`函数来设置超时，如下面的代码片段所示：

```go
func TimeoutExample(ctx context.Context, args Args) {
    const timeout = 10 * time.Second
    ctx, cancel := context.WithTimeout(ctx, timeout)
    defer cancel()
    resp, err := SomeOperation(ctx, args)
}
```

在前面的示例中，我们将`SomeOperation`函数的超时时间设置为`10`秒，因此完成操作不应超过 10 秒。

设置超时并不是限制请求处理时间的唯一方法。对此的另一种解决方案是设置一个`time.Duration`结构（例如，值为 10 秒），一个截止时间表示确切的时间点（例如，2074 年 1 月 1 日，00:00:00）。以下是一个使用截止时间的代码示例，与之前的代码示例中的相同操作：

```go
deadline := time.Parse(time.RFC3339, "2074-01-01T00:00:00Z")
ctx, cancel := context.WithDeadline(ctx, deadline)
defer cancel()
resp, err := SomeOperation(ctx, args)
```

技术上，超时和截止时间都帮助我们实现相同的目标——为特定操作设置时间限制。你可以根据自己的喜好选择使用任一格式。

### 回退

现在我们来讨论另一个客户端-服务器通信失败场景——当客户端尝试操作，即使在多次重试后也没有收到成功的响应。在这种情况下，客户端有三个可能的选择：

+   如果有调用者，向其返回错误

+   如果错误对系统是致命的，则引发恐慌

+   如果可能，执行替代的备份操作

最后一个选项被称为**回退**——一种在某些操作无法按预期执行时可以执行的替代逻辑。

让我们以我们的评分服务为例。在我们的服务中，我们通过从评分存储库中读取提供的记录的所有评分来实现`GetAggregatedRating`端点。现在，让我们考虑一个由于某些问题（例如 MySQL 数据库不可用）而无法检索评分的失败场景。如果没有回退逻辑，我们就无法处理传入的请求，并需要向我们的调用者返回错误。

回退的一个例子是使用`map`结构，并在数据库读取错误时返回它们。以下代码片段提供了一个这样的回退逻辑示例：

```go
    ratings, err := c.repo.Get(ctx, recordID, recordType)
    if err != nil && err == repository.ErrNotFound {
        return 0, ErrNotFound
    } else if err != nil {
        log.Printf("Failed to get ratings for %v %v: %v", recordID, recordType, err)
        log.Printf("Fallback: returning locally cached ratings for %v %v", recordID, recordType)
        return c.getCachedRatings(recordID, recordType)
    }
```

使用回退是**优雅降级**的一个例子——这是一种处理应用程序故障的方式，即使应用程序仍然以有限模式执行其操作。在我们的例子中，即使推荐功能不可用，电影服务也会继续处理获取电影详情的请求，为用户提供有限但正常的功能。

当设计新的服务或功能时，请自问哪些操作在出现故障时可以被回退操作所替代。此外，检查哪些特性和操作是绝对必要的，哪些可以在任何故障发生时关闭，例如系统过载或由于故障而丢失系统的一部分。另外，一个好的做法是发出与故障相关的额外有用信息，例如日志和指标，并在代码中明确指出回退是故意的，就像前面的例子一样。

### 速率限制和节流

正如我们在本章开头讨论的那样，可能存在一种情况，即微服务过载并且无法再处理传入的请求。我们如何防止或减轻此类问题？

防止此类问题的流行方法是在并行处理请求数量上设置一个硬限制。这种技术被称为**速率限制**，可以在多个级别上应用：

+   **客户端级别**：客户端限制同时发出的请求数量。

+   **服务器级别**：服务器限制同时传入的请求数量。

+   **网络/中间级别**：服务器和其客户端之间的请求数量由它们之间的某些逻辑或中间组件（例如，由负载均衡器）控制。

当客户端或服务器超过配置的请求数量时，请求的结果将是一个错误，该错误应包括一个特殊代码或消息，指示请求已被速率限制。

HTTP 协议中速率限制的一个例子是内置的状态码，`429 Too Many Requests`。当客户端收到带有此代码的响应时，它应该通过减少调用速率或等待一段时间直到服务器可以再次处理请求来考虑这一点。

客户端和服务器级别的速率限制通常由每个服务实例单独执行：每个实例跟踪当前发出的或传入的请求数量。这些模型的缺点是无法在全局服务级别配置限制。如果你将每个服务客户端实例配置为每秒不超过 100 个请求，那么如果有 1000 个客户端实例，你仍然可能会接收到 10 万个同时请求。如此高的同时请求数量很容易使你的服务过载。

网络级别的速率限制可能解决此问题：如果以集中方式（例如，通过处理服务之间请求的负载均衡器）执行速率限制，执行速率限制的组件可以跟踪所有服务实例的总请求数量。

虽然网络级别的速率限制器提供了更多的配置灵活性，但它们通常需要额外的集中式组件（例如负载均衡器）。因此，我们将演示如何使用基于客户端的更简单的方法。

有一个流行的 Go 语言包实现了速率限制，称为`golang.org/x/time/rate`。该包实现了`b`，每次请求时减 1，并以每秒配置的速率`r`个元素进行补充。例如，对于 b = 100 和 r = 50，令牌桶算法创建一个容量为 100 的桶，并以每秒 50 的速率补充。在任何时刻，它不允许超过 100 个并发请求（最大数量由当前桶的大小控制）。

下面是一个在 Go 中使用基于令牌桶的速率限制器的示例：

```go
package main
import (
    "fmt"
    "golang.org/x/time/rate"
)
func main() {
    limit := 3
    burst := 3
    limiter := rate.NewLimiter(rate.Limit(limit), burst)
    for i := 0; i < 100; i++ {
        if limiter.Allow() {
            fmt.Println("allowed")
        } else {
            fmt.Println("not allowed")
        }
    }
}
```

此代码会打印`allowed`三次，然后持续打印`not allowed` 97 次，除非执行时间超过 1 秒。

让我们通过*第五章*中实现的 gRPC API 处理器来展示如何使用这种速率限制器。gRPC 协议允许我们定义**拦截器**——在每个请求上执行的操作，可以修改 gRPC 服务器对该请求的响应。要将 gRPC 速率限制器添加到电影服务的 gRPC 处理器中，执行以下步骤：

1.  打开`movie/cmd/main.go`文件，并在其导入中添加以下代码：

    ```go
    “github.com/grpc-ecosystem/go-grpc-middleware/ratelimit"
    ```

1.  将带有`grpc.NewServer`调用的行替换为以下代码：

    ```go
        const limit = 100
    ```

    ```go
        const burst = 100
    ```

    ```go
        l := newLimiter(100, 100)
    ```

    ```go
        srv := grpc.NewServer(grpc.UnaryInterceptor(ratelimit.UnaryServerInterceptor(l)))
    ```

1.  然后，将以下结构定义添加到文件中：

    ```go
    type limiter struct {
    ```

    ```go
        l *rate.Limiter
    ```

    ```go
    }
    ```

    ```go
    func newLimiter(limit int, burst int) *limiter {
    ```

    ```go
        return &limiter{rate.NewLimiter(rate.Limit(limit), burst)}
    ```

    ```go
    }
    ```

    ```go
    func (l *limiter) Limit() bool {
    ```

    ```go
        return l.l.Allow()
    ```

    ```go
    }
    ```

我们的速率限制器使用来自`github.com/grpc-ecosystem/go-grpc-middleware/ratelimit`包的速率限制 gRPC 服务器拦截器。其接口与我们来自`golang.org/x/time/rate`的限制器略有不同，因此我们添加了一个结构来将它们连接起来。现在，我们的 gRPC 服务器允许每秒最多 100 个请求，并在超过限制时返回一个带有`codes.ResourceExhausted`特殊代码的错误。这确保了服务不会因为大量请求的突然增加而超载——如果有人一次性请求 100 万部电影详情，我们不会对元数据服务进行 100 万次调用并超载其数据库。

请记住，速率限制是一种强大的技术；然而，它需要谨慎使用，因为设置限制过低会使系统对用户过于限制，拒绝过多的请求。为了计算服务的公平速率限制设置，您需要定期进行基准测试，了解其逻辑的最大吞吐量。

让我们转到基于自动化的可靠性技术的下一个主题，描述如何优雅地终止服务的执行。

## 优雅关闭

在本节中，我们将讨论服务关闭事件的优雅处理。服务关闭可以由多个事件触发：

+   手动中断执行（例如，当用户在运行服务进程的终端中输入*Ctrl* + *C*/*Cmd* + *C*时，进程从操作系统接收`SIGINT`信号）

+   操作系统通过（例如，通过`SIGTERM`或`SIGKILL`信号）终止执行

+   服务代码中的 panic

通常，服务的执行突然终止可能会导致以下负面后果：

+   **丢弃请求**：入站 API 请求可能在完全处理之前被丢弃，从而导致服务调用者的错误。

+   **连接问题**：在关闭过程中，服务网络连接可能无法正确关闭，从而导致多种负面影响。例如，未关闭数据库连接可能导致所谓的**连接泄漏**情况，此时数据库会保留连接以供服务使用，而不是允许其他实例重用该连接。

为了防止这些问题，您需要确保您的服务通过执行一系列操作来优雅地关闭，以最大限度地减少对服务和其组件的负面影响。执行**优雅关闭**时，服务在终止前会运行一些额外的逻辑，例如以下内容：

+   尽可能完成尽可能多的未完成操作，例如未处理请求

+   关闭所有打开的网络连接并释放任何共享资源，例如网络套接字

Go 服务的优雅关闭逻辑通常按以下方式实现：

1.  服务通过调用`os/signal`包的`Notify`函数订阅关闭事件。

1.  当服务从操作系统接收到`SIGINT`或`SIGTERM`事件，表明服务即将被终止时，它执行一系列必要的操作来关闭所有打开的连接并完成所有挂起的任务。

1.  一旦所有操作完成，服务将结束执行。

这里是一个您可以添加到任何 Go 服务`main`函数中的代码示例，例如我们在*第二章*中实现的服务：

```go
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)
    var wg sync.WaitGroup
    wg.Add(1)
    go func() {
        defer wg.Done()
        s := <-sigChan
        log.Printf("Received signal %v, attempting graceful shutdown", s)
        // Graceful shutdown logic.
    }()
    wg.Wait()
```

通过使用内置的`recover`函数，也有一种优雅地处理 Go 代码中 panic 的方法。以下代码片段演示了如何在`main`函数内部处理 panic 并执行任何自定义逻辑，例如关闭任何打开的连接：

```go
func main() {
    defer func() {
        if err := recover(); err != nil {
            log.Printf("Panic occurred, attempting graceful shutdown")
            // Graceful shutdown logic.
        }
    }()
    panic("panic example")
}
```

在我们的代码中，我们通过调用`recover`函数并检查它是否返回非空错误来检查是否存在服务 panic。在 panic 的情况下，我们可以执行任何额外的操作，例如保存任何未保存的数据或终止任何打开的连接。

要优雅地终止 Go gRPC 服务器的执行，你需要调用 `GracefulStop` 函数而不是 `Stop`。与 `Stop` 函数不同，`GracefulStop` 会等待所有请求处理完毕，从而帮助减少关闭对客户端的负面影响。

如果你有一些长时间运行的组件，例如 Kafka 消费者或执行长时间运行任务的任何后台 goroutine，你可以使用内置的 `context.Context` 结构来传达服务终止信号。`context.Context` 结构提供了一种名为 **上下文取消** 的功能——通过发送与上下文关联的通道中的特定事件来通知不同组件关于执行取消的能力。

让我们更新我们的评分服务代码，以说明如何实现上下文取消和 gRPC 服务的优雅关闭：

1.  打开评分服务的 `main.go` 文件，找到执行 `context.Background()` 函数调用的行。将其替换为以下代码：

```go
ctx, cancel := context.WithCancel(context.Background())
```

我们创建了一个上下文实例和 `cancel` 函数，我们将在服务关闭时调用它来通知我们的组件，例如服务注册表，关于即将到来的终止。

1.  在调用 `srv.Serve` 函数之前立即添加以下代码：

```go
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)
    var wg sync.WaitGroup
    wg.Add(1)
    go func() {
        defer wg.Done()
        s := <-sigChan
        cancel()
        log.Printf("Received signal %v, attempting graceful shutdown", s)
        srv.GracefulStop()
        log.Println("Gracefully stopped the gRPC server")
    }()
```

在我们的代码中，我们让评分服务监听进程中断和终止信号，并启动后台 goroutine，持续监听相关通知。一旦它收到任一信号，它就会调用我们在上一步获得的 `cancel` 函数。调用此函数的结果将是一个通知，该通知将被发送到使用我们的上下文初始化的组件，例如服务注册表。

1.  让我们在 `main` 函数的末尾添加以下行来完成最后的润色：

```go
wg.Wait()
```

现在我们来测试我们刚刚实现的代码。运行评分服务，然后通过按 *Ctrl + C*/*Cmd + C*（取决于你的操作系统）来终止它。你应该会看到以下消息：

```go
2022/10/13 08:55:05 Received signal interrupt, attempting graceful shutdown
2022/10/13 08:55:05 Gracefully stopped the gRPC server
```

在 Go 微服务开发中，传达终止和中断事件是一种常见做法，并且是实现优雅关闭逻辑的一种优雅方式。在设计和服务实现时，提前考虑在服务终止时需要关闭或反初始化的可能资源，例如任何网络客户端和连接。优雅关闭逻辑可以防止服务突然终止的负面影响。它还可以减少服务中可能出现的错误数量，并提高你的操作体验。

到目前为止，我们已经审查了一些自动化技术，以提高我们服务的可靠性并减少各种故障场景的症状。现在，我们可以继续本章的下一节，涵盖与开发流程和文化相关的另一个可靠性工作方面。改进你的开发流程对于长期实现高可靠性至关重要，本节将为你提供一些有价值的技巧和想法，你可以在微服务开发中利用它们。

# 通过开发流程和文化实现可靠性

在本节中，我们将描述一些基于开发流程和文化变革提高服务可靠性的技术。你将学习如何建立改进和审查服务可靠性的流程，如何高效地从任何服务相关的问题和事件中学习，以及如何衡量你的服务可靠性。我们将涵盖整个行业广泛使用的流程和实践，概述每个流程最重要的思想。本节将比前一节更具理论性；然而，它应该同样有用。

首先，我们将提供一个概述，介绍设置监控服务问题的机制所必需的值班流程。

## 值班流程

当你的服务开始处理生产流量或开始响应用户请求时，你的第一个可靠性目标应该是尽早发现任何问题或事件。高效的检测应该是自动的——程序在检测大多数问题时总是比人类更有效率。每个自动检测都应该通知一个或多个工程师关于事件的信息，以便工程师可以采取措施减轻事件的影响。

建立这种通知工程师关于服务事件机制的流程被称为**值班**。此流程有助于确保在任何时刻，服务事件都由负责该服务的工程师承认和解决。

值班流程背后的主要思想如下：

+   工程师可以被分组到**值班轮换**中。每个参与值班流程的工程师都会反复被分配一个连续的**班次**（通常持续 1 周），在此期间，他们负责定期处理有关服务级别事件的通知。

+   值班轮换可以有一个**升级策略**——在事件未解决的情况下升级事件的过程。首先，事件报告给轮换的**主要**值班工程师。如果主要工程师不可用，事件报告给**次要**工程师，依此类推。

+   可以有一个*影子*角色，通常分配给新工程师。这个角色不需要对事件做出任何响应，但可以用来熟悉值班流程并订阅实时事件通知。

+   每个事件都会触发一个或多个通知，通知值班工程师关于问题。除非事件自行解决（例如，如果服务停止接收过多请求并开始正常运作），否则每个通知都必须由负责的值班工程师确认。

+   您还可以为轮换设置一个*升级策略*——如果负责的值班工程师在配置的时间内未确认事件通知，则触发事件升级的机制。通常，升级策略遵循工程层级报告链——如果没有任何工程师确认事件，事件首先触发通知给最近的工程经理，然后是经理报告的人，依此类推，直到达到最高级别（这甚至可能是某些公司的 CTO）。

值班流程对于大多数科技公司和团队来说是常见的，而且大多数公司之间的值班流程都很相似。一些流行的解决方案提供了触发各种类型通知的机制，例如短信、电子邮件，甚至电话。您还可以配置值班轮换并将它们分配给不同的服务。最流行的值班管理解决方案之一是**PagerDuty**——一个提供一系列自动化值班操作工具的平台，以及与数百个服务的集成，包括 Slack、Zoom 等。PagerDuty 提供了我们之前列出的所有功能，允许工程师为他们的服务配置值班轮换，并以不同的方式通知他们关于事件的信息。此外，它提供了一个 API，可以用于访问事件数据和从代码中触发新事件。

我们不会深入探讨本章中 PagerDuty 的功能和集成细节——我建议您查看他们网站上的官方 PagerDuty 文档，[`developer.pagerduty.com/docs`](https://developer.pagerduty.com/docs)。我还建议您在为您的服务建立值班流程之前阅读*第十二章*。这将帮助您了解可能的故障检测机制以及您可以在项目中利用的工具。

让我们讨论在微服务环境中建立值班流程的常见挑战：

+   **轮换所有权**：不同的服务可能由不同的团队维护，因此一个公司内部可能有多个值班轮换。一个好的做法是在每个生产服务和相关的值班轮换之间建立明确的映射，以便清楚地知道每个事件应该报告给哪个轮换。在*第十三章*中，我们将介绍这个方面的所有权问题。

+   **跨服务问题**：一些问题，如数据库或网络故障，可能跨越多个服务，因此拥有一些能够帮助解决跨越单个服务边界的任何问题的集中式团队变得很重要。

一些公司可能有数千个微服务，因此集中式的事件响应团队变得至关重要。例如，Uber 有一个名为*Ring0*的专用工程师团队，能够处理任何广泛的事件，并协调跨多个团队的问题缓解。拥有这样的团队有助于显著减少事件缓解时间。

为了更好地了解在工程师检测和确认事件后会发生什么，我们现在将转到下一个主题：事件管理。

## 事件管理

一旦事件被工程师检测和确认，还有两种其他类型的工作对于提高服务或系统可靠性是必要的——缓解和预防。除非问题自行解决或由于某些外部变化（例如，外部 API 由拥有团队修复），否则需要缓解来解决开放问题。预防工作对于确保问题不再发生是有用的。如果没有适当的预防响应来应对事件，你可能会反复修复相同的问题，浪费时间和影响系统用户的使用体验。

为了使事件缓解过程快速高效，尤其是在一个大型团队中，工程师可能对系统的理解程度不同，应该有足够的文档描述在发生事件时应采取哪些行动。这种文档称为**运行手册**，应该为尽可能多的可检测事件类型准备。每当值班工程师收到事件通知时，运行手册应该清楚地说明缓解事件的步骤。

一个好的运行手册应该简短、简洁，并提供任何工程师都容易理解的明确可执行步骤。让我们以这个例子为例：

```go
rating_service_fd_limit_reached:
  mitigation: Restart the service
```

如果事件缓解需要进一步调查，请包括任何有用的链接，例如指向相关应用程序日志和仪表板的链接。你应该争取尽可能低的缓解时间——也称为**修复时间**（**TTR**）——以提高服务的可用性并改善其整体健康状况。

一旦事件得到缓解，关注预防工作以确保您采取所有行动消除其原因，以及必要时改进检测和缓解机制。跨行业中的多家公司使用编写文档的过程称为 **事件后分析** 来组织围绕事件的学习，并确保每个事件都涉及足够的相关未来预防工作。事件后分析通常包括以下数据：

+   事件标题和摘要

+   作者

+   何时以及如何检测和缓解事件

+   事件背景，以文本或一组图表的形式呈现，有助于理解

+   根本原因

+   事件影响

+   事件时间线

+   经验教训

+   行动项

在著名的谷歌 *《站点可靠性工程（SRE）》* 书籍中提供了一个优秀的故障分析文档示例，您可以在以下链接中熟悉它：[`sre.google/sre-book/example-postmortem/`](https://sre.google/sre-book/example-postmortem/).

要找到事件的根本原因，您可以使用称为 **五问法** 的技术。该技术的理念是持续询问导致先前问题的原因，直到找到根本原因。让我们以下面的 **根本原因分析** （**RCA**）为例来理解这个技术：

**事件**：评级服务向其 API 调用者返回内部错误

**根本原因分析**：

1.  评级服务开始向其 API 调用者返回内部错误，因为评级数据库不可用。

1.  评级数据库因意外的高请求负载而变得不可用。

1.  评级服务意外的高请求负载是由电影服务中的应用程序错误引起的。

在这个例子中，我们通过使用五问法，逐步找到每个先前问题的根本原因，直到在仅仅三步中找到事件的根本原因。这个技术非常强大且易于使用，并且可以帮助您快速找到复杂问题的根本原因。

确保您包括并跟踪您事件中的行动项。仅捕获事件细节和识别原因不足以确保防止事件发生。优先处理行动项有助于确保最关键的行动项尽早得到解决。

现在，让我们转向基于定期测试您可能的服务故障场景的下一个可靠性流程。

## 可靠性演练

如许多系统管理员所知，仅对数据进行备份以保证其持久性是不够的。您还需要确保在发生任何故障的情况下能够从备份中恢复数据。同样的原则适用于您服务的任何部分——为了知道您的服务对特定故障具有弹性，您需要定期进行练习，称为 **演练**。

你可以执行许多可能类型的演练。例如，在数据库备份的例子中，如果你在数据库中存储了任何持久数据，你可以定期测试备份和恢复数据的能力，以验证你的服务是否能够容忍数据库可用性问题。另一个例子是网络演练。你可以通过更新服务路由配置或任何其他网络设置来模拟网络问题，例如连接丢失，以检查你的服务在网络不可用的情况下会如何表现。

执行可靠性演练有多个好处：

+   **检测意外的服务故障**：通过执行故障演练，你可以检测到一些在常规模式下不会发生的意外服务错误和恐慌。这些问题将在一个受控环境中呈现，工程师可以随时停止演练，并尽早解决检测到的错误。

+   **检测意外的服务依赖性**：可靠性演练经常揭示服务之间意外的依赖关系，例如**传递依赖**（服务 A 依赖于服务 B，而服务 B 依赖于服务 C）或甚至**循环依赖**（两个服务需要彼此才能运行）。

+   **更快地减轻未来事件的能力**：通过了解服务在故障情况下的运行方式和如何解决相关问题，你投资于提高未来事件缓解的能力。

演练通常作为**计划中的事件**进行——这些事件提前宣布，并遵循常规事件管理流程，包括编写事后报告。演练的事后报告应包括与常规事件相同的条目，重点在于改进缓解和预防体验。此外，工程师应专注于审查和更新服务操作手册，确保事件缓解说明准确且最新。

到目前为止，我们已经讨论了最重要的服务可靠性技术。还有许多与服务可靠性相关的话题值得探讨——其中一些与事件检测相关的内容，我们将在本书的*第十二章*中进行介绍。如果你对这个话题感兴趣，我强烈建议你阅读谷歌的*《站点可靠性工程（SRE）*》一书，它提供了各种可靠性相关技术的全面指南。你可以通过以下链接找到这本书的在线版本：[`sre.google/sre-book/table-of-contents`](https://sre.google/sre-book/table-of-contents)。书中描述的实践适用于任何微服务，因此你可以在构建任何类型的系统时始终将其作为参考。

# 摘要

在本章中，我们讨论了可靠性的主题，描述了一系列可以帮助你使你的微服务对各种类型的故障更具弹性的技术和实践。你学习了一些有用的技术来自动化服务的错误响应，并减少各种类型问题（如服务过载和意外的服务关闭）的负面影响。

在本章的最后部分，我们讨论了基于工程流程和文化变化的可靠性技术，例如引入值班和事件管理流程，以及进行定期的可靠性演练。从阅读本章中你获得的知识应该有助于你为编写可靠的微服务建立一个坚实的基础。

在下一章中，我们将继续我们的可靠性主题之旅，重点关注收集服务遥测数据，如日志、指标和跟踪。服务遥测数据是设置服务事件检测的主要工具，我们将说明如何在你的微服务代码中处理每种类型的遥测数据。

# 进一步阅读

如果你想了解更多，请参考以下资源：

+   *带有抖动的超时、重试和退避*: https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter

+   *令牌桶速率限制算法*: [`zh.wikipedia.org/wiki/Token_bucket`](https://zh.wikipedia.org/wiki/Token_bucket)

+   *PagerDuty 文档*: [`developer.pagerduty.com/docs`](https://developer.pagerduty.com/docs)

+   *事件事后分析*: [`www.pagerduty.com/resources/learn/incident-postmortem/`](https://www.pagerduty.com/resources/learn/incident-postmortem/)

+   *Google 网站可靠性工程 (SRE)* 网站: [`sre.google/`](https://sre.google/)

+   *Google 网站可靠性工程 (SRE)* 书籍: [`sre.google/sre-book/table-of-contents`](https://sre.google/sre-book/table-of-contents)
