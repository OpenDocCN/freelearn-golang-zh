["```go\n// Perform an HTTP request to load a page and search for a string\npackage main\n\nimport (\n   \"fmt\"\n   \"io/ioutil\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n   \"strings\"\n   \"time\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 3 {\n      fmt.Println(\"Search for a keyword in the contents of a URL\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url> <keyword>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com NanoDano\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n   needle := os.Args[2] // Like searching for a needle in a haystack\n\n   // Create a custom http client to override default settings. Optional\n   // Use http.Get() instead of client.Get() to use default client.\n   client := &http.Client{\n      Timeout: 30 * time.Second, // Default is forever!\n      // CheckRedirect - Policy for following HTTP redirects\n      // Jar - Cookie jar holding cookies\n      // Transport - Change default method for making request\n   }\n\n   response, err := client.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   // Read response body\n   body, err := ioutil.ReadAll(response.Body)\n   if err != nil {\n      log.Fatal(\"Error reading HTTP body. \", err)\n   }\n\n   // Search for string\n   if strings.Contains(string(body), needle) {\n      fmt.Println(\"Match found for \" + needle + \" in URL \" + url)\n   } else {\n      fmt.Println(\"No match found for \" + needle + \" in URL \" + url)\n   }\n} \n```", "```go\n<a href=\"mailto:nanodano@devdungeon.com\">\n<a href=\"mailto:nanodano@devdungeon.com?subject=Hello\">\n```", "```go\n// Search through a URL and find mailto links with email addresses\npackage main\n\nimport (\n   \"fmt\"\n   \"io/ioutil\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n   \"regexp\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"Search for emails in a URL\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   // Read the response\n   body, err := ioutil.ReadAll(response.Body)\n   if err != nil {\n      log.Fatal(\"Error reading HTTP body. \", err)\n   }\n\n   // Look for mailto: links using a regular expression\n   re := regexp.MustCompile(\"\\\"mailto:.*?[?\\\"]\")\n   matches := re.FindAllString(string(body), -1)\n   if matches == nil {\n      // Clean exit if no matches found\n      fmt.Println(\"No emails found.\")\n      os.Exit(0)\n   }\n\n   // Print all emails found\n   for _, match := range matches {\n      // Remove \"mailto prefix and the trailing quote or question mark\n      // by performing a slice operation to extract the substring\n      cleanedMatch := match[8 : len(match)-1]\n      fmt.Println(cleanedMatch)\n   }\n} \n```", "```go\n// Perform an HTTP HEAD request on a URL and print out headers\npackage main\n\nimport (\n   \"fmt\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n)\n\nfunc main() {\n   // Load URL from command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(os.Args[0] + \" - Perform an HTTP HEAD request to a URL\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Perform HTTP HEAD\n   response, err := http.Head(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   // Print out each header key and value pair\n   for key, value := range response.Header {\n      fmt.Printf(\"%s: %s\\n\", key, value[0])\n   }\n} \n```", "```go\nSet-Cookie: preferred_background=blue\nSet-Cookie: session_id=PZRNVYAMDFECHBGDSSRLH\n```", "```go\nCookie: preferred_background=blue; session_id=PZRNVYAMDFECHBGDSSRLH\n```", "```go\npackage main\n\nimport (\n   \"fmt\"\n   \"io/ioutil\"\n   \"log\"\n   \"net/http\"\n)\n\nvar url = \"https://www.example.com\"\n\nfunc main() {\n   // Create the HTTP request\n   request, err := http.NewRequest(\"GET\", url, nil)\n   if err != nil {\n      log.Fatal(\"Error creating HTTP request. \", err)\n   }\n\n   // Set cookie\n   request.Header.Set(\"Cookie\", \"session_id=<SESSION_TOKEN>\")\n\n   // Create the HTTP client, make request and print response\n   httpClient := &http.Client{}\n   response, err := httpClient.Do(request)\n   data, err := ioutil.ReadAll(response.Body)\n   fmt.Printf(\"%s\\n\", data)\n} \n```", "```go\n// Search through a URL and find HTML comments\npackage main\n\nimport (\n   \"fmt\"\n   \"io/ioutil\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n   \"regexp\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"Search for HTML comments in a URL\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL and get response\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n   body, err := ioutil.ReadAll(response.Body)\n   if err != nil {\n      log.Fatal(\"Error reading HTTP body. \", err)\n   }\n\n   // Look for HTML comments using a regular expression\n   re := regexp.MustCompile(\"<!--(.|\\n)*?-->\")\n   matches := re.FindAllString(string(body), -1)\n   if matches == nil {\n      // Clean exit if no matches found\n      fmt.Println(\"No HTML comments found.\")\n      os.Exit(0)\n   }\n\n   // Print all HTML comments found\n   for _, match := range matches {\n      fmt.Println(match)\n   }\n} \n```", "```go\n// Look for unlisted files on a domain\npackage main\n\nimport (\n   \"bufio\"\n   \"fmt\"\n   \"log\"\n   \"net/http\"\n   \"net/url\"\n   \"os\"\n   \"strconv\"\n)\n\n// Given a base URL (protocol+hostname) and a filepath (relative URL)\n// perform an HTTP HEAD and see if the path exists.\n// If the path returns a 200 OK print out the path\nfunc checkIfUrlExists(baseUrl, filePath string, doneChannel chan bool) {\n   // Create URL object from raw string\n   targetUrl, err := url.Parse(baseUrl)\n   if err != nil {\n      log.Println(\"Error parsing base URL. \", err)\n   }\n   // Set the part of the URL after the host name\n   targetUrl.Path = filePath\n\n   // Perform a HEAD only, checking status without\n   // downloading the entire file\n   response, err := http.Head(targetUrl.String())\n   if err != nil {\n      log.Println(\"Error fetching \", targetUrl.String())\n   }\n\n   // If server returns 200 OK file can be downloaded\n   if response.StatusCode == 200 {\n      log.Println(targetUrl.String())\n   }\n\n   // Signal completion so next thread can start\n   doneChannel <- true\n}\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 4 {\n      fmt.Println(os.Args[0] + \" - Perform an HTTP HEAD request to a URL\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \n         \" <wordlist_file> <url> <maxThreads>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" wordlist.txt https://www.devdungeon.com 10\")\n      os.Exit(1)\n   }\n   wordlistFilename := os.Args[1]\n   baseUrl := os.Args[2]\n   maxThreads, err := strconv.Atoi(os.Args[3])\n   if err != nil {\n      log.Fatal(\"Error converting maxThread value to integer. \", err)\n   }\n\n   // Track how many threads are active to avoid\n   // flooding a web server\n   activeThreads := 0\n   doneChannel := make(chan bool)\n\n   // Open word list file for reading\n   wordlistFile, err := os.Open(wordlistFilename)\n   if err != nil {\n      log.Fatal(\"Error opening wordlist file. \", err)\n   }\n\n   // Read each line and do an HTTP HEAD\n   scanner := bufio.NewScanner(wordlistFile)\n   for scanner.Scan() {\n      go checkIfUrlExists(baseUrl, scanner.Text(), doneChannel)\n      activeThreads++\n\n      // Wait until a done signal before next if max threads reached\n      if activeThreads >= maxThreads {\n         <-doneChannel\n         activeThreads -= 1\n      }\n   }\n\n   // Wait for all threads before repeating and fetching a new batch\n   for activeThreads > 0 {\n      <-doneChannel\n      activeThreads -= 1\n   }\n\n   // Scanner errors must be checked manually\n   if err := scanner.Err(); err != nil {\n      log.Fatal(\"Error reading wordlist file. \", err)\n   }\n} \n```", "```go\n// Change HTTP user agent\npackage main\n\nimport (\n   \"log\"\n   \"net/http\"\n)\n\nfunc main() {\n   // Create the request for use later\n   client := &http.Client{}\n   request, err := http.NewRequest(\"GET\", \n      \"https://www.devdungeon.com\", nil)\n   if err != nil {\n      log.Fatal(\"Error creating request. \", err)\n   }\n\n   // Override the user agent\n   request.Header.Set(\"User-Agent\", \"_Custom User Agent_\")\n\n   // Perform the request, ignore response.\n   _, err = client.Do(request)\n   if err != nil {\n      log.Fatal(\"Error making request. \", err)\n   }\n} \n```", "```go\ngo get https://github.com/PuerkitoBio/goquery  \n```", "```go\n<a href=\"https://www.devdungeon.com\">DevDungeon</a>  \n```", "```go\n// Load a URL and list all links found\npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"Find all links in a web page\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   // Extract all links\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      log.Fatal(\"Error loading HTTP response body. \", err)\n   }\n\n   // Find and print all links\n   doc.Find(\"a\").Each(func(i int, s *goquery.Selection) {\n      href, exists := s.Attr(\"href\")\n      if exists {\n         fmt.Println(href)\n      }\n   })\n} \n```", "```go\n// Load a URL and list all documents \npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n   \"strings\"\n)\n\nvar documentExtensions = []string{\"doc\", \"docx\", \"pdf\", \"csv\", \n   \"xls\", \"xlsx\", \"zip\", \"gz\", \"tar\"}\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"Find all links in a web page\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   // Extract all links\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      log.Fatal(\"Error loading HTTP response body. \", err)\n   }\n\n   // Find and print all links that contain a document\n   doc.Find(\"a\").Each(func(i int, s *goquery.Selection) {\n      href, exists := s.Attr(\"href\")\n      if exists && linkContainsDocument(href) {\n         fmt.Println(href)\n      }\n   })\n} \n\nfunc linkContainsDocument(url string) bool {\n   // Split URL into pieces\n   urlPieces := strings.Split(url, \".\")\n   if len(urlPieces) < 2 {\n      return false\n   }\n\n   // Check last item in the split string slice (the extension)\n   for _, extension := range documentExtensions {\n      if urlPieces[len(urlPieces)-1] == extension {\n         return true\n      }\n   }\n   return false\n} \n```", "```go\npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"List all headings (h1-h6) in a web page\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      log.Fatal(\"Error loading HTTP response body. \", err)\n   }\n\n   // Print title before headings\n   title := doc.Find(\"title\").Text()\n   fmt.Printf(\"== Title ==\\n%s\\n\", title)\n\n   // Find and list all headings h1-h6\n   headingTags := [6]string{\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"}\n   for _, headingTag := range headingTags {\n      fmt.Printf(\"== %s ==\\n\", headingTag)\n      doc.Find(headingTag).Each(func(i int, heading *goquery.Selection) {\n         fmt.Println(\" * \" + heading.Text())\n      })\n   }\n\n} \n```", "```go\npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n   \"strings\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"List all words by frequency from a web page\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      log.Fatal(\"Error loading HTTP response body. \", err)\n   }\n\n   // Find and list all headings h1-h6\n   wordCountMap := make(map[string]int)\n   doc.Find(\"p\").Each(func(i int, body *goquery.Selection) {\n      fmt.Println(body.Text())\n      words := strings.Split(body.Text(), \" \")\n      for _, word := range words {\n         trimmedWord := strings.Trim(word, \" \\t\\n\\r,.?!\")\n         if trimmedWord == \"\" {\n            continue\n         }\n         wordCountMap[strings.ToLower(trimmedWord)]++\n\n      }\n   })\n\n   // Print all words along with the number of times the word was seen\n   for word, count := range wordCountMap {\n      fmt.Printf(\"%d | %s\\n\", count, word)\n   }\n\n} \n```", "```go\n<script src=\"img/jquery.min.js\"></script>  \n```", "```go\n/ajax/libs/jquery/3.2.1/jquery.min.js\n```", "```go\npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"os\"\n)\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"List all JavaScript files in a webpage\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <url>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   url := os.Args[1]\n\n   // Fetch the URL\n   response, err := http.Get(url)\n   if err != nil {\n      log.Fatal(\"Error fetching URL. \", err)\n   }\n\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      log.Fatal(\"Error loading HTTP response body. \", err)\n   }\n\n   // Find and list all external scripts in page\n   fmt.Println(\"Scripts found in\", url)\n   fmt.Println(\"==========================\")\n   doc.Find(\"script\").Each(func(i int, script *goquery.Selection) {\n\n      // By looking only at the script src we are limiting\n      // the search to only externally loaded JavaScript files.\n      // External files might be hosted on the same domain\n      // or hosted remotely\n      src, exists := script.Attr(\"src\")\n      if exists {\n         fmt.Println(src)\n      }\n\n      // script.Text() will contain the raw script text\n      // if the JavaScript code is written directly in the\n      // HTML source instead of loaded from a separate file\n   })\n} \n```", "```go\n// Crawl a website, depth-first, listing all unique paths found\npackage main\n\nimport (\n   \"fmt\"\n   \"github.com/PuerkitoBio/goquery\"\n   \"log\"\n   \"net/http\"\n   \"net/url\"\n   \"os\"\n   \"time\"\n)\n\nvar (\n   foundPaths  []string\n   startingUrl *url.URL\n   timeout     = time.Duration(8 * time.Second)\n)\n\nfunc crawlUrl(path string) {\n   // Create a temporary URL object for this request\n   var targetUrl url.URL\n   targetUrl.Scheme = startingUrl.Scheme\n   targetUrl.Host = startingUrl.Host\n   targetUrl.Path = path\n\n   // Fetch the URL with a timeout and parse to goquery doc\n   httpClient := http.Client{Timeout: timeout}\n   response, err := httpClient.Get(targetUrl.String())\n   if err != nil {\n      return\n   }\n   doc, err := goquery.NewDocumentFromReader(response.Body)\n   if err != nil {\n      return\n   }\n\n   // Find all links and crawl if new path on same host\n   doc.Find(\"a\").Each(func(i int, s *goquery.Selection) {\n      href, exists := s.Attr(\"href\")\n      if !exists {\n         return\n      }\n\n      parsedUrl, err := url.Parse(href)\n      if err != nil { // Err parsing URL. Ignore\n         return\n      }\n\n      if urlIsInScope(parsedUrl) {\n         foundPaths = append(foundPaths, parsedUrl.Path)\n         log.Println(\"Found new path to crawl: \" +\n            parsedUrl.String())\n         crawlUrl(parsedUrl.Path)\n      }\n   })\n}\n\n// Determine if path has already been found\n// and if it points to the same host\nfunc urlIsInScope(tempUrl *url.URL) bool {\n   // Relative url, same host\n   if tempUrl.Host != \"\" && tempUrl.Host != startingUrl.Host {\n      return false // Link points to different host\n   }\n\n   if tempUrl.Path == \"\" {\n      return false\n   }\n\n   // Already found?\n   for _, existingPath := range foundPaths {\n      if existingPath == tempUrl.Path {\n         return false // Match\n      }\n   }\n   return true // No match found\n}\n\nfunc main() {\n   // Load command line arguments\n   if len(os.Args) != 2 {\n      fmt.Println(\"Crawl a website, depth-first\")\n      fmt.Println(\"Usage: \" + os.Args[0] + \" <startingUrl>\")\n      fmt.Println(\"Example: \" + os.Args[0] + \n         \" https://www.devdungeon.com\")\n      os.Exit(1)\n   }\n   foundPaths = make([]string, 0)\n\n   // Parse starting URL\n   startingUrl, err := url.Parse(os.Args[1])\n   if err != nil {\n      log.Fatal(\"Error parsing starting URL. \", err)\n   }\n   log.Println(\"Crawling: \" + startingUrl.String())\n\n   crawlUrl(startingUrl.Path)\n\n   for _, path := range foundPaths {\n      fmt.Println(path)\n   }\n   log.Printf(\"Total unique paths crawled: %d\\n\", len(foundPaths))\n} \n```"]