- en: Web Scraping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬取
- en: Information gathering from the web can be useful for many situations. Websites
    can provide a wealth of information. The information can be used to help when
    performing a social engineering attack or a phishing attack. You can find names
    and emails for potential targets, or collect keywords and headers that can help
    to quickly understand the topic or business of a website. You can also potentially
    learn the location of the business, find images and documents, and analyze other
    aspects of a website using web scraping techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络中收集信息在许多情况下都是有用的。网站可以提供丰富的信息。这些信息可以用于在进行社会工程攻击或钓鱼攻击时提供帮助。您可以找到潜在目标的姓名和电子邮件，或者收集关键词和标题，这些可以帮助快速了解网站的主题或业务。您还可以通过网络爬取技术潜在地了解企业的位置，找到图像和文档，并分析网站的其他方面。
- en: Learning about the target allows you to create a believable pretext. Pretexting
    is a common technique attackers use to trick an unsuspecting victim into complying
    with a request that compromises the user, their account, or their machine in some
    kind of way. For example, someone researches a company and finds out that it is
    a large company with a centralized IT support department in a specific city. They
    can call or email people at the company, pretending to be a support technician,
    and ask them to perform actions or provide their password. Information from a
    company's public website can contain many details used to set up a pretexting
    situation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 了解目标可以让您创建一个可信的借口。借口是攻击者用来欺骗毫无戒心的受害者，使其遵从某种方式上损害用户、其账户或其设备的请求的常见技术。例如，有人调查一家公司，发现它是一家在特定城市拥有集中式IT支持部门的大公司。他们可以打电话或给公司的人发电子邮件，假装是支持技术人员，并要求他们执行操作或提供他们的密码。公司公共网站上的信息可能包含许多用于设置借口情况的细节。
- en: Web crawling is another aspect of scraping, which involves following hyperlinks
    to other pages. Breadth-first crawling refers to finding as many different websites
    as you can and following them to find more sites. Depth-first crawling refers
    to crawling a single site to find all pages possible before moving on to the next
    site.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Web爬行是爬取的另一个方面，它涉及跟随超链接到其他页面。广度优先爬行是指尽可能找到尽可能多的不同网站，并跟随它们以找到更多的站点。深度优先爬行是指在转移到下一个站点之前，爬取单个站点以找到所有可能的页面。
- en: In this chapter, we will cover web scraping and web crawling. We will walk you
    through examples of basic tasks such as finding links, documents, and images,
    looking for hidden files and information, and using a powerful third-party package
    named `goquery`. We will also discuss techniques for mitigating scraping of your
    own websites.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖网络爬取和网络爬行。我们将通过示例向您介绍一些基本任务，例如查找链接、文档和图像，寻找隐藏文件和信息，并使用一个名为`goquery`的强大的第三方包。我们还将讨论减轻对您自己网站的爬取的技术。
- en: 'In this chapter, we will specifically cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将具体涵盖以下主题：
- en: Web scraping fundamentals
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络爬取基础知识
- en: String matching
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串匹配
- en: Regular expressions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Extracting HTTP headers from a response
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从响应中提取HTTP头
- en: Using cookies
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用cookies
- en: Extracting HTML comments from a page
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从页面中提取HTML注释
- en: Searching for unlisted files on a web server
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Web服务器上搜索未列出的文件
- en: Modifying your user agent
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改您的用户代理
- en: Fingerprinting web applications and servers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指纹识别Web应用程序和服务器
- en: Using the goquery package
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用goquery包
- en: Listing all links in a page
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中的所有链接
- en: Listing all document links in a page
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中的所有文档链接
- en: Listing title and headings of a page
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面的标题和标题
- en: Calculating most common words used on a page
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算页面上使用最频繁的单词
- en: Listing all external JavaScript sources of a page
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出页面中所有外部JavaScript源
- en: Depth-first crawling
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度优先爬行
- en: Breadth-first crawling
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广度优先爬行
- en: Protecting against web scraping
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止网络爬取
- en: Web scraping fundamentals
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬取基础知识
- en: Web scraping, as used in this book, is the process of extracting information
    from an HTML-structured page that is intended to be viewed by a human and not
    consumed programmatically. Some services provide an API that is efficient for
    programmatic use, but some websites only provide their information in HTML pages.
    These web scraping examples demonstrate various ways of extracting information
    from HTML. We'll look at basic string matching, then regular expressions, and
    then a powerful package named `goquery`, for web scraping.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Web爬取，如本书中所使用的，是从HTML结构化页面中提取信息的过程，这些页面是为人类查看而不是以编程方式消费的。一些服务提供了高效的用于编程使用的API，但有些网站只提供他们的信息在HTML页面中。这些网络爬取示例演示了从HTML中提取信息的各种方法。我们将看一下基本的字符串匹配，然后是正则表达式，然后是一个名为`goquery`的强大包，用于网络爬取。
- en: Finding strings in HTTP responses with the strings package
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用strings包在HTTP响应中查找字符串
- en: To get started, let's look at making a basic HTTP request and searching for
    a string using the standard library. First, we will create `http.Client` and set
    any custom variables; for example, whether or not the client should follow redirects,
    what set of cookies it should use, or what transport to use.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们看一下如何进行基本的HTTP请求并使用标准库搜索字符串。首先，我们将创建`http.Client`并设置任何自定义变量；例如，客户端是否应该遵循重定向，应该使用哪组cookies，或者应该使用哪种传输。
- en: The `http.Transport` type implements the network request operations to perform
    the HTTP request and get a response. By default, `http.RoundTripper` is used,
    and this executes a single HTTP request. For the majority of use cases, the default
    transport is just fine. By default, the HTTP proxy from the environment is used,
    but the proxy can also be specified in the transport. This might be useful if
    you want to use multiple proxies. This example does not use a custom `http.Transport`
    type, but I wanted to highlight how `http.Transport` is an embedded type within
    `http.Client`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`http.Transport`类型实现了执行HTTP请求和获取响应的网络请求操作。默认情况下，使用`http.RoundTripper`，这执行单个HTTP请求。对于大多数用例，默认传输就足够了。默认情况下，使用环境中的HTTP代理，但也可以在传输中指定代理。如果要使用多个代理，这可能很有用。此示例不使用自定义的`http.Transport`类型，但我想强调`http.Transport`是`http.Client`中的嵌入类型。'
- en: We are creating a custom `http.Client` type, but only to override the `Timeout`
    field. By default, there is no timeout and an application could hang forever.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个自定义的 `http.Client` 类型，但只是为了覆盖 `Timeout` 字段。默认情况下，没有超时，应用程序可能会永远挂起。
- en: 'Another embedded type that can be overridden within `http.Client` is the `http.CookieJar`
    type. Two functions the `http.CookieJar` interface requires are: `SetCookies()`
    and `Cookies()`. The standard library comes with the `net/http/cookiejar` package,
    and it contains a default implementation of `CookieJar`. One use case for multiple
    cookie jars is to log in and store multiple sessions with a website. You can log
    in as many users, and store each session in a cookie jar and use each one, as
    needed. This example does not use a custom cookie jar.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 `http.Client` 中覆盖的另一种嵌入类型是 `http.CookieJar` 类型。`http.CookieJar` 接口需要的两个函数是：`SetCookies()`
    和 `Cookies()`。标准库附带了 `net/http/cookiejar` 包，并且其中包含了 `CookieJar` 的默认实现。多个 cookie
    jar 的一个用例是登录并存储与网站的多个会话。您可以登录多个用户，并将每个会话存储在一个 cookie jar 中，并根据需要使用每个会话。此示例不使用自定义
    cookie jar。
- en: HTTP responses contain the body as a reader interface. We can extract the data
    from the reader using any function that accepts a reader interface. This includes
    functions such as the `io.Copy()`, `io.ReadAtLeast()`, `io.ReadlAll()`, and `bufio`
    buffered readers. In this example, `ioutil.ReadAll()` is used to quickly store
    the full contents of the HTTP response into a byte-slice variable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 响应包含作为读取器接口的主体。我们可以使用接受读取器接口的任何函数从读取器中提取数据。这包括函数，如 `io.Copy()`、`io.ReadAtLeast()`、`io.ReadlAll()`
    和 `bufio` 缓冲读取器。在此示例中，`ioutil.ReadAll()` 用于快速将 HTTP 响应的全部内容存储到字节切片变量中。
- en: 'The following is the code implementation of this example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using regular expressions to find email addresses in a page
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式在页面中查找电子邮件地址
- en: A regular expression, or regex, is actually a form of language in its own right.
    Essentially, it is a special string that expresses a text search pattern. You
    may be familiar with the asterisk (`*`) when using a shell. Commands such as `ls
    *.txt` use a simple regular expression. The asterisk in this case represents *anything*;
    so any string would match as long as it ended with `.txt`. Regular expressions
    have other symbols besides the asterisk, like the period (`.`), which matches
    any single character as opposed to the asterisk, which will match a string of
    any length. There are even more powerful expressions that can be crafted with
    the handful of symbols that are available.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式，或者 regex，实际上是一种独立的语言形式。本质上，它是一个表达文本搜索模式的特殊字符串。在使用 shell 时，您可能熟悉星号（`*`）。诸如
    `ls *.txt` 的命令使用简单的正则表达式。在这种情况下，星号代表*任何东西*；因此只要以 `.txt` 结尾，任何字符串都会匹配。正则表达式除了星号之外还有其他符号，比如句号（`.`），它匹配任何单个字符，而不是星号，星号将匹配任意长度的字符串。甚至可以使用少量可用的符号来构建更强大的表达式。
- en: Regular expressions have a reputation for being slow. The implementation used
    is guaranteed to run in linear time, as opposed to exponential time, based on
    the input length. This means it will run faster than many other implementations
    of regular expressions that do not provide that guarantee, such as Perl. Russ
    Cox, one of Go's authors, published a deep comparison of the two different approaches
    in 2007, which is available at [https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html).
    This is very important for our use case of searching the contents of an HTML page.
    If the regular expression ran in exponential time, based on the input length,
    it could take quite literally years to perform a search of certain expressions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式以慢而著称。所使用的实现保证以线性时间运行，而不是基于输入长度的指数时间。这意味着它将比许多其他不提供该保证的正则表达式实现运行得更快，比如
    Perl。Go 的作者之一 Russ Cox 在 2007 年发表了两种不同方法的深度比较，可在[https://swtch.com/~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html)上找到。这对于我们搜索
    HTML 页面内容的用例非常重要。如果正则表达式基于输入长度运行时间呈指数增长，可能需要很长时间才能执行某些表达式的搜索。
- en: Learn more about regular expressions in general from [https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)
    and the relevant Go documentation at [https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从[https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression)和相关的
    Go文档[https://golang.org/pkg/regexp/](https://golang.org/pkg/regexp/)中了解更多关于正则表达式的一般知识。
- en: This example uses a regular expression that searches for email address links
    embedded in HTML. It will search for any `mailto` links and extract the email
    address. We'll use the default HTTP client and call `http.Get()` instead of creating
    a custom client to modify the timeout.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用正则表达式搜索嵌入在 HTML 中的电子邮件地址链接。它将搜索任何 `mailto` 链接并提取电子邮件地址。我们将使用默认的 HTTP 客户端，并调用
    `http.Get()`，而不是创建自定义客户端来修改超时。
- en: 'A typical email link looks like one of these:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的电子邮件链接看起来像这样：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The regular expression used is in this example is this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例中使用的正则表达式是：
- en: '`"mailto:.*?["?]`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`"mailto:.*?["?]`'
- en: 'Let''s break this down and examine each part:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解并检查每个部分：
- en: '`"mailto:`: This whole piece is just a string literal. The first character
    is a quotation mark (`"`) and has no special meaning in the regular expression.
    It is treated like as a regular character. This means that the regex will begin
    by searching for a quotation mark character first. After the quotation mark is
    the text `mailto` with a colon (`:`). The colon has no special meaning either.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"mailto:`：整个片段只是一个字符串文字。第一个字符是引号（`"`），在正则表达式中没有特殊含义。它被视为普通字符。这意味着正则表达式将首先搜索引号字符。引号后面是文本
    `mailto` 和一个冒号（`:`）。冒号也没有特殊含义。'
- en: '`.*?`: The period (`.`) means match any character except a newline. The asterisk
    means continue matching based on the previous symbol (the period) for zero or
    more characters. Directly after the asterisk, is a question mark (`?`). This question
    mark tells the asterisk to be non-greedy. It will match the shortest string possible.
    Without it, the asterisk will continue to match as long as possible, while still
    satisfying the full regular expression. We only want the email address itself
    and not any query parameters such as `?subject`, so we are telling it to do a
    non-greedy or short match.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.*?`：句点（`.`）表示匹配除换行符以外的任何字符。星号表示基于前一个符号（句点）继续匹配零个或多个字符。在星号之后，是一个问号（`?`）。这个问号告诉星号不要贪婪。它将匹配可能的最短字符串。没有它，星号将继续匹配尽可能长的字符串，同时仍满足完整的正则表达式。我们只想要电子邮件地址本身，而不是任何查询参数，比如`?subject`，所以我们告诉它进行非贪婪或短匹配。'
- en: '`["?]`: The last piece of the regular expression is the `["?]` set. The brackets
    tell the regex to match any character encapsulated by the brackets. We only have
    two characters: the quotation mark and the question mark. The question mark here
    has no special meaning and is treated as a regular character. The two characters
    inside the brackets are the two possible characters that deliminate the end of
    the email address. By default, the regex would go with whichever one came last
    and return the longest string possible because the asterisk that preceded it would
    have been greedy. However, because we added the other question mark in the previous
    section directly after the asterisk, it will perform a non-greedy search and stop
    at the first thing that matches a character inside the brackets.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`["?]`：正则表达式的最后一部分是`["?]`集合。括号告诉正则表达式匹配括号内封装的任何字符。我们只有两个字符：引号和问号。这里的问号没有特殊含义，被视为普通字符。括号内的两个字符是电子邮件地址结束的两个可能字符。默认情况下，正则表达式将选择最后一个字符并返回最长的字符串，因为前面的星号会变得贪婪。然而，因为我们在前一节直接在星号后面添加了另一个问号，它将执行非贪婪搜索，并在第一个匹配括号内的字符的地方停止。'
- en: Using this technique means that we will only find emails that are explicitly
    linked using an `<a>` tag in the HTML. It will not find emails that are just written
    as plaintext in the page. Creating a regular expression to search for an email
    string based on a pattern such as `<word>@<word>.<word>` may seem simple, but
    the nuances between different regular expression implementations and the complex
    variations that emails can have make it difficult to craft a regular expression
    that catches all valid email combinations. If you do a quick search online for
    an example, you will see how many variations there are and how complex they get.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术意味着我们只会找到在HTML中使用`<a>`标签明确链接的电子邮件。它不会找到在页面中以纯文本形式编写的电子邮件。创建一个正则表达式来搜索基于模式的电子邮件字符串，比如`<word>@<word>.<word>`，可能看起来很简单，但不同正则表达式实现之间的细微差别以及电子邮件可能具有的复杂变化使得很难制定一个能捕捉到所有有效电子邮件组合的正则表达式。如果您快速在网上搜索一个示例，您会看到有多少变化以及它们变得多么复杂。
- en: If you are creating some kind of web service it is important to verify a person's
    email account by sending them an email and having them respond or verify with
    a link in some way. I do not recommend that you ever rely solely on a regular
    expression to determine if an email is valid, and I also recommend that you be
    extremely careful about using regular expressions to perform client-side email
    validation. A user may have a weird email address that is technically valid and
    you may prevent them from signing up to your service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在创建某种网络服务，重要的是通过发送电子邮件并要求他们以某种方式回复或验证链接来验证一个人的电子邮件帐户。我不建议您仅仅依赖正则表达式来确定电子邮件是否有效，我还建议您在使用正则表达式执行客户端电子邮件验证时要非常小心。用户可能有一个在技术上有效的奇怪电子邮件地址，您可能会阻止他们注册到您的服务。
- en: 'Here are some examples of email addresses that are actually valid according
    to *RFC 822* from 1982:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是根据1982年*RFC 822*实际有效的电子邮件地址的一些示例：
- en: '`*.*@example.com`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*.*@example.com`'
- en: '`$what^the.#!$%@example.com`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$what^the.#!$%@example.com`'
- en: '`!#$%^&*=()@example.com`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`!#$%^&*=()@example.com`'
- en: '`"!@#$%{}^&~*()|/="@example.com`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"!@#$%{}^&~*()|/="@example.com`'
- en: '`"hello@example.com"@example.com`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"hello@example.com"@example.com`'
- en: In 2001, *RFC 2822* replaced *RFC 822*. Out of all the preceding examples, only
    the last two containing an at (`@`) symbol are considered invalid by the newer
    *RFC 2822*. All of the other examples are still valid. Read the original RFCs
    at [https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)
    and [https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，*RFC 2822*取代了*RFC 822*。在所有先前的示例中，只有最后两个包含at（`@`）符号的示例被新的*RFC 2822*认为是无效的。所有其他示例仍然有效。在[https://www.ietf.org/rfc/rfc822.txt](https://www.ietf.org/rfc/rfc822.txt)和[https://www.ietf.org/rfc/rfc2822.txt](https://www.ietf.org/rfc/rfc2822.txt)上阅读原始RFC。
- en: 'The following is the code implementation of this example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该示例的代码实现：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Extracting HTTP headers from an HTTP response
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从HTTP响应中提取HTTP标头
- en: 'HTTP headers contain metadata and descriptive information about the request
    and response. You can potentially learn a lot about a server by inspecting the
    HTTP headers it serves with a response. You can learn the following things about
    the server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP标头包含有关请求和响应的元数据和描述信息。通过检查服务器提供的HTTP标头，您可以潜在地了解有关服务器的很多信息。您可以了解服务器的以下信息：
- en: Caching system
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存系统
- en: Authentication
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份验证
- en: Operating system
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统
- en: Web server
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web服务器
- en: Response type
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应类型
- en: Framework or content management system
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架或内容管理系统
- en: Programming language
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编程语言
- en: Spoken language
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 口头语言
- en: Security headers
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全标头
- en: Cookies
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cookies
- en: Not every web server will return all of those headers, but it is helpful to
    learn as much as you can from the headers. Popular frameworks such as WordPress
    and Drupal will return an `X-Powered-By` header telling you whether it is WordPress
    or Drupal and what version.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个网络服务器都会返回所有这些标头，但从标头中尽可能多地学习是有帮助的。流行的框架，如WordPress和Drupal，将返回一个`X-Powered-By`标头，告诉您它是WordPress还是Drupal以及版本。
- en: The session cookie can give away a lot of information too. A cookie named `PHPSESSID`
    tells you it is most likely a PHP application. Django's default session cookie
    is named `sessionid`, that of Java is `JSESSIONID`, and the session cookie of
    Ruby on Rail follows the `_APPNAME_session` pattern. You can use these clues to
    fingerprint web servers. If you only want the headers and don't need the whole
    body of a page, you can always use the HTTP `HEAD` method instead of HTTP `GET`.
    The `HEAD` method will return only headers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 会话cookie也可以透露很多信息。名为`PHPSESSID`的cookie告诉您它很可能是一个PHP应用程序。Django的默认会话cookie的名称是`sessionid`，Java的是`JSESSIONID`，Ruby
    on Rail的会话cookie遵循`_APPNAME_session`的模式。您可以使用这些线索来识别Web服务器。如果您只想要头部而不需要页面的整个主体，您可以始终使用HTTP
    `HEAD`方法而不是HTTP `GET`。`HEAD`方法将只返回头部。
- en: 'This example makes a `HEAD` request to a URL and prints out all of its headers.
    The `http.Response` type contains a map of strings to strings named `Header`,
    which contain the key-value pair for each HTTP header:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子对URL进行了一个`HEAD`请求，并打印出了它的所有头部。`http.Response`类型包含一个名为`Header`的字符串到字符串的映射，其中包含每个HTTP头的键值对：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Setting cookies with an HTTP client
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HTTP客户端设置cookie
- en: Cookies are an essential component of modern web applications. Cookies are sent
    back and forth between the client and server as HTTP headers. Cookies are just
    text key-value pairs that are stored by the browser client. They are used to store
    persistent data on the client. They can be used to store any text value, but are
    commonly used to store preferences, tokens, and session information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Cookie是现代Web应用程序的一个重要组成部分。Cookie作为HTTP头在客户端和服务器之间来回发送。Cookie只是由浏览器客户端存储的文本键值对。它们用于在客户端上存储持久数据。它们可以用于存储任何文本值，但通常用于存储首选项、令牌和会话信息。
- en: Session cookies usually store a token that matches the token the server has.
    When a user logs in, the server creates a session with an identifying token tied
    to that user. The server then sends the token back to the user in the form of
    a cookie. When the client sends the session token in the form of a cookie, the
    server looks and finds a matching token in the session store, which may be a database,
    a file, or in memory. The session token requires sufficient entropy to ensure
    that it is unique and attackers cannot guess it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 会话cookie通常存储与服务器相匹配的令牌。当用户登录时，服务器会创建一个带有与该用户相关联的标识令牌的会话。然后，服务器以cookie的形式将令牌发送回给用户。当客户端以cookie的形式发送会话令牌时，服务器会查找并在会话存储中找到匹配的令牌，这可能是数据库、文件或内存中。会话令牌需要足够的熵来确保它是唯一的，攻击者无法猜测。
- en: If a user is on a public Wi-Fi network and visits a website that does not use
    SSL, anyone nearby can see the HTTP requests in plaintext. An attacker could steal
    the session cookie and use it in their own requests. When a cookie is sidejacked
    in this fashion, the attacker can impersonate the victim. The server will treat
    them as the already logged in user. The attacker may never learn the password
    and does not need to.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户在公共Wi-Fi网络上，并访问一个不使用SSL的网站，附近的任何人都可以看到明文的HTTP请求。攻击者可以窃取会话cookie并在自己的请求中使用它。当以这种方式sidejacked
    cookie时，攻击者可以冒充受害者。服务器将把他们视为已登录的用户。攻击者可能永远不会知道密码，也不需要知道。
- en: For this reason, it can be useful to log out of websites occasionally and destroy
    any active sessions. Some websites allow you to manually destroy all active sessions.
    If you run a web service, I recommend that you set a reasonable expiration time
    for sessions. Bank websites do a good job of this usually enforcing a short 10-15
    minute expiration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定期注销网站并销毁任何活动会话可能是有用的。一些网站允许您手动销毁所有活动会话。如果您运行一个Web服务，我建议您为会话设置合理的过期时间。银行网站通常做得很好，通常强制执行短暂的10-15分钟过期时间。
- en: There is a `Set-Cookie` header that a server sends to the client when creating
    a new cookie. The client then sends the cookies back to the server using the `Cookie`
    header.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器在创建新cookie时向客户端发送一个`Set-Cookie`头。然后客户端使用`Cookie`头将cookie发送回服务器。
- en: 'Here is a simple example of cookie headers sent from a server:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务器发送的cookie头的一个简单示例：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is an example header from a client:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自客户端的一个示例头部：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are other attributes that a cookie can contain, such as the `Secure` and
    `HttpOnly` flags discussed in [Chapter 9](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml),
    *Web Applications*. Other attributes include an expiration date, a domain, and
    a path. This example is only presenting the simplest application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Cookie还可以包含其他属性，例如在[第9章](f15910a1-239e-49a5-b4d9-3881a524bfa9.xhtml)中讨论的`Secure`和`HttpOnly`标志，*Web应用程序*。其他属性包括到期日期、域和路径。这个例子只是展示了最简单的应用程序。
- en: In this example, a simple request is made with a custom session cookie. The
    session cookie is what allows you to be *logged in* when making a request to a
    website. This example should serve as a reference for how to make a request with
    a cookie and not a standalone tool. First, the URL is defined just before the
    `main` function. Then, the HTTP request is created first with the HTTP `GET` method
    specified. A nil body is provided since `GET` requests generally don't require
    a body. The new request is then updated with a new header, the cookie. In this
    example, `session_id` is the name of the session cookie, but that will vary depending
    on the web application being interacted with.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，使用自定义会话cookie进行了一个简单的请求。会话cookie是在向网站发出请求时允许您*登录*的东西。这个例子应该作为如何使用cookie发出请求的参考，而不是一个独立的工具。首先，在`main`函数之前定义URL。然后，首先创建HTTP请求，指定HTTP
    `GET`方法。由于`GET`请求通常不需要主体，因此提供了一个空主体。然后，使用一个新的头部，cookie，更新新的请求。在这个例子中，`session_id`是会话cookie的名称，但这将取决于正在交互的Web应用程序。
- en: Once the request is prepared, an HTTP client is created to actually make the
    request and process the response. Note that the HTTP request and the HTTP client
    are separate and independent entities. For example, you can reuse a request multiple
    times, use a request with different clients, and use multiple requests with a
    single client. This allows you to create multiple request objects with different
    session cookies if you need to manage multiple client sessions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求准备好，就会创建一个HTTP客户端来实际发出请求并处理响应。请注意，HTTP请求和HTTP客户端是独立的实体。例如，您可以多次重用一个请求，使用不同的客户端使用一个请求，并使用单个客户端进行多个请求。这允许您创建多个具有不同会话cookie的请求对象，如果需要管理多个客户端会话。
- en: 'The following is the code implementation of this example:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finding HTML comments in a web page
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中查找HTML注释
- en: HTML comments can sometimes hold amazing pieces of information. I have personally
    seen websites with the admin username and password in HTML comments. I have also
    seen an entire menu commented out, but the links still worked and could be reached
    directly. You never know what kind of information a careless developer might leave
    behind.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HTML注释有时可能包含惊人的信息。我个人见过在HTML注释中包含管理员用户名和密码的网站。我还见过整个菜单被注释掉，但链接仍然有效，可以直接访问。您永远不知道一个粗心的开发人员可能留下什么样的信息。
- en: If you are going to leave comments in your code, it is always ideal to leave
    them in the server-side code and not in the client-facing HTML and JavaScript.
    Comment in the PHP, Ruby, Python, or whatever backend code you have. You never
    want to give the client more information than they need in the code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要在代码中留下评论，最好将它们留在服务器端代码中，而不是在面向客户端的HTML和JavaScript中。在PHP、Ruby、Python或其他后端代码中进行注释。您永远不希望在代码中向客户端提供比他们需要的更多信息。
- en: 'The regular expression used in this program consists of a few special sequences.
    Here is the full regular expression. It essentially says, "match anything between
    the `<!--` and `-->` strings." Let''s examine it piece by piece:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序中使用的正则表达式由几个特殊序列组成。以下是完整的正则表达式。它基本上是说，“匹配`<!--`和`-->`之间的任何内容。”让我们逐个检查它：
- en: '`<!--(.|\n)*?-->`: The beginning and the end start with `<!--` and `-->`, which
    are the designations for opening and closing an HTML comment. Those are plain
    characters and not special characters to the regular expression.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<!--(.|\n)*?-->`：开头和结尾分别是`<!--`和`-->`，这是HTML注释的开始和结束标记。这些是普通字符，而不是正则表达式的特殊字符。'
- en: '`(.|\n)*?`: This can be broken down into two pieces:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(.|\n)*?`：这可以分解为两部分：'
- en: '`(.|\n)`: The first part has a few special characters. The parentheses, `()`,
    enclose a set of options. The pipe, `|`, separates the options. The options themselves
    are the dot, `.`, and the newline character, `\n`. The dot means match any character,
    except a newline. Because an HTML comment can span multiple lines, we want to
    match any character, including a newline character. The whole piece, `(.|\n)`
    means match the dot or a newline character.'
  id: totrans-95
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(.|\n)`：第一部分有一些特殊字符。括号`()`括起一组选项。管道`|`分隔选项。选项本身是点`.`和换行字符`\n`。点表示匹配任何字符，除了换行符。因为HTML注释可以跨多行，我们希望匹配任何字符，包括换行符。整个部分`(.|\n)`表示匹配点或换行符。'
- en: '`*?`: The asterisk means continue matching the previous character or expression
    zero or more times. Immediately preceding the asterisk is the set of parentheses,
    so it will continue trying to match `(.|\n)`. The question mark tells the asterisk
    to be non-greedy, or return the smallest match possible. Without the question
    mark, to designate it as non-greedy; it will match the largest thing possible,
    which means it will start at the beginning of the first comment in the page, and
    end at the ending of the very last comment in the page, including everything in
    between.'
  id: totrans-96
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*?`：星号表示继续匹配前一个字符或表达式零次或多次。紧接在星号之前的是括号集，因此它将继续尝试匹配`(.|\n)`。问号告诉星号是非贪婪的，或者返回可能的最小匹配。没有问号，以指定它为非贪婪；它将匹配可能的最大内容，这意味着它将从页面中第一个注释的开头开始，并在页面中最后一个注释的结尾结束，包括中间的所有内容。'
- en: Try running this program against some websites and see what kind of HTML comments
    you find. You might be surprised at what kind of information you can uncover.
    For example, the MailChimp signup forms come with an HTML comment that actually
    gives you tips on bypassing the bot signup prevention. The MailChimp signup form
    uses a honeypot field that should not be filled out or it assumes the form was
    submitted by a bot. See what you can find.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行此程序针对一些网站，并查看您能找到什么样的HTML注释。您可能会对您能发现的信息感到惊讶。例如，MailChimp注册表单附带了一个HTML注释，实际上为您提供了绕过机器人注册预防的提示。MailChimp注册表单使用了一个蜜罐字段，不应该填写，否则它会假定该表单是由机器人提交的。看看您能找到什么。
- en: 'This example will first fetch the URL provided, then use the regular expression
    we walked through earlier to search for HTML comments. Every match found is then
    printed out to standard output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例首先获取提供的URL，然后使用我们之前讨论过的正则表达式搜索HTML注释。然后将找到的每个匹配打印到标准输出：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finding unlisted files on a web server
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网络服务器上查找未列出的文件
- en: There is a popular program called DirBuster, which penetration testers use for
    finding unlisted files. DirBuster is an OWASP project that comes preinstalled
    on Kali, the popular penetration testing Linux distribution. With nothing but
    the standard library, we can create a fast, concurrent, and simple clone of DirBuster
    with just a few lines. More information about DirBuster is available at [https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为DirBuster的流行程序，渗透测试人员用于查找未列出的文件。DirBuster是一个OWASP项目，预装在流行的渗透测试Linux发行版Kali上。只需使用标准库，我们就可以创建一个快速、并发和简单的DirBuster克隆，只需几行代码。有关DirBuster的更多信息，请访问[https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project](https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project)。
- en: This program is a simple clone of DirBuster that searches for unlisted files
    based on a word list. You will have to create your own word list. A small list
    of example filenames will be provided here to give you some ideas and to use as
    a starting list. Build your list of files based on your own experience and based
    on the source code. Some web applications have files with specific names that
    will allow you to fingerprint which framework is being used. Also look for backup
    files, configuration files, version control files, changelog files, private keys,
    application logs, and anything else that is not intended to be public. You can
    also find prebuilt word lists on the internet, including DirBuster's lists.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序是DirBuster的一个简单克隆，它基于一个单词列表搜索未列出的文件。你将不得不创建自己的单词列表。这里提供了一小部分示例文件名，以便给你一些想法，并用作起始列表。根据你自己的经验和源代码构建你的文件列表。一些Web应用程序有特定名称的文件，这将允许你指纹识别使用的框架。还要寻找备份文件、配置文件、版本控制文件、更改日志文件、私钥、应用程序日志以及任何不打算公开的东西。你也可以在互联网上找到预先构建的单词列表，包括DirBuster的列表。
- en: 'Here is a sample list of files that you could search for:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个你可以搜索的文件的示例列表：
- en: '`.gitignore`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.gitignore`'
- en: '`.git/HEAD`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.git/HEAD`'
- en: '`id_rsa`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id_rsa`'
- en: '`debug.log`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug.log`'
- en: '`database.sql`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`database.sql`'
- en: '`index-old.html`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index-old.html`'
- en: '`backup.zip`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backup.zip`'
- en: '`config.ini`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config.ini`'
- en: '`settings.ini`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.ini`'
- en: '`settings.php.bak`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.php.bak`'
- en: '`CHANGELOG.txt`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CHANGELOG.txt`'
- en: This program will search a domain with the provided word list and report any
    files that do not return a 404 NOT FOUND response. The word list should have filenames
    separated with a newline and have one filename per line. When providing the domain
    name as a parameter, the trailing slash is optional, and the program will behave
    properly with or without the trailing slash on the domain name. The protocol must
    be specified though, so that the request knows whether to use HTTP or HTTPS.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序将使用提供的单词列表搜索一个域，并报告任何没有返回404 NOT FOUND响应的文件。单词列表应该用换行符分隔文件名，并且每行一个文件名。在提供域名作为参数时，尾随斜杠是可选的，程序将在有或没有域名尾随斜杠的情况下正常运行。但是协议必须被指定，这样请求才知道是使用HTTP还是HTTPS。
- en: The `url.Parse()` function is used to create a proper URL object. With the URL
    type, you can independently modify `Path` without modifying `Host` or `Scheme`.
    This provides an easy way to update the URL without resorting to manual string
    manipulation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`url.Parse()`函数用于创建一个正确的URL对象。使用URL类型，你可以独立修改`Path`而不修改`Host`或`Scheme`。这提供了一种简单的方法来更新URL，而不必求助于手动字符串操作。'
- en: 'To read the file line by line, a scanner is used. By default, scanners split
    by newlines, but they can be overridden by calling `scanner.Split()` and providing
    a custom split function. We use the default behavior since the words are expected
    to be provided on separate lines:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了逐行读取文件，使用了一个scanner。默认情况下，scanner按照换行符分割，但可以通过调用`scanner.Split()`并提供自定义分割函数来覆盖。我们使用默认行为，因为单词应该是在单独的行上提供的。
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Changing the user agent of a request
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改请求的用户代理
- en: A common technique to block scrapers and crawlers is to block certain user agents.
    Some services will blacklist certain user agents that contain keywords such as `curl`
    and `python`. You can get around most of these by simply changing your user agent
    to `firefox`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的阻止爬虫和网络爬虫的技术是阻止特定的用户代理。一些服务会把包含关键词如`curl`和`python`的特定用户代理列入黑名单。你可以通过简单地将你的用户代理更改为`firefox`来绕过大部分这些限制。
- en: To set the user agent, you must first create the HTTP request object. The header
    must be set before making the actual request. This means that you can't use the
    shortcut convenience functions such as `http.Get()`. We have to create the client
    and then create a request, and then use the client to `client.Do()` the request.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置用户代理，你必须首先创建HTTP请求对象。在实际请求之前必须设置头部。这意味着你不能使用`http.Get()`等快捷便利函数。我们必须创建客户端，然后创建一个请求，然后使用客户端来`client.Do()`请求。
- en: This example creates an HTTP request with `http.NewRequest()`, and then modifies
    the request headers to override the `User-Agent` header. You can use this to hide,
    fake, or be honest. To be a good web citizen, I recommend that you create a unique
    user agent for your crawler so that webmasters can throttle or block your bot.
    I also recommend that you include a website or email address in the user agent
    so that webmasters can request to be skipped by your scraper.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用`http.NewRequest()`创建了一个HTTP请求，然后修改请求头来覆盖`User-Agent`头部。你可以用这个来隐藏、伪装或者诚实。为了成为一个良好的网络公民，我建议你为你的爬虫创建一个独特的用户代理，这样网站管理员可以限制或者阻止你的机器人。我还建议你在用户代理中包含一个网站或者电子邮件地址，这样网站管理员可以请求跳过你的爬虫。
- en: 'The following is the code implementation of this example:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个例子的代码实现：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fingerprinting web application technology stacks
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指纹识别Web应用程序技术栈
- en: Fingerprinting a web application is when you try to identify the technology
    that is being used to serve a web application. Fingerprinting can be done at several
    levels. At a lower level, HTTP headers can give clues to what operating system,
    such as Windows or Linux, and what web server, such as Apache or nginx, is running.
    The headers may also give information about the programming language or framework
    being used at the application level. At a higher level, the web application can
    be fingerprinted to identify which JavaScript libraries are being used, whether
    any analytics platforms are being included, any ad networks are being displayed,
    the caching layers in use, and other information. We will first look at the HTTP
    headers, and then cover more complex methods of fingerprinting.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别Web应用程序是指尝试识别用于提供Web应用程序的技术。指纹识别可以在几个级别进行。在较低级别，HTTP头可以提供关于正在运行的操作系统（如Windows或Linux）和Web服务器（如Apache或nginx）的线索。头部还可以提供有关应用程序级别使用的编程语言或框架的信息。在较高级别，Web应用程序可以被指纹识别以确定正在使用哪些JavaScript库，是否包括任何分析平台，是否显示任何广告网络，正在使用的缓存层等信息。我们将首先查看HTTP头部，然后涵盖更复杂的指纹识别方法。
- en: Fingerprinting is a critical step in an attack or penetration test because it
    helps narrow down options and determine which paths to take. Identifying what
    technologies are being used also lets you search for known vulnerabilities. If
    a web application is not kept up to date, a simple fingerprinting and vulnerability
    search may be all that is needed for finding and exploiting an already-known vulnerability.
    If nothing else, it helps you learn about the target.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别是攻击或渗透测试中的关键步骤，因为它有助于缩小选项并确定要采取的路径。识别正在使用的技术还让您可以搜索已知的漏洞。如果一个Web应用程序没有及时更新，简单的指纹识别和漏洞搜索可能就足以找到并利用已知的漏洞。如果没有其他办法，它也可以帮助您了解目标。
- en: Fingerprinting based on HTTP response headers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于HTTP响应头的指纹识别
- en: I recommend that you inspect the HTTP headers first since they are simple key-value
    pairs, and generally, there are only a few returned with each request. It doesn't
    take very long to go through the headers manually, so you can inspect them first
    before moving on to the application. Fingerprinting at the application level is
    more complicated and we'll talk about that in a moment. Earlier in this chapter,
    there was a section about extracting HTTP headers and printing them out for inspection
    (the *Extracting HTTP headers from an HTTP response* section). You can use that
    program to dump the headers of different web pages and see what you can find.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您首先检查HTTP头，因为它们是简单的键值对，通常每个请求只返回几个。手动浏览头部不会花费太长时间，所以您可以在继续应用程序之前首先检查它们。应用程序级别的指纹识别更加复杂，我们稍后会谈论这个。在本章的前面，有一个关于提取HTTP头并打印它们以供检查的部分（*从HTTP响应中提取HTTP头*部分）。您可以使用该程序来转储不同网页的头部并查看您能找到什么。
- en: The basic idea is simple. Look for keywords. Some headers in particular contain
    the most obvious clues, such as the `X-Powered-By`, `Server`, and `X-Generator`
    headers. The `X-Powered-By` header can contain the name of the framework or **Content
    Management System** (**CMS**) being used, such as WordPress or Drupal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想很简单。寻找关键字。特别是一些头部包含最明显的线索，例如`X-Powered-By`、`Server`和`X-Generator`头部。`X-Powered-By`头部可以包含正在使用的框架或**内容管理系统**（**CMS**）的名称，例如WordPress或Drupal。
- en: There are two basic steps to examining the headers. First, you need to get the
    headers. Use the example provided earlier in this chapter for extracting HTTP
    headers. The second step is to do a string search to look for the keywords. You
    can use `strings.ToUpper()` and `strings.Contains()` to search directly for keywords,
    or use regular expressions. Refer to the earlier examples in this chapter that
    explain how to use regular expressions. Once you are able to search through the
    headers, you just need to be able to generate the list of keywords to search for.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 检查头部有两个基本步骤。首先，您需要获取头部。使用本章前面提供的示例来提取HTTP头。第二步是进行字符串搜索以查找关键字。您可以使用`strings.ToUpper()`和`strings.Contains()`直接搜索关键字，或者使用正则表达式。请参考本章前面的示例，了解如何使用正则表达式。一旦您能够搜索头部，您只需要能够生成要搜索的关键字列表。
- en: 'There are many keywords you can look for. What you search for will depend on
    what you are looking for. I''ll try to cover several broad categories to give
    you ideas on what to look for. The first thing you can try to identify is the
    operating system that the host is running. Here is a sample list of keywords that
    you can find in HTTP headers to indicate the operating system:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多关键字可以搜索。您搜索的内容将取决于您要寻找的内容。我将尝试涵盖几个广泛的类别，以便给您一些寻找内容的想法。您可以尝试识别的第一件事是主机正在运行的操作系统。以下是一个示例关键字列表，您可以在HTTP头部中找到，以指示操作系统：
- en: '`Linux`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Linux`'
- en: '`Debian`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Debian`'
- en: '`Fedora`'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fedora`'
- en: '`Red Hat`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Red Hat`'
- en: '`CentOS`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CentOS`'
- en: '`Ubuntu`'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ubuntu`'
- en: '`FreeBSD`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FreeBSD`'
- en: '`Win32`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Win32`'
- en: '`Win64`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Win64`'
- en: '`Darwin`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Darwin`'
- en: 'Here is a list of keywords that will help you determine which web server is
    being used. This is by no means an exhaustive list, but does cover several keywords
    that will yield results if you search the internet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关键字，可以帮助您确定正在使用哪种Web服务器。这绝不是一个详尽的列表，但涵盖了几个关键字，如果您在互联网上搜索，将会产生结果：
- en: '`Apache`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Apache`'
- en: '`Nginx`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nginx`'
- en: '`Microsoft-IIS`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Microsoft-IIS`'
- en: '`Tomcat`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tomcat`'
- en: '`WEBrick`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WEBrick`'
- en: '`Lighttpd`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lighttpd`'
- en: '`IBM HTTP Server`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IBM HTTP Server`'
- en: 'Determining which programming language is being used can make a big difference
    in your attack choices. Scripted languages such as PHP are vulnerable to different
    things than a Java server or an ASP.NET application. Here are a few sample keywords
    you can use to search in HTTP headers to identify which language is powering an
    application:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 确定正在使用的编程语言可以在攻击选择上产生很大的影响。像PHP这样的脚本语言对不同的东西都是脆弱的，与Java服务器或ASP.NET应用程序不同。以下是一些示例关键字，您可以使用它们在HTTP头中搜索，以确定哪种语言支持应用程序：
- en: '`Python`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Python`'
- en: '`Ruby`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ruby`'
- en: '`Perl`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Perl`'
- en: '`PHP`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PHP`'
- en: '`ASP.NET`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ASP.NET`'
- en: 'Session cookies are also big giveaways as to what framework or language is
    being used. For example, `PHPSESSID` indicates PHP and `JSESSIONID` indicates
    Java. Here are a few session cookies you can search for:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 会话cookie也是确定使用的框架或语言的重要线索。例如，`PHPSESSID`表示PHP，`JSESSIONID`表示Java。以下是一些会话cookie，您可以搜索：
- en: '`PHPSESSID`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PHPSESSID`'
- en: '`JSESSIONID`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JSESSIONID`'
- en: '`session`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`session`'
- en: '`sessionid`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sessionid`'
- en: '`CFID/CFTOKEN`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CFID/CFTOKEN`'
- en: '`ASP.NET_SessionId`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ASP.NET_SessionId`'
- en: Fingerprinting web applications
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指纹识别Web应用程序
- en: Fingerprinting web applications in general covers a much broader scope than
    just looking at the HTTP headers. You can do basic keyword searches in the HTTP
    headers, as just discussed, and learn a lot, but there is also a wealth of information
    in the HTML source code and the contents, or simply the existence, of other files
    on the server.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，指纹识别Web应用程序涵盖的范围要比仅查看HTTP头部要广泛得多。您可以在HTTP头部中进行基本的关键字搜索，就像刚才讨论的那样，并且可以学到很多，但是在HTML源代码和服务器上的其他文件的内容或简单存在中也有大量信息。
- en: In the HTML source code, you can look for clues such as the structure of pages
    themselves and the names of classes and IDs of HTML elements. AngularJS applications
    have distinct HTML attributes, such as `ng-app`, that can be used as keywords
    for fingerprinting. Angular is also generally included with a `script` tag, the
    same way other frameworks such as jQuery are included. The `script` tags can also
    be inspected for other clues. Look for things such as Google Analytics, AdSense,
    Yahoo ads, Facebook, Disqus, Twitter, and other third-party JavaScript embedded.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML源代码中，你可以寻找一些线索，比如页面本身的结构以及HTML元素的类和ID的名称。AngularJS应用程序具有独特的HTML属性，比如`ng-app`，可以用作指纹识别的关键词。Angular通常也包含在`script`标签中，就像其他框架如jQuery一样。`script`标签也可以被检查以寻找其他线索。寻找诸如Google
    Analytics、AdSense、Yahoo广告、Facebook、Disqus、Twitter和其他第三方嵌入的JavaScript等内容。
- en: Simply looking at the file extensions in URLs can tell you what language is
    being used. For example, `.php`, `.jsp`, and `.asp` indicate that PHP, Java, and
    ASP are being used, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过URL中的文件扩展名就可以告诉你正在使用的是什么语言。例如，`.php`、`.jsp`和`.asp`分别表示正在使用PHP、Java和ASP。
- en: We also looked at a program that finds HTML comments in a web page. Some frameworks
    and CMSes leave an identifiable footer or hidden HTML comment. Sometimes the marker
    is in the form of a small image.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了一个在网页中查找HTML注释的程序。一些框架和CMS会留下可识别的页脚或隐藏的HTML注释。有时标记以小图片的形式出现。
- en: Directory structure can also be another giveaway. It requires familiarity with
    different frameworks first. For example, Drupal stores site information in a directory
    called `/sites/default`. If you attempt to visit that URL and you get a 403 FORBIDDEN
    response and not a 404 NOT FOUND error, you likely found a Drupal-based website.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 目录结构也可以是另一个线索。首先需要熟悉不同的框架。例如，Drupal将站点信息存储在一个名为`/sites/default`的目录中。如果你尝试访问该URL，并且收到403
    FORBIDDEN的响应而不是404 NOT FOUND错误，那么你很可能找到了一个基于Drupal的网站。
- en: Look for files such as `wp-cron.php`. In the *Finding unlisted files on a web
    server* section, we looked at finding unlisted files with the DirBuster clone.
    Find a list of unique files that you can use to fingerprint web applications and
    add them to your word list. You can figure out which files to look for by inspecting
    the code bases for different web frameworks. For example, the source code for
    WordPress and Drupal are publicly available. Use the program discussed earlier
    in this chapter for finding unlisted files to search for files. Other unlisted
    files that you can search for are related to documentation, such as `CHANGELOG.txt`,
    `readme.txt`, `readme.md`, `readme.html`, `LICENSE.txt`, `install.txt`, or `install.php`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找诸如`wp-cron.php`之类的文件。在*在Web服务器上查找未列出的文件*部分，我们研究了使用DirBuster克隆来查找未列出的文件。找到一组可以用来指纹识别Web应用程序的唯一文件，并将它们添加到你的单词列表中。你可以通过检查不同Web框架的代码库来确定要查找哪些文件。例如，WordPress和Drupal的源代码是公开可用的。使用本章早些时候讨论的用于查找未列出文件的程序来搜索文件。你还可以搜索与文档相关的其他未列出的文件，比如`CHANGELOG.txt`、`readme.txt`、`readme.md`、`readme.html`、`LICENSE.txt`、`install.txt`或`install.php`。
- en: It is possible to get even more detail out of a web application by fingerprinting
    the version of an application that is running. It is much easier if you have access
    to the source code. I will use WordPress as an example since is it so ubiquitous
    and the source is available on GitHub at [https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指纹识别正在运行的应用程序的版本，可以更详细地了解Web应用程序。如果你可以访问源代码，这将更容易。我将以WordPress为例，因为它是如此普遍，并且源代码可以在GitHub上找到[https://github.com/WordPress/WordPress](https://github.com/WordPress/WordPress)。
- en: The goal is to find the differences between versions. WordPress is a good example
    because they all come with the `/wp-admin/` directory that contains all the administrative
    interfaces. Inside `/wp-admin/`, there are the `css` and `js` folders with style
    sheets and scripts in them, respectively. These files are publicly accessible
    when a site is hosted on a server. Use the `diff` command on these folders to
    identify which versions introduce new files, which versions remove files, and
    which versions modify existing files. With all that information combined, you
    can generally narrow down applications to a specific version or to at least a
    small range of versions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找出版本之间的差异。WordPress是一个很好的例子，因为它们都带有包含所有管理界面的`/wp-admin/`目录。在`/wp-admin/`目录中，有`css`和`js`文件夹，分别包含样式表和脚本。当网站托管在服务器上时，这些文件是公开可访问的。对这些文件夹使用`diff`命令，以确定哪些版本引入了新文件，哪些版本删除了文件，哪些版本修改了现有文件。将所有这些信息结合起来，通常可以将应用程序缩小到特定版本，或者至少缩小到一小范围的版本。
- en: 'As a contrived example, let''s say version 1.0 contains only one file: `main.js`.
    Version 1.1 introduces a second file: `utility.js`. Version 1.3 removes both of
    those and replaces them with a single file: `master.js`. You can make HTTP requests
    to the web server for all three files: `main.js`, `utility.js`, and `master.js`.
    Based on which files are found with a 200 OK error and which files return a 404
    NOT FOUND error, you can determine which version is running.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 举个假设的例子，假设版本1.0只包含一个文件：`main.js`。版本1.1引入了第二个文件：`utility.js`。版本1.3删除了这两个文件，并用一个文件`master.js`替换了它们。你可以向Web服务器发出HTTP请求获取这三个文件：`main.js`、`utility.js`和`master.js`。根据哪些文件返回200
    OK错误，哪些文件返回404 NOT FOUND错误，你可以确定正在运行的是哪个版本。
- en: If the same files are present across multiple versions, you can inspect deeper
    into the contents of the files. Either do a byte-by-byte comparison or hash the
    files and compare the checksums. Hashing and examples of hashing are covered in [Chapter
    6](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml), *Cryptography*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果相同的文件存在于多个版本中，你可以深入检查文件的内容。可以进行逐字节比较或对文件进行哈希处理并比较校验和。哈希处理和哈希处理的示例在[第6章](f68073f0-8cc8-40b5-af0e-795ce30e5271.xhtml)
    *密码学*中有介绍。
- en: Sometimes, identifying the version can be much simpler than that whole process
    just described. Sometimes there is a `CHANGELOG.txt` or `readme.html` file that
    will tell you exactly which version is running without having to do any work.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，识别版本可能比刚才描述的整个过程简单得多。有时会有一个`CHANGELOG.txt`或`readme.html`文件，它会告诉您确切地运行的是哪个版本，而无需做任何工作。
- en: How to prevent fingerprinting of your applications
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何防止您的应用程序被指纹识别
- en: As demonstrated earlier, there are multiple ways to fingerprint applications
    at many different levels of the technology stack. The first question you should
    really ask yourself is, "Do I need to prevent fingerprinting?" In general, trying
    to prevent fingerprinting is a form of obfuscation. Obfuscation is a bit controversial,
    but I think everyone does agree that obfuscation is not security in the same way
    that encoding is not encryption. It may slow down, limit information, or confuse
    an attacker temporarily, but it does not truly prevent any vulnerability from
    being exploited. Now, I'm not saying that there is no benefit at all from obfuscation,
    but it can never be relied on by itself. Obfuscation is simply a thin layer of
    concealment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所示，有多种方法可以在技术堆栈的许多不同级别上对应用程序进行指纹识别。您真正应该问自己的第一个问题是，“我需要防止指纹识别吗？”一般来说，试图防止指纹识别是一种混淆形式。混淆有点具有争议性，但我认为每个人都同意混淆不是加密的安全性。它可能会暂时减慢、限制信息或使攻击者困惑，但它并不能真正防止利用任何漏洞。现在，我并不是说混淆根本没有好处，但它永远不能单独依赖。混淆只是一层薄薄的掩饰。
- en: Obviously, you don't want to give away too much information about your application,
    such as debug output or configuration settings, but some information is going
    to be available no matter what when a service is available on the network. You
    will have to make a choice about how much time and effort you want to put into
    hiding information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，您不希望透露太多关于您的应用程序的信息，比如调试输出或配置设置，但无论如何，当服务在网络上可用时，一些信息都将可用。您将不得不在隐藏信息方面做出选择，需要投入多少时间和精力。
- en: Some people go as far as outputting false information to mislead attackers.
    Personally, putting out fake headers is not on my checklist of things to do when
    hardening a server. One thing I recommend that you do is to remove any extra files
    as mentioned earlier. Files such as changelog files, default setting files, installation
    files, and documentation files should all be removed before deployment. Don't
    publicly serve the files that are not required for the application to work.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人甚至会输出错误信息来误导攻击者。就我个人而言，在加固服务器时，输出虚假标头并不在我的清单上。我建议您做的一件事是在部署之前删除任何额外的文件，就像之前提到的那样。在部署之前，应删除诸如更改日志文件、默认设置文件、安装文件和文档文件等文件。不要公开提供不需要应用程序工作的文件。
- en: Obfuscation is a topic that warrants its own chapter or even its own book. There
    are obfuscation competitions dedicated to awarding the most creative and bizarre
    forms of obfuscation. There are some tools that help you obfuscate JavaScript
    code, but on the flip side, there are also deobfuscation tools.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆是一个值得单独章节甚至单独一本书的话题。有一些专门颁发最有创意和奇异的混淆形式的混淆竞赛。有一些工具可以帮助您混淆JavaScript代码，但另一方面也有反混淆工具。
- en: Using the goquery package for web scraping
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用goquery包进行网络抓取
- en: The `goquery` package is not part of the standard library, but is available
    on GitHub. It is intended to work similar to jQuery—a popular JavaScript framework
    for interacting with the HTML DOM. As demonstrated in the previous sections, trying
    to search with string matching and regular expressions is both tedious and complicated.
    The `goquery` package makes it much easier to work with HTML content and search
    for specific elements. The reason I suggest this package is because it is modelled
    after the very popular jQuery framework that many people are already familiar
    with.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`goquery`包不是标准库的一部分，但可以在GitHub上找到。它旨在与jQuery类似——这是一个用于与HTML DOM交互的流行JavaScript框架。正如前面所示，尝试使用字符串匹配和正则表达式进行搜索既繁琐又复杂。`goquery`包使得处理HTML内容和搜索特定元素变得更加容易。我建议这个包的原因是它是基于非常流行的jQuery框架建模的，许多人已经熟悉它。'
- en: 'You can get the `goquery` package with the `go get` command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`go get`命令获取`goquery`包：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The documentation is available at [https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文档可在[https://godoc.org/github.com/PuerkitoBio/goquery](https://godoc.org/github.com/PuerkitoBio/goquery)找到。
- en: Listing all hyperlinks in a page
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出页面中的所有超链接
- en: 'For the introduction to the `goquery` package, we''ll look at a common and
    simple task. We will find all hyperlinks in a page and print them out. A typical
    link looks something like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍`goquery`包时，我们将看一个常见且简单的任务。我们将找到页面中的所有超链接并将它们打印出来。典型的链接看起来像这样：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In HTML, the `a` tag stands for **anchor** and the `href` attribute stands
    for **hyperlink reference**. It is possible to have an anchor tag with no `href`
    attribute but only a `name` attribute. These are called bookmarks, or named anchors,
    and are used to jump to a location on the same page. We will ignore these since
    they only link within the same page. The `target` attribute is just an optional
    one specifying which window or tab to open the link in. We are only interested
    in the `href` value for this example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTML中，`a`标签代表**锚点**，`href`属性代表**超链接引用**。可能会有一个没有`href`属性但只有`name`属性的锚标签。这些被称为书签，或命名锚点，用于跳转到同一页面上的位置。我们将忽略这些，因为它们只在同一页面内链接。`target`属性只是一个可选项，用于指定在哪个窗口或选项卡中打开链接。在这个例子中，我们只对`href`值感兴趣：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finding documents in a web page
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中查找文档
- en: Documents are also points of interest. You might want to scrape a web page and
    look for documents. Word processor documents, spreadsheets, slideshow decks, CSV,
    text, and other files can contain useful information for a variety of purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 文档也是感兴趣的点。您可能希望抓取一个网页并查找文档。文字处理器文档、电子表格、幻灯片演示文稿、CSV、文本和其他文件可能包含各种目的的有用信息。
- en: The following example will search through a URL and search for documents based
    on the file extensions in the links. A global variable is defined at the top for
    convenience with the list of all extensions that should be searched for. Customize
    the list of extensions to search for your target file types. Consider extending
    the application to take a list of file extensions in from a file instead of being
    hardcoded. What other file extensions would you look for when trying to find sensitive
    information?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将通过URL搜索并根据链接中的文件扩展名搜索文档。在顶部定义了一个全局变量，方便列出应搜索的所有扩展名。自定义要搜索的扩展名列表以搜索目标文件类型。考虑扩展应用程序以从文件中获取文件扩展名列表，而不是硬编码。在尝试查找敏感信息时，您会寻找哪些其他文件扩展名？
- en: 'The following is the code implementation of this example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此示例的代码实现：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Listing page title and headings
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 列出页面标题和标题
- en: Headings are the primary structural elements that define the hierarchy of a
    web page, with `<h1>` being the top level and `<h6>` being the lowest or deepest
    level of the hierarchy. The title, defined in the `<title>` tag, of an HTML page
    is what gets displayed in the browser title bar, and it is not part of the rendered
    page.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 标题是定义网页层次结构的主要结构元素，`<h1>`是最高级别，`<h6>`是最低级别。在HTML页面的`<title>`标签中定义的标题是显示在浏览器标题栏中的内容，它不是渲染页面的一部分。
- en: By listing the title and headings, you can quickly get an idea of what the topic
    of the page is, assuming that they properly formatted their HTML. There is only
    supposed to be one `<title>` and one `<h1>` tag, but not everyone conforms to
    the standards.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过列出标题和标题，您可以快速了解页面的主题是什么，假设它们正确格式化了他们的HTML。应该只有一个`<title>`和一个`<h1>`标签，但并非每个人都符合标准。
- en: 'This program loads a web page and then prints the title and all headings to
    standard output. Try running this program against a few URLs and see whether you
    are able to get a quick idea of the contents just by looking at the headings:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序加载网页，然后将标题和所有标题打印到标准输出。尝试运行此程序针对几个URL，并查看是否能够通过查看标题快速了解内容：
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Crawling pages on the site that store the most common words
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取存储最常见单词的站点上的页面
- en: This program prints out a list of all the words used on a web page along with
    the count of how many times each word appeared in the page. This will search all
    paragraph tags. If you search the whole body, it will treat all the HTML code
    as words, which clutters the data and does not really help you understand the
    content of the site. It trims the spaces, commas, periods, tabs, and newlines
    from strings. It also converts all words to lowercase in an attempt to normalize
    the data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序打印出网页上使用的所有单词列表，以及每个单词在页面上出现的次数。这将搜索所有段落标签。如果搜索整个正文，它将将所有HTML代码视为单词，这会使数据混乱，并且实际上并不帮助您了解站点的内容。它会修剪字符串中的空格、逗号、句号、制表符和换行符。它还会尝试将所有单词转换为小写以规范化数据。
- en: 'For each paragraph it finds, it will split the text contents apart. Each word
    is stored in a map that maps the string to an integer count. In the end, the map
    is printed out, listing each word and how many times it was seen on the page:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于它找到的每个段落，它将拆分文本内容。每个单词存储在将字符串映射到整数计数的映射中。最后，映射被打印出来，列出了每个单词以及在页面上看到了多少次：
- en: '[PRE15]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Printing a list of external JavaScript files in a page
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在页面中打印外部JavaScript文件列表
- en: Inspecting the URLs of JavaScript files that are included on a page can help
    if you are trying to fingerprint an application or determine what third-party
    libraries are being loaded. This program will list the external JavaScript files
    referenced in a web page. External JavaScript files might be hosted on the same
    domain, or might be loaded from a remote site. It inspects the `src` attribute
    of all the `script` tags.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 检查包含在页面上的JavaScript文件的URL可以帮助您确定应用程序的指纹或确定加载了哪些第三方库。此程序将列出网页中引用的外部JavaScript文件。外部JavaScript文件可能托管在同一域上，也可能从远程站点加载。它检查所有`script`标签的`src`属性。
- en: 'For example, if an HTML page had the following tag:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果HTML页面具有以下标签：
- en: '[PRE16]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The URL of the `src` attribute is what would be printed:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`src`属性的URL将被打印：'
- en: '[PRE17]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that URLs in the `src` attribute may be fully qualified or relative URLs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`src`属性中的URL可能是完全限定的或相对URL。
- en: 'The following program loads a URL and then looks for all the `script` tags.
    It will print the `src` attribute for each script it finds. This will only look
    for scripts that are linked externally. To print inline scripts, refer to the
    comment at the bottom of the file regarding `script.Text()`. Try running this
    against some websites you visit frequently and see how many external and third-party
    scripts they embed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下程序加载URL，然后查找所有`script`标签。它将打印它找到的每个脚本的`src`属性。这将仅查找外部链接的脚本。要打印内联脚本，请参考文件底部关于`script.Text()`的注释。尝试运行此程序针对您经常访问的一些网站，并查看它们嵌入了多少外部和第三方脚本：
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This example looks for external scripts referenced by the `src` attribute, but
    some scripts are written directly in the HTML between the opening and closing
    `script` tags. These types of inline script won't have a `src` attribute referencing.
    Get inline script text using the `.Text()` function on the `goquery` object. Refer
    to the bottom of this example, where `script.Text()` is mentioned.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例查找由`src`属性引用的外部脚本，但有些脚本直接在HTML中的开放和关闭`script`标签之间编写。这些类型的内联脚本不会有引用`src`属性。使用`goquery`对象上的`.Text()`函数获取内联脚本文本。请参考此示例底部，其中提到了`script.Text()`。
- en: The reason this program does not print out the inline scripts and instead focuses
    only on the externally loaded scripts is because that is where a lot of vulnerabilities
    are introduced. Loading remote JavaScript is risky and should be done with trusted
    sources only. Even then, we don't get 100% assurance that the remote content provider
    will never be compromised and serve malicious code. Consider a large corporation
    such as Yahoo! who has acknowledged publicly that their systems have been compromised
    in the past. Yahoo! also has an ad network that hosts a **Content Delivery Network**
    (**CDN**) that serves JavaScript files to a large network of websites. This would
    be a prime target for attackers. Consider these risks when including remote JavaScript
    files in a sensitive customer portal.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序不打印内联脚本，而是只关注外部加载的脚本，因为那是引入许多漏洞的地方。加载远程JavaScript是有风险的，应该只能使用受信任的来源。即使如此，我们也不能百分之百保证远程内容提供者永远不会被入侵并提供恶意代码。考虑雅虎这样的大公司，他们公开承认他们的系统过去曾受到入侵。雅虎还有一个托管**内容传送网络**（**CDN**）的广告网络，为大量网站提供JavaScript文件。这将是攻击者的主要目标。在包含远程JavaScript文件时，请考虑这些风险。
- en: Depth-first crawling
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度优先爬行
- en: Depth-first crawling is when you prioritize links on the same domain over links
    that lead to other domains. In this program, external links are completely ignored,
    and only paths on the same domain or relative links are followed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 深度优先爬行是指优先考虑相同域上的链接，而不是指向其他域的链接。在这个程序中，外部链接完全被忽略，只有相同域上的路径或相对链接被跟踪。
- en: In this example, unique paths are stored in a slice and printed all together
    at the end. Any errors encountered during the crawl are ignored. Errors are encountered
    often due to malformed links, and we don't want the whole program to exit on errors
    like that.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，唯一的路径被存储在一个切片中，并在最后一起打印出来。在爬行过程中遇到的任何错误都会被忽略。由于链接格式不正确，经常会遇到错误，我们不希望整个程序在这样的错误上退出。
- en: Instead of trying to parse URLs manually using string functions, the `url.Parse()`
    function is utilized. It does the work of splitting apart the host from the path.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不要试图使用字符串函数手动解析URL，而是利用`url.Parse()`函数。它会将主机与路径分开。
- en: 'When crawling, any query strings and fragments are ignored to reduce duplicates.
    Query strings are designated with the question mark in the URL, and fragments,
    also called bookmarks, are designated with the pound or hash sign. This program
    is single-threaded and does not use goroutines:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬行时，忽略任何查询字符串和片段以减少重复。查询字符串在URL中用问号标记，片段，也称为书签，用井号标记。这个程序是单线程的，不使用goroutines：
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Breadth-first crawling
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广度优先爬行
- en: Breadth-first crawling is when priority is given to finding new domains and
    spreading out as far as possible, as opposed to continuing through a single domain
    in a depth-first manner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 广度优先爬行是指优先考虑查找新域并尽可能扩展，而不是以深度优先的方式继续通过单个域。
- en: Writing a breadth-first crawler will be left as an exercise for the reader based
    on the information provided in this chapter. It is not very different from the
    depth-first crawler in the previous section, except that it should prioritize
    URLs that point to domains that have not been seen before.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个广度优先爬行器将根据本章提供的信息留给读者作为练习。它与上一节的深度优先爬行器并没有太大的不同，只是应该优先考虑指向以前未见过的域的URL。
- en: There are a couple of notes to keep in mind. If you're not careful and you don't
    set a maximum limit, you could potentially end up crawling petabytes of data!
    You might choose to ignore subdomains, or you can enter a site that has infinite
    subdomains and you will never leave.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要记住。如果不小心，不设置最大限制，你可能最终会爬行宠字节的数据！你可能选择忽略子域，或者你可以进入一个具有无限子域的站点，你永远不会离开。
- en: How to protect against web scraping
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何防止网页抓取
- en: It is difficult, if not impossible, to completely prevent web scraping. If you
    serve the information from the web server, there will be a way to extract the
    data programmatically somehow. There are only hurdles you can put in the way.
    It amounts to obfuscation, which you could argue is not worth the effort.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全防止网页抓取是困难的，甚至是不可能的。如果你从Web服务器提供信息，总会有一种方式可以以编程方式提取数据。你只能设置障碍。这相当于混淆，你可以说这不值得努力。
- en: JavaScript makes it more difficult, but not impossible since Selenium can drive
    real web browsers, and frameworks such as PhantomJS can be used to execute the
    JavaScript.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript使得这更加困难，但并非不可能，因为Selenium可以驱动真实的Web浏览器，而像PhantomJS这样的框架可以用来执行JavaScript。
- en: Requiring authentication can help limit the amount of scraping done. Rate limiting
    can also provide some relief. Rate limiting can be done using tools such as iptables
    or done at the application level, based on the IP address or user session.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 需要身份验证可以帮助限制抓取的数量。速率限制也可以提供一些缓解。可以使用诸如iptables之类的工具进行速率限制，也可以在应用程序级别进行，基于IP地址或用户会话。
- en: Checking the user agent provided by the client is a shallow measure, but can
    help a bit. Discard requests that come with user agents that include keywords
    such as `curl`, `wget`, `go`, `python`, `ruby`, and `perl`. Blocking or ignoring
    these requests can prevent simple bots from scraping your site, but the client
    can fake or omit their user agent so that it is easy to bypass.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 检查客户端提供的用户代理是一个浅显的措施，但可以有所帮助。丢弃带有关键字的用户代理的请求，如`curl`，`wget`，`go`，`python`，`ruby`和`perl`。阻止或忽略这些请求可以防止简单的机器人抓取您的网站，但客户端可以伪造或省略他们的用户代理，以便轻松绕过。
- en: If you want to take it even further, you can make the HTML ID and class names
    dynamic so that they can't be used to find specific information. Change your HTML
    structure and naming frequently to play the *cat-and-mouse* game to make it more
    work than it is worth for the scraper. This is not a real solution, and I wouldn't
    recommend it, but it is worth mentioning, as it is annoying in the eyes of the
    scraper.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更进一步，你可以使HTML的ID和类名动态化，这样它们就不能用来查找特定信息。经常改变你的HTML结构和命名，玩起*猫鼠游戏*，让爬虫的工作变得不值得。这并不是一个真正的解决方案，我不建议这样做，但是值得一提，因为这会让爬虫感到恼火。
- en: You can use JavaScript to check information about the client, such as screen
    size, before presenting data. If the screen size is 1 x 1 or 0 × 0, or something
    strange, you can assume that it is a bot and refuse to render content.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用JavaScript来检查关于客户端的信息，比如屏幕尺寸，在呈现数据之前。如果屏幕尺寸是1 x 1或0 × 0，或者是一些奇怪的尺寸，你可以假设这是一个机器人，并拒绝呈现内容。
- en: Honeypot forms are another method of detecting bot behavior. Hide form fields
    with CSS or a `hidden` attribute, and check whether values have been provided
    in those fields. If data is in these fields, assume that a bot is filling out
    all the fields and ignore the request.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 蜜罐表单是检测机器人行为的另一种方法。使用CSS或`hidden`属性隐藏表单字段，并检查这些字段是否提供了值。如果这些字段中有数据，就假设是机器人在填写所有字段并忽略该请求。
- en: Another option is to use images to store information instead of text. For example,
    if you output only the image of a pie chart, it is much more difficult for someone
    to scrape the data than when you output the data as a JSON object and have JavaScript
    render the pie chart. The scraper can grab the JSON data directly. Text can be
    placed in images as well to prevent text from being scraped and to prevent keyword
    text searches, but **Optical Character Recognition** (**OCR**) can get around
    that with some extra effort.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用图像来存储信息而不是文本。例如，如果你只输出一个饼图的图像，对于某人来说要爬取数据就会更加困难，而当你将数据输出为JSON对象并让JavaScript渲染饼图时，情况就不同了。爬虫可以直接获取JSON数据。文本也可以放在图像中，以防止文本被爬取和防止关键字文本搜索，但是**光学字符识别**（**OCR**）可以通过一些额外的努力来解决这个问题。
- en: Depending on the application, some of the preceding techniques can be useful.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序，前面提到的一些技术可能会有用。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Having read this chapter, you should now understand the fundamentals of web
    scraping, such as performing an HTTP `GET` request and searching for a string
    using string matching or regular expressions to find HTML comments, emails, and
    other keywords. You should also understand how to extract the HTTP headers and
    set custom headers to set cookies and custom user agent strings. Moreover, you
    should understand the basic concepts of fingerprinting and have some idea of how
    to gather information about a web application based on the source code provided.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你现在应该了解了网络爬虫的基础知识，比如执行HTTP `GET`请求和使用字符串匹配或正则表达式查找HTML注释、电子邮件和其他关键字。你还应该了解如何提取HTTP头并设置自定义头以设置cookie和自定义用户代理字符串。此外，你应该了解指纹识别的基本概念，并对如何根据提供的源代码收集有关Web应用程序的信息有一些想法。
- en: Having worked through this chapter, you should also understand the basics of
    using the `goquery` package to find HTML elements in the DOM in a jQuery style.
    You should feel comfortable finding links in a web page, finding documents, listing
    title and headers, finding JavaScript files, and finding the difference between
    breadth-first and depth-first crawling.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这一章的学习，你应该也了解了使用`goquery`包在DOM中以jQuery风格查找HTML元素的基础知识。你应该能够轻松地在网页中找到链接，找到文档，列出标题和标题，找到JavaScript文件，并找到广度优先和深度优先爬取之间的区别。
- en: A note about scraping public websites—be respectful. Don't produce unreasonable
    amounts of traffic to websites by sending huge batches or letting a crawler go
    uninhibited. Set reasonable rate limits and maximum page count limits on programs
    you write as to not overburden remote servers. If you are scraping for data, always
    check to see if an API is available instead. APIs are much more efficient and
    intended to be used programmatically.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于爬取公共网站的一点说明——要尊重。不要通过发送大批量请求或让爬虫不受限制地运行来给网站带来不合理的流量。在你编写的程序中设置合理的速率限制和最大页面计数限制，以免过度拖累远程服务器。如果你是为了获取数据而进行爬取，请始终检查是否有API可用。API更高效，旨在以编程方式使用。
- en: Can you think of any other way to apply the tools examined in this chapter?
    Can you think of any additional features you can add to the examples provided?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到其他应用本章中所讨论的工具的方式吗？你能想到可以添加到提供的示例中的其他功能吗？
- en: In the next chapter, we will look at the methods of host discovery and enumeration.
    We will cover things such as TCP sockets, proxies, port scanning, banner grabbing,
    and fuzzing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨主机发现和枚举的方法。我们将涵盖诸如TCP套接字、代理、端口扫描、横幅抓取和模糊测试等内容。
