<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introducing Docker</h1>
                </header>
            
            <article>
                
<p>Before we go any further with this book, we need to look at a little thing called Docker, before we begin don't forget to clone the example code repository <a href="https://github.com/building-microservices-with-go/chapter3.git">https://github.com/building-microservices-with-go/chapter3.git</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Containers with Docker</h1>
                </header>
            
            <article>
                
<p>Docker is a platform that has risen to prominence in the last three years; it was born out of the desire to simplify the process of building, shipping, and running applications. Docker is not the inventor of the container, <span>Jacques Gélinas created the VServer project back in 2001, and since then the other main projects have been LXC from IBM and rkt from CoreOS.</span></p>
<p>If you would like to read more about the history, then I recommend this excellent blog post by Redhat: <a href="http://rhelblog.redhat.com/2015/08/28/the-history-of-containers">http://rhelblog.redhat.com/2015/08/28/the-history-of-containers</a>, this section is going to concentrate on Docker which is by far the most popular current technology.</p>
<p><span>The concept of a container is process isolation and application packaging. To quote Docker:</span></p>
<div class="packt_quote"><span><span>A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings.<br/>
...<br/>
Containers isolate software from its surroundings, for example, differences between development and staging environments and help reduce conflicts between teams running different software on the same infrastructure.</span></span></div>
<p>Where they benefit application development is that we can take advantage of this when deploying these applications as it allows us to pack them closer together, saving on hardware resources.</p>
<p>From a development and test lifecycle, containers give us the capability to run production code on our development machines with no complicated setup; it also allows us to create that <kbd>Clean Room</kbd> environment without having different instances of the same database installed to trial new software.</p>
<p>Containers have become the primary choice for packaging microservices, and as we progress through the examples in this book, you will learn how invaluable it is to your workflow.</p>
<p>Containers work by isolating processes and filesystems from each other. Unless explicitly specified, containers cannot access each other's file systems. They also cannot interact with one another via TCP or UDP sockets unless again specified.</p>
<p>Docker is made up of many parts; however, at its core is the Docker Engine, a lightweight application runtime with features for orchestration, scheduling networking, and security. Docker Engine can be installed anywhere on a physical or virtual host, and it supports both Windows and Linux. Containers allow developers to package large or small amounts of code and their dependencies together into an isolated package.</p>
<p> </p>
<p>We can also draw from a huge array of pre-created images, just about all software vendors from MySQL to IBM's WebSphere have an official image that is available for us to use.</p>
<p>Docker also uses Go, in fact nearly all of the code that goes into the Docker Engine and other applications are written in Go.</p>
<p>Rather than write an essay on how Docker works, let's examine each of the features by example. By the end of this chapter, we will take one of the simple examples that we created in <a href="ba3a8742-94e7-4e47-8a47-1324a277a7f9.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Microservices</em>, and create a Docker image for it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Docker</h1>
                </header>
            
            <article>
                
<p>Head over to <a href="https://docs.docker.com/engine/installation/"><span class="URLPACKT">https://docs.docker.com/engine/installation/</span></a> and install the correct version of Docker on your machine. You will find versions for Mac, Windows, and Linux.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running our first container</h1>
                </header>
            
            <article>
                
<p>To validate Docker has been installed correctly, let's run our first container, <strong>hello-world</strong> is actually an image, an image is an immutable snapshot of a container. Once we start these with the following command they become containers, think of it like types and instances, a type defines fields and methods making up behavior. An instance is a living instantiation of this type, you can assign other types to the fields and call the methods to perform actions.</p>
<pre>
<strong>$ docker run --rm hello-world</strong>  
</pre>
<p>The first thing you should see is:</p>
<pre>
<strong>Unable to find image 'hello-world:latest' locally</strong><br/><strong>latest: Pulling from library/hello-world</strong><br/><strong>c04b14da8d14: Pull complete </strong><br/><strong>Digest: sha256:0256e8a36e2070f7bf2d0b0763dbabdd67798512411de4cdcf9431a1feb60fd9</strong><br/><strong>Status: Downloaded newer image for hello-world:latest</strong>
</pre>
<p>When you execute a <kbd>docker run</kbd> the first thing the engine does is check to see if you have the image installed locally. If it doesn't then it connects to the default registry, in this case, <a href="https://hub.docker.com/">https://hub.docker.com/</a> to retrieve it.</p>
<p>Once the image has been downloaded, the daemon can create a container from the downloaded image, all the output is streamed to the output on your terminal:</p>
<pre>
<strong>Hello from Docker!</strong><br/><strong>This message shows that your installation appears to be working correctly.</strong>  
</pre>
<p class="mce-root">The <kbd>--rm</kbd> flag tells the Docker engine to remove the container and delete any resources such as volumes it was using on exit. Unless we would like to re-start a container at some point it is good practice to use the <kbd>--rm</kbd> flag to keep our filesystem clean, otherwise, all of the temporary volumes which are created will sit around and consume space.Let<span>'s try something a little more complicated, this time, we will start a container and create a shell inside of it to show how you can navigate to the internal file system. Execute the following command in your terminal:</span></p>
<pre>
<strong>$ docker run -it --rm alpine:latest sh</strong>  
</pre>
<p>Alpine is a lightweight version of Linux and is perfect for running Go applications. The <kbd>-it</kbd> flags stand for <strong>interactive terminal</strong> it maps the standard in from your terminal to the input of the running container. The <kbd>sh</kbd> statement after the name of the image we want to run is the name of the command we would like to execute in the container when it starts.</p>
<p>If all went well, you should now be inside a shell of the container. If you check the current directory by executing the <kbd>ls</kbd> command, you will see the following, which hopefully is not the directory you were in before running the command:</p>
<pre>
<strong>bin      etc      lib      media    proc     run      srv      tmp      var</strong><br/><strong>dev      home     linuxrc  mnt      root     sbin     sys      usr</strong>
</pre>
<p>This is the root folder of the newly started container, containers are immutable, so any changes you make to the file system in a running container is disposed of when the container is stopped. While this may seem to be a problem, there are solutions for persisting data, which we will look at in a little bit, however, for now, the important concept to remember is that:</p>
<div class="packt_quote">"Containers are immutable instances of images, and the data volumes are by default non-persistent"</div>
<p>You need to remember this when designing your services, to illustrate how this works take a look at this simple example.</p>
<p>Open another terminal and execute the following command:</p>
<pre>
<strong>$ docker ps</strong>  
</pre>
<p>You should see the following output:</p>
<pre>
<strong>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</strong><br/><strong>43a1bea0009e        alpine:latest       "sh"                6 minutes ago       Up 6 minutes                            tiny_galileo</strong>
</pre>
<p>The <kbd>docker ps</kbd> command queries the engine and returns a list of the containers, by default this only shows the running containers, however, if we add the <kbd>-a</kbd> flag we can also see stopped containers.</p>
<p>The Alpine Linux container that we started earlier is currently running, so jump back to your previous terminal window and create a file in the root file system:</p>
<pre>
<strong>$ touch mytestfile.txt</strong>  
</pre>
<p>If we list the directory structure again, we can see that a file has been created in the root of the file system:</p>
<pre>
<strong>bin             lib             mytestfile.txt  sbin            usr</strong><br/><strong>dev             linuxrc         proc            srv             var</strong><br/><strong>etc             media           root            sys</strong><br/><strong>home            mnt             run             tmp</strong>  
</pre>
<p>Now exit the container using the <kbd>exit</kbd> command and run <kbd>docker ps</kbd> again, you should see the following output:</p>
<pre>
<strong>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</strong>  
</pre>
<p>If we add the <kbd>-a</kbd> flag command to see stopped containers too, we should see the container we started earlier:</p>
<pre>
<strong>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES</strong><br/><strong>518c8ae7fc94        alpine:latest       "sh"                5 seconds ago       Exited (0) 2 seconds ago                       pensive_perlman</strong>  
</pre>
<p>Now, start another container again using the <kbd>docker run</kbd> command and list the directory contents in the root folder.</p>
<p>No <kbd>mytestfile.txt</kbd> right? The reason this does not exist is because of the principle we were discussing earlier, which I think is important to mention again as if this is the first time you have used Docker it will catch you out:</p>
<div class="packt_quote">"Containers are immutable instances of images, and the data volumes are by default non-persistent."</div>
<p>There is something worth noting, however, unless you explicitly remove a container it will persist in a stopped state on the Docker host.</p>
<p>Removing containers is important to remember for two reasons; the first is that if you do not remember this, you will fill up the disk on your host quickly as every time you create a container Docker will allocate space on the host for the container volumes. The second is that the container can be restarted.</p>
<p>Restarted that sounds cool, in fact, it is a handy feature, not something you should use in your production environment, for that you need to remember the golden rule and design your application accordingly:</p>
<div class="packt_quote">"Containers are immutable instances of images, and the data volumes are by default non-persistent."</div>
<p>However, the use of Docker extends far beyond simply running applications for your microservices. It is an awesome way to manage your development dependencies without cluttering up your development machine. We will look at that a little later on, but for now, we are interested in how we can restart a stopped container.</p>
<p>If we execute the <kbd>docker ps -a</kbd> command, we will see that we now have two stopped containers. The oldest one is the first container we started to which we added our <kbd>mytestfile.txt</kbd>. This is the one we want to restart, so grab the ID of the container and execute the following command:</p>
<pre>
<strong>$ docker start -it [container_id] sh</strong>  
</pre>
<p>Again, you should be in a shell at the root of the container if you check the directory contents what do you think you will find?</p>
<p>That's right, <kbd>mytestfile.txt</kbd>; this is because when you restarted the container, the engine remounted the volumes that were attached the first time you ran the command. These are the same volumes you mutated to add the file as mentioned earlier.</p>
<p>So we can restart our container; however, I just want to repeat the golden rule one last time:</p>
<div class="packt_quote">"Containers are immutable instances of images, and the data volumes are by default non-persistent."</div>
<p>When running in a production environment, you cannot ensure that you can restart a container. There are a million reasons for this, one of the main ones that we will look at more in depth when we look at orchestration is that containers are generally run on a cluster of hosts. Since there is no guarantee which host the container will be restarted on or even that the host the container was previously running on actually exists. There are many projects that attempt to solve this, but the best approach is to avoid the complexity altogether. If you need to persist files, then store them in something that is designed for the job such as Amazon S3 or Google Cloud Storage. Design your applications around this principle and you will spend far less time panicking when the inevitable happens, and your super sensitive data container disappears.</p>
<p>OK, before we look at Docker volumes in more depth let's clean up after ourselves.</p>
<p>Exit your container and get back to the shell on the Docker host. If we run <kbd>docker ps -a</kbd> ,we will see that there are two stopped containers. To remove these, we can use the <kbd>docker rm containerid</kbd> command.</p>
<p>Run this now using the first <kbd>containerid</kbd> in your list, if this is successful, the container ID you asked to be removed would be echoed back to you, and the container will is deleted.</p>
<p>If you want to remove all the stopped containers you can use the following command:</p>
<pre>
<strong>$ docker rm -v $(docker ps -a -q)</strong>  
</pre>
<p>The <kbd>docker ps -a -q</kbd> the <kbd>-a</kbd> flag will list all the containers including the stopped ones, <kbd>-q</kbd> will return a list of the container IDs rather than the full details. We are passing this as a parameter list to <kbd>docker rm</kbd>, which will remove all the containers in the list.</p>
<p>To avoid having to remove a container we can use the <kbd>--rm</kbd> flag when starting a new container. This flag tells Docker to remove the container when it stops.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker volumes</h1>
                </header>
            
            <article>
                
<p>We have seen how Docker containers are immutable; however, there are some instances when you may wish to write some files to a disk or when you want to read data from a disk such as in a development setup. Docker has the concept of volumes, which can be mounted either from the host running the Docker machine or from another Docker container.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Union filesystem</h1>
                </header>
            
            <article>
                
<p>To keep our images efficient and compact Docker uses the concept of a Union File System. The Union filesystem allows us to represent a logical file system by grouping different directories and or files together. It uses a <strong>Copy on Write</strong> technique, which copies the layer when we modify the file system, this way we only use about 1MB of space when creating a new image. When data is written to the file system Docker copies the layer and puts it on the top of the stack. When building images and extending existing images we are leveraging this technique, also when starting an image and creating a container the only difference is this writable layer, which means we do not need to copy all the layers every time and fill up our disk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mounting volumes</h1>
                </header>
            
            <article>
                
<p>The <kbd>-v</kbd>, or <kbd>--volume</kbd> parameter allows you to specify a pair of values corresponding to the file system you wish to mount on the host and the path where you would like to mount the volume inside the container.</p>
<p>Let's try our example from earlier, but this time mounting a volume on the local file system:</p>
<pre>
<strong>$ docker run -it -v $(pwd):/host alpine:latest /bin/sh</strong>  
</pre>
<p>If you change into the host folder, you will see that there is access to the same folder from where you ran the <kbd>docker run</kbd> command. The syntax for the values for <kbd>-v</kbd> is <kbd>hostfolder:destinationfolder</kbd>, one thing I think is important to point out is that these paths need to be absolute, and you cannot use a relative path like <kbd>./</kbd> or <kbd>../foldername</kbd>. The volume you have just mounted has read/write access, any changes you make will be synchronized to the folder on the host so be careful to not go running <kbd>rm -rf *</kbd>. Creating Volumes on a production environment should be used very sparingly, I would advise that where possible you avoid doing it all together as in a production environment there is no guarantee if a container dies and is re-created that it will be replaced on the same host where it was previously. This means that any changes you have made to the volume will be lost.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker ports</h1>
                </header>
            
            <article>
                
<p>When running web applications inside a container, it is quite common that we will need to expose some ports to the outside world. By default, a Docker container is completely isolated, and if you start a server running on port <kbd>8080</kbd> inside your container unless you explicitly specify that port is accessible from the outside, it will not be accessible.</p>
<p>Mapping ports is a good thing from a security perspective as we are operating on a principle of no trust. It is also effortless to expose these ports. Using one of the examples we created in <a href="ba3a8742-94e7-4e47-8a47-1324a277a7f9.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Microservices</em>, let's see just how easy this is.</p>
<p>Move to the folder where you checked out the sample code, and run the following Docker command:</p>
<pre>
<strong>$ docker run -it --rm -v $(pwd):/src -p 8080:8080 -w /src golang:alpine /bin/sh</strong>  
</pre>
<p>The <kbd>-w</kbd> flag we are passing is to set the working directory that means that any command we run in the container will be run inside this folder. When we start the shell, you will see that rather than having to change into the folder we specify in the second part of the volume mounting we are already in that folder and can run our application. We are also using a slightly different image this time. We are not using <kbd>alpine:latest</kbd>, which is a lightweight version of Linux, we are using <kbd>golang:alpine</kbd>, which is a version of Alpine with the most recent Go tools installed.</p>
<p>If we start our application using the <kbd>go run main.go</kbd> command; we should see the following output:</p>
<pre>
<strong>2016/09/02 05:53:13 Server starting on port 8080</strong>  
</pre>
<p>Now change to another shell and try to curl the API endpoint:</p>
<pre>
<strong>$ curl -XPOST localhost:8080/helloworld -d '{"name":"Nic"}'</strong>  
</pre>
<p>You should see something like the following message returned:</p>
<pre>
<strong>{"message":"Hello Nic"}</strong>  
</pre>
<p>If we run the <kbd>docker ps</kbd> command to inspect the running containers, we will see that there are no ports exposed. Go back to your previous terminal window and kill the command and then exit the container.</p>
<p>This time, when we start it, we will add the <kbd>-p</kbd> argument to specify the port. Like volumes, this takes a pair of values separated by a colon <kbd>(:)</kbd>. The first is the destination port on the host that we would like to bind to the second is the source port on the Docker container to which our application is bound.</p>
<p>Because this binds to the port on the host machine, in the same way that you would not be able to start the program locally twice because of the port binding, you cannot do this with the host port mappings in Docker either. Of course, you can start multiple instances of your code in separate containers and bind to different ports, and we will see how you can do that in just a bit.</p>
<p>But first let's take a look at that port command, rather than starting a container and creating a shell to run our application we can do this in one command by replacing the <kbd>/bin/sh</kbd> command with our <kbd>go run</kbd> command. Give that a try and see if you can get your application running.</p>
<p>Got it?</p>
<p>You should have typed something like the following:</p>
<pre>
<strong>$ docker run -it --rm -v $(pwd):/src -w /src -p 8080:8080 golang:alpine go run reading_writing_json_8.go</strong>  
</pre>
<p>Now try your <kbd>curl</kbd> to send some data to the API again, you should see the following output:</p>
<pre>
<strong>{"message":"Hello Nic"}</strong>  
</pre>
<p>Like volumes, you can specify multiple instances of the <kbd>-p</kbd> argument, which enables you to set up the binding for multiple ports.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing a container starting with an explicit name</h1>
                </header>
            
            <article>
                
<p>Containers that start with a name parameter are not automatically removed even if you specify the <kbd>--rm</kbd> argument. To remove a container started in this way, we must manually use the <kbd>docker rm</kbd> command. If we append the <kbd>-v</kbd> option to the command, we can also remove the volumes that are associated with it. We should really do this now, or when we try to recreate the container later in the chapter, you might be left a little puzzled:</p>
<pre>
<strong>$ docker rm -v server</strong>  
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker networking</h1>
                </header>
            
            <article>
                
<p>I never intended this chapter to be a full reproduction of the official Docker documentation; I am just trying to explain some of the key concepts that will help you as you progress through the rest of this book.</p>
<p>Docker networking is an interesting topic, and by default, Docker supports the following network modes:</p>
<ul>
<li>bridge</li>
<li>host</li>
<li>none</li>
<li>overlay</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bridge networking</h1>
                </header>
            
            <article>
                
<p>The bridge network is the default network that your containers will connect to when you launch them; this is how we were able to join our containers together in the last example. To facilitate this, Docker uses some of the core Linux capabilities such as networking namespaces and virtual Ethernet interfaces (or <kbd>veth</kbd> interfaces).</p>
<p>When the Docker engine starts, it creates the <kbd>docker0</kbd> virtual interface on the host machine. The <kbd>docker0</kbd> interface is a virtual Ethernet bridge that automatically forwards packets between any other network interfaces that are attached to it. When a container starts it creates a <kbd>veth</kbd> pair, it gives one to the container, which becomes its <kbd>eth0,</kbd> and the other connects to the <kbd>docker0</kbd> bridge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Host networking</h1>
                </header>
            
            <article>
                
<p>The host network is essentially the same network that the Docker engine is running on. When you connect a container to the host network all of the ports that are exposed by the container are automatically mapped to the hosts, it also shares the IP address of the host. While this may seem like a nice convenience, Docker was always designed to be capable of running multiple instances of the same container on the engine, and since you can only bind a socket to one port in Linux using the <kbd>host network</kbd> limits this feature.</p>
<p>The host network can also pose a security risk to your container as it is no longer protected by the principle of no trust and you no longer have the ability to explicitly control if a port is exposed or not. That being said, due to the efficiencies of host networking it may in some instances be appropriate to connect a container to the host network if you anticipate that it is going to heavily use the network. An API gateway might be one such example, this container would still be possible to route requests to other API containers that are sitting on the bridge network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">No network</h1>
                </header>
            
            <article>
                
<p>Removing your container from any network might in some instances be something you wish to do. Consider the situation where you have an application that only processes data stored in a file. Utilizing the principle of no trust, we may determine that the securest thing to do is to not connect it to any container and to only allow it to write to a volume that is mounted on the host. Attaching your container to the <kbd>none</kbd> network provides exactly this capability, and while the use case might be somewhat limited it is there, and it's nice to know about it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overlay network</h1>
                </header>
            
            <article>
                
<p>The Docker overlay network is a unique Docker network that is used to connect containers running on separate hosts to one another. With the bridge network as we have already learned, network communication is localized to the Docker host and this is generally fine when you are developing software. When you run your code in production however, all this changes, as you will typically be running multiple hosts, each running multiple containers as part of your high availability setup. The containers still need to talk to one another, and while we could route all traffic through an <strong>ESB</strong> (<strong>enterprise service bus</strong>), this is a little bit of an anti-pattern in the microservice world. The recommended approach as we will see in a later chapter, is for the service to be responsible for its own discovery and load balancing client calls. The Docker overlay network solves this problem, it is in effect a network tunnel between machines which passes the traffic unmodified over the physical network. The problem with the overlay is that you can no longer rely on Docker to update the <kbd>etc/hosts</kbd> file for you, and you must depend on a dynamic service registry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom network drivers</h1>
                </header>
            
            <article>
                
<p>Docker also supports plugins for networking, based around its open source <kbd>libnetwork</kbd> project, you can write custom networking plugins that can replace the networking subsystem of the Docker engine. They also give the capability for you to connect non-Docker applications to your container network such as a physical database server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weaveworks</h1>
                </header>
            
            <article>
                
<p>Weaveworks is one of the most popular plugins, it gives you the capability to securely link your Docker hosts and also provides a whole host of additional tools such as service discovery with weavedns and visualization with weavescope, so you can see how your network is connected together.</p>
<p><a href="https://www.weave.works"><span class="URLPACKT">https://www.weave.works</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project Calico</h1>
                </header>
            
            <article>
                
<p>Project Calico attempts to solve the speed and efficiency problems that using virtual LANs, bridging, and tunneling can cause. It achieves this by connecting your containers to a vRouter, which then routes traffic directly over the L3 network. This can give huge advantages when you are sending data between multiple data centers as there is no reliance on NAT and the smaller packet sizes reduce CPU utilization.</p>
<p><a href="https://www.projectcalico.org"><span class="URLPACKT">https://www.projectcalico.org</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating custom bridge networks</h1>
                </header>
            
            <article>
                
<p>Implementing a custom overlay network is beyond the scope of this book, however, understanding how you can create custom bridge networks is something that we should look at as Docker-Compose, which we are going to introduce later in this chapter, utilizes these concepts.</p>
<p>Like many of the Docker tools, creating a bridge network is quite straightforward. To see the currently running networks on your Docker engine, we can execute the following command:</p>
<pre>
<strong>$ docker network ls</strong>  
</pre>
<p>The output should be something like the following:</p>
<pre>
<strong>NETWORK ID          NAME                DRIVER              SCOPE</strong><br/><strong>8e8c0cc84f66        bridge              bridge              local               </strong><br/><strong>0c2ecf158a3e        host                host                local               </strong><br/><strong>951b3fde8001        none                null                local     </strong>  
</pre>
<p>You will find that there are three networks created by default, which is three of the ones we discussed earlier. Because these are default networks, we are unable to remove these, Docker requires these networks to function correctly and allowing you to remove them would be a bad thing indeed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a bridge network</h1>
                </header>
            
            <article>
                
<p>To create a bridge network, we can use the following command:</p>
<pre>
<strong>$ docker network create testnetwork</strong>  
</pre>
<p>Run this now in your terminal and list the networks again to see the results.</p>
<p>You will see that there is now a fourth network in your list that uses the bridge driver and that has the name you specified as one of the arguments. By default, when you create a network, it uses the <kbd>bridge</kbd> as a default driver, of course, it is possible to create a network to a custom driver, and this can be easily facilitated by specifying the additional argument, <kbd>-d drivername</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting containers to a custom network</h1>
                </header>
            
            <article>
                
<p>To connect a container to a custom network, let's again use the example application that we created in <a href="ba3a8742-94e7-4e47-8a47-1324a277a7f9.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Microservices</em>:</p>
<pre>
<strong>$ docker run -it --rm -v $(pwd):/src -w /src --name server --network=testnetwork golang:alpine go run main.go</strong>  
</pre>
<p>Did you get the error message that the name is already in use because you forgot to remove the container in the earlier section? If so, it might be time to head back a few pages.</p>
<p>Assuming all went well, you should see the server starting message, now let's try to curl the container using the same command we executed earlier:</p>
<pre>
<strong>$ docker run --rm appropriate/curl:latest curl -i -XPOST server:8080/helloworld -d '{"name":"Nic"}'</strong>  
</pre>
<p>You should have received the following error message:</p>
<pre>
<strong>curl: (6) Couldn't resolve host 'server'</strong>
</pre>
<p>This was expected, have a go to see if you can update the <kbd>docker run</kbd> command to make it work with our API container.</p>
<p>Got it?</p>
<p>If not, here is the modified command with the added network argument:</p>
<pre>
<strong>$ docker run --rm --network=testnetwork appropriate/curl:latest curl -i -XPOST server:8080/helloworld -d '{"name":"Nic"}'</strong>  
</pre>
<p>This command should have worked just fine the second time, and you should see the expected output. Now remove the server container, and we will take a look at how you can write your own Docker files.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing Dockerfiles</h1>
                </header>
            
            <article>
                
<p>Dockerfiles are the recipes for our images; the define the base image, software to be installed and give us the capability to set the various structure that our application needs.</p>
<p>In this section, we are going to look at how we can create a Docker file for our example API. Again, this is not going to be a comprehensive overview of how Dockerfiles work as there are many books and online resources that exist for that explicit purpose. What we will do is to look at the salient points that will give us the basics.</p>
<p>The first thing we are going to do is build our application code as when we package this into a Docker file we will be executing a binary, not using the <kbd>go run</kbd> command. The image we are going to create will have only the software installed that we need to run our application. Limiting the software installed is a Docker best practice when creating images as it reduces the attack surface by only including what is necessary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building application code for Docker</h1>
                </header>
            
            <article>
                
<p>We are going to execute a slightly different command for creating our files from the usual <kbd>go build</kbd>:</p>
<pre>
<strong>$ CGO_ENABLED=0 GOOS=linux GOARCH=386 go build -a -installsuffix cgo -ldflags '-s' -o server</strong>  
</pre>
<p>In the preceding command, we are passing the argument <kbd>-ldflags '-s'</kbd>, this argument passes the <kbd>-s</kbd> argument to the linker when we build the application and tells it to statically link all dependencies. This is very useful when we use the popular Scratch container as a base; Scratch is the lightest base you can get it has no application frameworks or applications this is opposed to Ubuntu, which takes about 150MB. The difference between Scratch and Ubuntu is that Scratch does not have access to the standard C library <kbd>GLibC</kbd>.</p>
<p>If we do not build a static binary, then it will not execute if we try to run it in a Scratch container. The reason for this is that while you may think that your Go application is a static binary it still has a dependency on <kbd>GLibC</kbd>, both the <kbd>net</kbd> and the <kbd>os/user</kbd> packages link to <kbd>GLibC</kbd> so if we are to run our application with a Scratch base image we need to statically link this. The benefit, however, is an incredibly small image, we end up with an image which is roughly 4MB in size, exactly the size of our compile Go application.</p>
<p>Because the Docker engine is running on Linux, we also need to build our Go binary for the Linux architecture. Even if you are using Docker for Mac or Docker for Windows, what is happening under the hood is that the Docker engine is running a lightweight virtual machine on either <kbd>HyperV</kbd> or the Mac's <kbd>xhyve</kbd> virtual machine.</p>
<p>If you are not using Linux to run your go build command and since Go has excellent capability for cross-platform compilation, you don't need to do much. All you do need to do is prefix the architecture variables <kbd>GOOS=linux GOARCH=386</kbd> to your go build command as we did in the earlier example.</p>
<p>Now that we have created a binary for our application, let's take a look at the Docker file:</p>
<pre>
<strong>1 FROM scratch</strong><br/><strong>2 MAINTAINER jackson.nic@gmail.com</strong><br/><strong>3</strong><br/><strong>4 EXPOSE 8080</strong><br/><strong>5</strong><br/><strong>6 COPY ./server ./</strong><br/><strong>7 </strong><br/><strong>8 ENTRYPOINT ./server</strong>  
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">FROM</h1>
                </header>
            
            <article>
                
<p>The <kbd>FROM</kbd> instruction set the base image for subsequent instructions. You can use any image that is either stored in a remote registry or locally on your Docker Engine. When you execute <kbd>docker build</kbd>, if you do not already have this image, then Docker will pull it from the registry as the first step of the build process. The format for the <kbd>FROM</kbd> command is the same as you would use when issuing a <kbd>docker run</kbd> command it is either:</p>
<ul>
<li><kbd>FROM image</kbd> // assuming latest</li>
<li><kbd>FROM image:tag</kbd> // where you can specify a tag to use</li>
</ul>
<p>In <strong>line 1</strong>, we are using the image name scratch, this is a particular kind of image, which is basically a blank canvas. We could use Ubuntu or Debian or Alpine or pretty much anything really, but since all we need to run our Go application is the application itself then we can use scratch to produce the smallest possible image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MAINTAINER</h1>
                </header>
            
            <article>
                
<p>The <kbd>MAINTAINER</kbd> instruction allows you to set the author of the generated image. This is an optional instruction; however, it can be good practice to include this even if you are not planning on publishing your image to the public registry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">EXPOSE</h1>
                </header>
            
            <article>
                
<p>The <kbd>EXPOSE</kbd> instruction informs Docker that the container listens on the specified networks ports at runtime. Expose does not make the ports accessible to the host; this function still needs to be performed with the <kbd>-p</kbd> mapping.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">COPY</h1>
                </header>
            
            <article>
                
<p>The <kbd>COPY</kbd> instruction copies files from the source in the first part of this instruction to the destination specified in the second part:</p>
<ul>
<li><kbd>COPY &lt;src&gt; &lt;dest&gt;</kbd></li>
<li><kbd>COPY ["&lt;src"&gt;, "&lt;dest&gt;"]</kbd> // useful when paths contain whitespace</li>
</ul>
<p>The <kbd>&lt;src&gt;</kbd> in the <kbd>COPY</kbd> instruction may contain wildcards with the matching done using Go's <kbd>filepath.Match</kbd> rules.</p>
<p>Note:</p>
<ul>
<li><kbd>&lt;src&gt;</kbd> must be part of the context for the build, you cannot specify relative folders such as <kbd>../;</kbd></li>
<li>A root <kbd>/</kbd> specified in the <kbd>&lt;src&gt;</kbd> will be the root of the context</li>
<li>A root <kbd>/</kbd> specified in the <kbd>&lt;dest&gt;</kbd> will map to the containers root file system</li>
<li>Specifying a <kbd>COPY</kbd> instruction without a destination will copy the file or folder into the <kbd>WORKDIR</kbd> with the same name as the original</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ENTRYPOINT</h1>
                </header>
            
            <article>
                
<p>An <kbd>ENTRYPOINT</kbd> allows you to configure the executable that you would like to run when your container starts. Using <kbd>ENTRYPOINT</kbd> makes it possible to specify arguments as part of the <kbd>docker run</kbd> command which is appended to the <kbd>ENTRYPOINT</kbd>.</p>
<p><kbd>ENTRYPOINT</kbd> has two forms:</p>
<ul>
<li><kbd>ENTRYPOINT ["executable", "param1", "param2"]</kbd> // preferred form</li>
<li><kbd>ENTRYPOINT command param1 param2</kbd> //shell form</li>
</ul>
<p>For example, in our Docker file, we are specifying the <kbd>ENTRYPOINT ./server</kbd>. This is our Go binary that we would like to run. When we start our container with the following <kbd>docker run helloworld</kbd> command, we do not need to explicitly tell the container to execute the binary and launch the server. We can, however, pass additional arguments to the application via the <kbd>docker run</kbd> command arguments; these would then be appended to the <kbd>ENTRYPOINT</kbd> before the application is run. For example:</p>
<pre>
<strong>$ docker run --rm helloworld --config=/configfile.json</strong>  
</pre>
<p>The preceding command would append the arguments to the executed statement defined in the entry point, which would be the equivalent of executing the following shell command:</p>
<pre>
<strong>$ ./server --config=configfile.json</strong>  
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CMD</h1>
                </header>
            
            <article>
                
<p>The CMD instruction has three forms:</p>
<ul>
<li><kbd>CMD ["executable", "param1", "param2"]</kbd> // exec form</li>
<li><kbd>CMD ["param1", "param2"]</kbd> // append default parameters to <kbd>ENTRYPOINT</kbd></li>
<li><kbd>CMD command param1 param2</kbd> // shell form</li>
</ul>
<p>When <kbd>CMD</kbd> is used to provide default arguments for the <kbd>ENTRYPOINT</kbd> instruction then both the <kbd>CMD</kbd> and <kbd>ENTRYPOINT</kbd> instructions should be specified using the <kbd>JSON</kbd> array format.</p>
<p>If we specify a default value for <kbd>CMD</kbd>, we can still override it by passing the command arguments to the <kbd>docker run</kbd> command.</p>
<p>Only one <kbd>CMD</kbd> instruction is permitted in a Docker file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Good practice for creating Dockerfiles</h1>
                </header>
            
            <article>
                
<p>Taking all of this into account, we need to remember how the union file system works in Docker and how we can leverage it to create small and compact images. Every time we issue a command in the Dockerfile, Docker will create a new layer. When we mutate this command, the layer must be completely recreated and potentially all the following layers too, which can dramatically slow down your build. It is therefore recommended a good practice that you should attempt to group your commands as tightly as possible to reduce the possibility of this occurring.</p>
<p>Quite often, you will see Dockerfiles which instead of having a separate <kbd>RUN</kbd> command for every command we would like to execute, we chain these using standard bash formatting.</p>
<p>For example, consider the following, which would install software from a package manager.</p>
<p><strong>Bad Practice:</strong></p>
<pre>
<strong>RUN apt-get update</strong><br/><strong>RUN apt-get install -y wget</strong><br/><strong>RUN apt-get install -y curl</strong><br/><strong>RUN apt-get install -y nginx</strong>  
</pre>
<p><strong>Good Practice:</strong></p>
<pre>
<strong>RUN apt-get update &amp;&amp; \</strong><br/><strong>    apt-get install -y wget curl nginx</strong>
</pre>
<p>The second example would only create one layer, which in turn would create a much smaller and more compact image, it is also good practice to organize your COPY statements placing the statement which changes the least further up in the Dockerfile, this way you avoid invalidation of subsequent layers even if there are no changes to these layers.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building images from Dockerfiles</h1>
                </header>
            
            <article>
                
<p>To build an image from our Dockerfile, we can execute a straightforward command:</p>
<pre>
<strong>$ docker build -t testserver .</strong>  
</pre>
<p>Breaking this down the <kbd>-t</kbd> argument is the tag we wish to give the container, this takes the form name:tag, If we omit the <kbd>tag</kbd> portion of the argument as we have in our example command, then the tag <kbd>latest</kbd> will be automatically assigned.</p>
<p>If you run <kbd>docker images</kbd>, you will see that our <kbd>testserver</kbd> image has been given this tag.</p>
<p>The final argument is the context we would like to send to the Docker Engine. When you run a Docker build, the context is automatically forwarded to the server. This may seem strange, but you have to remember that it is not uncommon that the Docker Engine will not be running on your local machine, and therefore it will not have access to your local filesystem. For this reason, we should be careful about where we are setting our context as it can mean that potentially a large amount of data is being sent to the engine, which will slow things down. Context then becomes the root for your <kbd>COPY</kbd> commands.</p>
<p>Now that we have our running container, let's test it out. Why not start a container from our newly built image and check the API by curling the endpoint:</p>
<pre>
<strong>$ docker run --rm -p 8080:8080 testserver</strong><br/><strong>$ curl -XPOST localhost:8080/helloworld -d '{"name":"Nic"}'</strong>
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker build context</h1>
                </header>
            
            <article>
                
<p>When we run our Docker build command, we set the context path as the final argument. What actually happens when the command executes is that the context is transferred to the server. This can cause problems if you have a large source folder, so it is good practice to only send the files you need to be packaged inside the container or the files you need when building the container. There are two ways we can mitigate this problem. The first is to ensure that our context only has the files on it we require. Since this is not always possible we have a secondary option of using a <kbd>.dockerignore</kbd> file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker Ignore files</h1>
                </header>
            
            <article>
                
<p>The <kbd>.dockerignore</kbd> file is similar to a git ignore file before the CLI sends the context to the Engine, it excludes files and directories that match patterns in the <kbd>.dockerignore</kbd> file. It uses the patterns which are defined in Go's <kbd>filepath.Match</kbd> rules you can find more information about them in the following Go documentation: <a href="https://godoc.org/path/filepath#Match"><span class="URLPACKT">https://godoc.org/path/filepath#Match</span></a></p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Rule</strong></p>
</td>
<td>
<p><strong>Behavior</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd># comment</kbd></p>
</td>
<td>
<p>Ignored.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>*/temp*</kbd></p>
</td>
<td>
<p>Exclude files and directories whose names start with temp in any immediate subdirectory of the root. For example, the plain file <kbd>/somedir/temporary.txt</kbd> is excluded, as is the directory <kbd>/somedir/temp</kbd>.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>*/*/temp*</kbd></p>
</td>
<td>
<p>Exclude files and directories starting with temp from any subdirectory that is two levels below the root. For example, <kbd>/somedir/subdir/temporary.txt</kbd> is excluded.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>temp?</kbd></p>
</td>
<td>
<p>Exclude files and directories in the root directory whose names are a one-character extension of temp. For example, <kbd>/tempa</kbd> and <kbd>/tempb</kbd> are excluded.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><a href="https://docs.docker.com/engine/reference/builder/#/dockerignore-file"><span class="URLPACKT">https://docs.docker.com/engine/reference/builder/#/dockerignore-file</span></a></p>
<p><a href="https://docs.docker.com/engine/reference/builder/#/dockerignore-file"><span class="URLPACKT">﻿</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Daemons in containers</h1>
                </header>
            
            <article>
                
<p>One of the things you might be used to when deploying an application to a VM or physical server is to use a Daemon runner such as <kbd>initd</kbd> or <kbd>systemd</kbd> to ensure that the application is started in the background and continues to run even if it crashes. This is an anti-pattern when you are using Docker containers, for Docker to successfully stop the application it will attempt to kill the process running with PID 1. Daemons will generally start with PID 1 and start your application with another process ID, which will mean they are not killed when you stop the Docker container. This can cause containers to hang when the <kbd>docker stop</kbd> command is executed.</p>
<p>In the instance that you need to ensure that your application keeps running even after a crash then you delegate this responsibility to the orchestrator who is starting your Docker container. We will learn more about this when we look at orchestration in a later chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker Compose</h1>
                </header>
            
            <article>
                
<p>That was all super easy-ish, let's now take a look at a compelling feature of Docker that allows you to start multiple containers at once with your stack definition stored in a handy YAML file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Docker Compose on Linux</h1>
                </header>
            
            <article>
                
<p>If you have either Docker for Mac or Docker for Windows installed then it already comes bundled with <kbd>docker-compose</kbd>, if however, you are using Linux, then you may need to install this yourself as it does not come as part of the default Docker package.</p>
<p>To install Docker Compose on Linux, execute the following command in your terminal:</p>
<pre>
<strong>$ curl -L https://github.com/docker/compose/releases/download/1.8.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose  &amp;&amp; chmod +x /usr/local/bin/docker-compose</strong>  
</pre>
<p>Before we look at how we can run our application with <kbd>docker-compose</kbd>, let's take a look at the file we are going to run and some of the important facets of it:</p>
<pre>
<strong>1 version: '2'                                                                          </strong><br/><strong>2 services:                                                                             </strong><br/><strong>3   testserver:                                                                       </strong><br/><strong>4     image: testserver                                                             </strong><br/><strong>5   curl:                                                                             </strong><br/><strong>6     image: appropriate/curl                                                       </strong><br/><strong>7     entrypoint: sh -c  "sleep 3 &amp;&amp; curl -XPOST testserver:8080/helloworld -d '{\"name\":\"Nic\"}'"                                                                      </strong>
</pre>
<p>Docker Compose files are written in YAML, inside this file you can define services that will make up your application. In our simple example, we are only describing two services. The first is our example code that we have just built and the second is a simple service that curls this API. As a production example, this is not particularly useful I admit, but it is only intended to show how to set up these files. As we progress through later chapters, we will heavily rely on compose files to create our databases and other data stores that make up our application.</p>
<p><strong>Line 1</strong> defines the version of the Docker compose file we are using, version 2 is the latest version and is a breaking change from version 1 which along with the <kbd>--link</kbd> directive is now deprecated and will be removed in a future release.</p>
<p>In <strong>line 2</strong> we define the services. Services are the containers that you would like to start with your stack. Each service has to have a unique name to the compose file, but not necessarily to all the containers running on your Docker Engine. To avoid conflicts when starting a stack, we can pass <kbd>-p projectname</kbd> to the <kbd>docker-compose up</kbd> command; this will prefix the name of any of our containers with the specified project name.</p>
<p>The minimum information you need to specify for a service is the image, which is the image you wish to start a container from. In the same way that <kbd>docker run</kbd> works, this can either be a local image on the Docker Engine or it can be a reference to an image in a remote registry. When you start a stack, compose will check to see if the image is available locally and if not it will automatically pull it from the registry.</p>
<p><strong>Line 6</strong> defines our second service; this is simply going to execute a command to curl a request to the API exposed by the first service.</p>
<p>In this service definition block, we are both specifying the image and an entry point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service startup</h1>
                </header>
            
            <article>
                
<p class="mce-root">The previous command looks a little weird, but there is a gotcha with Docker compose, which quite a few people fall foul too, there is no real way for compose to know when an application is running. Even if we use the <kbd>depends-on</kbd> configuration, we are only informing compose that there are dependencies and that it should control the start order of the services.</p>
<pre class="mce-root">
<strong><br/></strong>sh -c  "sleep 3 &amp;&amp; curl -XPOST testserver:8080/helloworld -d '{\"name\":\"Nic\"}'"
</pre>
<p> </p>
<p>All compose will do is check that the container has been started. The general problem occurs with a misunderstanding that a container being started equals it is ready to receive requests. More often than not this is not the case, it can take time for your application to start and be ready to accept requests. If you have a dependency like we have specified in our entry point to curl the endpoint in another service, then we cannot assume that the dependent service is ready for requests before we execute our command. We will cover a pattern for dealing with this in <a href="74445ff8-eb01-4a2f-a910-0551e7d85a5f.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em><span class="cdp-organizer-chapter-title">Microservice Frameworks</span></em>, but for now we can be aware that:</p>
<div class="packt_quote">"Container started, and service ready is not the same thing."</div>
<p>In our simple example, we know that it roughly takes a second or so for the service to start, so we will just sleep for three seconds to give it plenty of time to get ready before executing our command. This method is not good practice, and it is only to illustrate how we can use compose to link services. In reality, you would probably never start a single command like we are here in your compose file.</p>
<p>When you use a Docker network, Docker automatically adds a mapping to the containers <kbd>resolve.conf</kbd> pointing to the built in Docker DNS server, we can then contact other containers connected to the same network by referencing them by name. Looking at our curl command, this DNS capability is exactly what allows us to use the hostname testserver.</p>
<p>OK, time to test it out, run the following command from your terminal:</p>
<pre>
<strong>$ docker-compose up</strong>  
</pre>
<p>All being well you should see the following message returned in the output:</p>
<pre>
<strong>{"message":"Hello Nic"}</strong>  
</pre>
<p><em><span class="KeyPACKT">Ctrl</span></em> + <em><span class="KeyPACKT">C</span></em> will exit compose, however, since we did run this with the <kbd>docker run</kbd> command and passed the arguments <kbd>--rm</kbd> to remove the container, we need to ensure that we clean up after ourselves. To remove any stopped container that you have started with <kbd>docker-compose</kbd>, we can use the particular compose command <kbd>rm</kbd> and pass the <kbd>-v</kbd> argument to remove any associated volumes:</p>
<pre>
<strong>$ docker-compose rm -v</strong>  
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Specifying the location of a compose file</h1>
                </header>
            
            <article>
                
<p>Whenever you run <kbd>docker-compose</kbd>, it looks for a file named <kbd>docker-compose.yml</kbd> in the current folder as a default file. To specify an alternate file, we can pass the <kbd>-f</kbd> argument to compose with a path to the compose file we would like to load:</p>
<pre>
<strong>$ docker-compose -f ./docker-compose.yml up</strong>  
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Specifying a project name</h1>
                </header>
            
            <article>
                
<p>As we discussed earlier when we start <kbd>docker-compose</kbd>, it will create services with the given names in your Compose file appending the project name <kbd>default</kbd> to them. If we need to run multiple instances of this compose file, then <kbd>docker-compose</kbd> will not start another instance as it will check to see if any services are running with the given names first. To override this, we can specify the project name replacing the default name of <kbd>default</kbd>. To do this we just need to specify the <kbd>-p projectname</kbd> argument to our command as follows:</p>
<pre>
<strong>$ docker-compose -p testproject up</strong>  
</pre>
<p>This will then create two containers:</p>
<ul>
<li><kbd>testproject_testserver</kbd></li>
<li><kbd>testproject_curl</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In summary, we have learned how to work with Docker in this chapter, and while this is only a brief overview, I suggest you head over to the documentation and read more in depth on the concepts of Dockerfiles, Composefiles, the Docker Engine, and Docker Compose. Docker is an invaluable tool for development, testing, and production and as we progress through the following chapters, we will use these concepts extensively. In the next chapter, we are going to look at testing, which builds on all of the things you have learned so far.</p>


            </article>

            
        </section>
    </body></html>