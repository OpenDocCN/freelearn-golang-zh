<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>When you try to define what testing is, you will come up with a multitude of answers, and many of us will not understand the full benefits of testing until we've been burnt by buggy software or we have tried to change a complex code base which has no tests.</p>
<p>When I tried to define testing, I came up with the following:</p>
<div class="packt_quote">"The art of a good night's sleep is knowing you will not get woken by a support call and the piece of mind from being able to confidently change your software in an always moving market."</div>
<p>OK, so I am trying to be funny, but the concept is correct. Nobody enjoys debugging poorly written code, and indeed, nobody enjoys the stress caused when a system fails. Starting out with a mantra of quality first can alleviate many of these problems.</p>
<p>Over the last 20 years, techniques like TDD have become commonplace. In some instances, it is not as common as I would like, but at least people are talking about testing now. In some ways, we have the Agile Alliance to thank for this:</p>
<div class="packt_quote">the principle of releasing little and often provides significant business benefits; the downside (or the benefit, depending on your viewpoint) to releasing little and often is that you can no longer spend three months running through a regression test suite before you release to market.</div>
<p>In my office, context switching is one of the biggest complaints. Nobody enjoys having to drop what they are doing to investigate a problem on work that they or even a colleague may have carried out months or years ago. We want to be moving forward; and to ensure we can do that, we have to make sure that what we have previously delivered meets the specification and is of high enough quality to meet the client's requirement.</p>
<p>I also mentioned a change in my definition, and one of the biggest problems with change is the concern that the change you are making may have an undesirable effect on another part of the system. This effect applies to microservices as well as large monolithic systems.</p>
<p>What if I also told you that the side effect of code that is easy to test is probably well-written code that is loosely coupled and has the right abstractions?</p>
<p>Testing, however, is not just about the developer: there is a definite need for manual testing by people detached from the code base. This exploratory testing can bring out missing requirements or incorrect assumptions. In itself, this is a specialized field and way beyond the scope of this book, so we are going to concentrate on the kind of testing that you should be doing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The testing pyramid</h1>
                </header>
            
            <article>
                
<p>Mike Cohn is credited with having created the concept of a testing pyramid in his book <em>Succeeding with Agile</em>. The concept is that your cheapest (fastest) tests to run, which will be your unit tests, go at the bottom of the pyramid; service level integration tests are on top of this, and at the very top, you place full end-to-end tests, which are the costliest element. Because this is a pyramid, the number of tests gets smaller as you move up the pyramid.</p>
<div class="CDPAlignCenter CDPAlign"><img height="147" width="226" src="assets/e41fa6ed-58a7-46a2-a5af-8358f58a61ee.png"/></div>
<p>In the early days of automated testing, all the testing was completed at the top of the pyramid. While this did work from a quality perspective, it meant the process of debugging the area at fault would be incredibly complicated and time-consuming. If you were lucky, there might be a complete failure which could be tracked down to a stack trace. If you were unlucky, then the problem would be behavioral; and even if you knew the system inside out, it would involve plowing through thousands of lines of code and manually repeating the action to reproduce the failure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Outside-in development</h1>
                </header>
            
            <article>
                
<p>When writing tests, I like to follow a process called outside-in development. With outside-in development, you start by writing your tests almost at the top of the pyramid, determine what the functionality is going to be for the story you are working on, and then write some failing test for this story. Then you work on implementing the unit tests and code which starts to get the various steps in the behavioral tests to pass.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/e16cc217-a7e6-4d93-a49e-1c0efc11e146.png"/></div>
<p>This initial specification also becomes the living documentation for your system. We will go into more detail as to how you can create this later in this chapter, but more often than not it is written in a language like <strong>Gherkin</strong> and is defined by working in a group with a domain specialist like a product owner, a developer, and a testing expert. The intention behind Gherkin is to create a universal language that everyone understands. This ubiquitous language uses verbs and nouns that have special meaning to the team, and which is almost always domain-specific, but should also be understandable to outsiders.</p>
<pre>
<strong>Feature</strong>: As a user when I call the search endpoint, I would like to receive a list of kittens
</pre>
<p>The feature is the story which, in an agile environment is owned by the product owner. The feature is then broken down into scenarios which explain in greater detail the qualities that the code must have to be acceptable.</p>
<pre>
<strong>Scenario</strong>: Invalid query<br/> Given I have no search criteria <br/> When I call the search endpoint <br/> Then I should receive a bad request message 
</pre>
<p>When we get to the section on BDD in a little while we will examine this in greater depth; we will also look at a framework for Go for writing and executing Cucumber specifications. Now, however, I am going to break the rules of outside in development by showing you how to write great unit tests in Go. The concepts we are about to learn will be greatly beneficial when we do start to look at BDD, so I think it is best we cover them first. Like the previous chapters, it will be useful to you when reading through this chapter to have the source code handy; you can clone the code from the following location: <a href="https://github.com/building-microservices-with-go/chapter4.git">https://github.com/building-microservices-with-go/chapter4.git</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit tests</h1>
                </header>
            
            <article>
                
<p>Our unit tests go right down to the bottom of the pyramid. This book was never intended to be a lesson in TDD, there are plenty of better places to learn that. We will, however, take a look at the testing framework which is built into Go. Before we do that, let's just remind ourselves of the three laws of testing as defined by the awesome Uncle Bob Martin in his book <em>Clean Code</em>:</p>
<ul>
<li><strong>First law</strong>: You may not write production code until you have written a failing unit test</li>
<li><strong>Second law</strong>: You may not write more of a unit test than is sufficient to fail, and not compiling is failing</li>
<li><strong>Third law</strong>: You may not write more production code than is sufficient to pass the currently failing test</li>
</ul>
<p>One of the most effective ways to test a microservice in Go is not to fall into the trap of trying to execute all the tests through the HTTP interface. We need to develop a pattern that avoids creating a physical web server for testing our handlers, the code to create this kind of test is slow to run and incredibly tedious to write. What need to be doing is to test our handlers and the code within them as unit tests. These tests will run far quicker than testing through the web server, and if we think about the coverage, we will be able to test the wiring of the handlers in the Cucumber tests that execute a request to the running server which overall gives us 100% coverage of our code.</p>
<p><kbd>main.go</kbd></p>
<pre>
  10 func main() { <br/>  11   err := http.ListenAndServe(":2323", &amp;handlers.SearchHandler{}) <br/>  12   if err != nil { <br/>  13     log.Fatal(err) <br/>  14   } <br/>  15 } 
</pre>
<p>You will see in the main function that we have split the handlers out into a separate package. Breaking up the code in this way allows us to test these in isolation, so let's go ahead and write some unit tests for our <kbd>SearchHandler</kbd>.</p>
<p>The convention is that we define our test files in the same folder as the package to which they belong, and we name them the same as the file they are testing followed by <kbd>_test</kbd>. For our example, we are going to write some tests for our <kbd>SearchHandler</kbd> which lives in the <kbd>handlers/search.go</kbd> file; therefore our test file will be named <kbd>handlers/search_test.go</kbd>.</p>
<p>The signature for a test method looks like this:</p>
<pre>
func TestXxx(*testing.T) 
</pre>
<p>The name of the test must have a particular name beginning with <kbd>Test</kbd> and then immediately following this an uppercase character or number. So we could not call our test <kbd>TestmyHandler</kbd>, but we could name it <kbd>Test1Handler</kbd> or <kbd>TestMyHandler</kbd>.</p>
<p>Again, drawing on Uncle Bob's wisdom, we need to think about the names of our tests as carefully as we would the names for our methods in our production code.</p>
<p>The first test we are going to write is the one who will validate that search criteria have been sent with the request and the implementation is going to look like this:</p>
<pre>
   9 func TestSearchHandlerReturnsBadRequestWhenNoSearchCriteriaIsSent(t *testing.T) { <br/>  10   handler := SearchHandler{} <br/>  11   request := httptest.NewRequest("GET", "/search", nil) <br/>  12   response := httptest.NewRecorder() <br/>  13 <br/>  14   handler.ServeHTTP(response, request) <br/>  15 <br/>  16   if response.Code != http.StatusBadRequest { <br/>  17     t.Errorf("Expected BadRequest got %v", response.Code) <br/>  18   } <br/>  19 } 
</pre>
<p>The <kbd>net/http/httptest</kbd> package has two fantastic convenience methods for us <kbd>NewRequest</kbd> and <kbd>NewResponse</kbd>, if you are familiar with unit testing concepts, then one of the fundamentals isolate dependency. Often we replace the dependencies with Mocks or Spies which allow us to test the behavior of our code without having to execute code in the dependencies. These two functions enable us to do exactly this; they generate Mock versions of the dependent objects <kbd>http.Request</kbd> and <kbd>http.ResponseWriter</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">httptest.NewRequest</h1>
                </header>
            
            <article>
                
<p>The first line we need to pay attention to is line <strong>11</strong>: the <kbd>net/http/httptest</kbd> package has some nice convenience methods for us. The <kbd>NewRequest</kbd> method returns an incoming server request which we can then pass to our <kbd>http.Handler</kbd>:</p>
<pre>
func NewRequest(method, target string, body io.Reader) *http.Request 
</pre>
<p>We can pass parameters to the method and the target, which is either the path or an absolute URL. If we only pass a path, then <kbd>example.com</kbd> will be used as our host setting. Finally, we can give it an <kbd>io.Reader</kbd> file which will correspond to the body of the request; if we do not pass a nil value then <kbd>Request.ContentLength</kbd> is set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">httptest.NewRecorder</h1>
                </header>
            
            <article>
                
<p>In line <strong>12</strong> we are creating a <kbd>ResponseRecorder</kbd> type: this is going to be our instance of <kbd>ResponseWriter</kbd> that we pass to the handler. Because a handler has no return function to validate correct operation, we need to check what has been written to the output. The <kbd>ResponseRecorder</kbd> type is an implementation of <kbd>http.ResponseWriter</kbd> which does just that: it records all the mutations we make so that it is later possible to make our assertions against it.</p>
<pre>
type ResponseRecorder struct { <br/>         Code      int           // the HTTP response code from WriteHeader <br/>         HeaderMap http.Header   // the HTTP response headers <br/>         Body      *bytes.Buffer // if non-nil, the bytes.Buffer to append written data to <br/>         Flushed   bool <br/>         // contains filtered or unexported fields <br/> } 
</pre>
<p>All we then need to do is to call the <kbd>ServeHTTP</kbd> method with our dummy request and response and then assert that we have the correct outcome.</p>
<p>Go does not have an assertion library as you would find with RSpec or JUnit. We will look at a third-party framework later in this chapter, but for now, let's concentrate on the standard packages.</p>
<p>In line <strong>16</strong>, we are checking to see if the response code returned from the handler is equal to the expected code <kbd>http.BadRequest</kbd>. If it is not, then we call the Errorf method on the testing framework.</p>
<p><strong>ErrorF</strong></p>
<pre>
func (c *T) Errorf(format string, args ...interface{}) 
</pre>
<p>The <kbd>Errorf</kbd> function takes the parameters of a format string and a variadic list of parameters; internally this calls the <kbd>Logf</kbd> method before calling <kbd>Fail</kbd>.</p>
<p>If we run our tests by running the command <kbd>go test -v -race ./...</kbd> , we should see the following output:</p>
<pre>
<strong>=== RUN   TestSearchHandlerReturnsBadRequestWhenNoSearchCriteriaIsSent</strong><br/><strong>--- FAIL: TestSearchHandlerReturnsBadRequestWhenNoSearchCriteriaIsSent (0.00s)</strong><br/><strong>     search_test.go:17: Expected BadRequest got 200</strong><br/><strong> FAIL</strong><br/><strong> exit status 1</strong><br/><strong> FAIL    github.com/nicholasjackson/building-microservices-in-<br/> go/chapter5/handlers    0.016s</strong>
</pre>
<p>The <kbd>-v</kbd> flag will print the output in a verbose style, and it will also print all the text written to the output by the application, even if the test succeeds.</p>
<p>The <kbd>-race</kbd> flag enables Go's race detector which holds discover bugs with concurrency problems. A data race occurs when two Go routines access the same variable concurrently, and at least one of the accesses is a write. The race flag adds a small overhead to your test run, so I recommend you add it to all executions.</p>
<p>Using <kbd>-./...</kbd> as our final parameter allows us to run all our tests in the current folder as well as the child folders, it saves us from manually having to construct a list of packages or files to test.</p>
<p>Now we have a failing test we can go ahead and write the implementation to make the test pass:</p>
<pre>
 18   decoder := json.NewDecoder(r.Body) <br/> 19   defer r.Body.Close() <br/> 20 <br/> 21   request := new(searchRequest) <br/> 22   err := decoder.Decode(request) <br/> 23   if err != nil { <br/> 24     http.Error(rw, "Bad Request", http.StatusBadRequest) <br/> 25     return <br/> 26   } 
</pre>
<p>When we rerun the tests, we can see that they have succeeded:</p>
<pre>
<strong>=== RUN   TestSearchHandlerReturnsBadRequestWhenNoSearchCriteriaIsSent</strong><br/><strong>--- PASS: TestSearchHandlerReturnsBadRequestWhenNoSearchCriteriaIsSent (0.00s)</strong><br/><strong>PASS</strong><br/><strong>ok      github.com/nicholasjackson/building-microservices-in-go/chapter5/handlers    1.022s</strong>
</pre>
<p>This output is awesome; but what if passing a query to the request with a blank string constitutes a failure? Time to write another test:</p>
<pre>
  23 func TestSearchHandlerReturnsBadRequestWhenBlankSearchCriteriaIsSent(t *testing.T) { <br/>  24   handler := SearchHandler{} <br/>  25   data, _ := json.Marshal(searchRequest{}) <br/>  26   request := httptest.NewRequest("POST", "/search", bytes.NewReader(data)) <br/>  27   response := httptest.NewRecorder() <br/>  28 <br/>  29   handler.ServeHTTP(response, request) <br/>  30 <br/>  31   if response.Code != http.StatusBadRequest { <br/>  32     t.Errorf("Expected BadRequest got %v", response.Code) <br/>  33   } <br/>  34 } 
</pre>
<p>This test is very similar to the last one; the only difference is that we are passing some JSON in the request body. While this test will fail correctly, we should take the lead from Uncle Bob and refactor this to make it more readable:</p>
<pre>
  21 func TestSearchHandlerReturnsBadRequestWhenBlankSearchCriteriaIsSent(t *testing.T) { <br/>  22   r, rw, handler := setupTest(&amp;searchRequest{}) <br/>  23 <br/>  24   handler.ServeHTTP(rw, r) <br/>  25 <br/>  26   if rw.Code != http.StatusBadRequest { <br/>  27     t.Errorf("Expected BadRequest got %v", rw.Code) <br/>  28   } <br/>  29 } 
</pre>
<p>We have refactored our test to add a setup method which is shared across the two tests, the intention behind this is to keep our tests focused on three core areas:</p>
<ul>
<li>Setup</li>
<li>Execute</li>
<li>Assert</li>
</ul>
<p>Bad tests with duplicated code can be worse than bad code: your tests should be clear, easy to understand and contain the same care that you would add to your production code.</p>
<p>Now, if the test fails, we can go ahead and update our code to implement the feature:</p>
<pre>
23 if err != nil || len(request.Query) &lt; 1 { <br/>24     http.Error(rw, "Bad Request", http.StatusBadRequest) <br/>25     return <br/>26   } 
</pre>
<p>All we needed to do was make a simple modification to our <kbd>if</kbd> statement. As our system grows in complexity and we find more cases for what constitutes an invalid search query, we will refactor this into a separate method; but, for now, this is the minimum we need to do to make the test pass.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dependency injection and mocking</h1>
                </header>
            
            <article>
                
<p>To get the tests that return items from the <kbd>Search</kbd> handler to pass, we are going to need a data store. Whether we implement our data store in a database or a simple in-memory store we do not want to run our tests against the actual data store as we will be checking both data store and our handler. For this reason, we are going to need to manage the dependencies on our handler so that we can replace them in our tests. To do this, we are going to use a technique called dependency injection where we will pass our dependencies into our handler rather than creating them internally.</p>
<p>This method allows us to replace these dependencies with stubs or mocks when we are testing the handler, making it possible to control the behavior of the dependency and check how the calling code responds to this.</p>
<p>Before we do anything, we need to create our dependency. In our simple example, we are going to create an in-memory data store which has a single method:</p>
<pre>
Search(string) []Kitten 
</pre>
<p>To replace the type with a mock, we need to change our handler to depend on an interface which represents our data store. We can then interchange this with either an actual data store or a mock instance of the store without needing to change the underlying code:</p>
<pre>
 type Store interface { <br/>     Search(name string) []Kitten <br/> } 
</pre>
<p>We can now go ahead and create the implementation for this. Since this is a simple example, we are going to hardcode our list of kittens as a slice and the search method will just select from this slice when the criteria given as a parameter matches the name of the kitten.</p>
<p>OK, great; we now have our data store created, so let's see how we are going to modify our handler to accept this dependency. It is quite simple: because we created a struct which implements the <kbd>ServeHTTP</kbd> method, we can just add our dependencies onto this struct:</p>
<pre>
 Search { <br/>     Store data.Store <br/> } 
</pre>
<p>Note how we are using a reference to the interface rather than the concrete type, which allows us to interchange this object with anything that implements the store interface.</p>
<p>Now, back to our unit tests: we would like to ensure that, when we call the <kbd>ServeHTTP</kbd> method with a search string, we are querying the data store and returning the kittens from it.</p>
<p>To do this, we are going to create a mock instance of our data store. We could create the mock ourselves; however, there is an excellent package by Matt Ryer who incidentally is also a Packt author. Testify (<a href="https://github.com/stretchr/testify.git">https://github.com/stretchr/testify.git</a>) has a fully featured mocking framework with assertions. It also has an excellent package for testing the equality of objects in our tests and removes quite a lot of the boilerplate code we have to write.</p>
<p>In the data package, we are going to create a new file called <kbd>mockstore.go</kbd>. This structure will be our mock implementation of the data store:</p>
<pre>
5 // MockStore is a mock implementation of a datastore for testing purposes <br/>6 type MockStore struct { <br/>7   mock.Mock <br/>8 } <br/>9 <br/>10 //Search returns the object which was passed to the mock on setup <br/>11 func (m *MockStore) Search(name string) []Kitten { <br/>12   args := m.Mock.Called(name) <br/>13 <br/>14   return args.Get(0).([]Kitten) <br/>15 } 
</pre>
<p>In line <strong>6</strong>, we are defining our <kbd>MockStore</kbd> object. There is nothing unusual about this, except you will note that it is embedding the <kbd>mock.Mock</kbd> type. Embedding Mock will give us all of the methods on the <kbd>mock.Mock</kbd> struct.</p>
<p>When we write the implementation for our search method, we are first calling the <kbd>Called</kbd> method and passing it the arguments that are sent to <kbd>Search</kbd>. Internally, the mock package is recording that this method was called and with what parameters so that we can later write an assertion against it:</p>
<pre>
 args := m.Mock.Called(name) 
</pre>
<p>Finally, we are returning <kbd>args.Get(0).(Kitten)</kbd>. When we call the <kbd>Called</kbd> method, the mock returns us a list of arguments that we provided in the setup. We are casting this to our output type and returning to the caller. Let's take a quick look at the test method and see how this works.</p>
<p>Line <strong>57</strong> is the start of our test setup. The first thing we are going to do is to create an instance of our <kbd>MockStore</kbd>. We then set this as a dependency for our <kbd>Search</kbd> handler. If we skip back up the file to line <strong>38</strong>, you will see that we are calling the <kbd>On</kbd> method on our <kbd>mockStore</kbd>. The <kbd>On</kbd> method is a setup method for the mock and has the signature:</p>
<pre>
func (c *Call) On(methodName string, arguments ...interface{}) *Call 
</pre>
<p>If we do not call the <kbd>On</kbd> method with the parameter <kbd>Search</kbd> then when we call the <kbd>Search</kbd> method in our code we will get an exception from the test saying that <kbd>Search</kbd> has been called yet has not been setup. One of the reasons why I like to use mocking rather than a simple Stub is this ability to assert that a method has been called and we can explicitly dictate the behavior that the code under test is allowed to exhibit. This way we can ensure that we are not doing work the output of which has not been tested.</p>
<p>In our instance, we are setting up the condition that, when the <kbd>Search</kbd> method is called with the parameter <kbd>Fat Freddy's Cat</kbd>, we would like to return an array of kittens.</p>
<p>The assertion is that we are calling the <kbd>Search</kbd> method on the data store and passing it the query that was sent in the HTTP response. Using assertions in this way is a handy technique as it allows us to test the unhappy path such as when a data store may not be able to return data due to an internal error or another reason. If we were trying to test this with an integration test, it could be tough to persuade the database to fail on demand.</p>
<p>Why don't you spend five minutes as a little exercise and go ahead and write this code to finish off?</p>
<p>Did it all work? Don't worry if not, you can just check out the example code to see where you have gone wrong, but I hope that the process is useful. You can see how you can take a measured approach through two layers of testing to produce a working application. These tests are now your safety net: whenever you change the code to add a new feature, you can be sure that you are not breaking something unintentionally.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code coverage</h1>
                </header>
            
            <article>
                
<p>Code coverage is an excellent metric to ensure that the code you are writing has adequate coverage.</p>
<p>The most simplistic way of getting a readout of test coverage is to execute our tests with the <kbd>-cover</kbd> option:</p>
<pre>
<strong>go test -cover ./...</strong>  
</pre>
<p>If we run this against our example code in the root folder of our example code we will see the following output:</p>
<pre>
<strong>$go test -cover ./...</strong><br/><strong>? github.com/building-microservices-with-go/chapter4 [no test files]</strong><br/><strong>ok github.com/building-microservices-with-go/chapter4/data 0.017s coverage: 20.0% of statements</strong><br/><strong>ok github.com/building-microservices-with-go/chapter4/features 0.018s coverage: 0.0% of statements [no tests to run]</strong><br/><strong>ok github.com/building-microservices-with-go/chapter4/handlers 0.018s coverage: 100.0% of statements</strong>
</pre>
<p>Now our handlers look beautiful: we have 100% coverage of this package. However, our data package is only reporting 20% coverage. Before we get too alarmed at this, let's take a look at what we are trying to test.</p>
<p>If we first examine the <kbd>datastore.go</kbd> file, this is only an interface and therefore does not have any test files; however, the <kbd>memorystore.go</kbd> does. This file is well covered with 100% test coverage for this file. The files that are letting us down are our mock class and our MongoDB implementation.</p>
<p>Now the mock type I am not too bothered about, but the Mongo store is an interesting problem.</p>
<p>This type would be incredibly difficult to test due to the dependency on the MongoDB. We could create a mock implementation of the Mongo package to test our code, but this could be more complicated than the implementation. There are however some critical areas where we could make mistakes in this class. Consider line <strong>26</strong>:</p>
<pre>
c := s.DB("kittenserver").C("kittens") 
</pre>
<p>This line retrieves the collection <kbd>kittens</kbd> from the database <kbd>kittenserver</kbd>. If we make a simple spelling mistake here, then our application will not work. We do not want to wait until this code gets out into production to see that this is happening. We also do not want to have to manually test this as, in a larger application, this could be considerably time-consuming. Integration tests are really where our Cucumber tests shine. If you remember, we are writing some very high-level end-to-end tests to make sure that the input into our API results in the correct output. Because this is running against an actual database, if we had made such an error, then it would be picked up. So, while the Go coverage report states that we are not covered, it's because we have higher level tests that the Go test is not looking at, so we are covered. The central area where we could run into problems would be by omitting line <strong>23</strong>.</p>
<p>If we do not close the connection to the database after we have opened it, we are going to be leaking connections; after a while, we may find that we can no longer open another as the pool is exhausted. There is no simple way to test this, but there is, however, a way to catch the problem post-deploy. When we look at logging and monitoring in <a href="bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Logging and</em> <em>Monitoring</em>, we will see how we can expose such information to help us ensure our production system is functioning correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Behavioral Driven Development</h1>
                </header>
            
            <article>
                
<p><strong>Behavioral Driven Development</strong> (<strong>BDD</strong>) and is a technique often executed by an application framework called <strong>Cucumber</strong>. It was developed by Dan North and was designed to create a common ground between developers and product owners. In theory, it should be possible to test complete coverage of the system with BDD; however, since this would create a significant number of slow running tests it is not the best approach. What we should be doing is defining the boundaries of our system, and we can save the granularity for our unit tests.</p>
<p>In our Three Amigos group, we discuss the facets of the feature and what the essential qualities of it are and start to write scenarios.</p>
<p><strong>Sad path</strong></p>
<pre>
<strong>Scenario</strong>: User passes no search criteria    <br/>  Given I have no search criteria <br/>  When I call the search endpoint <br/>  Then I should receive a bad request message 
</pre>
<p><strong>Happy path</strong></p>
<pre>
<strong>Scenario</strong>: User passes valid search criteria    <br/>  Given I have valid search criteria <br/>  When I call the search endpoint <br/>  Then I should receive a list of kittens 
</pre>
<p>These scenarios are quite a simple example, but I think you can understand how, when using this language with non-technical people, it would be quite straightforward to come up with these descriptions. From an automation perspective what we then do is to write the steps which correspond to each of these <kbd>Given</kbd>, <kbd>When</kbd>, and <kbd>Then</kbd> statements.</p>
<p>For this book, we are going to take a look at the GoDog framework which allows us to implement the step definitions in Go. We will first need to install the application you can do this by running the command: <kbd>fgo get github.com/DATA-DOG/godog/cmd/godog</kbd></p>
<p>If we look at <kbd>features/search.feature</kbd> we can see that we have implemented this feature and the scenarios.</p>
<p>If we run the <kbd>godog ./</kbd> command to run these tests without first creating the features, we should see the following error message:</p>
<pre>
<strong>Feature: As a user when I call the search endpoint, I would like to receive a list of kittens</strong><br/><br/><strong>Scenario: Invalid query                       <br/># features/search.feature:4</strong><br/><strong>    Given I have no search criteria</strong><br/><strong>    When I call the search endpoint</strong><br/><strong>    Then I should receive a bad request message</strong><br/><br/><strong>Scenario: Valid query                     <br/># features/search.feature:9</strong><br/><strong>    Given I have valid search criteria</strong><br/><strong>    When I call the search endpoint</strong><br/><strong>    Then I should receive a list of kittens</strong><br/><br/><strong>2 scenarios (2 undefined)</strong><br/><strong>6 steps (6 undefined)</strong><br/><strong>321.121µs</strong>
</pre>
<p>You can implement step definitions for undefined steps with these snippets:</p>
<pre>
func iHaveNoSearchCriteria() error { <br/>    return godog.ErrPending <br/>} <br/><br/>func iCallTheSearchEndpoint() error { <br/>    return godog.ErrPending <br/>} <br/><br/>func iShouldReceiveABadRequestMessage() error { <br/>    return godog.ErrPending <br/>} <br/><br/>func iHaveAValidSearchCriteria() error { <br/>    return godog.ErrPending <br/>} <br/><br/>func iShouldReceiveAListOfKittens() error { <br/>    return godog.ErrPending <br/>} <br/><br/>func FeatureContext(s *godog.Suite) { <br/>    s.Step(`^I have no search criteria$`, iHaveNoSearchCriteria) <br/>    s.Step(`^I call the search endpoint$`, iCallTheSearchEndpoint) <br/>    s.Step(`^I should receive a bad request message$`, iShouldReceiveABadRequestMessage) <br/>    s.Step(`^I have a valid search criteria$`, iHaveAValidSearchCriteria) <br/>    s.Step(`^I should receive a list of kittens$`, iShouldReceiveAListOfKittens) <br/>} 
</pre>
<p>Usefully, this gives us the boilerplate to perform our steps; once we implement this and rerun the command we get a different message:</p>
<pre>
<strong>Feature: As a user when I call the search endpoint, I would like to receive a list of kittens</strong><br/><br/><strong>   Scenario: Invalid query                       # search.feature:4</strong><br/><strong>     Given I have no search criteria             # search_test.go:6 -&gt; github.com/nicholasjackson/building-microservices-in-go/chapter5/features.iHaveNoSearchCriteria</strong><br/><strong>     TODO: write pending definition</strong><br/><strong>     When I call the search endpoint</strong><br/><strong>     Then I should receive a bad request message</strong><br/><br/><strong>   Scenario: Valid query                     # search.feature:9</strong><br/><strong>     Given I have a valid search criteria    # search_test.go:18 -&gt; github.com/nicholasjackson/building-microservices-in-go/chapter5/features.iHaveAValidSearchCriteria</strong><br/><strong>       TODO: write pending definition</strong><br/><strong>     When I call the search endpoint</strong><br/><strong>     Then I should receive a list of kittens</strong><br/><br/><strong> 2 scenarios (2 pending)</strong><br/><strong> 6 steps (2 pending, 4 skipped)</strong><br/><strong> 548.978µs</strong>  
</pre>
<p>We can now start filling in the details for the steps which should fail as we have not yet written our code.</p>
<p>We can implement our code using plain Go which gives us the capability to use any of the interfaces and packages. Take a look at the example which corresponds to the method <kbd>iCallTheSearchEndpoint</kbd>:</p>
<pre>
23 func iCallTheSearchEndpoint() error { <br/>24   var request []byte <br/>25 <br/>26   response, err = http.Post("http://localhost:2323", "application/json", bytes.NewReader(request)) <br/>27   return err <br/>28 } <br/>29 <br/>30 func iShouldReceiveABadRequestMessage() error { <br/>31   if response.StatusCode != http.StatusBadRequest { <br/>32     return fmt.Errorf("Should have recieved a bad response") <br/>33   } <br/>34 <br/>35   return nil <br/>36 } 
</pre>
<p>Now we have some tests implemented we should run Cucumber tests as some of the steps should be passing. To test the system, we need to start our main application; we could split our main function out into a <kbd>StartServer</kbd> function, which could be called directly from Cucumber. However, that is omitting the fact that we forgot to call <kbd>StartServer</kbd> in the main function. For this reason, the best approach is to test the complete application in our Cucumber test from the outside in.</p>
<p>To do this, we are going to add a couple of new functions to the <kbd>features/search_test.go</kbd> file:</p>
<pre>
59 func FeatureContext(s *godog.Suite) { <br/>60   s.Step(`^I have no search criteria$`, iHaveNoSearchCriteria) <br/>61   s.Step(`^I call the search endpoint$`, iCallTheSearchEndpoint) <br/>62   s.Step(`^I should receive a bad request message$`, iShouldReceiveABadRequestMessage) <br/>63   s.Step(`^I have a valid search criteria$`, iHaveAValidSearchCriteria) <br/>64   s.Step(`^I should receive a list of kittens$`, iShouldReceiveAListOfKittens) <br/>65 <br/>66   s.BeforeScenario(func(interface{}) { <br/>67     startServer() <br/>68     fmt.Printf("Server running with pid: %v", server.Process.Pid) <br/>69   }) <br/>70 <br/>71   s.AfterScenario(func(interface{}, error) { <br/>72     server.Process.Kill() <br/>73   }) <br/>74 } <br/>75 <br/>76 var server *exec.Cmd <br/>77 <br/>78 func startServer() { <br/>79   server = exec.Command("go", "run", "../main.go") <br/>80   go server.Run() <br/>81   time.Sleep(3 * time.Second) <br/>82 } 
</pre>
<p>In line <strong>66</strong>, we are using the <kbd>BeforeScenario</kbd> method on <kbd>godog</kbd>: this allows us to run a function before our scenario starts. We would use this for clearing up any data in the data store, but in our simple example, we are just going to start our application server. Later on in this chapter, we will look at a more complex example, which uses Docker Compose to start a stack of containers containing our server and a database.</p>
<p>The <kbd>startServer</kbd> function spawns a new process to run <kbd>go run ../main.go</kbd>. We have to run this in <kbd>gofunc</kbd> as we do not want the test to block. Line <strong>81</strong> contains a small pause to see if our server has started. In reality, we should be checking the health endpoint of the API, but for now, this will suffice.</p>
<p>Line <strong>71</strong> will execute after the scenario has finished and tears down our server. If we don't do this then the next time we try to start our server it will fail as the process will already be running and bound to the port.</p>
<p>Let's go ahead and run our Cucumber tests, and the output should look something like this:</p>
<pre>
<strong>Feature: As a user when I call the search endpoint, I would like to receive a list of kittens</strong><br/><strong> Server running with pid: 91535</strong><br/><strong>   Scenario: Invalid query                       # search.feature:4</strong><br/><strong>     Given I have no search criteria             # search_test.go:17 -&gt; github.com/building-microservices-with-go/chapter4/features.iHaveNoSearchCriteria</strong><br/><strong>     When I call the search endpoint             # search_test.go:25 -&gt; github.com/building-microservices-with-go/chapter4/features.iCallTheSearchEndpoint</strong><br/><strong>     Then I should receive a bad request message # search_test.go:32 -&gt; github.com/building-microservices-with-go/chapter4/features.iShouldReceiveABadRequestMessage</strong><br/><strong> Server running with pid: 91615</strong><br/><strong>   Scenario: Valid query                     # search.feature:9</strong><br/><strong>     Given I have a valid search criteria    # search_test.go:40 -&gt; github.com/building-microservices-with-go/chapter4/features.iHaveAValidSearchCriteria</strong><br/><strong>     Do not have a valid criteria</strong><br/><strong>     When I call the search endpoint</strong><br/><strong>     Then I should receive a list of kittens</strong><br/><br/><strong> --- Failed scenarios:</strong><br/><br/><strong>     search.feature:10</strong><br/><br/><strong> 2 scenarios (1 passed, 1 failed)</strong><br/><strong> 6 steps (3 passed, 1 failed, 2 skipped)</strong><br/><strong> 6.010954682s</strong><br/><strong> make: *** [cucumber] Error 1</strong>  
</pre>
<p>Perfect! We are getting there, some of the steps are passing now, and one of the features is passing. We can now go ahead finish these tests off, but first, we need to look at how we can use Docker Compose to test against a real database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing with Docker Compose</h1>
                </header>
            
            <article>
                
<p>So far this has been relatively simple implementation, but it is not particularly useful as a real-world example. It is going to be pretty rare that you find yourself implementing an in-memory data store with only three items in it. More often than not you are going to be using a functioning database. Of course, the integration between a real database and our code needs testing; we need to ensure that the connection to the data store is correct and that the query we are sending to it is valid.</p>
<p>To do this, we need to spin up a real database and to do that we can use Docker-Compose as it is a fantastic way of starting our dependencies.</p>
<p>In our sample file <kbd>docker-compose.yml</kbd>, we have the following:</p>
<pre>
version: '2' <br/>services: <br/>  mongodb: <br/>    image: mongo <br/>    ports: <br/>      - 27017:27017 
</pre>
<p>When we run <kbd>docker-compose up</kbd> command, we will download the image of MongoDB and start an instance exposing these ports on our local host.</p>
<p>We now need to create a new struct in our project which is going to implement the store interface. We can then execute commands against the real database as opposed to using a mock or a simple in-memory store.</p>
<p>The implementation for <kbd>MongoStore</kbd> is quite straight forward. Looking at the file in <kbd>data/monogstore.go</kbd>, you will see that we have two additional methods not defined in our interface, namely:</p>
<pre>
DelleteAllKittens <br/>InsertKittens 
</pre>
<p>These are here because we need them for the setup of our functional tests.</p>
<p>If we look at our file <kbd>features/search_test.go</kbd> you will see that we have added a couple of extra calls to the <kbd>FeatureContext</kbd> method in our setup.</p>
<p>The first thing we are doing is to call the <kbd>waitForDB</kbd> method: because we cannot control when our Mongo instance is going to be ready to accept connections, we need to wait for it before kicking off the tests. The process is that we will try to create an instance to our <kbd>MongoStore</kbd> using the convenience method <kbd>NewMongoStore</kbd>, internally this is doing the following work:</p>
<pre>
10 // NewMongoStore creates an instance of MongoStore with the given connection string <br/>11 func NewMongoStore(connection string) (*MongoStore, error) { <br/>12   session, err := mgo.Dial(connection) <br/>13   if err != nil { <br/>14     return nil, err <br/>15   } <br/>16 <br/>17   return &amp;MongoStore{session: session}, nil <br/>18 } 
</pre>
<p>The <kbd>Dial</kbd> method attempts to connect to the instance of MongoDB specified in the connection string. If this connection fails, then an error is returned. In our code, if we receive an error, we return this to the caller of <kbd>NewMongoStore</kbd> with a nil instance of our struct. The <kbd>waitForDB</kbd> method works by repeatedly attempting to create this connection until it no longer receives an error. To avoid spamming the database while it is trying to start, we sleep for one second after every failed attempt, to a maximum time of 10 seconds. This method will block the main Go routine, but this is by design as we do not want the tests to execute until we are sure we have this connection:</p>
<pre>
 98 func waitForDB() { <br/> 99   var err error <br/>100 <br/>101   for i := 0; i &lt; 10; i++ { <br/>102     store, err = data.NewMongoStore("localhost") <br/>103     if err == nil { <br/>104       break <br/>105     } <br/>106     time.Sleep(1 * time.Second) <br/>107   } <br/>108 } 
</pre>
<p>We have also added some code to the <kbd>BeforeScenario</kbd> setup: the first thing we are going to do is wipe our database clearing down any previous test data. Clearing data is an incredibly important step as, should we have had any methods which mutate the data; we would not get predictable test results after each run.</p>
<p>Finally, we insert the <kbd>setupData</kbd> method into our test data, and then we will proceed to execute the tests.</p>
<p>We now have quite a few things to do before we can test our code, we need to run docker-compose, perform our tests and then stop docker-compose. An efficient way of scripting this process is to use Makefiles, Makefiles have been around forever and are still the primary software build mechanisms for many applications. They allow us to define commands in a simple text file which we can then run by executing <strong>make [command]</strong>.</p>
<p>If we look at the Cucumber command in the Makefile in the example repository, we can see how we script the steps that need to be carried out to run our tests. <span>W</span>e are starting our Mongo instance with <kbd>docker-compose</kbd>, running our Cucumber tests and then tearing the database down again:</p>
<pre>
cucumber: <br/>    docker-compose up -d <br/>    cd features &amp;&amp; godog ./ <br/>    docker-compose stop 
</pre>
<p>If you are wondering why we still need the <kbd>waitForDB</kbd> method when we are starting our database before running the tests, remember that Docker only knows when the primary process is executed. There can be a considerable lag between the commencement of the process and it being ready to accept connections. To run this we run <kbd>make cucumber</kbd> from the command line the result should be passing cucumber tests:</p>
<pre class="mce-root">
$make cucumber<br/>docker-compose up -d<br/>chapter4_mongodb_1 is up-to-date<br/>cd features &amp;&amp; godog ./<br/><strong>Feature</strong>: As a user when I call the search endpoint, I would like to receive a list of kittens<br/>Server running with pid: 88200<br/>  <strong>Scenario</strong>: Invalid query # search.feature:4<br/>    Given I have no search criteria # search_test.go:21 -&gt; github.com/building-microservices-with-go/chapter4/features.iHaveNoSearchCriteria<br/>    When I call the search endpoint # search_test.go:29 -&gt; github.com/building-microservices-with-go/chapter4/features.iCallTheSearchEndpoint<br/>    Then I should receive a bad request message # search_test.go:40 -&gt; github.com/building-microservices-with-go/chapter4/features.iShouldReceiveABadRequestMessage<br/>Server running with pid: 88468<br/>  <strong>Scenario</strong>: Valid query # search.feature:9<br/>    Given I have a valid search criteria # search_test.go:48 -&gt; github.com/building-microservices-with-go/chapter4/features.iHaveAValidSearchCriteria<br/>    When I call the search endpoint # search_test.go:29 -&gt; github.com/building-microservices-with-go/chapter4/features.iCallTheSearchEndpoint<br/>    Then I should receive a list of kittens # search_test.go:54 -&gt; github.com/building-microservices-with-go/chapter4/features.iShouldReceiveAListOfKittens<br/><br/>2 scenarios (2 passed)<br/>6 steps (6 passed)<br/>7.028664s<br/>docker-compose stop<br/>Stopping chapter4_mongodb_1 ... done
</pre>
<p>That is all for this section, we have learned that, with a few well-placed patterns, it is easy to write a robust test suite that will keep us safe and sound asleep instead of getting up in the middle of the night to diagnose a broken system. In the next section, we are going to look at some of the fantastic features of Go to ensure that our code is fast and optimized.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benchmarking and profiling</h1>
                </header>
            
            <article>
                
<p>Go has two excellent ways to analyze the performance of your code. We have benchmark tests and the fantastic pprof.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benchmarks</h1>
                </header>
            
            <article>
                
<p>Benchmarking is a way of measuring the performance of your code by executing it multiple times with a fixed workload. We took a look at this briefly in <a href="ba3a8742-94e7-4e47-8a47-1324a277a7f9.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Microservices</em>, where we ascertained that the <kbd>json.Marshal</kbd> method was slower than the <kbd>json.Encode</kbd> method. While this is a useful feature, I find it tough to work out what I should benchmark. If I am writing an algorithm, then this is relatively straightforward. However, when writing a microservice that is predominately interacting with a database, it is far more challenging.</p>
<p>To demonstrate how easy it is to execute benchmarks in Go, take a look at <kbd>chandlers/search_bench_test.go</kbd>:</p>
<pre>
  11 func BenchmarkSearchHandler(b *testing.B) { <br/>  12   mockStore = &amp;data.MockStore{} <br/>  13   mockStore.On("Search", "Fat Freddy's Cat").Return([]data.Kitten{ <br/>  14     data.Kitten{ <br/>  15       Name: "Fat Freddy's Cat", <br/>  16     }, <br/>  17   }) <br/>  18 <br/>  19   search := Search{DataStore: mockStore} <br/>  20 <br/>  21   for i := 0; i &lt; b.N; i++ { <br/>  22     r := httptest.NewRequest("POST", "/search", <br/>         bytes.NewReader([]byte(`{"query":"Fat Freddy's Cat"}`))) <br/>  23     rr := httptest.NewRecorder() <br/>  24     search.ServeHTTP(rr, r) <br/>  25   } <br/>  26 } 
</pre>
<p>The most important part of this code is hidden away at line <strong>21</strong>:</p>
<pre>
for n := 0; n &lt; b.N; n++ 
</pre>
<p>When running a benchmark, Go needs to run it multiple times to get an accurate reason. The number of times that the benchmark will run is the field <kbd>N</kbd> on the benchmark's struct. Before setting this number, Go will execute a few iterations of your code to get an approximate measurement of the execution time.</p>
<p>We would execute our benchmark using the <kbd>go test-bench -benchmem</kbd> command:</p>
<pre>
<strong>go test -bench=. -benchmem</strong><br/><strong>BenchmarkSearchHandler-8          50000         43183 ns/op       49142 B/op          68 allocs/op</strong><br/><strong>PASS</strong><br/><strong>ok      github.com/building-microservices-with-go/chapter4/handlers    2.495s</strong>  
</pre>
<p>Here we are passing an additional flag to see the memory allocations for each execution. We know that our handler when running with the mock takes 43,183 nanoseconds or 0.043183 milliseconds to execute and performs 68 memory allocations. It'd be good if the code would run this fast when running in real life, but we might have to wait a few years before we see this level of speed from an API connected to a database.</p>
<p>One of the other nice features of benchmark tests is that we can run them and it outputs profiles which can be used with pprof:</p>
<pre>
<strong>go test -bench=. -cpuprofile=cpu.prof -blockprofile=block.prof -memprofile=mem.prof</strong>  
</pre>
<p>The output of this command will give us more information about where this time and memory is being consumed and can help us to optimize our code correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Profiling</h1>
                </header>
            
            <article>
                
<p>When we wish to take a look at the speed of our program, the best technique we can employ is profiling. Profiling automatically samples your running application while it is executing; and then we can compute that data, such as the running time of a particular function, into a statistical summary called a profile.</p>
<p>Go supports three different types of profiling:</p>
<ul>
<li><strong>CPU</strong>: Identifies the tasks which require the most CPU time</li>
<li><strong>Heap</strong>: Identifies the statements responsible for allocating the most memory</li>
<li><strong>Blocking</strong>: Identifies the operations responsible for blocking Go routines for the longest time</li>
</ul>
<p>If we would like to enable profiling on our application, we can do one of two things:</p>
<ul>
<li>Add <kbd>import "net/http/pprof"</kbd> to your startup file</li>
<li>Manually start profiling</li>
</ul>
<p>The first option is the most straightforward. You only add it to the beginning of your main Go file and, if you are not already running an HTTP web server, start one:</p>
<pre>
import _ "net/http/pprof" <br/><br/>go func() { <br/>     log.Println(http.ListenAndServe("localhost:6060", nil)) <br/> }() 
</pre>
<p>This method then exposes various paths on your HTTP server at <kbd>/debug/pprof/</kbd> which can then be accessed via a URL. The side effect of this, however, is that when this import statement is in your go file, then you will be profiling, which not only could slow down your application, but you also don't want to expose this information for public consumption.</p>
<p>Another method of profiling is to start the profiler when you start your application by passing some additional command line flags:</p>
<pre>
19 var cpuprofile = flag.String("cpuprofile", "", "write cpu profile to <br/>   file") <br/>20 var memprofile = flag.String("memprofile", "", "write memory profile  <br/>   to file") <br/>21 var store *data.MongoStore <br/>22 <br/>23 func main() { <br/>24   flag.Parse() <br/>25 <br/>26   if *cpuprofile != "" { <br/>27     fmt.Println("Running with CPU profile") <br/>28     f, err := os.Create(*cpuprofile) <br/>29     if err != nil { <br/>30       log.Fatal(err) <br/>31     } <br/>32     pprof.StartCPUProfile(f) <br/>33   } <br/>34 <br/>35   sigs := make(chan os.Signal, 1) <br/>36   signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) <br/>37 <br/>38   go func() { <br/>39     &lt;-sigs <br/>40     fmt.Println("Finished") <br/>41     if *memprofile != "" { <br/>42       f, err := os.Create(*memprofile) <br/>43       if err != nil { <br/>44         log.Fatal(err) <br/>45       } <br/>46       runtime.GC() <br/>47       pprof.Lookup("heap").WriteTo(f, 0) <br/>48       defer f.Close() <br/>49     } <br/>50     if *cpuprofile != "" { <br/>51       pprof.StopCPUProfile() <br/>52     } <br/>53 <br/>54     os.Exit(0) <br/>55   }() 
</pre>
<p>On line <strong>26</strong>, we are checking whether we have specified an output file for CPU profiling, and if so, we are creating the file and then starting the profiler with <kbd>pprof.StartCPUProfile(f)</kbd>, and passing it a reference to the file:</p>
<pre>
func StartCPUProfile(w io.Writer) error 
</pre>
<p>The <kbd>StartCPUProfile</kbd> function enables the CPU profiling for the current process and buffers the output to <kbd>w</kbd>. While running, the CPU profiler will stop your application roughly 100 times per second and record the data.</p>
<p>To profile heap allocations, we use a slightly different command:</p>
<pre>
pprof.Lookup("heap").WriteTo(f, 0) <br/>func Lookup(name string) *Profile 
</pre>
<p>The <kbd>Lookup()</kbd> function returns the profile with the given name, or if no such profile exists, the predefined profiles available are:</p>
<pre>
goroutine    - stack traces of all current goroutines <br/>heap         - a sampling of all heap allocations <br/>threadcreate - stack traces that led to the creation of new OS threads <br/>block        - stack traces that led to blocking on synchronization primitives <br/><br/>fuc (p *Profile) WriteTo(w io.Writer, debug int) error 
</pre>
<p><kbd>WriteTo</kbd> outputs the profile to the given write in the pprof format. If we set the debug flag to <kbd>1</kbd>, then <kbd>WriteTo</kbd> will add comments to function names and line numbers instead of just hexadecimal addresses that pprof uses. These comments are so you can read the file without needing any special tooling.</p>
<p>If you look at the example code in the folder <kbd>benchmark</kbd>, you will find an example profile and the binary from which it was generated.</p>
<p>We can now run the pprof tool to examine what is going on. To do this, we need to run the tool on the command line and provide a reference to the binary that the profile was executed against, and also the profile:</p>
<pre>
<strong>go tool pprof ./kittenserver ./cpu.prof</strong>  
</pre>
<p>The simplest command we can run is <kbd>top</kbd>. Top will show us the functions which consumed the most CPU during the execution of our application:</p>
<pre>
Entering interactive mode (type "help" for commands) <br/> (pprof) top <br/> 24460ms of 42630ms total (57.38%) <br/> Dropped 456 nodes (cum &lt;= 213.15ms) <br/> Showing top 10 nodes out of 163 (cum &gt;= 790ms) <br/>       flat  flat%   sum%        cum   cum% <br/>    16110ms 37.79% 37.79%    16790ms 39.39%  syscall.Syscall <br/>     2670ms  6.26% 44.05%     2670ms  6.26%  runtime._ExternalCode <br/>     1440ms  3.38% 47.43%     1560ms  3.66%  syscall.Syscall6 <br/>      900ms  2.11% 49.54%      900ms  2.11%  runtime.epollctl <br/>      830ms  1.95% 51.49%     2370ms  5.56%  runtime.mallocgc <br/>      610ms  1.43% 52.92%     1470ms  3.45%  runtime.pcvalue <br/>      510ms  1.20% 54.12%      510ms  1.20%  runtime.heapBitsSetType <br/>      470ms  1.10% 55.22%     2810ms  6.59%  runtime.gentraceback <br/>      470ms  1.10% 56.32%      470ms  1.10%  runtime.memmove <br/>      450ms  1.06% 57.38%      790ms  1.85%  runtime.deferreturn <br/> (pprof) 
</pre>
<p>The main offender in this instance is <kbd>syscall.Syscall</kbd>. If we look this up in the documentation, we find that package syscall contains an interface to the low-level operating system primitives.</p>
<p>On its own this output is not particularly useful, so let's generate a call graph which will show us more detail. We can do this again from the pprof tool. However, we do need to install Graphviz first. If you are using macOS then you can install this with brew:</p>
<pre>
<strong>brew install graphviz</strong>  
</pre>
<p>If you are using a Linux based system and have the apt package manager, you can use that:</p>
<pre>
<strong>apt-get install graphviz</strong>  
</pre>
<p>The output for this looks like this <kbd>benchmark/cpu.png</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/621e4bd5-e5a2-4e16-aadf-0fc8dc90d0e0.png"/></div>
<p>That is quite something! However, we can see that <kbd>syscall.Syscall</kbd> is in the largest font as it is responsible for consuming the most CPU. If we start here at the bottom and start tracing backward, we can see that the root of this seems to be our data store's search function.</p>
<p>To get a closer look at exactly where this is happening, we can use the list command and then pass the name of an object or method we would like to investigate further:</p>
<pre>
<strong>Entering interactive mode (type "help" for commands)</strong><br/><strong>(pprof) list Search</strong><br/><strong>Total: 42.63s</strong><br/><strong>ROUTINE ======================== github.com//building-microservices-with-go/chapter4/data.(*MongoStore).Search in github.com/building-microservices-with-go/chapter4/data/mongostore.go</strong><br/><strong>    40ms      7.92s (flat, cum) 18.58% of Total</strong><br/><strong>       .         .    16:</strong><br/><strong>       .         .    17:    return &amp;MongoStore{session: session}, nil</strong><br/><strong>       .         .    18:}</strong><br/><strong>       .         .    19:</strong><br/><strong>       .         .    20:// Search returns Kittens from the MongoDB <br/>                      instance which have the name name</strong><br/><strong>    40ms       40ms     21:func (m *MongoStore) Search(name string) <br/>                        []Kitten {</strong><br/><strong>       .     270ms    22:    s := m.session.Clone()</strong><br/><strong>      .      10ms    23:    defer s.Close()</strong><br/><strong>      .         .    24:</strong><br/><strong>      .      20ms    25:    var results []Kitten</strong><br/><strong>      .      70ms    26:    c := s.DB("kittenserver").C("kittens")</strong><br/><strong>      .     7.30s    27:    err := c.Find(Kitten{Name: <br/>                              name}).All(&amp;results)</strong><br/><strong>       .         .    28:    if err != nil {</strong><br/><strong>       .         .    29:        return nil</strong><br/><strong>       .         .    30:    }</strong><br/><strong>       .         .    31:</strong><br/><strong>       .     210ms    32:    return results</strong><br/><strong>       .         .    33:}</strong><br/><strong>       .         .    34:</strong><br/><strong>       .         .    35:// DeleteAllKittens deletes all the kittens <br/>                      from the datastore</strong><br/><strong>      .         .    36:func (m *MongoStore) DeleteAllKittens() {</strong><br/><strong>      .         .    37:    s := m.session.Clone()</strong> 
</pre>
<p>When we do this with the search method, we can see that a huge percentage of our CPU cycles were spent executing queries to MongoDB. If we look at the other route from <kbd>syscall.Syscall</kbd>, it shows that another large consumer is the <kbd>http.ResponseWriter</kbd> interface.</p>
<p>This output all makes sense as we are not doing anything too clever in our API; it just retrieves some data from a database. The nice thing about pprof is that we can use the same commands to query the heap usage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned some best practice approaches to testing microservices in Go. We have looked at the testing package, including some special features for dealing with requests and responses. We have also looked at writing integration tests with Cucumber.</p>
<p>Ensuring that your code works without fault, however, is only part of the job, we also need to make sure that our code is performant, and Go has some excellent tools for managing this too.</p>
<p>I would always recommend that you test your code and that you do this religiously. As for performance optimization, this is open for debate, no doubt you have heard comments that premature optimization is the root of all evil. However, this quote from Donald Knuth is much-misunderstood: he did not mean that you should never optimize until you have a problem; he said that you should only optimize what matters. With pprof, we have an easy way to figure out what, if anything, actually matters. Include the practice of profiling into your development routine, and you will have faster and more efficient applications, profiling is also an excellent technique to understand your application better when you are trying to track down that tricky bug.</p>
<div class="packt_quote">"Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%."<br/>
- Donald Knuth</div>


            </article>

            
        </section>
    </body></html>