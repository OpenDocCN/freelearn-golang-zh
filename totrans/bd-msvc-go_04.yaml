- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you try to define what testing is, you will come up with a multitude of
    answers, and many of us will not understand the full benefits of testing until
    we've been burnt by buggy software or we have tried to change a complex code base
    which has no tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I tried to define testing, I came up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The art of a good night''s sleep is knowing you will not get woken by a support
    call and the piece of mind from being able to confidently change your software
    in an always moving market."'
  prefs: []
  type: TYPE_NORMAL
- en: OK, so I am trying to be funny, but the concept is correct. Nobody enjoys debugging
    poorly written code, and indeed, nobody enjoys the stress caused when a system
    fails. Starting out with a mantra of quality first can alleviate many of these
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the last 20 years, techniques like TDD have become commonplace. In some
    instances, it is not as common as I would like, but at least people are talking
    about testing now. In some ways, we have the Agile Alliance to thank for this:'
  prefs: []
  type: TYPE_NORMAL
- en: the principle of releasing little and often provides significant business benefits;
    the downside (or the benefit, depending on your viewpoint) to releasing little
    and often is that you can no longer spend three months running through a regression
    test suite before you release to market.
  prefs: []
  type: TYPE_NORMAL
- en: In my office, context switching is one of the biggest complaints. Nobody enjoys
    having to drop what they are doing to investigate a problem on work that they
    or even a colleague may have carried out months or years ago. We want to be moving
    forward; and to ensure we can do that, we have to make sure that what we have
    previously delivered meets the specification and is of high enough quality to
    meet the client's requirement.
  prefs: []
  type: TYPE_NORMAL
- en: I also mentioned a change in my definition, and one of the biggest problems
    with change is the concern that the change you are making may have an undesirable
    effect on another part of the system. This effect applies to microservices as
    well as large monolithic systems.
  prefs: []
  type: TYPE_NORMAL
- en: What if I also told you that the side effect of code that is easy to test is
    probably well-written code that is loosely coupled and has the right abstractions?
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing, however, is not just about the developer: there is a definite need
    for manual testing by people detached from the code base. This exploratory testing
    can bring out missing requirements or incorrect assumptions. In itself, this is
    a specialized field and way beyond the scope of this book, so we are going to
    concentrate on the kind of testing that you should be doing.'
  prefs: []
  type: TYPE_NORMAL
- en: The testing pyramid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mike Cohn is credited with having created the concept of a testing pyramid in
    his book *Succeeding with Agile*. The concept is that your cheapest (fastest)
    tests to run, which will be your unit tests, go at the bottom of the pyramid;
    service level integration tests are on top of this, and at the very top, you place
    full end-to-end tests, which are the costliest element. Because this is a pyramid,
    the number of tests gets smaller as you move up the pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e41fa6ed-58a7-46a2-a5af-8358f58a61ee.png)'
  prefs: []
  type: TYPE_IMG
- en: In the early days of automated testing, all the testing was completed at the
    top of the pyramid. While this did work from a quality perspective, it meant the
    process of debugging the area at fault would be incredibly complicated and time-consuming.
    If you were lucky, there might be a complete failure which could be tracked down
    to a stack trace. If you were unlucky, then the problem would be behavioral; and
    even if you knew the system inside out, it would involve plowing through thousands
    of lines of code and manually repeating the action to reproduce the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Outside-in development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing tests, I like to follow a process called outside-in development.
    With outside-in development, you start by writing your tests almost at the top
    of the pyramid, determine what the functionality is going to be for the story
    you are working on, and then write some failing test for this story. Then you
    work on implementing the unit tests and code which starts to get the various steps
    in the behavioral tests to pass.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e16cc217-a7e6-4d93-a49e-1c0efc11e146.png)'
  prefs: []
  type: TYPE_IMG
- en: This initial specification also becomes the living documentation for your system.
    We will go into more detail as to how you can create this later in this chapter,
    but more often than not it is written in a language like **Gherkin** and is defined
    by working in a group with a domain specialist like a product owner, a developer,
    and a testing expert. The intention behind Gherkin is to create a universal language
    that everyone understands. This ubiquitous language uses verbs and nouns that
    have special meaning to the team, and which is almost always domain-specific,
    but should also be understandable to outsiders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The feature is the story which, in an agile environment is owned by the product
    owner. The feature is then broken down into scenarios which explain in greater
    detail the qualities that the code must have to be acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When we get to the section on BDD in a little while we will examine this in
    greater depth; we will also look at a framework for Go for writing and executing
    Cucumber specifications. Now, however, I am going to break the rules of outside
    in development by showing you how to write great unit tests in Go. The concepts
    we are about to learn will be greatly beneficial when we do start to look at BDD,
    so I think it is best we cover them first. Like the previous chapters, it will
    be useful to you when reading through this chapter to have the source code handy;
    you can clone the code from the following location: [https://github.com/building-microservices-with-go/chapter4.git](https://github.com/building-microservices-with-go/chapter4.git)'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our unit tests go right down to the bottom of the pyramid. This book was never
    intended to be a lesson in TDD, there are plenty of better places to learn that.
    We will, however, take a look at the testing framework which is built into Go.
    Before we do that, let''s just remind ourselves of the three laws of testing as
    defined by the awesome Uncle Bob Martin in his book *Clean Code*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First law**: You may not write production code until you have written a failing
    unit test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second law**: You may not write more of a unit test than is sufficient to
    fail, and not compiling is failing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third law**: You may not write more production code than is sufficient to
    pass the currently failing test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most effective ways to test a microservice in Go is not to fall into
    the trap of trying to execute all the tests through the HTTP interface. We need
    to develop a pattern that avoids creating a physical web server for testing our
    handlers, the code to create this kind of test is slow to run and incredibly tedious
    to write. What need to be doing is to test our handlers and the code within them
    as unit tests. These tests will run far quicker than testing through the web server,
    and if we think about the coverage, we will be able to test the wiring of the
    handlers in the Cucumber tests that execute a request to the running server which
    overall gives us 100% coverage of our code.
  prefs: []
  type: TYPE_NORMAL
- en: '`main.go`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will see in the main function that we have split the handlers out into a
    separate package. Breaking up the code in this way allows us to test these in
    isolation, so let's go ahead and write some unit tests for our `SearchHandler`.
  prefs: []
  type: TYPE_NORMAL
- en: The convention is that we define our test files in the same folder as the package
    to which they belong, and we name them the same as the file they are testing followed
    by `_test`. For our example, we are going to write some tests for our `SearchHandler`
    which lives in the `handlers/search.go` file; therefore our test file will be
    named `handlers/search_test.go`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The signature for a test method looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The name of the test must have a particular name beginning with `Test` and then
    immediately following this an uppercase character or number. So we could not call
    our test `TestmyHandler`, but we could name it `Test1Handler` or `TestMyHandler`.
  prefs: []
  type: TYPE_NORMAL
- en: Again, drawing on Uncle Bob's wisdom, we need to think about the names of our
    tests as carefully as we would the names for our methods in our production code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test we are going to write is the one who will validate that search
    criteria have been sent with the request and the implementation is going to look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `net/http/httptest` package has two fantastic convenience methods for us
    `NewRequest` and `NewResponse`, if you are familiar with unit testing concepts,
    then one of the fundamentals isolate dependency. Often we replace the dependencies
    with Mocks or Spies which allow us to test the behavior of our code without having
    to execute code in the dependencies. These two functions enable us to do exactly
    this; they generate Mock versions of the dependent objects `http.Request` and
    `http.ResponseWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: httptest.NewRequest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first line we need to pay attention to is line **11**: the `net/http/httptest`
    package has some nice convenience methods for us. The `NewRequest` method returns
    an incoming server request which we can then pass to our `http.Handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can pass parameters to the method and the target, which is either the path
    or an absolute URL. If we only pass a path, then `example.com` will be used as
    our host setting. Finally, we can give it an `io.Reader` file which will correspond
    to the body of the request; if we do not pass a nil value then `Request.ContentLength`
    is set.
  prefs: []
  type: TYPE_NORMAL
- en: httptest.NewRecorder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In line **12** we are creating a `ResponseRecorder` type: this is going to
    be our instance of `ResponseWriter` that we pass to the handler. Because a handler
    has no return function to validate correct operation, we need to check what has
    been written to the output. The `ResponseRecorder` type is an implementation of
    `http.ResponseWriter` which does just that: it records all the mutations we make
    so that it is later possible to make our assertions against it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: All we then need to do is to call the `ServeHTTP` method with our dummy request
    and response and then assert that we have the correct outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Go does not have an assertion library as you would find with RSpec or JUnit.
    We will look at a third-party framework later in this chapter, but for now, let's
    concentrate on the standard packages.
  prefs: []
  type: TYPE_NORMAL
- en: In line **16**, we are checking to see if the response code returned from the
    handler is equal to the expected code `http.BadRequest`. If it is not, then we
    call the Errorf method on the testing framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**ErrorF**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `Errorf` function takes the parameters of a format string and a variadic
    list of parameters; internally this calls the `Logf` method before calling `Fail`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run our tests by running the command `go test -v -race ./...` , we should
    see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `-v` flag will print the output in a verbose style, and it will also print
    all the text written to the output by the application, even if the test succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: The `-race` flag enables Go's race detector which holds discover bugs with concurrency
    problems. A data race occurs when two Go routines access the same variable concurrently,
    and at least one of the accesses is a write. The race flag adds a small overhead
    to your test run, so I recommend you add it to all executions.
  prefs: []
  type: TYPE_NORMAL
- en: Using `-./...` as our final parameter allows us to run all our tests in the
    current folder as well as the child folders, it saves us from manually having
    to construct a list of packages or files to test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a failing test we can go ahead and write the implementation to
    make the test pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we rerun the tests, we can see that they have succeeded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This output is awesome; but what if passing a query to the request with a blank
    string constitutes a failure? Time to write another test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This test is very similar to the last one; the only difference is that we are
    passing some JSON in the request body. While this test will fail correctly, we
    should take the lead from Uncle Bob and refactor this to make it more readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We have refactored our test to add a setup method which is shared across the
    two tests, the intention behind this is to keep our tests focused on three core
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assert
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bad tests with duplicated code can be worse than bad code: your tests should
    be clear, easy to understand and contain the same care that you would add to your
    production code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if the test fails, we can go ahead and update our code to implement the
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All we needed to do was make a simple modification to our `if` statement. As
    our system grows in complexity and we find more cases for what constitutes an
    invalid search query, we will refactor this into a separate method; but, for now,
    this is the minimum we need to do to make the test pass.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection and mocking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get the tests that return items from the `Search` handler to pass, we are
    going to need a data store. Whether we implement our data store in a database
    or a simple in-memory store we do not want to run our tests against the actual
    data store as we will be checking both data store and our handler. For this reason,
    we are going to need to manage the dependencies on our handler so that we can
    replace them in our tests. To do this, we are going to use a technique called
    dependency injection where we will pass our dependencies into our handler rather
    than creating them internally.
  prefs: []
  type: TYPE_NORMAL
- en: This method allows us to replace these dependencies with stubs or mocks when
    we are testing the handler, making it possible to control the behavior of the
    dependency and check how the calling code responds to this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do anything, we need to create our dependency. In our simple example,
    we are going to create an in-memory data store which has a single method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To replace the type with a mock, we need to change our handler to depend on
    an interface which represents our data store. We can then interchange this with
    either an actual data store or a mock instance of the store without needing to
    change the underlying code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can now go ahead and create the implementation for this. Since this is a
    simple example, we are going to hardcode our list of kittens as a slice and the
    search method will just select from this slice when the criteria given as a parameter
    matches the name of the kitten.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, great; we now have our data store created, so let''s see how we are going
    to modify our handler to accept this dependency. It is quite simple: because we
    created a struct which implements the `ServeHTTP` method, we can just add our
    dependencies onto this struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note how we are using a reference to the interface rather than the concrete
    type, which allows us to interchange this object with anything that implements
    the store interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, back to our unit tests: we would like to ensure that, when we call the
    `ServeHTTP` method with a search string, we are querying the data store and returning
    the kittens from it.'
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we are going to create a mock instance of our data store. We could
    create the mock ourselves; however, there is an excellent package by Matt Ryer
    who incidentally is also a Packt author. Testify ([https://github.com/stretchr/testify.git](https://github.com/stretchr/testify.git))
    has a fully featured mocking framework with assertions. It also has an excellent
    package for testing the equality of objects in our tests and removes quite a lot
    of the boilerplate code we have to write.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the data package, we are going to create a new file called `mockstore.go`.
    This structure will be our mock implementation of the data store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In line **6**, we are defining our `MockStore` object. There is nothing unusual
    about this, except you will note that it is embedding the `mock.Mock` type. Embedding
    Mock will give us all of the methods on the `mock.Mock` struct.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we write the implementation for our search method, we are first calling
    the `Called` method and passing it the arguments that are sent to `Search`. Internally,
    the mock package is recording that this method was called and with what parameters
    so that we can later write an assertion against it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we are returning `args.Get(0).(Kitten)`. When we call the `Called`
    method, the mock returns us a list of arguments that we provided in the setup.
    We are casting this to our output type and returning to the caller. Let's take
    a quick look at the test method and see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Line **57** is the start of our test setup. The first thing we are going to
    do is to create an instance of our `MockStore`. We then set this as a dependency
    for our `Search` handler. If we skip back up the file to line **38**, you will
    see that we are calling the `On` method on our `mockStore`. The `On` method is
    a setup method for the mock and has the signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If we do not call the `On` method with the parameter `Search` then when we call
    the `Search` method in our code we will get an exception from the test saying
    that `Search` has been called yet has not been setup. One of the reasons why I
    like to use mocking rather than a simple Stub is this ability to assert that a
    method has been called and we can explicitly dictate the behavior that the code
    under test is allowed to exhibit. This way we can ensure that we are not doing
    work the output of which has not been tested.
  prefs: []
  type: TYPE_NORMAL
- en: In our instance, we are setting up the condition that, when the `Search` method
    is called with the parameter `Fat Freddy's Cat`, we would like to return an array
    of kittens.
  prefs: []
  type: TYPE_NORMAL
- en: The assertion is that we are calling the `Search` method on the data store and
    passing it the query that was sent in the HTTP response. Using assertions in this
    way is a handy technique as it allows us to test the unhappy path such as when
    a data store may not be able to return data due to an internal error or another
    reason. If we were trying to test this with an integration test, it could be tough
    to persuade the database to fail on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Why don't you spend five minutes as a little exercise and go ahead and write
    this code to finish off?
  prefs: []
  type: TYPE_NORMAL
- en: 'Did it all work? Don''t worry if not, you can just check out the example code
    to see where you have gone wrong, but I hope that the process is useful. You can
    see how you can take a measured approach through two layers of testing to produce
    a working application. These tests are now your safety net: whenever you change
    the code to add a new feature, you can be sure that you are not breaking something
    unintentionally.'
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code coverage is an excellent metric to ensure that the code you are writing
    has adequate coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most simplistic way of getting a readout of test coverage is to execute
    our tests with the `-cover` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this against our example code in the root folder of our example code
    we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our handlers look beautiful: we have 100% coverage of this package. However,
    our data package is only reporting 20% coverage. Before we get too alarmed at
    this, let''s take a look at what we are trying to test.'
  prefs: []
  type: TYPE_NORMAL
- en: If we first examine the `datastore.go` file, this is only an interface and therefore
    does not have any test files; however, the `memorystore.go` does. This file is
    well covered with 100% test coverage for this file. The files that are letting
    us down are our mock class and our MongoDB implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Now the mock type I am not too bothered about, but the Mongo store is an interesting
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This type would be incredibly difficult to test due to the dependency on the
    MongoDB. We could create a mock implementation of the Mongo package to test our
    code, but this could be more complicated than the implementation. There are however
    some critical areas where we could make mistakes in this class. Consider line
    **26**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This line retrieves the collection `kittens` from the database `kittenserver`.
    If we make a simple spelling mistake here, then our application will not work.
    We do not want to wait until this code gets out into production to see that this
    is happening. We also do not want to have to manually test this as, in a larger
    application, this could be considerably time-consuming. Integration tests are
    really where our Cucumber tests shine. If you remember, we are writing some very
    high-level end-to-end tests to make sure that the input into our API results in
    the correct output. Because this is running against an actual database, if we
    had made such an error, then it would be picked up. So, while the Go coverage
    report states that we are not covered, it's because we have higher level tests
    that the Go test is not looking at, so we are covered. The central area where
    we could run into problems would be by omitting line **23**.
  prefs: []
  type: TYPE_NORMAL
- en: If we do not close the connection to the database after we have opened it, we
    are going to be leaking connections; after a while, we may find that we can no
    longer open another as the pool is exhausted. There is no simple way to test this,
    but there is, however, a way to catch the problem post-deploy. When we look at
    logging and monitoring in [Chapter 7](bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml),
    *Logging and* *Monitoring*, we will see how we can expose such information to
    help us ensure our production system is functioning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral Driven Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Behavioral Driven Development** (**BDD**) and is a technique often executed
    by an application framework called **Cucumber**. It was developed by Dan North
    and was designed to create a common ground between developers and product owners.
    In theory, it should be possible to test complete coverage of the system with
    BDD; however, since this would create a significant number of slow running tests
    it is not the best approach. What we should be doing is defining the boundaries
    of our system, and we can save the granularity for our unit tests.'
  prefs: []
  type: TYPE_NORMAL
- en: In our Three Amigos group, we discuss the facets of the feature and what the
    essential qualities of it are and start to write scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sad path**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Happy path**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: These scenarios are quite a simple example, but I think you can understand how,
    when using this language with non-technical people, it would be quite straightforward
    to come up with these descriptions. From an automation perspective what we then
    do is to write the steps which correspond to each of these `Given`, `When`, and
    `Then` statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this book, we are going to take a look at the GoDog framework which allows
    us to implement the step definitions in Go. We will first need to install the
    application you can do this by running the command: `fgo get github.com/DATA-DOG/godog/cmd/godog`'
  prefs: []
  type: TYPE_NORMAL
- en: If we look at `features/search.feature` we can see that we have implemented
    this feature and the scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the `godog ./` command to run these tests without first creating
    the features, we should see the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can implement step definitions for undefined steps with these snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Usefully, this gives us the boilerplate to perform our steps; once we implement
    this and rerun the command we get a different message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can now start filling in the details for the steps which should fail as we
    have not yet written our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement our code using plain Go which gives us the capability to use
    any of the interfaces and packages. Take a look at the example which corresponds
    to the method `iCallTheSearchEndpoint`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now we have some tests implemented we should run Cucumber tests as some of the
    steps should be passing. To test the system, we need to start our main application;
    we could split our main function out into a `StartServer` function, which could
    be called directly from Cucumber. However, that is omitting the fact that we forgot
    to call `StartServer` in the main function. For this reason, the best approach
    is to test the complete application in our Cucumber test from the outside in.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we are going to add a couple of new functions to the `features/search_test.go`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In line **66**, we are using the `BeforeScenario` method on `godog`: this allows
    us to run a function before our scenario starts. We would use this for clearing
    up any data in the data store, but in our simple example, we are just going to
    start our application server. Later on in this chapter, we will look at a more
    complex example, which uses Docker Compose to start a stack of containers containing
    our server and a database.'
  prefs: []
  type: TYPE_NORMAL
- en: The `startServer` function spawns a new process to run `go run ../main.go`.
    We have to run this in `gofunc` as we do not want the test to block. Line **81**
    contains a small pause to see if our server has started. In reality, we should
    be checking the health endpoint of the API, but for now, this will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Line **71** will execute after the scenario has finished and tears down our
    server. If we don't do this then the next time we try to start our server it will
    fail as the process will already be running and bound to the port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and run our Cucumber tests, and the output should look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! We are getting there, some of the steps are passing now, and one of
    the features is passing. We can now go ahead finish these tests off, but first,
    we need to look at how we can use Docker Compose to test against a real database.
  prefs: []
  type: TYPE_NORMAL
- en: Testing with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far this has been relatively simple implementation, but it is not particularly
    useful as a real-world example. It is going to be pretty rare that you find yourself
    implementing an in-memory data store with only three items in it. More often than
    not you are going to be using a functioning database. Of course, the integration
    between a real database and our code needs testing; we need to ensure that the
    connection to the data store is correct and that the query we are sending to it
    is valid.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we need to spin up a real database and to do that we can use Docker-Compose
    as it is a fantastic way of starting our dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our sample file `docker-compose.yml`, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: When we run `docker-compose up` command, we will download the image of MongoDB
    and start an instance exposing these ports on our local host.
  prefs: []
  type: TYPE_NORMAL
- en: We now need to create a new struct in our project which is going to implement
    the store interface. We can then execute commands against the real database as
    opposed to using a mock or a simple in-memory store.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation for `MongoStore` is quite straight forward. Looking at the
    file in `data/monogstore.go`, you will see that we have two additional methods
    not defined in our interface, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: These are here because we need them for the setup of our functional tests.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at our file `features/search_test.go` you will see that we have added
    a couple of extra calls to the `FeatureContext` method in our setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we are doing is to call the `waitForDB` method: because we
    cannot control when our Mongo instance is going to be ready to accept connections,
    we need to wait for it before kicking off the tests. The process is that we will
    try to create an instance to our `MongoStore` using the convenience method `NewMongoStore`,
    internally this is doing the following work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dial` method attempts to connect to the instance of MongoDB specified
    in the connection string. If this connection fails, then an error is returned.
    In our code, if we receive an error, we return this to the caller of `NewMongoStore`
    with a nil instance of our struct. The `waitForDB` method works by repeatedly
    attempting to create this connection until it no longer receives an error. To
    avoid spamming the database while it is trying to start, we sleep for one second
    after every failed attempt, to a maximum time of 10 seconds. This method will
    block the main Go routine, but this is by design as we do not want the tests to
    execute until we are sure we have this connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have also added some code to the `BeforeScenario` setup: the first thing
    we are going to do is wipe our database clearing down any previous test data.
    Clearing data is an incredibly important step as, should we have had any methods
    which mutate the data; we would not get predictable test results after each run.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we insert the `setupData` method into our test data, and then we will
    proceed to execute the tests.
  prefs: []
  type: TYPE_NORMAL
- en: We now have quite a few things to do before we can test our code, we need to
    run docker-compose, perform our tests and then stop docker-compose. An efficient
    way of scripting this process is to use Makefiles, Makefiles have been around
    forever and are still the primary software build mechanisms for many applications.
    They allow us to define commands in a simple text file which we can then run by
    executing **make [command]**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the Cucumber command in the Makefile in the example repository,
    we can see how we script the steps that need to be carried out to run our tests.
    We are starting our Mongo instance with `docker-compose`, running our Cucumber
    tests and then tearing the database down again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are wondering why we still need the `waitForDB` method when we are starting
    our database before running the tests, remember that Docker only knows when the
    primary process is executed. There can be a considerable lag between the commencement
    of the process and it being ready to accept connections. To run this we run `make
    cucumber` from the command line the result should be passing cucumber tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: That is all for this section, we have learned that, with a few well-placed patterns,
    it is easy to write a robust test suite that will keep us safe and sound asleep
    instead of getting up in the middle of the night to diagnose a broken system.
    In the next section, we are going to look at some of the fantastic features of
    Go to ensure that our code is fast and optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking and profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go has two excellent ways to analyze the performance of your code. We have benchmark
    tests and the fantastic pprof.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarking is a way of measuring the performance of your code by executing
    it multiple times with a fixed workload. We took a look at this briefly in [Chapter
    1](ba3a8742-94e7-4e47-8a47-1324a277a7f9.xhtml), *Introduction to Microservices*,
    where we ascertained that the `json.Marshal` method was slower than the `json.Encode`
    method. While this is a useful feature, I find it tough to work out what I should
    benchmark. If I am writing an algorithm, then this is relatively straightforward.
    However, when writing a microservice that is predominately interacting with a
    database, it is far more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how easy it is to execute benchmarks in Go, take a look at `chandlers/search_bench_test.go`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important part of this code is hidden away at line **21**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: When running a benchmark, Go needs to run it multiple times to get an accurate
    reason. The number of times that the benchmark will run is the field `N` on the
    benchmark's struct. Before setting this number, Go will execute a few iterations
    of your code to get an approximate measurement of the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would execute our benchmark using the `go test-bench -benchmem` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here we are passing an additional flag to see the memory allocations for each
    execution. We know that our handler when running with the mock takes 43,183 nanoseconds
    or 0.043183 milliseconds to execute and performs 68 memory allocations. It'd be
    good if the code would run this fast when running in real life, but we might have
    to wait a few years before we see this level of speed from an API connected to
    a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the other nice features of benchmark tests is that we can run them and
    it outputs profiles which can be used with pprof:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output of this command will give us more information about where this time
    and memory is being consumed and can help us to optimize our code correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we wish to take a look at the speed of our program, the best technique
    we can employ is profiling. Profiling automatically samples your running application
    while it is executing; and then we can compute that data, such as the running
    time of a particular function, into a statistical summary called a profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go supports three different types of profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU**: Identifies the tasks which require the most CPU time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heap**: Identifies the statements responsible for allocating the most memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blocking**: Identifies the operations responsible for blocking Go routines
    for the longest time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we would like to enable profiling on our application, we can do one of two
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Add `import "net/http/pprof"` to your startup file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually start profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first option is the most straightforward. You only add it to the beginning
    of your main Go file and, if you are not already running an HTTP web server, start
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This method then exposes various paths on your HTTP server at `/debug/pprof/`
    which can then be accessed via a URL. The side effect of this, however, is that
    when this import statement is in your go file, then you will be profiling, which
    not only could slow down your application, but you also don't want to expose this
    information for public consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method of profiling is to start the profiler when you start your application
    by passing some additional command line flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'On line **26**, we are checking whether we have specified an output file for
    CPU profiling, and if so, we are creating the file and then starting the profiler
    with `pprof.StartCPUProfile(f)`, and passing it a reference to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `StartCPUProfile` function enables the CPU profiling for the current process
    and buffers the output to `w`. While running, the CPU profiler will stop your
    application roughly 100 times per second and record the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To profile heap allocations, we use a slightly different command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Lookup()` function returns the profile with the given name, or if no such
    profile exists, the predefined profiles available are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`WriteTo` outputs the profile to the given write in the pprof format. If we
    set the debug flag to `1`, then `WriteTo` will add comments to function names
    and line numbers instead of just hexadecimal addresses that pprof uses. These
    comments are so you can read the file without needing any special tooling.'
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the example code in the folder `benchmark`, you will find an
    example profile and the binary from which it was generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run the pprof tool to examine what is going on. To do this, we need
    to run the tool on the command line and provide a reference to the binary that
    the profile was executed against, and also the profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest command we can run is `top`. Top will show us the functions which
    consumed the most CPU during the execution of our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The main offender in this instance is `syscall.Syscall`. If we look this up
    in the documentation, we find that package syscall contains an interface to the
    low-level operating system primitives.
  prefs: []
  type: TYPE_NORMAL
- en: 'On its own this output is not particularly useful, so let''s generate a call
    graph which will show us more detail. We can do this again from the pprof tool.
    However, we do need to install Graphviz first. If you are using macOS then you
    can install this with brew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using a Linux based system and have the apt package manager, you
    can use that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this looks like this `benchmark/cpu.png`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/621e4bd5-e5a2-4e16-aadf-0fc8dc90d0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: That is quite something! However, we can see that `syscall.Syscall` is in the
    largest font as it is responsible for consuming the most CPU. If we start here
    at the bottom and start tracing backward, we can see that the root of this seems
    to be our data store's search function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a closer look at exactly where this is happening, we can use the list
    command and then pass the name of an object or method we would like to investigate
    further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: When we do this with the search method, we can see that a huge percentage of
    our CPU cycles were spent executing queries to MongoDB. If we look at the other
    route from `syscall.Syscall`, it shows that another large consumer is the `http.ResponseWriter`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: This output all makes sense as we are not doing anything too clever in our API;
    it just retrieves some data from a database. The nice thing about pprof is that
    we can use the same commands to query the heap usage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned some best practice approaches to testing microservices
    in Go. We have looked at the testing package, including some special features
    for dealing with requests and responses. We have also looked at writing integration
    tests with Cucumber.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that your code works without fault, however, is only part of the job,
    we also need to make sure that our code is performant, and Go has some excellent
    tools for managing this too.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would always recommend that you test your code and that you do this religiously.
    As for performance optimization, this is open for debate, no doubt you have heard
    comments that premature optimization is the root of all evil. However, this quote
    from Donald Knuth is much-misunderstood: he did not mean that you should never
    optimize until you have a problem; he said that you should only optimize what
    matters. With pprof, we have an easy way to figure out what, if anything, actually
    matters. Include the practice of profiling into your development routine, and
    you will have faster and more efficient applications, profiling is also an excellent
    technique to understand your application better when you are trying to track down
    that tricky bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '"Programmers waste enormous amounts of time thinking about, or worrying about,
    the speed of noncritical parts of their programs, and these attempts at efficiency
    have a strong negative impact when debugging and maintenance are considered. We
    should forget about small efficiencies, say about 97% of the time: premature optimization
    is the root of all evil. Yet we should not pass up our opportunities in that critical
    3%."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Donald Knuth'
  prefs: []
  type: TYPE_NORMAL
