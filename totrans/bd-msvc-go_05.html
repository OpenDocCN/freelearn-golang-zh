<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Common Patterns</h1>
                </header>
            
            <article>
                
<p>Before we take a look at some frameworks which can help you build microservices in Go, we should first look at some of the design patterns that will help you avoid failure.</p>
<p>I am not talking about software design patterns like factories or facades, but architectural designs like load balancing and service discovery. If you have never worked with microservice architecture before, then you may not understand why these are needed, but I hope that by the end of the chapter you will have a solid understanding why these patterns are important and how you can apply them correctly. If you have already successfully deployed a microservice architecture, then this chapter will give you greater knowledge of the underlying patterns which make your system function. If you have not had much success with microservices, then possibly you did not understand that you need the patterns I am going to describe.</p>
<p>In general, there is something for everyone, and we are going to look at not just the core patterns but some of the fantastic open source software which can do most of the heavy lifting for us.</p>
<p>The examples referenced in this chapter can be found at: <a href="https://github.com/building-microservices-with-go/chapter5.git">https://github.com/building-microservices-with-go/chapter5.git</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Design for failure</h1>
                </header>
            
            <article>
                
<p>Anything that can go wrong will go wrong.</p>
<p>When we are building microservices, we should always be prepared for failure. There are many reasons for this, but the main one is that cloud computing networks can be flakey and you lose the ability to tune switching and routing, which would have given you an optimized system if you were running them in your data center. In addition to this, we tend to build microservice architectures to scale automatically, and this scaling causes services to start and stop in unpredictable ways.</p>
<p>What this means for our software is that we need to think about this failure up front while discussing upcoming features. We then need to design this into the software from the beginning, and as engineers, we need to understand these problems.</p>
<p>In his book <em>Designing Data-Intensive Applications</em>, Martin Kleppman makes the following comment:</p>
<div class="packt_quote">The bigger a system gets, the more likely it is that one of its components is broken. Over time, broken things get fixed, and new things break, but in a system with thousands of nodes, it is reasonable to assume that something is always broken. If the error handling strategy consists of only giving up such a large system would never work.</div>
<p>While this applies to more major systems, I would argue that the situation where you need to start considering failure due to connectivity and dependency begins once your estate reaches the size of <em>n+1</em>. This might seem a frighteningly small number, but it is incredibly relevant. Consider the following simplistic system:</p>
<p>You have a simple website which allows your users (who are all cat lovers) to register for updates from other cat lovers. The update is in the form of a simple daily e-mail, and you would like to send out a welcome e-mail once the form has been submitted by the user and the data saved into the database. Because you are a good microservice practitioner you have recognized that sending e-mails should not be the responsibility of the registration system, and instead you would like to devolve this to an external system. In the meantime, the service is growing in popularity; you have determined that you can save time and effort by leveraging the e-mail as an API service from MailCo. This has a simple RESTful contract and can support all your current needs, which allows you to get to market that little bit sooner.</p>
<p>The following diagram represents that simple microservice:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="347" width="467" class="image-border" src="assets/4b51cade-e920-46a1-9b70-63a6c227f348.png"/></div>
<p>Being a good software architect, you define an interface for this mail functionality which will serve as an abstraction for the actual implementation. This concept will allow you to replace MailCo quickly at a later date. Sending e-mails is fast, so there is no need to do anything clever. We can make the call to MailCo synchronously during the registration request.</p>
<p>This application seems like a simple problem to solve, and you get the work done in record time. The site is hosted on AWS and configured with ElasticScale so, no matter what load you get, you will be sleeping peacefully with no worry that the site could go down.</p>
<p>One evening your CEO is out at an event for tech startups which is being covered by the news network, CNN. She gets talking to the reporter who, also being a cat lover, decides he would like to feature the service in a special report which will air tomorrow evening.</p>
<p>The excitement is unreal; this is the thing that will launch the service into the stratosphere. You and the other engineers check the system just for peace of mind, make sure the auto scale is configured correctly, then kick back with some pizza and beer to watch the program.</p>
<p>When the program airs and your product is shown to the nation, you can see the number of users on the site in Google Analytics. It looks great: request times are small, the cloud infrastructure is doing its job, and this has been a total success. Until of course, it isn't. After a few minutes, the request queueing starts to climb, and the services are still scaling, but now the alarms are going off due to a high number of errors and transaction processing time. More and more users are entering the site and trying to register but very few are successful, this is the worst kind of disaster you could have ever wished for.</p>
<p>I won't mention the look on the face of the CEO; you have all probably seen that look at some point in your careers; and if not, when you do you will know what I am talking about. It is a cross between, anger, hatred, and confusion as to how they hired such idiots.</p>
<p>You aren't an idiot; software is complex, and with complexity, it is easy to make mistakes.</p>
<p>So, you start to investigate the problem, quickly you see that while your service and database have been operating correctly, the bottleneck is MailCo's e-mail API. This started the blockage and, because you were executing a synchronous request, your service started blocking too.</p>
<p>So, your moment of glory was taken down by a single bottleneck with a third-party API. Now you understand why you need to plan for failure. Let's take a look at how you can implement failure driven design patterns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Patterns</h1>
                </header>
            
            <article>
                
<p>The truth about microservices is that they are not hard you only need to understand the core software architectural patterns which will help you succeed. In this section, we are going to take a look at some of these patterns and how we can implement them in Go.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event processing</h1>
                </header>
            
            <article>
                
<p>In our case study, we failed due to a downstream synchronous process failing, and that blocked the upstream. The first question we should ask ourselves is "Does this call need to be synchronous?" In the case of sending an e-mail, the answer is almost always, No. The best way to deal with this is to take a fire and forget approach; we would just add the request with all the details of the mail onto a highly available queue which would guarantee at least once delivery and move on. There would be a separate worker processing the queue records and sending these on to the third-party API.</p>
<p>In the instance that the third party starts to experience problems, we can happily stop processing the queue without causing any problems for our registration service.</p>
<p>Regarding user experience, this potentially means that the when the user clicks the register button they would not instantly receive their welcome e-mail. However, e-mail is not an instantaneous system, so some delay is to be expected. You could enhance your user experience further: what if adding an item to the queue returns the approximate queue length back to the calling system. When you are designing for failure, you may take a call that if the queue is over <em>n</em> items, you could present a friendly message to the user letting them know you are busy now but rest assured your welcome e-mail is on its way.</p>
<p>We will look at the implementation of this pattern further in <a href="2952a830-163e-4610-8554-67498ec77e1e.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Event-Driven Architecture</em>, but at the moment there are a few key concepts that we need to cover.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event processing with at least once delivery</h1>
                </header>
            
            <article>
                
<p>Event processing is a model which allows you to decouple your microservices by using a message queue. Rather than connect directly to a service which may or may not be at a known location, you broadcast and listen to events which exist on a queue, such as Redis, Amazon SQS, NATS.io, Rabbit, Kafka, and a whole host of other sources.</p>
<p>To use our example of sending a welcome e-mail, instead of making a direct call to the downstream service using its REST or RPC interface, we would add an event to a queue containing all the details that the recipient would need to process this message.</p>
<p>Our message may look like:</p>
<pre>
{ <br/>  "id": "ABCDERE2342323SDSD", <br/>  "queue" "registration.welcome_email", <br/>  "dispatch_date": "2016-03-04 T12:23:12:232", <br/>  "payload": { <br/>    "name": "Nic Jackson", <br/>    "email": "mail@nicholasjackson.io" <br/>  } <br/>} 
</pre>
<p>We add the message to the queue and then wait for an ACK from the queue to let us know that the message has been received. Of course, we would not know if the message has been delivered but receiving the ACK should be enough for us to notify the user and proceed.</p>
<p>The message queue is a highly distributed and scalable system, and it should be capable of processing millions of messages so we do not need to worry about it not being available. At the other end of the queue, there will be a worker who is listening for new messages pertaining to it. When it receives such a message, it processes the message and then removes it from the queue.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="298" width="548" class="image-border" src="assets/d6f3bfdc-300c-459f-8841-0ae61cea8b16.png"/></div>
<p>Of course, there is always the possibility that the receiving service can not process the message which could be due to a direct failure or bug in the email service or it could be that the message which was added to the queue is not in a format which can be read by the email service. We need to deal with both of these issues independently, let us start with handing errors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling Errors</h1>
                </header>
            
            <article>
                
<p>It is not uncommon for things to go wrong with distributed systems and we should factor this into our software design, in the instance that a valid message can not be processed one standard approach is to retry processing the message, normally with a delay. We can add the message back onto the queue augmenting it with the error message which occurred at the time as seen in the following example:</p>
<pre>
{ <br/> "id": "ABCDERE2342323SDSD", <br/> "queue" "registration.welcome_email", <br/> "dispatch_date": "2016-03-04 T12:23:12:232", <br/> "payload": { <br/> "name": "Nic Jackson", <br/> "email": "mail@nicholasjackson.io" <br/> }, <br/> "error": [{<br/>   "status_code": 3343234,<br/>   "message": "Message rejected from mail API, quota exceeded",<br/>   "stack_trace": "mail_handler.go line 32 ...",<br/>   "date": "2016-03-04 T12:24:01:132"<br/> }]<br/>} 
</pre>
<p>It is important to append the error every time we fail to process a message as it gives us the history of what went wrong, it also provides us with the capability to understand how many times we have tried to process the message because after we exceed this threshold we do not want to continue to retry we need to move this message to a second queue where we can use it for diagnostic information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dead Letter Queue</h1>
                </header>
            
            <article>
                
<p>This second queue is commonly called a dead letter queue, a dead letter queue is specific to the queue from where the message originated, if we had a queue named <kbd>order_service_emails</kbd> then we would create a second queue called <kbd>order_service_emails_deadletter</kbd>. The purpose of this is so that we can examine the failed messages on this queue to assist us with debugging the system, there is no point in knowing an error has occurred if we do not know what that error is and because we have been appending the error details direct to the message body we have this history right where we need it.</p>
<p>We can see that the message has failed because we have exceeded our quota in the mail API, we also have the date and time of when the error occurred. In this instance, because we have exceeded our quota with the email provider once we remove the issue with the email provider we can then move all of these messages from the dead letter queue back onto the main queue and they should then process correctly. Having the error information in a machine readable format allows us to handle the dead letter queue programmatically, we can explicitly select messages which relate to quota problem within a particular time window.</p>
<p>In the instance that a message can not be processed by the email service due to a bad message payload we typically do not retry processing of the message but add it directly to the dead letter queue. Again having this information allows us to diagnose why this issue might have occurred, it could be due to a contract change in the upstream service which has not been reflected in the downstream service. If this is the reason behind the failure we have the knowledge to correct the contract issue in the email service which is consuming the messages and again move the message back into the main queue for processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Idempotent transactions and message order</h1>
                </header>
            
            <article>
                
<p>While many message queues now offer <em>At Most Once Delivery</em> in addition to the <em>At Least Once</em>, the latter option is still the best for large throughput of messages. To deal with the fact that the downstream service may receive a message twice it needs to be able to handle this in its own logic. One method for ensuring that the same message is not processed twice is to log the message ID in a transactions table. When we receive a message, we will insert a row which contains the message ID and then we can check when we receive a message to see if it has already been processed and if it has to dispose of that message.</p>
<p>The other issue that can occur with messaging is receiving a message out of sequence if for some reason two messages which supersede each other are received in an incorrect order then you may end up with inconsistent data in the database. Consider this simple example, the front end service allows the update of user information a subset of which is forwarded to a second microservice. The user quickly updates their information twice which causes two messages to be dispatched to the second service, providing both messages arrive in the order by which they were dispatched then the second service will process both messages and the data will be in a consistent state. However, if they do not arrive in the correct order then the second service will be inconsistent to the first as it will save the older data as the most recent. Once potential way to avoid this issue is to again leverage the transaction table and to store the message dispatch_date in addition to the id. When the second service receives a message then it can not only check if the current message has been processed it can check that it is the most recent message and if not discard it.</p>
<p>Unfortunately, there is no one solution fits all with messaging we need to tailor the solution which matches the operating conditions of the service. For you as a microservice practitioner, you need to be aware that these conditions can exist and factor them into your solution designs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atomic transactions</h1>
                </header>
            
            <article>
                
<p>While storing data, a database can be ATOMIC: that is, all operations occur or none do. We cannot say the same with distributed transactions in microservices. When we used SOAP as our message protocol a decade or so ago, there was a proposal for a standard called <strong>Web Service-Transactions</strong> (<strong>WS-T</strong>). This aimed to provide the same functionality that you get from a database transaction, but in a distributed system. Thankfully SOAP is long gone unless you work in finance or another industry which deals with legacy systems, but the problem remains. In our previous example, we looked at how we can decouple the saving of the data and the sending of the e-mail by using a message queue with at least once delivery. What if we could solve the problem of atomicity in the same way, consider this example:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="331" width="608" class="image-border" src="assets/e0b953ae-9222-4881-8876-3797cef2fa28.png"/></div>
<p>We distribute both parts of our order process to the queue, a worker service persists the data to the database, and a service that is responsible for sending the confirmation e-mail. Both these services would subscribe to the same <kbd>new_order</kbd> message and take action when this is received. Distributed transactions do not give us the same kind of transaction that is found in a database. When part of a database transaction fails, we can roll back the other parts of the transaction. Using this pattern we would only remove the message from the queue if the process succeeded so when something fails, we keep retrying. This gives us a kind of eventually consistent transaction. My opinion on distributed transactions is to avoid them if possible; try to keep your behavior simple. However, when this is not possible then this pattern may just be the right one to apply.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Timeouts</h1>
                </header>
            
            <article>
                
<p>A timeout is an incredibly useful pattern while communicating with other services or data stores. The idea is that you set a limit on the response of a server and, if you do not receive a response in the given time, then you write a business logic to deal with this failure, such as retrying or sending a failure message back to the upstream service.</p>
<p>A timeout could be the only way of detecting a fault with a downstream service. However, no reply does not mean the server has not received and processed the message, or that it might not exist. The key feature of a timeout is to fail fast and to notify the caller of this failure.</p>
<p>There are many reasons why this is a good practice, not only from the perspective of returning early to the client and not keeping them waiting indefinitely but also from the point of view of load and capacity. Every connection that your service currently has active is one which cannot serve an active customer. Also, the capacity of your system is not infinite, it takes many resources to maintain a connection, and this also applies to the upstream service which is making a call to you. Timeouts are an effective hygiene factor in large distributed systems, where many small instances of a service are often clustered to achieve high throughput and redundancy. If one of these instances is malfunctioning and you, unfortunately, connect to it, then this can block an entirely functional service. The correct approach is to wait for a response for a set time and then if there is no response in this period, we should cancel the call, and try the next service in the list. The question of what duration your timeouts are set to do not have a simple answer. We also need to consider the different types of timeout which can occur in a network request, for example, you have:</p>
<ul>
<li>Connection Timeout - The time it takes to open a network connection to the server</li>
<li>Request Timeout - The time it takes for a server to process a request</li>
</ul>
<p>The request timeout is almost always going to be the longest duration of the two and I recommend the timeout is defined in the configuration of the service. While you might initially set it to an arbitrary value of, say 10 seconds, you can modify this after the system has been running in production, and you have a decent data set of transaction times to look at.</p>
<p>We are going to use the deadline package from eapache (<a href="https://github.com/eapache/go-resiliency/tree/master/deadline"><span class="URLPACKT">https://github.com/eapache/go-resiliency/tree/master/deadline</span></a>), recommended by the go-kit toolkit (<a href="https://gokit.io"><span class="URLPACKT">https://gokit.io</span></a>).</p>
<p>The method we are going to run loops from 0-100 and sleeps after each loop. If we let the function continue to the end, it would take 100 seconds.</p>
<p>Using the deadline package we can set our own timeout to cancel the long running operation after two seconds:</p>
<p><kbd>timeout/main.go</kbd></p>
<pre>
 24 func makeTimeoutRequest() { <br/> 25   dl := deadline.New(1 * time.Second) <br/> 26   err := dl.Run(func(stopper &lt;-chan struct{}) error { <br/> 27     slowFunction() <br/> 28     return nil <br/> 29   }) <br/> 30 <br/> 31   switch err { <br/> 32   case deadline.ErrTimedOut: <br/> 33     fmt.Println("Timeout") <br/> 34   default: <br/> 35     fmt.Println(err) <br/> 36   } <br/> 37 } 
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back off</h1>
                </header>
            
            <article>
                
<p>Typically, once a connection has failed, you do not want to retry immediately to avoid flooding the network or the server with requests. To allow this, it's necessary to implement a back-off approach to your retry strategy. A back-off algorithm waits for a set period before retrying after the first failure, this then increments with subsequent failures up to a maximum duration.</p>
<p>Using this strategy inside a client-called API might not be desirable as it contravenes the requirement to fail fast. However, if we have a worker process that is only processing a queue of messages, then this could be exactly the right strategy to add a little protection to your system.</p>
<p>We will look at the <kbd>go-resiliency</kbd> package and the <kbd>retrier</kbd> package.</p>
<p>To create a new retrier, we use the <kbd>New</kbd> function which has the signature:</p>
<pre>
func New(backoff []time.Duration, class Classifier) *Retrier 
</pre>
<p>The first parameter is an array of <kbd>Duration</kbd>. Rather than calculating this by hand, we can use the two built-in methods which will generate this for us:</p>
<pre>
func ConstantBackoff(n int, amount time.Duration) []time.Duration 
</pre>
<p>The <kbd>ConstantBackoff</kbd> function generates a simple back-off strategy of retrying <em>n</em> times and waiting for the given amount of time between each retry:</p>
<pre>
func ExponentialBackoff(n int, initialAmount time.Duration) []time.Duration 
</pre>
<p>The <kbd>ExponentialBackoff</kbd> function generates a simple back-off strategy of retrying <em>n</em> times doubling the time between each retry.</p>
<p>The second parameter is a <kbd>Classifier</kbd>. This allows us a nice amount of control over what error type is allowed to retry and what will fail immediately.</p>
<pre>
type DefaultClassifier struct{} 
</pre>
<p>The <kbd>DefaultClassifier</kbd> type is the simplest form: if there is no error returned then we succeed; if there is any error returned then the retrier enters the retry state.</p>
<pre>
type BlacklistClassifier []error 
</pre>
<p>The <kbd>BlacklistClassifier</kbd> type classifies errors based on a blacklist. If the error is in the given blacklist it immediately fails; otherwise, it will retry.</p>
<pre>
type WhitelistClassifier []error 
</pre>
<p>The <kbd>WhitelistClassifier</kbd> type is the opposite of the blacklist, and it will only retry when an error is in the given white list. Any other errors will fail.</p>
<p>The WhitelistClassifier might seem slightly complicated. However, every situation requires a different implementation. The strategy that you implement is tightly coupled to your use case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Circuit breaking</h1>
                </header>
            
            <article>
                
<p>We have looked at some patterns like timeouts and back-offs, which help protect our systems from cascading failure in the instance of an outage. However, now it's time to introduce another pattern which is complementary to this duo. Circuit breaking is all about failing fast, Michael Nygard in his book "Release It" says:</p>
<div class="packt_quote">''Circuit breakers are a way to automatically degrade functionality when the system is under stress."</div>
<p>One such example could be our frontend example web application. It is dependent on a downstream service to provide recommendations for kitten memes that match the kitten you are looking at currently. Because this call is synchronous with the main page load, the web server will not return the data until it has successfully returned recommendations. Now you have designed for failure and have introduced a timeout of five seconds for this call. However, since there is an issue with the recommendations system, a call which would ordinarily take 20 milliseconds is now taking 5,000 milliseconds to fail. Every user who looks at a kitten profile is waiting five seconds longer than usual; your application is not processing requests and releasing resources as quickly as normal, and its capacity is significantly reduced. In addition to this, the number of concurrent connections to the main website has increased due to the length of time it is taking to process a single page request; this is adding load to the front end which is starting to slow down. The net effect is going to be that, if the recommendations service does not start responding, then the whole site is headed for an outage.</p>
<p>There is a simple solution to this: you should stop attempting to call the recommendations service, return the website back to normal operating speeds, and slightly degrade the functionality of the profile page. This has three effects:</p>
<ul>
<li>You restore the browsing experience to other users on the site.</li>
<li>You slightly degrade the experience in one area.</li>
<li>You need to have a conversation with your stakeholders before you implement this feature as it has a direct impact on the system's business.</li>
</ul>
<p>Now in this instance, it should be a relatively simple sell. Let's assume that recommendations increase conversion by 1%; however, slow page loads reduce it by 90%. Then isn't it better to degrade by 1% instead of 90%? This example, is clear cut but what if the downstream service was a stock checking system; should you accept an order if there is a chance you do not have the stock to fulfill it?</p>
<p>Error behaviour is not a question that software engineering can answer on its own; business stakeholders need to be involved in this decision. In fact, I recommend that when you are planning the design of your systems, you talk about failure as part of your non-functional requirements and decide ahead of time what you will do when the downstream service fails.</p>
<p><strong>So how do they work?</strong></p>
<p>Under normal operations, like a circuit breaker in your electricity switch box, the breaker is closed and traffic flows normally. However, once the pre-determined error threshold has been exceeded, the breaker enters the open state, and all requests immediately fail without even being attempted. After a period, a further request would be allowed and the circuit enters a half-open state, in this state a failure immediately returns to the open state regardless of the <kbd>errorThreshold</kbd>. Once some requests have been processed without any error, then the circuit again returns to the closed state, and only if the number of failures exceeded the error threshold would the circuit open again.</p>
<p>That gives us a little more context to why we need circuit breakers, but how can we implement them in Go?</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="342" width="566" class="image-border" src="assets/553bafb9-1732-478c-976b-45097eb43bb1.png"/></div>
<p>Again, we are going to turn to the <kbd>go-resilience</kbd> package. Creating a circuit breaker is straight forward, the signature for the breaker is as follows:</p>
<pre>
func New(errorThreshold, successThreshold int, timeout time.Duration) *Breaker 
</pre>
<p>We construct our circuit breaker with three parameters:</p>
<ul>
<li>The first <kbd>errorThreshold</kbd>, is the number of times a request can fail before the circuit opens</li>
<li>The <kbd>successThreshold</kbd>, is the number of times that we need a successful request in the half-open state before we move back to open</li>
<li>The <kbd>timeout</kbd>, is the time that the circuit will stay in the open state before changing to half-open</li>
</ul>
<p>Run the following code:</p>
<pre>
 11   b := breaker.New(3, 1, 5*time.Second) <br/> 12 <br/> 13   for { <br/> 14     result := b.Run(func() error { <br/> 15       // Call some service <br/> 16       time.Sleep(2 * time.Second) <br/> 17       return fmt.Errorf("Timeout") <br/> 18     }) <br/> 19 <br/> 20     switch result { <br/> 21     case nil: <br/> 22       // success! <br/> 23     case breaker.ErrBreakerOpen: <br/> 24       // our function wasn't run because the breaker was open <br/> 25       fmt.Println("Breaker open") <br/> 26     default: <br/> 27       fmt.Println(result) <br/> 28     } <br/> 29 <br/> 30     time.Sleep(500 * time.Millisecond) <br/> 31   } 
</pre>
<p>If you run this code you should see the following output. After three failed requests the breaker enters the open state, then after our five-second interval, we enter the half-open state, and we are allowed to make another request. Unfortunately, this fails, and we again enter the fully open state, and we no longer even attempt to make the call:</p>
<pre>
<strong>Timeout</strong><br/><strong>Timeout</strong><br/><strong>Timeout</strong><br/><strong>Breaker open</strong><br/><strong>Breaker open</strong><br/><strong>Breaker open</strong><br/><strong>...</strong><br/><strong>Breaker open</strong><br/><strong>Breaker open</strong><br/><strong>Timeout</strong><br/><strong>Breaker open</strong><br/><strong>Breaker open</strong>  
</pre>
<p>One of the more modern implementations of circuit breaking and timeouts is the Hystix library from Netflix; Netflix is certainly renowned for producing some quality microservice architecture and the Hystrix client is something that has also been copied time and time again.</p>
<p>Hystrix is described as "a latency and fault tolerance library designed to isolate points of access to remote systems, services, and third-party libraries, stop cascading failure, and enable resilience in complex distributed systems where failure is inevitable."</p>
<p>(<a href="https://github.com/Netflix/Hystrix"><span class="URLPACKT">https://github.com/Netflix/Hystrix</span></a>)</p>
<p>For the implementation of this in Golang, check out the excellent package <a href="https://github.com/afex/hystrix-go"><span class="URLPACKT">https://github.com/afex/hystrix-go</span></a>. This is a nice clean implementation, which is a little cleaner than implementing <kbd>go-resiliency</kbd>. Another benefit of <kbd>hystrix-go</kbd> is that it will automatically export metrics to either the Hystrix dashboard to via StatsD. In <a href="bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Logging and Monitoring</em>, we will learn all about this just how important it is.</p>
<p>I hope you can see why this is an incredibly simple but useful pattern. However, there should be questions raised as to what you are going to do when you fail. Well these are microservices, and you will rarely only have a single instance of a service, so why not retry the call, and for that we can use a load balancer pattern.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Health checks</h1>
                </header>
            
            <article>
                
<p>Health checks should be an essential part of your microservices setup. Every service should expose a health check endpoint which can be accessed by the consul or another server monitor. Health checks are important as they allow the process responsible for running the application to restart or kill it when it starts to misbehave or fail. Of course, you must be incredibly careful with this and not set this too aggressively.</p>
<p>What you record in your health check is entirely your choice. However, I recommend you look at implementing these features:</p>
<ul>
<li>Data store connection status (general connection state, connection pool status)</li>
<li>Current response time (rolling average)</li>
<li>Current connections</li>
<li>Bad requests (running average)</li>
</ul>
<p>How you determine what would cause an unhealthy state needs to be part of the discussion you have when you are designing the service. For example, no connectivity to the database means the service is completely inoperable, it would report unhealthy and would allow the orchestrator to recycle the container. An exhausted connection pool could just mean that the service is under high load, and while it is not completely inoperable it could be suffering degraded performance and should just serve a warning.</p>
<p>The same goes for the current response time. This point I find interesting: when you load test your service once it has been deployed to production, you can build up a picture of the thresholds of operating health. These numbers can be stored in the config and used by the health check. For example, if you know that your service will run an average service request with a 50 milliseconds latency for 4,000 concurrent users; however at 5,000, this time grows to 500 milliseconds as you have exhausted the connection pool. You could set your SLA upper boundary to be 100 milliseconds; then you would start reporting degraded performance from your health check. This should, however, be a rolling average based on the normal distribution. It is always possible for one or two requests to greatly be outside the standard deviation of normal operation, and you do not want to allow this to skew your average which then causes the service to report unhealthy, when in fact the slow response was actually due to the upstream service having slow network connectivity, not your internal state.</p>
<p>When discussing health checks, Michael Nygard considers the pattern of a handshake, where each client would send a handshake request to the downstream service before connecting to check if it was capable of receiving its request. Under normal operating conditions and most of the time, this adds an enormous amount of chatter into your application, and I think this could be overkill. It also implies that you are using client-side load-balancing, as with a server side approach you would have no guarantees that the service you handshake is the one you connect to. That said Release It was written over 10 years ago and much has changed in technology. The concept however of the downstream service making a decision that it can or can't handle a request is a valid one. Why not instead call your internal health check as the first operation before processing a request? This way you could immediately fail and give the client the opportunity to attempt another endpoint in the cluster. This call would add almost no overhead to your processing time as all you are doing is reading the state from the health endpoint, not processing any data.</p>
<p>Let's look at how we could implement this by looking at the example code in <kbd>health/main.go</kbd> :</p>
<pre>
18 func main() {<br/>19   ma = ewma.NewMovingAverage()<br/>20<br/>21   http.HandleFunc("/", mainHandler)<br/>22   http.HandleFunc("/health", healthHandler)<br/>23<br/>24   http.ListenAndServe(":8080", nil)<br/>25 }
</pre>
<p>We are defining two handlers one which deals with our main request at the path <kbd>/</kbd> and one used for checking the health at the path <kbd>/health</kbd>.</p>
<p>The handler implements a simple moving average which records the time it takes for the handler to execute. Rather than just allow any request to be handled we are first checking on line <strong>30</strong> if the service is currently healthy which is checking if the current moving average is greater than a defined threshold if the service is not healthy we return the status code <kbd>StatusServiceUnavailable</kbd>S.</p>
<pre>
 27 func mainHandler(rw http.ResponseWriter, r *http.Request) {<br/> 28   startTime := time.Now()<br/> 29<br/> 30   if !isHealthy() {<br/> 31     respondServiceUnhealthy(rw)<br/> 32     return<br/> 33   }<br/> 34<br/> 35   rw.WriteHeader(http.StatusOK)<br/> 36   fmt.Fprintf(rw, "Average request time: %f (ms)\n", ma.Value()/1000000)<br/> 37<br/> 38   duration := time.Now().Sub(startTime)<br/> 39   ma.Add(float64(duration))<br/> 40 }
</pre>
<p>Looking greater in depth to the <kbd>respondServiceUnhealty</kbd> function, we can see it is doing more than just returning the HTTP status code.</p>
<pre>
55 func respondServiceUnhealthy(rw http.ResponseWriter) {<br/>56   rw.WriteHeader(http.StatusServiceUnavailable)<br/>57<br/>58   resetMutex.RLock()<br/>59   defer resetMutex.RUnlock()<br/>60<br/>61   if !resetting {<br/>62     go sleepAndResetAverage()<br/>63   }<br/>64 }
</pre>
<p>Lines <strong>58</strong> and <strong>59</strong> are obtaining a lock on the <kbd>resetMutex</kbd>, we need this lock as when the service is unhealthy we need to sleep to give the service time to recover and then reset the average. However, we do not want to call this every time the handler is called or once the service is marked unhealthy it would potentially never recover. The check and variable on line <strong>61</strong> ensures this does not happen however this variable is not safe unless marked with a mutex because we have multiple go routines.</p>
<pre>
63 func sleepAndResetAverage() {<br/>64   resetMutex.Lock()<br/>65   resetting = true<br/>66   resetMutex.Unlock()<br/>67<br/>68   time.Sleep(timeout)<br/>69   ma = ewma.NewMovingAverage()<br/>70<br/>71   resetMutex.Lock()<br/>72   resetting = false<br/>73   resetMutex.Unlock()<br/>74 }
</pre>
<p>The sleepAndResetAverage function waits for a predetermined length of time before resetting the moving average, during this time no work will be performed by the service which will hopefully give the overloaded service time to recover. Again we need to obtain a lock on the resetMutex before interacting with the resetting variable to avoid any race conditions when multiple go routines are trying to access this variable. Line <strong>69</strong> then resets the moving average back to 0 which will mean work will again be able to be handled by the service.</p>
<p>This example is just a simple implementation, as mentioned earlier we could add any metric that the service has available to it such as CPU memory, database connection state should we be using a database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Throttling</h1>
                </header>
            
            <article>
                
<p>Throttling is a pattern where you restrict the number of connections that a service can handle, returning an HTTP error code when this threshold has been exceeded. The full source code for this example can be found in the file <kbd>throttling/limit_handler.go</kbd>. The middleware pattern for Go is incredibly useful here: what we are going to do is to wrap the handler we would like to call, but before we call the handler itself, we are going to check to see if the server can honor the request. In this example, for simplicity, we are going only to limit the number of concurrent requests that the handler can serve, and we can do this with a simple buffered channel.</p>
<p>Our <kbd>LimitHandler</kbd> is quite a simple object:</p>
<pre>
  9 type LimitHandler struct { <br/> 10   connections chan struct{} <br/> 11   handler     http.Handler <br/> 12 } 
</pre>
<p>We have two private fields: one holds the number of connections as a buffered channel, and the second is the handler we are going to call after we have checked that the system is healthy. To create an instance of this object we are going to use the <kbd>NewLimitHandler</kbd> function. This takes the parameters connection, which is the number of connections we allow to process at any one time and the handler which would be called if successful:</p>
<pre>
16 func NewLimitHandler(connections int, next http.Handler) <br/>   *LimitHandler { <br/>17   cons := make(chan struct{}, connections) <br/>18   for i := 0; i &lt; connections; i++ { <br/>19     cons &lt;- struct{}{} <br/>20   } <br/>21 <br/>22   return &amp;LimitHandler{ <br/>23     connections: cons, <br/>24     handler:     next, <br/>25   } <br/>26 } 
</pre>
<p>This is quite straightforward: we create a buffered channel with the size equal to the number of concurrent connections, and then we fill that ready for use:</p>
<pre>
28 func (l *LimitHandler) ServeHTTP(rw http.ResponseWriter, r <br/>   *http.Request) { <br/>29   select { <br/>30   case &lt;-l.connections: <br/>31     l.handler.ServeHTTP(rw, r) <br/>32     l.connections &lt;- struct{}{} // release the lock <br/>32   default: <br/>33     http.Error(rw, "Busy", http.StatusTooManyRequests) <br/>34   } <br/>35 } 
</pre>
<p>If we look at the <kbd>ServeHTTP</kbd> method starting at line <strong>29</strong>, we have a <kbd>select</kbd> statement. The beauty of channel is that we can write a statement like this: if we cannot retrieve an item from the channel then we should return a busy error message to the client.</p>
<p>Another thing worth looking at in this example are the tests, in the test file which corresponds to this example <kbd>throttling/limit_handler_test.go</kbd>, we have quite a complicated test setup to check that multiple concurrent requests return an error when we hit the limit:</p>
<pre>
 14 func newTestHandler(ctx context.Context) http.Handler { <br/> 15   return http.HandlerFunc(func(rw http.ResponseWriter, r <br/>      *http.Request) { <br/> 16     rw.WriteHeader(http.StatusOK) <br/> 17     &lt;-r.Context().Done() <br/> 18   }) <br/> 19 } <br/><br/> 84 func TestReturnsBusyWhenConnectionsExhausted(t *testing.T) { <br/> 85   ctx, cancel := context.WithCancel(context.Background()) <br/> 86   ctx2, cancel2 := context.WithCancel(context.Background()) <br/> 87   handler := NewLimitHandler(1, newTestHandler(ctx)) <br/> 88   rw, r := setup(ctx) <br/> 89   rw2, r2 := setup(ctx2) <br/> 90 <br/> 91   time.AfterFunc(10*time.Millisecond, func() { <br/> 92     cancel() <br/> 93     cancel2() <br/> 94   }) <br/> 95 <br/> 96   waitGroup := sync.WaitGroup{} <br/> 97   waitGroup.Add(2) <br/> 98 <br/> 99   go func() { <br/>100     handler.ServeHTTP(rw, r) <br/>101     waitGroup.Done() <br/>102   }() <br/>103 <br/>104   go func() { <br/>105     handler.ServeHTTP(rw2, r2) <br/>106     waitGroup.Done() <br/>107   }() <br/>108 <br/>109   waitGroup.Wait() <br/>110 <br/>111   if rw.Code == http.StatusOK &amp;&amp; rw2.Code == http.StatusOK { <br/>112     t.Fatalf("One request should have been busy, request 1: %v, <br/>        request 2: %v", rw.Code, rw2.Code) <br/>113   } <br/>114 } 
</pre>
<p>If we look at line <strong>87</strong>, we can see that we are constructing our new <kbd>LimitHandler</kbd> and passing it a mock handler which will be called if the server is capable of accepting the request. You can see that, in line <strong>17</strong> of this handler, we will block until the done channel on the context has an item and that this context is a <kbd>WithCancel</kbd> context. The reason we need to do this is that, to test that one of our requests will be called and the other will not but <kbd>LimitHandler</kbd> will return <kbd>TooManyRequests</kbd>, we need to block the first request. To ensure that our test does eventually complete, we are calling the cancel methods for the contexts in a timer block which will fire after ten milliseconds. Things start to get a little complex as we need to call our handlers in a Go routine to ensure that they execute concurrently. However, before we make our assertion we need to make sure that they have completed. This is why we are setting up <kbd>WaitGroup</kbd> in line <strong>96</strong>, and decrementing this group after each handler has completed. Finally, we can just block on line <strong>109</strong> until everything is complete and then we can make our assertion. Let's take a closer look at the flow through this test:</p>
<ol>
<li>Block at line <strong>109</strong>.</li>
<li>Call <kbd>handler.ServeHTTP</kbd> twice concurrently.</li>
<li>One <kbd>ServeHTTP</kbd> method returns immediately with <kbd>http.TooManyRequests</kbd> and decrements the wait group.</li>
<li>Call cancel context allowing the one blocking <kbd>ServeHTTP</kbd> call to return and decrement the wait group.</li>
<li>Perform assertion.</li>
</ol>
<p>This flow is not the same as reading the code in a linear manner from top to bottom. Three concurrent routines are executing, and the flow of execution is not the same as the order of the statements in the code. Unfortunately, testing concurrent Go routines is always going to be a complicated issue. However, by performing these steps we have 100% coverage for our <kbd>LimitHandler</kbd>:</p>
<pre>
<strong>PASS                                                                                                                                        </strong><br/><strong>coverage: 100.0% of statements                                                                                                               </strong><br/><strong>ok      github.com/nicholasjackson/building-microservices-in-go/chapter5/health 0.033s</strong>  
</pre>
<p>Rather than just limiting the number of connections in this handler, we could implement anything we like: it would be relatively trivial to implement something which records the average execution time or CPU consumption and fail fast if the condition exceeds our requirements. Determining exactly what these requirements are is a complex topic on its own and your first guess will most likely be wrong. We need to run multiple load tests of our system and spend time looking at logging and performance statistics for the end point before we are in a situation to make an educated guess. However, this action could just save you from a cascading failure, and that is an excellent thing indeed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service discovery</h1>
                </header>
            
            <article>
                
<p>With monolithic applications, services invoke one another through language level methods or procedure calls. This was relatively straightforward and predictable behavior. However, once we realized that monolithic applications were not suitable for the scale and demand of modern software, we moved towards SOA or service-oriented architecture. We broke down this monolith into smaller chunks that typically served a particular purpose. To solve the problem with inter-service calls, SOA services ran at well-known fixed locations as the servers were large and quite often hosted in your data center or a leased rack in a data center. This meant that they did not change location very often, the IP addresses were often static, and even if a server did have to move, re-configuring of the IPs was always part of the deployment process.</p>
<p>With microservices all this changes, the application typically runs in a virtualized or containerized environment where the number of instances of a service and their locations can change dynamically, minute by minute. This gives us the ability to scale our application depending on the forces dynamically applied to it, but this flexibility does not come without its own share of problems. One of the main ones knows where your services are to contact them. A good friend of mine, a fantastic software architect and the author of the foreword of this book made this statement in one of his presentations once:</p>
<div class="packt_quote">"Microservices are easy; building microservice systems is hard."</div>
<p>Without the right patterns, it can almost be impossible, and one of the first ones you will most likely stumble upon even before you get your service out into production is service discovery.</p>
<p>Let's suppose you have a setup like this: you have three instances of the same service A, B, and C. Instance A and B are running on the same hardware, but service C is running in an entirely different data center. Because A and B are running on the same machine, they are accessible from the same IP address. However, because of this, they both cannot be bound to the same port. How is your client supposed to figure out all of this to make a simple call?</p>
<p>The solution is service discovery and the use of a dynamic service registry, like Consul or Etcd. These systems are highly scalable and have strongly consistent methods for storing the location of your services. The services register with the dynamic service registry upon startup, and in addition to the IP address and port they are running on, will also often provide metadata, like service version or other environmental parameters that can be used by a client when querying the registry. In addition to this, the consul has the capability to perform health checks on the service to ensure its availability. If the service fails a health check then it is marked as unavailable in the registry and will not be returned by any queries.</p>
<p>There are two main patterns for service discovery:</p>
<ul>
<li>Server-side discovery</li>
<li>Client-side discovery</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Server-side service discovery</h1>
                </header>
            
            <article>
                
<p>Server-side service discovery for inter-service calls within the same application, in my opinion, is a microservice anti-pattern. This is the method we used to call services in an SOA environment. Typically, there will be a reverse proxy which acts as a gateway to your services. It contacts the dynamic service registry and forwards your request on to the backend services. The client would access the backend services, implementing a known URI using either a subdomain or a path as a differentiator.</p>
<p>The problem with this approach is that the reverse proxy starts to become a bottleneck. You can scale your backend services quickly enough, but now you need to be monitoring and watching these servers. Also, this pattern introduces latency, even though it may be only one 20ms hop, this could quite easily cost you 10% of your capacity, which means you have 10% increase in cost in addition to the cost of running and maintaining these services. Then what about consistency: you are potentially going to have two different failure patterns in your code for downstream calls, one for internal services and one for external. This is only going to add to the confusion.</p>
<p>The biggest problem for me, however, is that you have to centralize this failure logic. A little later in this chapter, we are going to look at these patterns in depth, but we have already stated that your services will go wrong at some point and you will want to handle this failure. If you put this logic into a reverse proxy, then all services which want to access service A will be treated the same, regardless of whether the call is essential to the success or not.</p>
<p>To my mind, the worst implementation of this pattern is the one that abstracts all this knowledge from the client, retrying internally, and never letting the calling client know what is happening until success or catastrophic failure.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="446" width="510" class="image-border" src="assets/54a6c338-15b3-4d37-b5dc-842aa31b5b88.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Client-side service discovery</h1>
                </header>
            
            <article>
                
<p>While server-side service discovery might be an acceptable choice for your public APIs for any internal inter-service communication, I prefer the client-side pattern. This gives you greater control over what happens when a failure occurs. You can implement the business logic on a retry of a failure on a case-by-case basis, and this will also protect you against cascading failure.</p>
<p>In essence, the pattern is similar to its server-side partner. However, the client is responsible for the service discovery and load balancing. You still hook into a dynamic service registry to get the information for the services you are going to call. This logic is localized in each client, so it is possible to handle the failure logic on a case-by-case basis.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="458" width="501" class="image-border" src="assets/3c198550-4409-4ebb-8684-4f87182f1587.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load balancing</h1>
                </header>
            
            <article>
                
<p>When we discussed service discovery, we examined the concepts of server-side and client-side discovery. My personal preference is to look at client side for any internal calls as it affords you greater control over the logic of retries on a case by case basis. Why do I like client side load balancing? For many years server-side discovery was the only option, and there was also a preference for doing SSL termination on the load balancer due to the performance problems. This is not necessarily true anymore and as we will see when we look at the chapter on security. It is a good idea to use TLS secured connections internally. However, what about being able to do sophisticated traffic distribution? That can only be achieved if you have a central source of knowledge. I am not sure this is necessary: a random distribution will theoretically over time work out the same. However, there could be a benefit to only sending a certain number of connections to a particular host; but then how do you measure health? You can use layer 6 or 7, but as we have seen by using smart health checks, if the service is too busy then it can just reject a connection.</p>
<p>From the example looking at circuit breaking, I hope you can now start to see the potential this can give your system. So how do we implement load balancing in Go?</p>
<p>If we take a look at <kbd>loadbalancing/main.go</kbd>, I have created a simple implementation of a load balancer. We create it by calling <kbd>NewLoadBalancer</kbd> which has the following signature:</p>
<pre>
func NewLoadBalancer(strategy Strategy, endpoints []url.URL) *LoadBalancer 
</pre>
<p>This function takes two parameters: a <kbd>strategy</kbd>, an interface that contains the selection logic for the endpoints, and a list of endpoints.</p>
<p>To be able to implement multiple strategies for the load balancer, such as round-robin, random, or more sophisticated strategies like distributed statistics, across multiple instances you can define your own strategy which has the following interface:</p>
<pre>
 10 // Strategy is an interface to be implemented by loadbalancing <br/> 11 // strategies like round robin or random. <br/> 12 type Strategy interface { <br/> 13   NextEndpoint() url.URL <br/> 14   SetEndpoints([]url.URL) <br/> 15 } <br/><br/>NextEndpoint() url.URL 
</pre>
<p>This is the method which will return a particular endpoint for the strategy. It is not called directly, but it is called internally by the <kbd>LoadBalancer</kbd> package when you call the <kbd>GetEndpoint</kbd> method. This has to be a public method to allow for strategies to be included in packages outside of the <kbd>LoadBalancer</kbd> package:</p>
<pre>
SetEndpoints([]url.URL) 
</pre>
<p>This method will update the <kbd>Strategy</kbd> type with a list of the currently available endpoints. Again, this is not called directly but is called internally by the <kbd>LoadBalancer</kbd> package when you call the <kbd>UpdateEndpoints</kbd> method.</p>
<p>To use the <kbd>LoadBalancer</kbd> package, you just initialize it with your chosen strategy and a list of endpoints, then by calling <kbd>GetEndpoint</kbd>, you will receive the next endpoint in the list:</p>
<pre>
 56 func main() { <br/> 57   endpoints := []url.URL{ <br/> 58     url.URL{Host: "www.google.com"}, <br/> 59     url.URL{Host: "www.google.co.uk"}, <br/> 60   } <br/> 61 <br/> 62   lb := NewLoadBalancer(&amp;RandomStrategy{}, endpoints) <br/> 63 <br/> 64   fmt.Println(lb.GetEndpoint()) <br/> 65 } 
</pre>
<p>In the example code, we have implemented a simple <kbd>RandomStrategy</kbd>. Why not see if you can build a strategy which applies a <kbd>RoundRobinStrategy</kbd>?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caching</h1>
                </header>
            
            <article>
                
<p>One way you can improve the performance of your service is by caching results from databases and other downstream calls in an in-memory cache or a side cache like Redis, rather than by hitting a database every time.</p>
<p>Caches are designed to deliver massive throughput by storing precompiled objects in a fast-access data store, frequently based around a concept of a hash key. We know from looking at algorithm performance that a hash table has the average performance of O(1); that is as fast as it gets. Without going too in depth into Big O notation, this means it takes one iteration to be able to find the item you want in the collection.</p>
<p>What this means for you is that, not only can you reduce the load on your database, you can also reduce your infrastructure costs. Typically, a database is limited by the amount of data that can be read and written from the disk and the time it takes for the CPU to process this information. With an in-memory cache, this limitation is removed by using pre-aggregated data, which is stored in fast memory, not onto a state-full device like a disk. You also eliminate the problem with locking that many: databases suffer where one write can block many reads for a piece of information. This comes at the cost of consistency because you cannot guarantee that all your clients will have the same information at the same time. However, more often than not strong consistency is a vastly overvalued attribute of a database:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="459" width="382" class="image-border" src="assets/88b66cf2-d4ce-4def-9898-fcfa701ea7b7.png"/></div>
<p>Consider our list of kittens. If we are receiving a high throughput of users retrieving a list of kittens, and it has to make a call to the database every time just to ensure the list is always up to date, then this will be costly and can fast overwhelm a database when it is already experiencing high load. We first need to ask ourselves is it essential that all these clients receive the updated information at the same time or is a one second delay quite acceptable. More often than not it is acceptable, and the speed and cost benefits you gain are well worth the potential cost that a connecting client does not get the up-to-date information exactly after it has been written to the database.</p>
<p>Caching strategies can be calculated based on your requirements for this consistency. In theory, the longer your cache expiry, the greater your cost saving, and the faster your system is at the expense of reduced consistency. We have already talked about designing for failure and how you can implement graceful degradation of a system. In the same way, when you are planning a feature, you should be talking about consistency and the tradeoffs with performance and cost, and documenting this decision, as these decisions will greatly help create a more successful implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Premature optimization</h1>
                </header>
            
            <article>
                
<p>You have probably heard the phrase, so does that mean you should not implement caching until you need it? No; it means you should be attempting to predict the initial load that your system will be under at design time, and the growth in capacity over time, as you are considering the application lifecycle. When creating this design, you will be putting together this data, and you will not be able to reliably predict the speed at which a service will run at. However, you do know that a cache will be cheaper to operate than a data store; so, if possible, you should be designing to use the smallest and cheapest data store possible, and making provision to be able to extend your service by introducing caching at a later date. This way you only do the actual work necessary to get the service out of the door, but you have done the design up front to be able to extend the service when it needs to scale.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stale cache in times of database or downstream service failure</h1>
                </header>
            
            <article>
                
<p>The cache will normally have an end date on it. However, if you implement the cache in a way that the code decides to invalidate it, then you can potentially avoid problems if a downstream service or database disappears. Again, this is back to thinking about failure states and asking what is better: the user seeing slightly out-of-date information or an error page? If your cache has expired, the call to the downstream service fails. However, you can always decide to serve the stale cache back to the calling client. In some instances, this will be better than returning a 50x error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have now seen how we can use some rather cool patterns to make our microservices more resilient and to deal with the inevitable failure. We have also looked at how introducing a weak link can save the entire system from a cascading failure. Where and how you apply these patterns should start out with an educated guess, but you need to constantly look at logging and monitoring to ensure that your opinion is still relevant. In the next chapter, we are going to look at some fantastic frameworks for building microservices in Go and then in, <a href="bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>Logging and Monitoring</em>, we will look at some options and best practice for logging and monitoring your service.</p>


            </article>

            
        </section>
    </body></html>