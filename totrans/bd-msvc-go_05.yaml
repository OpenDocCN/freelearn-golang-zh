- en: Common Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见模式
- en: Before we take a look at some frameworks which can help you build microservices
    in Go, we should first look at some of the design patterns that will help you
    avoid failure.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看一些可以帮助你在Go中构建微服务的框架之前，我们应该首先看看一些可以帮助你避免失败的设计模式。
- en: I am not talking about software design patterns like factories or facades, but
    architectural designs like load balancing and service discovery. If you have never
    worked with microservice architecture before, then you may not understand why
    these are needed, but I hope that by the end of the chapter you will have a solid
    understanding why these patterns are important and how you can apply them correctly.
    If you have already successfully deployed a microservice architecture, then this
    chapter will give you greater knowledge of the underlying patterns which make
    your system function. If you have not had much success with microservices, then
    possibly you did not understand that you need the patterns I am going to describe.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是在谈论像工厂或外观这样的软件设计模式，而是在谈论像负载均衡和服务发现这样的架构设计。如果你以前从未使用过微服务架构，那么你可能不会理解为什么需要这些，但我希望到本章结束时，你将有一个坚实的理解，为什么这些模式很重要，以及如何正确地应用它们。如果你已经成功部署了微服务架构，那么本章将为你提供更多关于使你的系统运行的基础模式的知识。如果你在微服务方面没有取得太多成功，那么可能你没有理解你需要我即将描述的模式。
- en: In general, there is something for everyone, and we are going to look at not
    just the core patterns but some of the fantastic open source software which can
    do most of the heavy lifting for us.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个人都有适合的东西，我们将不仅查看核心模式，还会查看一些可以为我们做大部分繁重工作的出色开源软件。
- en: 'The examples referenced in this chapter can be found at: [https://github.com/building-microservices-with-go/chapter5.git](https://github.com/building-microservices-with-go/chapter5.git)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中引用的示例可以在以下链接找到：[https://github.com/building-microservices-with-go/chapter5.git](https://github.com/building-microservices-with-go/chapter5.git)
- en: Design for failure
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 失败设计
- en: Anything that can go wrong will go wrong.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 凡是可能出错的事情都会出错。
- en: When we are building microservices, we should always be prepared for failure.
    There are many reasons for this, but the main one is that cloud computing networks
    can be flakey and you lose the ability to tune switching and routing, which would
    have given you an optimized system if you were running them in your data center.
    In addition to this, we tend to build microservice architectures to scale automatically,
    and this scaling causes services to start and stop in unpredictable ways.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建微服务时，我们始终应该为失败做好准备。这有很多原因，但主要原因是云计算网络可能会出现故障，你失去了调整切换和路由的能力，如果你在数据中心运行它们，这将为你提供一个优化的系统。此外，我们倾向于构建微服务架构来自动扩展，这种扩展会导致服务以不可预测的方式启动和停止。
- en: What this means for our software is that we need to think about this failure
    up front while discussing upcoming features. We then need to design this into
    the software from the beginning, and as engineers, we need to understand these
    problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们软件的意义在于，在讨论即将推出的功能时，我们需要提前考虑这种失败。然后我们需要从软件开始设计它，并且作为工程师，我们需要理解这些问题。
- en: 'In his book *Designing Data-Intensive Applications*, Martin Kleppman makes
    the following comment:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书《设计数据密集型应用》中，马丁·克莱普曼提出了以下评论：
- en: The bigger a system gets, the more likely it is that one of its components is
    broken. Over time, broken things get fixed, and new things break, but in a system
    with thousands of nodes, it is reasonable to assume that something is always broken.
    If the error handling strategy consists of only giving up such a large system
    would never work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 系统越大，其组件出现故障的可能性就越大。随着时间的推移，故障会被修复，新的东西会出问题，但在拥有数千个节点的系统中，合理地假设总会有东西是出故障的。如果错误处理策略只是放弃这样一个庞大的系统，那么这种方法永远不会成功。
- en: 'While this applies to more major systems, I would argue that the situation
    where you need to start considering failure due to connectivity and dependency
    begins once your estate reaches the size of *n+1*. This might seem a frighteningly
    small number, but it is incredibly relevant. Consider the following simplistic
    system:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这适用于更重大的系统，但我认为，当你需要开始考虑由于连接性和依赖性导致的失败时，你的系统规模一旦达到*n+1*，这种情况就开始了。这个数字可能看起来很小，但它的相关性却非常强。考虑以下简单的系统：
- en: You have a simple website which allows your users (who are all cat lovers) to
    register for updates from other cat lovers. The update is in the form of a simple
    daily e-mail, and you would like to send out a welcome e-mail once the form has
    been submitted by the user and the data saved into the database. Because you are
    a good microservice practitioner you have recognized that sending e-mails should
    not be the responsibility of the registration system, and instead you would like
    to devolve this to an external system. In the meantime, the service is growing
    in popularity; you have determined that you can save time and effort by leveraging
    the e-mail as an API service from MailCo. This has a simple RESTful contract and
    can support all your current needs, which allows you to get to market that little
    bit sooner.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个简单的网站，允许您的用户（他们都是猫爱好者）注册接收其他猫爱好者的更新。更新以简单的每日电子邮件的形式提供，您希望在用户提交表单并将数据保存到数据库后发送欢迎邮件。因为您是一位优秀的微服务实践者，您已经意识到发送电子邮件不应是注册系统的责任，相反，您希望将这项任务委托给外部系统。在此期间，该服务越来越受欢迎；您确定可以通过利用MailCo的电子邮件API服务来节省时间和精力。这个服务有一个简单的RESTful协议，可以满足您当前的所有需求，这使您能够更快地进入市场。
- en: 'The following diagram represents that simple microservice:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表示了那个简单的微服务：
- en: '![](img/4b51cade-e920-46a1-9b70-63a6c227f348.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b51cade-e920-46a1-9b70-63a6c227f348.png)'
- en: Being a good software architect, you define an interface for this mail functionality
    which will serve as an abstraction for the actual implementation. This concept
    will allow you to replace MailCo quickly at a later date. Sending e-mails is fast,
    so there is no need to do anything clever. We can make the call to MailCo synchronously
    during the registration request.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一位优秀的软件架构师，您为这个邮件功能定义了一个接口，它将作为实际实现的抽象。这个概念将允许您在以后快速替换MailCo。发送电子邮件很快，所以没有必要做任何聪明的事情。我们可以在注册请求期间同步调用MailCo。
- en: This application seems like a simple problem to solve, and you get the work
    done in record time. The site is hosted on AWS and configured with ElasticScale
    so, no matter what load you get, you will be sleeping peacefully with no worry
    that the site could go down.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序看起来像是一个简单的问题，您以创纪录的时间完成了工作。该网站托管在AWS上，并配置了ElasticScale，所以无论您得到什么负载，您都会安心地睡觉，不用担心网站会崩溃。
- en: One evening your CEO is out at an event for tech startups which is being covered
    by the news network, CNN. She gets talking to the reporter who, also being a cat
    lover, decides he would like to feature the service in a special report which
    will air tomorrow evening.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一天晚上，您的CEO参加了一个由新闻网络CNN报道的科技初创公司活动。她与记者交谈，这位记者也是一个猫爱好者，决定在明天的晚间特别报道中介绍这项服务。
- en: The excitement is unreal; this is the thing that will launch the service into
    the stratosphere. You and the other engineers check the system just for peace
    of mind, make sure the auto scale is configured correctly, then kick back with
    some pizza and beer to watch the program.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 兴奋得难以置信；这正是将服务推向太空的东西。您和其他工程师只是为了安心检查系统，确保自动扩展配置正确，然后吃些披萨和啤酒，观看节目。
- en: 'When the program airs and your product is shown to the nation, you can see
    the number of users on the site in Google Analytics. It looks great: request times
    are small, the cloud infrastructure is doing its job, and this has been a total
    success. Until of course, it isn''t. After a few minutes, the request queueing
    starts to climb, and the services are still scaling, but now the alarms are going
    off due to a high number of errors and transaction processing time. More and more
    users are entering the site and trying to register but very few are successful,
    this is the worst kind of disaster you could have ever wished for.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当节目播出，您的产品向全国展示时，您可以在Google Analytics中看到网站上的用户数量。看起来很棒：请求时间短，云基础设施正在正常工作，这是一次完全的成功。当然，直到它不是。几分钟之后，请求排队开始上升，服务仍在扩展，但现在由于错误数量众多和交易处理时间过长，警报开始响起。越来越多的用户进入网站并尝试注册，但成功的人很少，这是您可能希望发生的最糟糕的灾难。
- en: I won't mention the look on the face of the CEO; you have all probably seen
    that look at some point in your careers; and if not, when you do you will know
    what I am talking about. It is a cross between, anger, hatred, and confusion as
    to how they hired such idiots.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会提及CEO脸上的表情；你们中的许多人可能在自己的职业生涯中见过这种表情；如果没有，当你看到的时候，你就会知道我在说什么。那是一种介于愤怒、仇恨和困惑之间的表情，他们怎么会雇佣这样的笨蛋。
- en: You aren't an idiot; software is complex, and with complexity, it is easy to
    make mistakes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你不是白痴；软件是复杂的，而复杂性很容易导致错误。
- en: So, you start to investigate the problem, quickly you see that while your service
    and database have been operating correctly, the bottleneck is MailCo's e-mail
    API. This started the blockage and, because you were executing a synchronous request,
    your service started blocking too.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你开始调查这个问题，很快你就看到，尽管你的服务和数据库一直在正常运行，但瓶颈是MailCo的电子邮件API。这导致了阻塞，因为你正在执行一个同步请求，所以你的服务也开始阻塞。
- en: So, your moment of glory was taken down by a single bottleneck with a third-party
    API. Now you understand why you need to plan for failure. Let's take a look at
    how you can implement failure driven design patterns.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你的辉煌时刻被一个第三方API的单个瓶颈击垮了。现在你理解了为什么你需要为故障做计划。让我们看看你如何可以实施故障驱动的设计模式。
- en: Patterns
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式
- en: The truth about microservices is that they are not hard you only need to understand
    the core software architectural patterns which will help you succeed. In this
    section, we are going to take a look at some of these patterns and how we can
    implement them in Go.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于微服务的真相是它们并不难，你只需要理解核心软件架构模式，这些模式将帮助你成功。在本节中，我们将探讨一些这些模式以及我们如何在Go中实现它们。
- en: Event processing
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件处理
- en: In our case study, we failed due to a downstream synchronous process failing,
    and that blocked the upstream. The first question we should ask ourselves is "Does
    this call need to be synchronous?" In the case of sending an e-mail, the answer
    is almost always, No. The best way to deal with this is to take a fire and forget
    approach; we would just add the request with all the details of the mail onto
    a highly available queue which would guarantee at least once delivery and move
    on. There would be a separate worker processing the queue records and sending
    these on to the third-party API.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究中，我们因为下游同步过程失败而失败，这阻碍了上游。我们应该问自己的第一个问题是“这个调用需要同步吗？”在发送电子邮件的情况下，答案几乎总是“不”。处理这个问题最好的方式是采取“点火后忘记”的方法；我们只需将包含邮件所有详细信息的请求添加到一个高可用队列中，这样可以保证至少一次的投递，然后继续。会有一个单独的工人在处理队列记录并将这些发送到第三方API。
- en: In the instance that the third party starts to experience problems, we can happily
    stop processing the queue without causing any problems for our registration service.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第三方开始出现问题，我们可以愉快地停止处理队列，而不会对我们的注册服务造成任何问题。
- en: 'Regarding user experience, this potentially means that the when the user clicks
    the register button they would not instantly receive their welcome e-mail. However,
    e-mail is not an instantaneous system, so some delay is to be expected. You could
    enhance your user experience further: what if adding an item to the queue returns
    the approximate queue length back to the calling system. When you are designing
    for failure, you may take a call that if the queue is over *n* items, you could
    present a friendly message to the user letting them know you are busy now but
    rest assured your welcome e-mail is on its way.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用户体验，这可能会意味着当用户点击注册按钮时，他们不会立即收到欢迎邮件。然而，电子邮件不是一个即时系统，所以一些延迟是可以预料的。你可以进一步改善用户体验：如果将项目添加到队列中返回队列的大致长度给调用系统，会怎样？当你设计故障时，你可能采取的决策是，如果队列中有超过*n*个项目，你可以向用户展示一个友好的消息，告诉他们你现在很忙，但请放心，你的欢迎邮件正在路上。
- en: We will look at the implementation of this pattern further in [Chapter 9](2952a830-163e-4610-8554-67498ec77e1e.xhtml),
    *Event-Driven Architecture*, but at the moment there are a few key concepts that
    we need to cover.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第9章](2952a830-163e-4610-8554-67498ec77e1e.xhtml)“事件驱动架构”中进一步探讨这个模式的实现，但到目前为止，有几个关键概念我们需要讨论。
- en: Event processing with at least once delivery
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 至少一次投递的事件处理
- en: Event processing is a model which allows you to decouple your microservices
    by using a message queue. Rather than connect directly to a service which may
    or may not be at a known location, you broadcast and listen to events which exist
    on a queue, such as Redis, Amazon SQS, NATS.io, Rabbit, Kafka, and a whole host
    of other sources.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事件处理是一个模型，它允许你通过使用消息队列来解耦你的微服务。而不是直接连接到一个可能或可能不在已知位置的服务，你广播并监听存在于队列上的事件，例如Redis、Amazon
    SQS、NATS.io、Rabbit、Kafka以及一大堆其他来源。
- en: To use our example of sending a welcome e-mail, instead of making a direct call
    to the downstream service using its REST or RPC interface, we would add an event
    to a queue containing all the details that the recipient would need to process
    this message.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以发送欢迎电子邮件的例子来说，我们不会直接调用下游服务的REST或RPC接口，而是将包含接收者处理此消息所需的所有详细信息的活动添加到一个队列中。
- en: 'Our message may look like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的消息可能看起来像：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We add the message to the queue and then wait for an ACK from the queue to let
    us know that the message has been received. Of course, we would not know if the
    message has been delivered but receiving the ACK should be enough for us to notify
    the user and proceed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将消息加入队列，然后等待队列的确认（ACK），以告知我们消息已被接收。当然，我们不知道消息是否已送达，但收到确认应该足以让我们通知用户并继续操作。
- en: The message queue is a highly distributed and scalable system, and it should
    be capable of processing millions of messages so we do not need to worry about
    it not being available. At the other end of the queue, there will be a worker
    who is listening for new messages pertaining to it. When it receives such a message,
    it processes the message and then removes it from the queue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列是一个高度分布式和可扩展的系统，它应该能够处理数百万条消息，所以我们不需要担心它不可用。在队列的另一端，将有一个工作者正在监听与其相关的新的消息。当它收到这样的消息时，它会处理该消息，然后将其从队列中移除。
- en: '![](img/d6f3bfdc-300c-459f-8841-0ae61cea8b16.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6f3bfdc-300c-459f-8841-0ae61cea8b16.png)'
- en: Of course, there is always the possibility that the receiving service can not
    process the message which could be due to a direct failure or bug in the email
    service or it could be that the message which was added to the queue is not in
    a format which can be read by the email service. We need to deal with both of
    these issues independently, let us start with handing errors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，接收服务可能无法处理消息，这可能是由于电子邮件服务直接失败或存在错误，或者可能是加入队列的消息格式不适合电子邮件服务读取。我们需要独立处理这两个问题，让我们从处理错误开始。
- en: Handling Errors
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理错误
- en: 'It is not uncommon for things to go wrong with distributed systems and we should
    factor this into our software design, in the instance that a valid message can
    not be processed one standard approach is to retry processing the message, normally
    with a delay. We can add the message back onto the queue augmenting it with the
    error message which occurred at the time as seen in the following example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中出现错误并不罕见，我们应该在软件设计中考虑这一点。在有效消息无法被处理的情况下，一个标准的方法是重试处理该消息，通常会有一个延迟。我们可以将消息连同当时发生的错误信息重新加入队列，如下面的示例所示：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is important to append the error every time we fail to process a message
    as it gives us the history of what went wrong, it also provides us with the capability
    to understand how many times we have tried to process the message because after
    we exceed this threshold we do not want to continue to retry we need to move this
    message to a second queue where we can use it for diagnostic information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们无法处理消息时，都重要地附加错误信息，因为它提供了错误的历史记录，它还提供了我们理解尝试处理消息次数的能力，因为当我们超过这个阈值后，我们不想继续重试，我们需要将此消息移动到第二个队列，在那里我们可以用它作为诊断信息。
- en: Dead Letter Queue
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 死信队列
- en: This second queue is commonly called a dead letter queue, a dead letter queue
    is specific to the queue from where the message originated, if we had a queue
    named `order_service_emails` then we would create a second queue called `order_service_emails_deadletter`.
    The purpose of this is so that we can examine the failed messages on this queue
    to assist us with debugging the system, there is no point in knowing an error
    has occurred if we do not know what that error is and because we have been appending
    the error details direct to the message body we have this history right where
    we need it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第二个队列通常被称为死信队列，死信队列是特定于消息原始队列的，如果我们有一个名为`order_service_emails`的队列，那么我们会创建一个名为`order_service_emails_deadletter`的第二个队列。这样做的目的是为了我们可以检查这个队列上的失败消息，以帮助我们调试系统，如果我们不知道错误是什么，那么知道发生了错误就没有意义，因为我们已经将错误详情直接附加到消息正文中，所以我们需要的这个历史记录就在我们需要的地方。
- en: We can see that the message has failed because we have exceeded our quota in
    the mail API, we also have the date and time of when the error occurred. In this
    instance, because we have exceeded our quota with the email provider once we remove
    the issue with the email provider we can then move all of these messages from
    the dead letter queue back onto the main queue and they should then process correctly.
    Having the error information in a machine readable format allows us to handle
    the dead letter queue programmatically, we can explicitly select messages which
    relate to quota problem within a particular time window.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过邮件API超出配额的情况看到消息失败，我们也有错误发生的时间和日期。在这种情况下，因为我们已经超过了电子邮件提供者的配额，一旦我们解决了电子邮件提供者的问题，我们就可以将这些消息从死信队列移回到主队列，并且它们应该会正确处理。错误信息以机器可读的格式存在，使我们能够以编程方式处理死信队列，我们可以在特定时间窗口内明确选择与配额问题相关的消息。
- en: In the instance that a message can not be processed by the email service due
    to a bad message payload we typically do not retry processing of the message but
    add it directly to the dead letter queue. Again having this information allows
    us to diagnose why this issue might have occurred, it could be due to a contract
    change in the upstream service which has not been reflected in the downstream
    service. If this is the reason behind the failure we have the knowledge to correct
    the contract issue in the email service which is consuming the messages and again
    move the message back into the main queue for processing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于消息负载不良，电子邮件服务无法处理消息，我们通常不会重试处理该消息，而是直接将其添加到死信队列。再次拥有这些信息使我们能够诊断为什么可能发生这个问题，这可能是由于上游服务中的合同变更尚未反映在下游服务中。如果这是失败的原因，我们就有了纠正消耗消息的电子邮件服务中合同问题的知识，然后将消息再次移回到主队列以进行处理。
- en: Idempotent transactions and message order
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 幂等事务和消息顺序
- en: While many message queues now offer *At Most Once Delivery* in addition to the
    *At Least Once*, the latter option is still the best for large throughput of messages.
    To deal with the fact that the downstream service may receive a message twice
    it needs to be able to handle this in its own logic. One method for ensuring that
    the same message is not processed twice is to log the message ID in a transactions
    table. When we receive a message, we will insert a row which contains the message
    ID and then we can check when we receive a message to see if it has already been
    processed and if it has to dispose of that message.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然现在许多消息队列除了提供**至少一次投递**外，还提供了**最多一次投递**，但对于大量消息的高吞吐量来说，后者仍然是最佳选择。为了处理下游服务可能会接收到消息两次的事实，它需要能够在其自身逻辑中处理这种情况。确保相同消息不会被处理两次的一种方法是在事务表中记录消息ID。当我们收到一条消息时，我们会插入一个包含消息ID的行，然后我们可以在收到消息时检查它是否已经被处理，以及是否需要处理该消息。
- en: The other issue that can occur with messaging is receiving a message out of
    sequence if for some reason two messages which supersede each other are received
    in an incorrect order then you may end up with inconsistent data in the database.
    Consider this simple example, the front end service allows the update of user
    information a subset of which is forwarded to a second microservice. The user
    quickly updates their information twice which causes two messages to be dispatched
    to the second service, providing both messages arrive in the order by which they
    were dispatched then the second service will process both messages and the data
    will be in a consistent state. However, if they do not arrive in the correct order
    then the second service will be inconsistent to the first as it will save the
    older data as the most recent. Once potential way to avoid this issue is to again
    leverage the transaction table and to store the message dispatch_date in addition
    to the id. When the second service receives a message then it can not only check
    if the current message has been processed it can check that it is the most recent
    message and if not discard it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递可能出现的另一个问题是，如果由于某种原因，两个相互替代的消息以错误的顺序接收，那么你可能会在数据库中得到不一致的数据。考虑这个简单的例子，前端服务允许更新用户信息的一部分，这部分信息被转发到第二个微服务。用户快速更新他们的信息两次，导致向第二个服务发送了两条消息。如果这两条消息按照发送的顺序到达，那么第二个服务将处理这两条消息，数据将处于一致状态。然而，如果它们没有按照正确的顺序到达，那么第二个服务将相对于第一个服务不一致，因为它会将旧数据保存为最新数据。一种避免这种问题的潜在方法是通过再次利用事务表，并存储消息的发送日期，除了id之外。当第二个服务接收到一条消息时，它不仅可以检查当前消息是否已被处理，还可以检查它是否是最新的消息，如果不是，则丢弃它。
- en: Unfortunately, there is no one solution fits all with messaging we need to tailor
    the solution which matches the operating conditions of the service. For you as
    a microservice practitioner, you need to be aware that these conditions can exist
    and factor them into your solution designs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，没有一种解决方案可以适用于所有消息传递需求，我们需要根据服务的运行条件定制解决方案。对于你作为微服务实践者来说，你需要意识到这些条件可能存在，并将它们纳入你的解决方案设计中。
- en: Atomic transactions
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子事务
- en: 'While storing data, a database can be ATOMIC: that is, all operations occur
    or none do. We cannot say the same with distributed transactions in microservices.
    When we used SOAP as our message protocol a decade or so ago, there was a proposal
    for a standard called **Web Service-Transactions** (**WS-T**). This aimed to provide
    the same functionality that you get from a database transaction, but in a distributed
    system. Thankfully SOAP is long gone unless you work in finance or another industry
    which deals with legacy systems, but the problem remains. In our previous example,
    we looked at how we can decouple the saving of the data and the sending of the
    e-mail by using a message queue with at least once delivery. What if we could
    solve the problem of atomicity in the same way, consider this example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储数据时，数据库可以是原子的：也就是说，所有操作要么都发生，要么都不发生。在微服务的分布式事务中，我们不能这样说。当我们大约十年前使用SOAP作为我们的消息协议时，有一个名为**Web服务-事务**（**WS-T**）的标准的提案。这个标准旨在提供与数据库事务相同的功能，但在分布式系统中。幸运的是，SOAP已经消失了，除非你在金融或其他处理遗留系统的行业中工作，但问题仍然存在。在我们之前的例子中，我们探讨了如何通过使用至少一次投递的消息队列来解耦数据的保存和电子邮件的发送。如果我们能够以同样的方式解决原子性问题，考虑这个例子：
- en: '![](img/e0b953ae-9222-4881-8876-3797cef2fa28.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e0b953ae-9222-4881-8876-3797cef2fa28.png)'
- en: We distribute both parts of our order process to the queue, a worker service
    persists the data to the database, and a service that is responsible for sending
    the confirmation e-mail. Both these services would subscribe to the same `new_order`
    message and take action when this is received. Distributed transactions do not
    give us the same kind of transaction that is found in a database. When part of
    a database transaction fails, we can roll back the other parts of the transaction.
    Using this pattern we would only remove the message from the queue if the process
    succeeded so when something fails, we keep retrying. This gives us a kind of eventually
    consistent transaction. My opinion on distributed transactions is to avoid them
    if possible; try to keep your behavior simple. However, when this is not possible
    then this pattern may just be the right one to apply.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将订单处理的两部分都分配到队列中，一个工作服务将数据持久化到数据库，另一个负责发送确认电子邮件的服务。这两个服务都会订阅相同的`new_order`消息，并在收到该消息时采取行动。分布式事务并不提供数据库中存在的同类型事务。当数据库事务的一部分失败时，我们可以回滚事务的其他部分。使用这种模式，我们只有在处理成功的情况下才会从队列中删除消息，因此当出现失败时，我们会持续重试。这给我们带来了一种最终一致的事务。我对分布式事务的看法是，如果可能的话，尽量避免使用；尽量保持你的行为简单。然而，当这种情况不可能时，这种模式可能就是适用的正确选择。
- en: Timeouts
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超时
- en: A timeout is an incredibly useful pattern while communicating with other services
    or data stores. The idea is that you set a limit on the response of a server and,
    if you do not receive a response in the given time, then you write a business
    logic to deal with this failure, such as retrying or sending a failure message
    back to the upstream service.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超时是在与其他服务或数据存储进行通信时一个非常实用的模式。其理念是，你为服务器的响应设置一个限制，如果在给定时间内没有收到响应，那么你将编写业务逻辑来处理这种失败，例如重试或向上游服务发送失败消息。
- en: A timeout could be the only way of detecting a fault with a downstream service.
    However, no reply does not mean the server has not received and processed the
    message, or that it might not exist. The key feature of a timeout is to fail fast
    and to notify the caller of this failure.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 超时可能是检测下游服务故障的唯一方式。然而，没有回复并不意味着服务器没有收到并处理了消息，或者它可能不存在。超时的关键特性是快速失败并通知调用者这种失败。
- en: 'There are many reasons why this is a good practice, not only from the perspective
    of returning early to the client and not keeping them waiting indefinitely but
    also from the point of view of load and capacity. Every connection that your service
    currently has active is one which cannot serve an active customer. Also, the capacity
    of your system is not infinite, it takes many resources to maintain a connection,
    and this also applies to the upstream service which is making a call to you. Timeouts
    are an effective hygiene factor in large distributed systems, where many small
    instances of a service are often clustered to achieve high throughput and redundancy.
    If one of these instances is malfunctioning and you, unfortunately, connect to
    it, then this can block an entirely functional service. The correct approach is
    to wait for a response for a set time and then if there is no response in this
    period, we should cancel the call, and try the next service in the list. The question
    of what duration your timeouts are set to do not have a simple answer. We also
    need to consider the different types of timeout which can occur in a network request,
    for example, you have:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种很好的做法，不仅从尽早向客户端返回并避免他们无限期等待的角度来看，而且从负载和容量的角度来看也是如此。你服务当前拥有的每个活动连接都是一个不能为活动客户提供服务的关系。此外，你系统的容量是无限的，维护一个连接需要许多资源，这也适用于向你发起调用的上游服务。超时是大规模分布式系统中的一个有效卫生因素，在这些系统中，许多服务的小实例通常被集群在一起以实现高吞吐量和冗余。如果这些实例中的一个出现故障，不幸的是你连接到了它，那么这可能会阻塞一个完全正常的服务。正确的方法是等待一定时间的响应，如果在这一时期内没有响应，我们应该取消调用，并尝试列表中的下一个服务。关于你的超时设置多长时间的问题没有简单的答案。我们还需要考虑网络请求中可能发生的不同类型的超时，例如，你有：
- en: Connection Timeout - The time it takes to open a network connection to the server
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接超时 - 打开到服务器的网络连接所需的时间
- en: Request Timeout - The time it takes for a server to process a request
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求超时 - 服务器处理请求所需的时间
- en: The request timeout is almost always going to be the longest duration of the
    two and I recommend the timeout is defined in the configuration of the service.
    While you might initially set it to an arbitrary value of, say 10 seconds, you
    can modify this after the system has been running in production, and you have
    a decent data set of transaction times to look at.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请求超时几乎总是两个中最长的时间，我建议在服务的配置中定义超时。虽然你最初可能将其设置为任意值，比如10秒，但在系统在生产环境中运行并积累了一定的交易时间数据集后，你可以修改这个值。
- en: We are going to use the deadline package from eapache ([https://github.com/eapache/go-resiliency/tree/master/deadline](https://github.com/eapache/go-resiliency/tree/master/deadline)),
    recommended by the go-kit toolkit ([https://gokit.io](https://gokit.io)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用eapache的`deadline`包（[https://github.com/eapache/go-resiliency/tree/master/deadline](https://github.com/eapache/go-resiliency/tree/master/deadline)），这是由go-kit工具包（[https://gokit.io](https://gokit.io)）推荐的。
- en: The method we are going to run loops from 0-100 and sleeps after each loop.
    If we let the function continue to the end, it would take 100 seconds.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要运行的方法从0-100循环，并在每次循环后暂停。如果我们让函数继续到末尾，它将需要100秒。
- en: 'Using the deadline package we can set our own timeout to cancel the long running
    operation after two seconds:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`deadline`包，我们可以设置自己的超时，在两秒后取消长时间运行的操作：
- en: '`timeout/main.go`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeout/main.go`'
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Back off
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 退避
- en: Typically, once a connection has failed, you do not want to retry immediately
    to avoid flooding the network or the server with requests. To allow this, it's
    necessary to implement a back-off approach to your retry strategy. A back-off
    algorithm waits for a set period before retrying after the first failure, this
    then increments with subsequent failures up to a maximum duration.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一旦连接失败，你不想立即重试，以避免用请求淹没网络或服务器。为了允许这样做，有必要在重试策略中实现退避方法。退避算法在第一次失败后等待设定的时间，然后随着后续失败的增加而增加，直到达到最大持续时间。
- en: Using this strategy inside a client-called API might not be desirable as it
    contravenes the requirement to fail fast. However, if we have a worker process
    that is only processing a queue of messages, then this could be exactly the right
    strategy to add a little protection to your system.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端调用的API中使用这种策略可能不是最佳选择，因为它违反了快速失败的要求。然而，如果我们有一个仅处理消息队列的工作进程，那么这可能是为您的系统添加一点保护的最佳策略。
- en: We will look at the `go-resiliency` package and the `retrier` package.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨`go-resiliency`包和`retrier`包。
- en: 'To create a new retrier, we use the `New` function which has the signature:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的重试器，我们使用`New`函数，其签名如下：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first parameter is an array of `Duration`. Rather than calculating this
    by hand, we can use the two built-in methods which will generate this for us:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是`Duration`数组。我们不必手动计算这个值，可以使用两个内置方法来生成这个值：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `ConstantBackoff` function generates a simple back-off strategy of retrying
    *n* times and waiting for the given amount of time between each retry:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConstantBackoff`函数生成一个简单的退避策略，重试*n*次，并在每次重试之间等待给定的时间：'
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `ExponentialBackoff` function generates a simple back-off strategy of retrying
    *n* times doubling the time between each retry.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExponentialBackoff`函数生成一个简单的退避策略，重试*n*次，并在每次重试之间加倍时间。'
- en: The second parameter is a `Classifier`. This allows us a nice amount of control
    over what error type is allowed to retry and what will fail immediately.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是`Classifier`。这允许我们对允许重试的错误类型和立即失败的错误类型有更多的控制。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `DefaultClassifier` type is the simplest form: if there is no error returned
    then we succeed; if there is any error returned then the retrier enters the retry
    state.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`DefaultClassifier`类型是最简单形式：如果没有返回错误，则成功；如果有任何错误返回，则重试器进入重试状态。'
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `BlacklistClassifier` type classifies errors based on a blacklist. If the
    error is in the given blacklist it immediately fails; otherwise, it will retry.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`BlacklistClassifier`类型根据黑名单对错误进行分类。如果错误在给定的黑名单中，它将立即失败；否则，它将重试。'
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `WhitelistClassifier` type is the opposite of the blacklist, and it will
    only retry when an error is in the given white list. Any other errors will fail.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`WhitelistClassifier`类型与黑名单相反，它只有在给定的白名单中存在错误时才会重试。任何其他错误都将失败。'
- en: The WhitelistClassifier might seem slightly complicated. However, every situation
    requires a different implementation. The strategy that you implement is tightly
    coupled to your use case.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`WhitelistClassifier`可能看起来稍微复杂一些。然而，每种情况都需要不同的实现。你实施的策略与你的用例紧密相关。'
- en: Circuit breaking
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电路断开
- en: 'We have looked at some patterns like timeouts and back-offs, which help protect
    our systems from cascading failure in the instance of an outage. However, now
    it''s time to introduce another pattern which is complementary to this duo. Circuit
    breaking is all about failing fast, Michael Nygard in his book "Release It" says:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了像超时和退避这样的模式，这些模式有助于保护我们的系统在出现故障时免受级联失败的影响。然而，现在是时候介绍另一个与这对互补的模式了。断路器完全是关于快速失败，Michael
    Nygard 在他的书《Release It》中说：
- en: '''''Circuit breakers are a way to automatically degrade functionality when
    the system is under stress."'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: “断路器是一种在系统处于压力下自动降低功能的方法。”
- en: One such example could be our frontend example web application. It is dependent
    on a downstream service to provide recommendations for kitten memes that match
    the kitten you are looking at currently. Because this call is synchronous with
    the main page load, the web server will not return the data until it has successfully
    returned recommendations. Now you have designed for failure and have introduced
    a timeout of five seconds for this call. However, since there is an issue with
    the recommendations system, a call which would ordinarily take 20 milliseconds
    is now taking 5,000 milliseconds to fail. Every user who looks at a kitten profile
    is waiting five seconds longer than usual; your application is not processing
    requests and releasing resources as quickly as normal, and its capacity is significantly
    reduced. In addition to this, the number of concurrent connections to the main
    website has increased due to the length of time it is taking to process a single
    page request; this is adding load to the front end which is starting to slow down.
    The net effect is going to be that, if the recommendations service does not start
    responding, then the whole site is headed for an outage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的例子可能是我们的前端示例网络应用程序。它依赖于下游服务提供与当前查看的小猫匹配的小猫搞笑图片推荐。因为这个调用与主页加载同步，所以直到成功返回推荐，服务器不会返回数据。现在你已经为失败设计了，并为这个调用引入了五秒的超时。然而，由于推荐系统存在问题，原本只需20毫秒的调用现在需要5000毫秒才能失败。每个查看小猫个人资料的用户都比平时多等了五秒；你的应用程序没有像正常那样快速处理请求和释放资源，其容量显著减少。此外，由于处理单个页面请求所需时间较长，主网站的并发连接数增加了；这正在给前端增加负载，使其开始变慢。最终的效果将是，如果推荐服务不开始响应，整个网站将面临中断。
- en: 'There is a simple solution to this: you should stop attempting to call the
    recommendations service, return the website back to normal operating speeds, and
    slightly degrade the functionality of the profile page. This has three effects:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，有一个简单的解决方案：你应该停止尝试调用推荐服务，将网站恢复到正常操作速度，并稍微降低个人资料页面的功能。这将产生三个效果：
- en: You restore the browsing experience to other users on the site.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将恢复网站上其他用户的浏览体验。
- en: You slightly degrade the experience in one area.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在某个领域稍微降低了用户体验。
- en: You need to have a conversation with your stakeholders before you implement
    this feature as it has a direct impact on the system's business.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实施此功能之前，你需要与你的利益相关者进行对话，因为它对系统的业务有直接影响。
- en: Now in this instance, it should be a relatively simple sell. Let's assume that
    recommendations increase conversion by 1%; however, slow page loads reduce it
    by 90%. Then isn't it better to degrade by 1% instead of 90%? This example, is
    clear cut but what if the downstream service was a stock checking system; should
    you accept an order if there is a chance you do not have the stock to fulfill
    it?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个例子应该相对简单。假设推荐可以提高转化率1%；然而，缓慢的页面加载会减少90%。那么，降低1%而不是90%不是更好吗？这个例子很明确，但如果下游服务是股票检查系统，你应该接受一个可能没有库存来履行的订单吗？
- en: Error behaviour is not a question that software engineering can answer on its
    own; business stakeholders need to be involved in this decision. In fact, I recommend
    that when you are planning the design of your systems, you talk about failure
    as part of your non-functional requirements and decide ahead of time what you
    will do when the downstream service fails.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 错误行为不是软件工程可以独自回答的问题；业务利益相关者需要参与这个决策。事实上，我建议在规划系统设计时，将失败作为非功能性需求的一部分进行讨论，并提前决定当下游服务失败时将采取什么措施。
- en: '**So how do they work?**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么它们是如何工作的呢？**'
- en: Under normal operations, like a circuit breaker in your electricity switch box,
    the breaker is closed and traffic flows normally. However, once the pre-determined
    error threshold has been exceeded, the breaker enters the open state, and all
    requests immediately fail without even being attempted. After a period, a further
    request would be allowed and the circuit enters a half-open state, in this state
    a failure immediately returns to the open state regardless of the `errorThreshold`.
    Once some requests have been processed without any error, then the circuit again
    returns to the closed state, and only if the number of failures exceeded the error
    threshold would the circuit open again.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常操作中，就像你家里的电闸箱中的断路器一样，断路器是闭合的，流量正常流动。然而，一旦预定的错误阈值被超过，断路器进入开启状态，所有请求立即失败，甚至没有尝试。经过一段时间后，会允许进一步的请求，此时电路进入半开启状态，在这个状态下，任何失败都会立即返回到开启状态，无论`errorThreshold`是多少。一旦处理了一些请求且没有错误，电路再次返回到闭合状态，只有当失败次数超过错误阈值时，电路才会再次开启。
- en: That gives us a little more context to why we need circuit breakers, but how
    can we implement them in Go?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了更多关于为什么我们需要断路器的背景信息，但我们在Go中如何实现它们呢？
- en: '![](img/553bafb9-1732-478c-976b-45097eb43bb1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/553bafb9-1732-478c-976b-45097eb43bb1.png)'
- en: 'Again, we are going to turn to the `go-resilience` package. Creating a circuit
    breaker is straight forward, the signature for the breaker is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次转向`go-resilience`包。创建断路器很简单，断路器的签名如下：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We construct our circuit breaker with three parameters:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建我们的断路器有三个参数：
- en: The first `errorThreshold`, is the number of times a request can fail before
    the circuit opens
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个`errorThreshold`，是电路开启之前请求可以失败的最大次数
- en: The `successThreshold`, is the number of times that we need a successful request
    in the half-open state before we move back to open
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`successThreshold`，是在我们回到开启状态之前，在半开启状态下需要成功的请求次数'
- en: The `timeout`, is the time that the circuit will stay in the open state before
    changing to half-open
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeout`，是电路在变为半开启状态之前保持开启状态的时间'
- en: 'Run the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you run this code you should see the following output. After three failed
    requests the breaker enters the open state, then after our five-second interval,
    we enter the half-open state, and we are allowed to make another request. Unfortunately,
    this fails, and we again enter the fully open state, and we no longer even attempt
    to make the call:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你应该看到以下输出。在三次失败请求后，断路器进入开启状态，然后在我们五秒的间隔后，我们进入半开启状态，并被允许再次发起请求。不幸的是，这次请求失败了，我们再次进入完全开启状态，并且我们不再尝试发起调用：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: One of the more modern implementations of circuit breaking and timeouts is the
    Hystix library from Netflix; Netflix is certainly renowned for producing some
    quality microservice architecture and the Hystrix client is something that has
    also been copied time and time again.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 电路断路和超时的一种更现代的实现是Netflix的Hystrix库；Netflix当然因生产高质量的微服务架构而闻名，Hystrix客户端也是被一次次复制的对象。
- en: Hystrix is described as "a latency and fault tolerance library designed to isolate
    points of access to remote systems, services, and third-party libraries, stop
    cascading failure, and enable resilience in complex distributed systems where
    failure is inevitable."
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hystrix被描述为“一个设计用于隔离对远程系统、服务和第三方库访问点的延迟和容错库，阻止级联故障，并在故障不可避免的复杂分布式系统中实现弹性。”
- en: ([https://github.com/Netflix/Hystrix](https://github.com/Netflix/Hystrix))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://github.com/Netflix/Hystrix](https://github.com/Netflix/Hystrix))
- en: For the implementation of this in Golang, check out the excellent package [https://github.com/afex/hystrix-go](https://github.com/afex/hystrix-go).
    This is a nice clean implementation, which is a little cleaner than implementing
    `go-resiliency`. Another benefit of `hystrix-go` is that it will automatically
    export metrics to either the Hystrix dashboard to via StatsD. In [Chapter 7](bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml),
    *Logging and Monitoring*, we will learn all about this just how important it is.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在Golang中的实现，请查看优秀的包[https://github.com/afex/hystrix-go](https://github.com/afex/hystrix-go)。这是一个很好的干净实现，比实现`go-resiliency`要干净一些。`hystrix-go`的另一个好处是它会自动将指标导出到Hystrix仪表板或StatsD。在[第7章](bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml)，*日志和监控*中，我们将了解到这一点的重要性。
- en: I hope you can see why this is an incredibly simple but useful pattern. However,
    there should be questions raised as to what you are going to do when you fail.
    Well these are microservices, and you will rarely only have a single instance
    of a service, so why not retry the call, and for that we can use a load balancer
    pattern.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能看到为什么这是一个极其简单但非常有用的模式。然而，应该提出一些问题，即当你失败时你将做什么。这些是微服务，你很少只有一个服务实例，所以为什么不重试调用，为此我们可以使用负载均衡器模式。
- en: Health checks
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: Health checks should be an essential part of your microservices setup. Every
    service should expose a health check endpoint which can be accessed by the consul
    or another server monitor. Health checks are important as they allow the process
    responsible for running the application to restart or kill it when it starts to
    misbehave or fail. Of course, you must be incredibly careful with this and not
    set this too aggressively.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查应该是你的微服务设置的一个基本部分。每个服务都应该暴露一个健康检查端点，该端点可以被consul或其他服务器监控器访问。健康检查很重要，因为它们允许负责运行应用程序的过程在应用程序开始出现异常或失败时重启或终止它。当然，你必须非常小心，不要过于激进地设置这个。
- en: 'What you record in your health check is entirely your choice. However, I recommend
    you look at implementing these features:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你在健康检查中记录的内容完全由你决定。然而，我建议你考虑实现以下功能：
- en: Data store connection status (general connection state, connection pool status)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储连接状态（一般连接状态，连接池状态）
- en: Current response time (rolling average)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前响应时间（滚动平均）
- en: Current connections
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前连接
- en: Bad requests (running average)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏请求（运行平均）
- en: How you determine what would cause an unhealthy state needs to be part of the
    discussion you have when you are designing the service. For example, no connectivity
    to the database means the service is completely inoperable, it would report unhealthy
    and would allow the orchestrator to recycle the container. An exhausted connection
    pool could just mean that the service is under high load, and while it is not
    completely inoperable it could be suffering degraded performance and should just
    serve a warning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何确定什么会导致不健康状态，需要成为你在设计服务时讨论的一部分。例如，无法连接到数据库意味着服务完全不可操作，它会报告不健康，并允许编排器回收容器。耗尽的连接池可能只是意味着服务在高负载下运行，虽然它不是完全不可操作，但它可能正在遭受性能下降，应该只发出警告。
- en: 'The same goes for the current response time. This point I find interesting:
    when you load test your service once it has been deployed to production, you can
    build up a picture of the thresholds of operating health. These numbers can be
    stored in the config and used by the health check. For example, if you know that
    your service will run an average service request with a 50 milliseconds latency
    for 4,000 concurrent users; however at 5,000, this time grows to 500 milliseconds
    as you have exhausted the connection pool. You could set your SLA upper boundary
    to be 100 milliseconds; then you would start reporting degraded performance from
    your health check. This should, however, be a rolling average based on the normal
    distribution. It is always possible for one or two requests to greatly be outside
    the standard deviation of normal operation, and you do not want to allow this
    to skew your average which then causes the service to report unhealthy, when in
    fact the slow response was actually due to the upstream service having slow network
    connectivity, not your internal state.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前的响应时间也是如此。我认为这一点很有趣：当你将服务部署到生产环境后进行负载测试，你可以构建出操作健康阈值的图像。这些数字可以存储在配置文件中，并由健康检查使用。例如，如果你知道你的服务将以50毫秒的延迟处理平均服务请求，对于4,000个并发用户；然而，当达到5,000个用户时，这个时间增长到500毫秒，因为你已经耗尽了连接池。你可以将你的服务级别协议（SLA）的上限设置为100毫秒；然后你将从健康检查开始报告性能下降。然而，这应该是一个基于正态分布的滚动平均值。总有可能有一个或两个请求会大大超出正常操作的标准差，你不想让这种情况扭曲你的平均值，从而导致服务报告不健康，而实际上缓慢的响应是由于上游服务网络连接缓慢，而不是你的内部状态。
- en: When discussing health checks, Michael Nygard considers the pattern of a handshake,
    where each client would send a handshake request to the downstream service before
    connecting to check if it was capable of receiving its request. Under normal operating
    conditions and most of the time, this adds an enormous amount of chatter into
    your application, and I think this could be overkill. It also implies that you
    are using client-side load-balancing, as with a server side approach you would
    have no guarantees that the service you handshake is the one you connect to. That
    said Release It was written over 10 years ago and much has changed in technology.
    The concept however of the downstream service making a decision that it can or
    can't handle a request is a valid one. Why not instead call your internal health
    check as the first operation before processing a request? This way you could immediately
    fail and give the client the opportunity to attempt another endpoint in the cluster.
    This call would add almost no overhead to your processing time as all you are
    doing is reading the state from the health endpoint, not processing any data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论健康检查时，Michael Nygard考虑了握手模式，其中每个客户端在连接到下游服务之前都会发送一个握手请求以检查其是否能够接收请求。在正常操作条件下，这会在你的应用程序中添加大量的冗余，我认为这可能有些过度。这也意味着你正在使用客户端负载均衡，因为如果使用服务器端方法，你无法保证你握手的那个服务就是你要连接的服务。尽管如此，《Release
    It》这本书是在10年前写的，技术已经发生了很大的变化。然而，下游服务决定它是否能够处理请求的概念是有效的。为什么不先调用你的内部健康检查，然后再处理请求呢？这样，你就可以立即失败，并给客户端尝试集群中另一个端点的机会。这个调用几乎不会增加你的处理时间，因为你只是在读取健康端点的状态，而不是处理任何数据。
- en: 'Let''s look at how we could implement this by looking at the example code in
    `health/main.go` :'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看`health/main.go`中的示例代码来了解如何实现这一点：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We are defining two handlers one which deals with our main request at the path
    `/` and one used for checking the health at the path `/health`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个处理器，一个用于处理路径`/`上的主要请求，另一个用于检查路径`/health`上的健康状态。
- en: The handler implements a simple moving average which records the time it takes
    for the handler to execute. Rather than just allow any request to be handled we
    are first checking on line **30** if the service is currently healthy which is
    checking if the current moving average is greater than a defined threshold if
    the service is not healthy we return the status code `StatusServiceUnavailable`S.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器实现了一个简单的移动平均值，它记录了处理器执行所需的时间。我们不是允许任何请求被处理，而是在第**30**行首先检查服务是否当前健康，这是检查当前移动平均值是否大于定义的阈值；如果服务不健康，我们返回状态码`StatusServiceUnavailable`。
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Looking greater in depth to the `respondServiceUnhealty` function, we can see
    it is doing more than just returning the HTTP status code.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深入查看`respondServiceUnhealty`函数，我们可以看到它不仅仅只是返回HTTP状态码。
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Lines **58** and **59** are obtaining a lock on the `resetMutex`, we need this
    lock as when the service is unhealthy we need to sleep to give the service time
    to recover and then reset the average. However, we do not want to call this every
    time the handler is called or once the service is marked unhealthy it would potentially
    never recover. The check and variable on line **61** ensures this does not happen
    however this variable is not safe unless marked with a mutex because we have multiple
    go routines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第**58**行和第**59**行正在获取对`resetMutex`的锁，我们需要这个锁，因为当服务不健康时，我们需要等待一段时间以给服务恢复的机会，然后重置平均值。然而，我们不希望在每次调用处理器或服务被标记为不健康时都调用这个操作，因为这可能会导致服务永远无法恢复。第**61**行的检查和变量确保了这种情况不会发生，然而，除非用互斥锁标记，否则这个变量并不安全，因为我们有多个goroutine。
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The sleepAndResetAverage function waits for a predetermined length of time before
    resetting the moving average, during this time no work will be performed by the
    service which will hopefully give the overloaded service time to recover. Again
    we need to obtain a lock on the resetMutex before interacting with the resetting
    variable to avoid any race conditions when multiple go routines are trying to
    access this variable. Line **69** then resets the moving average back to 0 which
    will mean work will again be able to be handled by the service.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`sleepAndResetAverage`函数在重置移动平均值之前会等待一个预定的时间长度，在这段时间内，服务将不会执行任何工作，这有望给过载的服务提供恢复的时间。同样，在交互重置变量之前，我们需要获取对`resetMutex`的锁，以避免多个goroutine尝试访问这个变量时的竞争条件。第**69**行将移动平均值重置为0，这意味着服务将再次能够处理工作。'
- en: This example is just a simple implementation, as mentioned earlier we could
    add any metric that the service has available to it such as CPU memory, database
    connection state should we be using a database.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子只是一个简单的实现，如前所述，我们可以添加服务可用的任何指标，例如CPU内存、数据库连接状态，如果我们使用数据库的话。
- en: Throttling
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限流
- en: 'Throttling is a pattern where you restrict the number of connections that a
    service can handle, returning an HTTP error code when this threshold has been
    exceeded. The full source code for this example can be found in the file `throttling/limit_handler.go`.
    The middleware pattern for Go is incredibly useful here: what we are going to
    do is to wrap the handler we would like to call, but before we call the handler
    itself, we are going to check to see if the server can honor the request. In this
    example, for simplicity, we are going only to limit the number of concurrent requests
    that the handler can serve, and we can do this with a simple buffered channel.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 限流是一种模式，其中你限制一个服务可以处理的连接数，当这个阈值被超过时，返回一个HTTP错误代码。这个例子的完整源代码可以在文件`throttling/limit_handler.go`中找到。Go的中间件模式在这里非常有用：我们将要做的是包装我们想要调用的处理器，但在调用处理器本身之前，我们将检查服务器是否能够满足请求。在这个例子中，为了简单起见，我们只限制处理器可以服务的并发请求数量，我们可以通过一个简单的带缓冲的通道来实现这一点。
- en: 'Our `LimitHandler` is quite a simple object:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`LimitHandler`是一个非常简单的对象：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We have two private fields: one holds the number of connections as a buffered
    channel, and the second is the handler we are going to call after we have checked
    that the system is healthy. To create an instance of this object we are going
    to use the `NewLimitHandler` function. This takes the parameters connection, which
    is the number of connections we allow to process at any one time and the handler
    which would be called if successful:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个私有字段：一个字段持有连接数，作为一个带缓冲的通道，另一个是在我们检查系统健康后将要调用的处理器。为了创建这个对象的实例，我们将使用`NewLimitHandler`函数。这个函数接受参数连接，这是我们允许在任何时候处理的连接数，以及如果成功将被调用的处理器：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This is quite straightforward: we create a buffered channel with the size equal
    to the number of concurrent connections, and then we fill that ready for use:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常直接：我们创建一个大小等于并发连接数的带缓冲通道，然后将其填充以供使用：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we look at the `ServeHTTP` method starting at line **29**, we have a `select`
    statement. The beauty of channel is that we can write a statement like this: if
    we cannot retrieve an item from the channel then we should return a busy error
    message to the client.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看从第**29**行开始的`ServeHTTP`方法，我们有一个`select`语句。通道的优点在于我们可以写出这样的语句：如果我们无法从通道中检索到项目，那么我们应该向客户端返回一个忙碌的错误信息。
- en: 'Another thing worth looking at in this example are the tests, in the test file
    which corresponds to this example `throttling/limit_handler_test.go`, we have
    quite a complicated test setup to check that multiple concurrent requests return
    an error when we hit the limit:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，另一个值得关注的点是测试，在对应这个例子的测试文件`throttling/limit_handler_test.go`中，我们有一个相当复杂的测试设置，用于检查当我们达到限制时，多个并发请求会返回一个错误：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we look at line **87**, we can see that we are constructing our new `LimitHandler`
    and passing it a mock handler which will be called if the server is capable of
    accepting the request. You can see that, in line **17** of this handler, we will
    block until the done channel on the context has an item and that this context
    is a `WithCancel` context. The reason we need to do this is that, to test that
    one of our requests will be called and the other will not but `LimitHandler` will
    return `TooManyRequests`, we need to block the first request. To ensure that our
    test does eventually complete, we are calling the cancel methods for the contexts
    in a timer block which will fire after ten milliseconds. Things start to get a
    little complex as we need to call our handlers in a Go routine to ensure that
    they execute concurrently. However, before we make our assertion we need to make
    sure that they have completed. This is why we are setting up `WaitGroup` in line
    **96**, and decrementing this group after each handler has completed. Finally,
    we can just block on line **109** until everything is complete and then we can
    make our assertion. Let''s take a closer look at the flow through this test:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看第 **87** 行，我们可以看到我们正在构建我们的新 `LimitHandler` 并传递一个模拟处理程序，如果服务器能够接受请求，这个处理程序将被调用。您可以看到，在这个处理程序的
    **17** 行，我们将阻塞，直到上下文的完成通道上有项目，并且这个上下文是一个 `WithCancel` 上下文。我们需要这样做的原因是，为了测试我们的请求之一将被调用而另一个则不会，但
    `LimitHandler` 将返回 `TooManyRequests`，我们需要阻塞第一个请求。为了确保我们的测试最终完成，我们在计时器块中调用上下文的取消方法，该计时器块将在十毫秒后触发。随着我们需要在
    Go 线程中调用我们的处理程序以确保它们并发执行，事情开始变得有些复杂。然而，在我们做出断言之前，我们需要确保它们已经完成。这就是为什么我们在第 **96**
    行设置 `WaitGroup` 的原因，并在每个处理程序完成后递减这个组。最后，我们只需在第 **109** 行阻塞，直到一切完成，然后我们可以做出断言。让我们更仔细地看看通过这个测试的流程：
- en: Block at line **109**.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第 **109** 行阻塞。
- en: Call `handler.ServeHTTP` twice concurrently.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时调用 `handler.ServeHTTP` 两次。
- en: One `ServeHTTP` method returns immediately with `http.TooManyRequests` and decrements
    the wait group.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 `ServeHTTP` 方法立即返回 `http.TooManyRequests` 并递减等待组。
- en: Call cancel context allowing the one blocking `ServeHTTP` call to return and
    decrement the wait group.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用取消上下文，允许阻塞的 `ServeHTTP` 调用返回并递减等待组。
- en: Perform assertion.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行断言。
- en: 'This flow is not the same as reading the code in a linear manner from top to
    bottom. Three concurrent routines are executing, and the flow of execution is
    not the same as the order of the statements in the code. Unfortunately, testing
    concurrent Go routines is always going to be a complicated issue. However, by
    performing these steps we have 100% coverage for our `LimitHandler`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流程与从上到下线性阅读代码的顺序不同。有三个并发线程正在执行，执行流程与代码中语句的顺序不同。不幸的是，测试并发 Go 线程始终是一个复杂的问题。然而，通过执行这些步骤，我们已经为我们的
    `LimitHandler` 实现了 100% 的覆盖率：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Rather than just limiting the number of connections in this handler, we could
    implement anything we like: it would be relatively trivial to implement something
    which records the average execution time or CPU consumption and fail fast if the
    condition exceeds our requirements. Determining exactly what these requirements
    are is a complex topic on its own and your first guess will most likely be wrong.
    We need to run multiple load tests of our system and spend time looking at logging
    and performance statistics for the end point before we are in a situation to make
    an educated guess. However, this action could just save you from a cascading failure,
    and that is an excellent thing indeed.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅仅限制这个处理程序中的连接数不同，我们可以实现任何我们喜欢的东西：实现记录平均执行时间或 CPU 消耗并在条件超过我们的要求时快速失败是非常简单的。确定这些要求的确切内容本身就是一个复杂的话题，您的第一个猜测很可能会出错。我们需要运行我们系统的多个负载测试，并花费时间查看端点的日志和性能统计，然后我们才能处于做出明智猜测的情况。然而，这个动作可能只是让您免于级联故障，这确实是一件好事。
- en: Service discovery
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: With monolithic applications, services invoke one another through language level
    methods or procedure calls. This was relatively straightforward and predictable
    behavior. However, once we realized that monolithic applications were not suitable
    for the scale and demand of modern software, we moved towards SOA or service-oriented
    architecture. We broke down this monolith into smaller chunks that typically served
    a particular purpose. To solve the problem with inter-service calls, SOA services
    ran at well-known fixed locations as the servers were large and quite often hosted
    in your data center or a leased rack in a data center. This meant that they did
    not change location very often, the IP addresses were often static, and even if
    a server did have to move, re-configuring of the IPs was always part of the deployment
    process.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在单体应用中，服务通过语言级别的函数或过程调用相互调用。这种行为相对简单且可预测。然而，一旦我们意识到单体应用不适合现代软件的规模和需求，我们就转向了
    SOA 或面向服务的架构。我们将这个单体分解成更小的块，这些块通常服务于特定的目的。为了解决服务间调用的问题，SOA 服务在已知固定的位置运行，因为服务器很大，通常托管在你的数据中心或数据中心租赁的机架上。这意味着它们的位置不会经常改变，IP
    地址通常是静态的，即使服务器需要移动，重新配置 IP 地址也总是部署过程的一部分。
- en: 'With microservices all this changes, the application typically runs in a virtualized
    or containerized environment where the number of instances of a service and their
    locations can change dynamically, minute by minute. This gives us the ability
    to scale our application depending on the forces dynamically applied to it, but
    this flexibility does not come without its own share of problems. One of the main
    ones knows where your services are to contact them. A good friend of mine, a fantastic
    software architect and the author of the foreword of this book made this statement
    in one of his presentations once:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务中，所有这些都发生了变化，应用程序通常在虚拟化或容器化环境中运行，其中服务的实例数量和位置可以每分钟动态变化。这使我们能够根据动态施加于它的力量来扩展我们的应用程序，但这种灵活性并非没有问题。主要问题之一是知道你的服务在哪里以便联系它们。我的一个好朋友，一位出色的软件架构师，也是这本书前言的作者，在一次他的演讲中说过：
- en: '"Microservices are easy; building microservice systems is hard."'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '"微服务本身很简单；构建微服务系统却很困难。"'
- en: Without the right patterns, it can almost be impossible, and one of the first
    ones you will most likely stumble upon even before you get your service out into
    production is service discovery.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 没有合适的模式，几乎可以说是无法实现的，你很可能会在服务上线生产之前就遇到第一个问题：服务发现。
- en: 'Let''s suppose you have a setup like this: you have three instances of the
    same service A, B, and C. Instance A and B are running on the same hardware, but
    service C is running in an entirely different data center. Because A and B are
    running on the same machine, they are accessible from the same IP address. However,
    because of this, they both cannot be bound to the same port. How is your client
    supposed to figure out all of this to make a simple call?'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个这样的设置：你有三个相同的服务实例 A、B 和 C。实例 A 和 B 在同一台硬件上运行，但服务 C 在一个完全不同的数据中心运行。由于 A
    和 B 在同一台机器上运行，它们可以通过相同的 IP 地址访问。然而，正因为如此，它们都不能绑定到相同的端口。你的客户端应该如何弄清楚所有这些信息，以便进行简单的调用？
- en: The solution is service discovery and the use of a dynamic service registry,
    like Consul or Etcd. These systems are highly scalable and have strongly consistent
    methods for storing the location of your services. The services register with
    the dynamic service registry upon startup, and in addition to the IP address and
    port they are running on, will also often provide metadata, like service version
    or other environmental parameters that can be used by a client when querying the
    registry. In addition to this, the consul has the capability to perform health
    checks on the service to ensure its availability. If the service fails a health
    check then it is marked as unavailable in the registry and will not be returned
    by any queries.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是服务发现和动态服务注册表的使用，如 Consul 或 Etcd。这些系统具有高度的可扩展性，并且有强一致性方法来存储服务的位置。服务在启动时向动态服务注册表注册，除了它们运行的
    IP 地址和端口外，通常还会提供元数据，如服务版本或其他环境参数，这些参数可以在客户端查询注册表时使用。此外，Consul 还具有对服务进行健康检查的能力，以确保其可用性。如果服务未通过健康检查，则它在注册表中标记为不可用，并且不会被任何查询返回。
- en: 'There are two main patterns for service discovery:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现有两种主要模式：
- en: Server-side discovery
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器端发现
- en: Client-side discovery
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端发现
- en: Server-side service discovery
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器端服务发现
- en: Server-side service discovery for inter-service calls within the same application,
    in my opinion, is a microservice anti-pattern. This is the method we used to call
    services in an SOA environment. Typically, there will be a reverse proxy which
    acts as a gateway to your services. It contacts the dynamic service registry and
    forwards your request on to the backend services. The client would access the
    backend services, implementing a known URI using either a subdomain or a path
    as a differentiator.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一应用程序内部进行服务间调用时，服务器端服务发现，在我看来，是一种微服务反模式。这是我们用于SOA环境中调用服务的方法。通常，将有一个反向代理作为您服务的网关。它联系动态服务注册表，并将您的请求转发到后端服务。客户端将访问后端服务，使用子域或路径作为区分器来实现已知的URI。
- en: 'The problem with this approach is that the reverse proxy starts to become a
    bottleneck. You can scale your backend services quickly enough, but now you need
    to be monitoring and watching these servers. Also, this pattern introduces latency,
    even though it may be only one 20ms hop, this could quite easily cost you 10%
    of your capacity, which means you have 10% increase in cost in addition to the
    cost of running and maintaining these services. Then what about consistency: you
    are potentially going to have two different failure patterns in your code for
    downstream calls, one for internal services and one for external. This is only
    going to add to the confusion.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，反向代理开始成为瓶颈。您可能能够快速扩展后端服务，但现在您需要监控和观察这些服务器。此外，这种模式引入了延迟，即使只有20毫秒的跳跃，这也可能轻易地消耗您10%的容量，这意味着您在运行和维护这些服务的成本之外，还要增加10%的成本。那么一致性如何呢？您可能会在代码中遇到两种不同的故障模式，一种是针对下游调用的内部服务，另一种是针对外部服务。这只会增加混乱。
- en: The biggest problem for me, however, is that you have to centralize this failure
    logic. A little later in this chapter, we are going to look at these patterns
    in depth, but we have already stated that your services will go wrong at some
    point and you will want to handle this failure. If you put this logic into a reverse
    proxy, then all services which want to access service A will be treated the same,
    regardless of whether the call is essential to the success or not.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对我来说，最大的问题是您必须集中化这个故障逻辑。在本章稍后，我们将深入探讨这些模式，但我们已经指出，您的服务在某个时刻会出错，您将希望处理这个故障。如果您将这个逻辑放入反向代理，那么所有想要访问服务A的服务都将被同等对待，无论调用是否对成功至关重要。
- en: To my mind, the worst implementation of this pattern is the one that abstracts
    all this knowledge from the client, retrying internally, and never letting the
    calling client know what is happening until success or catastrophic failure.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，这种模式的最大问题是它从客户端抽象出所有这些知识，内部重试，并且直到成功或灾难性故障，从不让调用客户端知道发生了什么。
- en: '![](img/54a6c338-15b3-4d37-b5dc-842aa31b5b88.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54a6c338-15b3-4d37-b5dc-842aa31b5b88.png)'
- en: Client-side service discovery
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端服务发现
- en: While server-side service discovery might be an acceptable choice for your public
    APIs for any internal inter-service communication, I prefer the client-side pattern.
    This gives you greater control over what happens when a failure occurs. You can
    implement the business logic on a retry of a failure on a case-by-case basis,
    and this will also protect you against cascading failure.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器端服务发现可能适合您公开API的任何内部服务间通信时，我更倾向于客户端模式。这使您能够更好地控制发生故障时的情况。您可以根据具体情况在失败重试时实现业务逻辑，这也可以保护您免受级联故障的影响。
- en: In essence, the pattern is similar to its server-side partner. However, the
    client is responsible for the service discovery and load balancing. You still
    hook into a dynamic service registry to get the information for the services you
    are going to call. This logic is localized in each client, so it is possible to
    handle the failure logic on a case-by-case basis.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，这种模式与其服务器端伙伴类似。然而，客户端负责服务发现和负载均衡。您仍然需要连接到动态服务注册表以获取将要调用的服务的相关信息。这种逻辑在每个客户端本地化，因此可以针对具体情况处理故障逻辑。
- en: '![](img/3c198550-4409-4ebb-8684-4f87182f1587.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c198550-4409-4ebb-8684-4f87182f1587.png)'
- en: Load balancing
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'When we discussed service discovery, we examined the concepts of server-side
    and client-side discovery. My personal preference is to look at client side for
    any internal calls as it affords you greater control over the logic of retries
    on a case by case basis. Why do I like client side load balancing? For many years
    server-side discovery was the only option, and there was also a preference for
    doing SSL termination on the load balancer due to the performance problems. This
    is not necessarily true anymore and as we will see when we look at the chapter
    on security. It is a good idea to use TLS secured connections internally. However,
    what about being able to do sophisticated traffic distribution? That can only
    be achieved if you have a central source of knowledge. I am not sure this is necessary:
    a random distribution will theoretically over time work out the same. However,
    there could be a benefit to only sending a certain number of connections to a
    particular host; but then how do you measure health? You can use layer 6 or 7,
    but as we have seen by using smart health checks, if the service is too busy then
    it can just reject a connection.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论服务发现时，我们考察了服务器端和客户端发现的概念。我个人的偏好是查看客户端进行任何内部调用，因为它允许你根据具体情况对重试逻辑有更大的控制。为什么我喜欢客户端负载均衡？多年来，服务器端发现是唯一的选择，由于性能问题，也倾向于在负载均衡器上进行SSL终止。这现在已经不再是必然的，当我们查看安全章节时，我们会看到这一点。在内部使用TLS加密连接是一个好主意。然而，关于能够进行复杂的流量分配，这只能在你有一个知识中心的情况下实现。我不确定这是否必要：理论上，随机分配最终会达到相同的效果。然而，只向特定主机发送一定数量的连接可能会有好处；但那么如何衡量健康状态呢？你可以使用第6层或第7层，但正如我们所看到的，通过使用智能健康检查，如果服务太忙，它可能会拒绝连接。
- en: From the example looking at circuit breaking, I hope you can now start to see
    the potential this can give your system. So how do we implement load balancing
    in Go?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从查看断路器的示例来看，我希望你现在可以开始看到这可以为你的系统带来的潜力。那么我们如何在Go中实现负载均衡？
- en: 'If we take a look at `loadbalancing/main.go`, I have created a simple implementation
    of a load balancer. We create it by calling `NewLoadBalancer` which has the following
    signature:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`loadbalancing/main.go`，我创建了一个简单的负载均衡器实现。我们通过调用`NewLoadBalancer`来创建它，该函数具有以下签名：
- en: '[PRE21]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This function takes two parameters: a `strategy`, an interface that contains
    the selection logic for the endpoints, and a list of endpoints.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受两个参数：一个`strategy`，它是一个包含端点选择逻辑的接口，以及一个端点列表。
- en: 'To be able to implement multiple strategies for the load balancer, such as
    round-robin, random, or more sophisticated strategies like distributed statistics,
    across multiple instances you can define your own strategy which has the following
    interface:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够实现负载均衡器的多种策略，例如轮询、随机或更复杂的策略，如分布式统计，跨多个实例，你可以定义自己的策略，该策略具有以下接口：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is the method which will return a particular endpoint for the strategy.
    It is not called directly, but it is called internally by the `LoadBalancer` package
    when you call the `GetEndpoint` method. This has to be a public method to allow
    for strategies to be included in packages outside of the `LoadBalancer` package:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是返回特定端点的方法。它不是直接调用的，而是在你调用`GetEndpoint`方法时，`LoadBalancer`包内部调用的。这必须是一个公开方法，以便允许策略包含在`LoadBalancer`包之外的包中：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This method will update the `Strategy` type with a list of the currently available
    endpoints. Again, this is not called directly but is called internally by the
    `LoadBalancer` package when you call the `UpdateEndpoints` method.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将更新`Strategy`类型，包含当前可用的端点列表。同样，这不是直接调用的，而是在你调用`UpdateEndpoints`方法时，`LoadBalancer`包内部调用的。
- en: 'To use the `LoadBalancer` package, you just initialize it with your chosen
    strategy and a list of endpoints, then by calling `GetEndpoint`, you will receive
    the next endpoint in the list:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`LoadBalancer`包，你只需用你选择的策略和端点列表初始化它，然后通过调用`GetEndpoint`，你将收到列表中的下一个端点：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the example code, we have implemented a simple `RandomStrategy`. Why not
    see if you can build a strategy which applies a `RoundRobinStrategy`?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例代码中，我们已经实现了一个简单的`RandomStrategy`。为什么不尝试构建一个应用`RoundRobinStrategy`的策略呢？
- en: Caching
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存
- en: One way you can improve the performance of your service is by caching results
    from databases and other downstream calls in an in-memory cache or a side cache
    like Redis, rather than by hitting a database every time.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 提高您服务性能的一种方法是通过在内存缓存或类似Redis的旁路缓存中缓存数据库和其他下游调用的结果，而不是每次都直接击中数据库。
- en: Caches are designed to deliver massive throughput by storing precompiled objects
    in a fast-access data store, frequently based around a concept of a hash key.
    We know from looking at algorithm performance that a hash table has the average
    performance of O(1); that is as fast as it gets. Without going too in depth into
    Big O notation, this means it takes one iteration to be able to find the item
    you want in the collection.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的设计是为了通过在快速访问的数据存储中存储预编译对象来提供巨大的吞吐量，通常基于哈希键的概念。从查看算法性能我们知道，哈希表的平均性能为O(1)；也就是说，这是最快的。不深入探讨大O符号，这意味着只需要一次迭代就能在集合中找到你想要的项目。
- en: 'What this means for you is that, not only can you reduce the load on your database,
    you can also reduce your infrastructure costs. Typically, a database is limited
    by the amount of data that can be read and written from the disk and the time
    it takes for the CPU to process this information. With an in-memory cache, this
    limitation is removed by using pre-aggregated data, which is stored in fast memory,
    not onto a state-full device like a disk. You also eliminate the problem with
    locking that many: databases suffer where one write can block many reads for a
    piece of information. This comes at the cost of consistency because you cannot
    guarantee that all your clients will have the same information at the same time.
    However, more often than not strong consistency is a vastly overvalued attribute
    of a database:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这对您的意义是，您不仅可以减少数据库的负载，还可以减少您的基础设施成本。通常，数据库受限于可以从磁盘读取和写入的数据量以及CPU处理这些信息所需的时间。使用内存缓存，通过使用预聚合数据，这些数据存储在快速内存中，而不是存储在像磁盘这样的状态设备上，可以消除这种限制。您还消除了许多数据库中存在的锁定问题：一次写入可以阻塞许多读取信息。这以一致性为代价，因为您无法保证所有客户端将同时拥有相同的信息。然而，更多情况下，强一致性是数据库中被高估的属性：
- en: '![](img/88b66cf2-d4ce-4def-9898-fcfa701ea7b7.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88b66cf2-d4ce-4def-9898-fcfa701ea7b7.png)'
- en: Consider our list of kittens. If we are receiving a high throughput of users
    retrieving a list of kittens, and it has to make a call to the database every
    time just to ensure the list is always up to date, then this will be costly and
    can fast overwhelm a database when it is already experiencing high load. We first
    need to ask ourselves is it essential that all these clients receive the updated
    information at the same time or is a one second delay quite acceptable. More often
    than not it is acceptable, and the speed and cost benefits you gain are well worth
    the potential cost that a connecting client does not get the up-to-date information
    exactly after it has been written to the database.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们的猫咪列表。如果我们正在接收大量用户检索猫咪列表，并且每次都必须调用数据库以确保列表始终是最新的，那么这将非常昂贵，并且当数据库已经承受高负载时，可能会迅速超出其处理能力。我们首先需要问自己，是否所有这些客户端都必须同时接收更新信息，或者一秒钟的延迟是否可以接受。通常情况下，一秒钟的延迟是可以接受的，并且你从中获得的速度和成本效益远远超过了连接客户端无法在信息写入数据库后立即获得最新信息的潜在成本。
- en: Caching strategies can be calculated based on your requirements for this consistency.
    In theory, the longer your cache expiry, the greater your cost saving, and the
    faster your system is at the expense of reduced consistency. We have already talked
    about designing for failure and how you can implement graceful degradation of
    a system. In the same way, when you are planning a feature, you should be talking
    about consistency and the tradeoffs with performance and cost, and documenting
    this decision, as these decisions will greatly help create a more successful implementation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存策略可以根据您对一致性的要求来计算。理论上，您的缓存过期时间越长，您的成本节约就越大，但您的系统速度会降低，这是以牺牲一致性为代价的。我们已经讨论了如何设计故障以及如何实现系统的优雅降级。同样，当您规划一个功能时，您应该讨论一致性以及与性能和成本的权衡，并记录这个决定，因为这些决定将极大地帮助创建一个更成功的实现。
- en: Premature optimization
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过早的优化
- en: You have probably heard the phrase, so does that mean you should not implement
    caching until you need it? No; it means you should be attempting to predict the
    initial load that your system will be under at design time, and the growth in
    capacity over time, as you are considering the application lifecycle. When creating
    this design, you will be putting together this data, and you will not be able
    to reliably predict the speed at which a service will run at. However, you do
    know that a cache will be cheaper to operate than a data store; so, if possible,
    you should be designing to use the smallest and cheapest data store possible,
    and making provision to be able to extend your service by introducing caching
    at a later date. This way you only do the actual work necessary to get the service
    out of the door, but you have done the design up front to be able to extend the
    service when it needs to scale.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过这个短语，那么这意味着你不需要在需要时才实现缓存吗？不；这意味着你应该在设计时尝试预测系统将承受的初始负载，以及随着时间的推移容量增长，正如你在考虑应用程序生命周期时一样。在创建这个设计时，你将汇集这些数据，并且你无法可靠地预测服务将运行的速度。然而，你知道缓存将比数据存储便宜；所以，如果可能的话，你应该设计使用可能的最小和最便宜的数据存储，并做出安排，以便在以后引入缓存来扩展你的服务。这样，你只做必要的实际工作来推出服务，但你在前端已经完成了设计，以便在需要扩展时扩展服务。
- en: Stale cache in times of database or downstream service failure
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库或下游服务故障时的陈旧缓存
- en: 'The cache will normally have an end date on it. However, if you implement the
    cache in a way that the code decides to invalidate it, then you can potentially
    avoid problems if a downstream service or database disappears. Again, this is
    back to thinking about failure states and asking what is better: the user seeing
    slightly out-of-date information or an error page? If your cache has expired,
    the call to the downstream service fails. However, you can always decide to serve
    the stale cache back to the calling client. In some instances, this will be better
    than returning a 50x error.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存通常会有一个结束日期。然而，如果你以这种方式实现缓存，即代码决定使其失效，那么如果下游服务或数据库消失，你可以避免潜在的问题。再次强调，这是回到考虑故障状态并思考什么更好：用户看到稍微过时的信息，还是错误页面？如果你的缓存已过期，对下游服务的调用将失败。但是，你总是可以选择将陈旧的缓存返回给调用客户端。在某些情况下，这比返回50x错误要好。
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have now seen how we can use some rather cool patterns to make our microservices
    more resilient and to deal with the inevitable failure. We have also looked at
    how introducing a weak link can save the entire system from a cascading failure.
    Where and how you apply these patterns should start out with an educated guess,
    but you need to constantly look at logging and monitoring to ensure that your
    opinion is still relevant. In the next chapter, we are going to look at some fantastic
    frameworks for building microservices in Go and then in, [Chapter 7](bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml),
    *Logging and Monitoring*, we will look at some options and best practice for logging
    and monitoring your service.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用一些相当酷的模式来使我们的微服务更加健壮，并处理不可避免的故障。我们也探讨了如何引入一个薄弱环节可以挽救整个系统免于级联故障。你应该如何以及在哪里应用这些模式应该从有根据的猜测开始，但你需要不断地查看日志和监控来确保你的观点仍然相关。在下一章中，我们将探讨一些用于在Go中构建微服务的出色框架，然后在[第7章](bcd70598-81c4-4f0c-8319-bc078e854db5.xhtml)，*日志和监控*中，我们将探讨一些日志和监控你服务的选项和最佳实践。
