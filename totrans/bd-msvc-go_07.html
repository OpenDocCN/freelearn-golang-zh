<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Logging and Monitoring</h1>
                </header>
            
            <article>
                
<p>Logging and monitoring are not advanced topics. However, they are one of those things that you do not realize just how important they are until you do not have them. Useful data about your service is essential to understanding the load and environment that your service is operating in so that you can make sure that it is finely tuned to give the very best performance.</p>
<p>Consider this example: when you first launch your service, you have an endpoint which returns a list of kittens. Initially, this service is responding promptly with a 20 ms response time; however, as people start to add items to the service, the speed slows to 200 ms. The first part of this problem is that you need to know about this slowdown. If you work in e-commerce, there is a direct correlation between the time it takes to process a request or a page to load and the likelihood that your customers will buy something.</p>
<p>One of the traditional methods for determining speed has always been to look at things from the edge; you use something such as Google Analytics, and you measure page load speed as experienced by the end user.</p>
<p>The first problem with this is that you have no idea where the slowdown originates. When we built monolithic applications, this was simple; the slowdown was either extra cruft which had been added to the HTML or it was the monolithic app server. So the app server may have some metrics output to a log file; however, due to the nature of the application only having one attached data store you did not have to look at many places before you found the source.</p>
<p>Everything changes with microservices; instead of one application, you may have 1,000, instead of one data store, you may have 100, and dozens of other associated services such as cloud storage queues and message routers. You could take the same approach to guess and test, but you will end up with a deep-seated hatred for yourself and all your colleagues who built the system.</p>
<p>Problem number 2: using Google Analytics will not easily tell you if the site is slowing down when it is under load. How will you know when you experience an outage? If the page is not loading because the back end server is not responding, then the JavaScript which fires data to Google Analytics will not fire. Can you even set up an alert in Google Analytics to fire an alarm when the average load time drops below a threshold?</p>
<p>Problem number 3: you don't have a website only an API; bye bye Google analytics.</p>
<p>Now, I am not saying you should not use Google Analytics; what I am saying is that it should form part of a larger strategy.</p>
<p>Stack traces and other application output which helps you diagnose a problem can be broken down into three categories::</p>
<ul>
<li><strong>Metrics</strong>: These are things such as time series data (for example, transaction or individual component timings).</li>
<li><strong>Text-based logs</strong>: Text-based records are your real old-fashioned logs which are spat out by things such as Nginx or a text log from your application software.</li>
<li><strong>Exceptions</strong>: Exceptions potentially could fall into the two previous categories; however, I like to break these out into a separate category since exceptions should be, well, exceptional.</li>
</ul>
<p>As always the source code for this chapter is available on GitHub you can find it at <a href="https://github.com/building-microservices-with-go/chapter7">https://github.com/building-microservices-with-go/chapter7</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging best practices</h1>
                </header>
            
            <article>
                
<p>In the free e-book, <em>The pragmatic logging handbook</em>, by Jon Gifford of Loggly (<span class="URLPACKT">www.loggly.com</span>), Jon proposes the following eight best practices to apply when determining your logging strategy:</p>
<ul>
<li>Treat application logging as an ongoing iterative process. Log at a high level and then add deeper instrumentation.</li>
<li>Always instrument anything that goes out of the process because distributed system problems are not well behaved.</li>
<li>Always log unacceptable performance. Log anything outside the range in which you expect your system to perform.</li>
<li>If possible, always log enough context for a complete picture of what happened from a single log event.</li>
<li>View machines as your end consumer, not humans. Create records that your log management solution can interpret.</li>
<li>Trends tell the story better than data points.</li>
<li>Instrumentation is NOT a substitute for profiling and vice versa.</li>
<li>Flying more slowly is better than flying blind. So the debate is not whether to instrument, just how much.</li>
</ul>
<p>I think there is one of these points which need a little more explanation, that is "Instrumentation is NOT a substitute for profiling and vice versa." What Jon is referring to is that while your application may have high levels of logging and monitoring, you should still run through a pre-release process of profiling the application code. We looked at tools like Go's profiler, and we have also done some basic performance testing with the bench tool. However for a production service a more thorough approach should be taken, it is beyond the scope of this book to look at performance testing in depth, however, I would encourage you to read "Performance Testing with JMeter 3" by Bayo Erinle published by Packt for further information on this topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metrics</h1>
                </header>
            
            <article>
                
<p>In my opinion, metrics are the most useful form of logging for day-to-day operations. Metrics are useful because we have simple numeric data. We can plot this onto a time series dashboard and quite quickly set up alerting from the output as the data is incredibly cheap to process and collect.</p>
<p>No matter what you are storing, the superior efficiency of metrics is that you are storing numeric data in a time-series database using a unique key as an identifier. Numeric data allows the computation and comparison of the data to be incredibly efficient. It also allows the data store to reduce the resolution of the data as time progresses, enabling you to have granular data when you need it most at the right time and retain historical reference data without requiring petabytes of data storage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of data best represented by metrics</h1>
                </header>
            
            <article>
                
<p>This is quite simple: it is the data that is meaningful when expressed by simple numbers, such as request timings and counts. How granular you want to be with your metrics depends upon your requirements; generally, when I am building a microservice I start with top line metrics such as request timings, success, and failure counts for handlers, and if I am using a datastore, then I would include these too. As the service develops and I start performance testing things, I will start to add new items that help me diagnose performance problems with the service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naming conventions</h1>
                </header>
            
            <article>
                
<p>Defining a naming convention is incredibly important, as once you start to collect data, a point will come where you need to analyze it. The key thing for me is not to define a convention for your service but a convention that is useful for your entire estate. When you start to investigate issues with your service, more often than not, you will find that the problem is not necessarily with your service but could be due to a multitude of things:</p>
<ul>
<li>Exhausted CPU on host server</li>
<li>Exhausted memory</li>
<li>Network latency</li>
<li>Slow data store queries</li>
<li>Latency with downstream service caused by any of the preceding factors</li>
</ul>
<p>I recommend you break up the name of your service using dot notation such as the following:</p>
<pre>
environment.host.service.group.segment.outcome 
</pre>
<ul>
<li><kbd>environment</kbd>: This is the working environment; for example: production, staging</li>
<li><kbd>host</kbd>: This is the hostname of the server running the application</li>
<li><kbd>service</kbd>: The name of your service</li>
<li><kbd>group</kbd>: This is the top level grouping; for an API, this might be handlers</li>
<li><kbd>segment</kbd>: The child level information for the group; this will typically be the name of the handler in the instance of an API</li>
<li><kbd>outcome</kbd>: This is something which denotes the result of the operation, in an API you may have called, success, or you may choose to use HTTP status codes</li>
</ul>
<p>Here is an example of how to use the following dot notation:</p>
<pre>
prod.server1.kittenserver.handlers.list.ok <br/>prod.server1.kittenserver.mysql.select_kittens.timing 
</pre>
<p>If your monitoring solution supports tags in addition to the event name, then I recommend you use tags for the environment and host, this will make querying the data store a little easier. For example, if I have a handler which lists kittens which are running on my production server then I may choose to add the following events to be emitted when the handler is called:</p>
<pre>
func (h *list) ServeHTTP(rw http.ResponseWriter, r *http.Request) { <br/>  event := startTimingEvent("kittens.handlers.list.timing", ["production", "192.168.2.2"]) <br/>  defer event.Complete() <br/> <br/>  dispatchIncrementEvent("kittens.handlers.list.called", ["production", "192.168.2.2"]) <br/> <br/>... <br/> <br/>  if err != nil { <br/>    dispatchIncrementEvent("kittens.handlers.list.failed", ["production", 192.168.2.2"]) <br/>   return` <br/>  } <br/> <br/>  dispatchIncrementEvent("kittens.handlers.list.success", ["production", 192.168.2.2"]) <br/>} 
</pre>
<p>This is a pseudo code, but you can see that we are dispatching three events from this handler:</p>
<ol>
<li>The first event is that we are going so send some timing information.</li>
<li>In the next, we are simply going to send an increment count which is simply going to state that the handler has been called.</li>
<li>Finally, we are going to check if the operation has been successful. If not, we increment our handler-failed metric; if successful, we increment our success metric.</li>
</ol>
<p>Naming our metrics in this way allows us to graph errors either on a granular level or makes it possible to write a query which is at a higher level. For example, we may be interested in the total number of failed requests for the entire service, not just this endpoint. Using this naming convention, we can query using wildcards; so to query all failures for this service, we could write a metric like the following code:</p>
<pre>
kittens.handlers.*.failed 
</pre>
<p>If we were interested in all failed requests to handlers for all services, we could write the following query:</p>
<pre>
*.handlers.*.failed 
</pre>
<p>Having a consistent naming convention for metrics is essential. Add this to your upfront design when building a service and implement this as a company-wide standard, not just on a team level. Let's take a look at some example code to see just how easy it is to implement <kbd>statsD</kbd>. If we take a look at <kbd>chapter7/main.go</kbd>, we can see that on line <strong>19</strong>, we are initializing our <kbd>statsD</kbd> client:</p>
<pre>
statsd, err := createStatsDClient(os.Getenv("STATSD") <br/>  if err != nil { <br/>    log.Fatal("Unable to create statsD client") <br/>  } <br/>... <br/> <br/>func createStatsDClient(address string) (*statsd.Client, error){ <br/>  return statsd.New(statsd.Address(address)) <br/>} 
</pre>
<p>We are using an open source package by Alex Cesaro (<a href="https://github.com/alexcesaro/statsd">https://github.com/alexcesaro/statsd</a>). This has a very simple interface; to create our client, we call the new function and pass it a list of options. In this instance, we are only passing through the address of the <kbd>statsD</kbd> server, which has been set by an environment variable:</p>
<pre>
func New(opts ...Option) (*Client, error) 
</pre>
<p>If we look at line <strong>27</strong> in the file <kbd>cserver/handlers/helloworld.go</kbd>, we are deferring the sending of the timing data until the handler completes:</p>
<pre>
defer h.statsd.NewTiming().Send(helloworldTiming) 
</pre>
<p>The start time will be the time of execution of the defer statement so this should be the first line of your file; the end time will be once the deferred statement executes. If this handler is middleware and you are calling a downstream in a chain, then remember that the execution time of all the downstream calls will also be included in this metric. To exclude this, we can create a new <kbd>Timing</kbd> in line <strong>27</strong> and then call the send method manually just before we execute the next middleware in the chain:</p>
<pre>
func (c *Client) NewTiming() Timing 
</pre>
<p>If you take a look at line <strong>35</strong>, you will see we are calling the increment method when the request completes successfully:</p>
<pre>
h.statsd.Increment(helloworldSuccess)    
</pre>
<p>The <kbd>Increment</kbd> function will increase the count for the given bucket by one, and these are fascinating metrics to have in your application as they give you a really interesting picture of the health and status:</p>
<pre>
func (c *Client) Increment(bucket string) 
</pre>
<p>The <kbd>statsD</kbd> client does not work synchronously, sending each metric when you make a call to the client; instead, it buffers all the calls, and there is an internal goroutine which sends the data at a predetermined interval. This makes the operation highly efficient, and you should not have to worry about any application slowdown.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage and querying</h1>
                </header>
            
            <article>
                
<p>There are multiple options for storing and querying metric data; you have the possibility for self-hosting, or you can utilize a software as a service. How you manage this is dependent upon your company's scale and the security requirement for your data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software as a service</h1>
                </header>
            
            <article>
                
<p>For software as a service (SaaS), I recommend looking at Datadog. To send metrics to Datadog, you have two options: one is to communicate with the API directly; the other is to run the Datadog collector as a container inside your cluster. The Datadog collector allows you to use <kbd>StatsD</kbd> as your data format and it supports a couple of nice extensions which standard <kbd>StatsD</kbd> does not, such as the ability to add additional tags or metadata to your metrics. Tagging allows you to categorize your data by user-defined tags, this allows you to keep your metric names specific to what they are monitoring without having to add environmental information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Self-hosted</h1>
                </header>
            
            <article>
                
<p>While it may be desirable to use a SaaS service for your production data, it is always useful to be able to run a server locally for local development. There are many options for backend data stores such as Graphite, Prometheus, InfluxDB, and ElasticSearch; however, when it comes to graphing, Grafana leads the way.</p>
<p>Let's spin up a Docker Compose stack for our list, kittenservice, so we can run through the simple steps of setting up Prometheus with Grafana with Docker Compose.</p>
<p>If we look at the Docker compose file, we can see that we have three entries:</p>
<ul>
<li><kbd>statsD</kbd></li>
<li><kbd>grafana</kbd></li>
<li><kbd>prometheus</kbd></li>
</ul>
<p>StatsD is not a <kbd>statsD</kbd> server as such but a <kbd>statsD</kbd> exporter; this exposes an endpoint which Prometheus can use to collect the statistics. Unlike Graphite, which you push metrics to, Prometheus pulls stats.</p>
<p>Prometheus is the database server which is used for collecting the data.</p>
<p>Grafana is what we will use for graphing our data.</p>
<p>If we take a look at the Docker Compose file <kbd>docker-compose.yml</kbd>, which is located at the root of our source repository, we will see that the Prometheus section requires some particular configuration:</p>
<pre>
prometheus: <br/>   image: prom/prometheus <br/>   links: <br/>     - statsd <br/>   volumes: <br/>     - ./prometheus.yml:/etc/prometheus/prometheus.yml <br/>   ports: <br/>     - 9090:9090 
</pre>
<p>We are mounting a volume which contains the Prometheus configuration. Let's take a look at it:</p>
<pre>
global: <br/>   scrape_interval:     15s <br/>  <br/> scrape_configs: <br/>   - job_name: 'statsd' <br/>     static_configs: <br/>       - targets: ['statsd:9102'] <br/>    <br/>   - job_name: 'prometheus' <br/>     static_configs: <br/>       - targets: ['localhost:9090'] 
</pre>
<p>The first part of this configuration sets the intervals for fetching the data from our sources and also the intervals upon which they will be evaluated. The default value for the scrape interval is one minute. We have reduced this for our example, as we are impatient and we would like to see our metrics update almost immediately after we have made a request to the server. However, in practice, we are not really interested in real-time data. A lag of a minute is OK. The next part is the scrape configs; these are the settings which define our data which we would like to import into Prometheus. The first element is our <kbd>statsD</kbd> collector; we point this to the collector defined in our <kbd>docker-compose</kbd> file. As we are using a link between our two containers, we can use the link name in the config. The next item is the configuration for Prometheus' performance metrics. We do not have to enable this; however, metrics are critical so it would make sense to monitor the health of our metrics database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grafana</h1>
                </header>
            
            <article>
                
<p>To display these metrics, we are going to use Grafana. If we start our stack by using the <kbd>make runserver</kbd> command and wait for a few moments for the server to start, we can then execute a few curls to the endpoint to start populating the system with data:</p>
<pre>
<strong>curl [docker host ip]:8091/helloworld -d '{"name": "Nic"}'</strong>  
</pre>
<p>Let's log into Grafana and have a look at some of the data we have collected. Point your browser at <kbd>[docker host ip]:3000</kbd> and you should be presented with a login screen. The default username and password is <kbd>admin</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="463" width="639" class="image-border" src="assets/f0845d43-63cd-4e9c-9631-915e01fb3197.png"/></div>
<p>Once you have logged in, the first thing we want to do is to configure our data source. Unfortunately, there seems to be no way to set up this automatically with configuration files. There is an API if you need to provision in an environment outside of your local machine; it should be pretty trivial to write something which syncs data using this:</p>
<p><a href="http://docs.grafana.org/http_api/data_source/"><span class="URLPACKT">http://docs.grafana.org/http_api/data_source/</span></a></p>
<p>Configuring the data source is relatively straightforward. All we need to do is to select Prometheus as our data type and then fill in the connection details. You need to ensure that you select proxy as opposed to direct. Proxy makes the calls for data from the Grafana server; direct will use your browser. Once we have done that, let's add the default dashboard for the Prometheus server:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="431" width="576" class="image-border" src="assets/83a732df-b4a7-44af-85c0-47849ec1c702.png"/></div>
<p>If you click the dashboards tab, you will see that you have the ability to import a pre-created dashboard. This is useful but what we want to do is create our own dashboard from our server. To do this, hit the dashboards link and then choose new dashboard. This will take you to the dashboards creation page. We are going to add a graph of our requests. So let's select the <span class="packt_screen">Graph</span> option. In the bottom panel, we have the ability to add the metrics we would like to show; if we already know the name of the dashboard, then all we need to do is type the expression into the box:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="363" width="563" class="image-border" src="assets/79023681-8e3f-4c46-91d2-a12e1d36ea2f.png"/></div>
<p>The metrics lookup allows us to search for metrics based on part of the name. If we type <kbd>kitten</kbd> into this box, then all the metrics from our simple API that have been tagged with kitten will show up in the box. For now, let's select the validation success metric. By default, this metric is a count of all the times that the metric was reported for the given time interval. This is why you see the graph. While this may be useful in some instances, what we would like to see is a nice bar chart showing the successes for a given period. To do this, we can use one of the many expressions to group this data:</p>
<pre>
increase(kittenserver_helloworld_success_counter{}[30s]) 
</pre>
<p>This expression will group the data into buckets of 30 seconds and will return the difference between the current and the previous bucket. In effect, what this gives us is a chart showing the number of successes every 30 seconds. To present the information, a bar graph would most likely be better, so we can change this option in the display tab. Changing the step setting to the same interval as the duration we set in our increase expression, will make the chart look a little more readable. Now add a second query for the timings of our hello world handler. This time we do not need to aggregate the data into buckets, as we are fine displaying it on the graph as it is. Timing metrics show three lines, the average (quartile, 0.5), the top 10% (quartile, 0.9), and the top 1% (quartile, 0.99). In general, we would like to see these lines quite tightly grouped together, which indicates little variance in our service calls. We do not see this in our graph, even though we are performing the same operation time and time again, due to line 149 in the code:</p>
<pre>
time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond) 
</pre>
<p>Our handler was running just too fast to measure &lt; 1 ms so I added a little random wait to make the graph more interesting:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="305" width="718" class="image-border" src="assets/7383b913-f642-4dc7-9eec-a028f330d4f8.png"/></div>
<p>That is the basics for simple metrics; for logging more detailed information, we need to fall back to trusty log files. The days of pulling data from servers are long gone, and in our highly distributed world this would be a nightmare. Thankfully, we have tools such as Elasticsearch and Kibana.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging</h1>
                </header>
            
            <article>
                
<p>When working with highly distributed containers, you may have 100 instances of your application running rather than one or two. This means that if you need to grep your log files, you will be doing this over hundreds of files instead of just a couple. In addition, Docker-based applications should be stateless and the scheduler may be moving them around on multiple hosts. This adds an extra layer of complexity to manage. To save the trouble, the best way to solve this problem is not to write the logs to disk in the first place. A distributed logging store, such as an ELK stack, or software as a service platform, such as Logmatic or Loggly, solve this problem for us and give us a fantastic insight into the health and operating condition of our system. Regarding the cost, you will most likely find that one of the SasS providers is cheaper than running and maintaining your ELK stack. However, your security needs may not always allow this. Retention is also an interesting problem while looking at logging. My personal preference is to only store log data for short periods of time, such as 30 days; this allows you to maintain diagnostic traces which could be useful for troubleshooting without the cost of maintaining historical data. For historical data, a metrics platform is best, as you can cheaply store this data over a period of years, which can be useful to compare current performance with that of a historic event.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed tracing with Correlation IDs</h1>
                </header>
            
            <article>
                
<p>In <a href="b4c0c222-3d5d-458d-bd03-25a2a9082230.xhtml" target="_blank">Chapter 2</a>, <em>Designing a Great API</em>, we looked at the header <kbd>X-Request-ID</kbd> which allows us to mark all the service calls for an individual request with the same ID so that we can later query them. This is an incredibly important concept when it comes to debugging a request as it can dramatically help you understand why a service may be failing or misbehaving by looking at the tree of requests and the parameters passed to them. If you take a look at the file <kbd>handlers/correlation.go</kbd>, we can implement this quite simply:</p>
<pre>
func (c *correlationHandler) ServeHTTP(rw http.ResponseWriter, r *http.Request) { <br/>  if r.Header.Get("X-Request-ID") == "" { <br/>    r.Header.Set("X-Request-ID", uuid.New().String()) <br/>  }<br/><br/>  c.next.ServeHTTP(rw, r)<br/>}
</pre>
<p>The handler is implemented using the middleware pattern when we wish to use it all we need to do is wrap the actual handler like so:</p>
<pre>
http.Handle("/helloworld", handlers.NewCorrelationHandler(validation))
</pre>
<p class="mce-root">Now every time a request is made to the <kbd>/helloworld</kbd> endpoint, the header <kbd>X-Request-ID</kbd> will be appended to the request with a random UUID if it is not already present. This is a very simple method of adding distributed tracing into your application, depending upon your requirements you may like to check out Zipkin is a distributed tracing system designed to trouble shoot latency, which is becoming incredibly popular <a href="http://zipkin.io">http://zipkin.io.</a> There are also tools from DataDog, NewRelic, and AWS X-Ray, it is too much to go into depth into these applications, however, please spend an hour and familiarize yourself with their capabilities as you never know when you are going to need them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch, Logstash, and Kibana (ELK)</h1>
                </header>
            
            <article>
                
<p>Elasticsearch, Logstash, and Kibana are pretty much the industry standard when it comes to logging verbose data. All of the output which would traditionally be streamed to a log file is stored in a central location which you can query with a graphical interface tool, Kibana.</p>
<p>If we look at our Docker Compose file, you will see three entries for our ELK stack:</p>
<pre>
elasticsearch: <br/>   image: elasticsearch:2.4.2 <br/>   ports: <br/>     - 9200:9200 <br/>     - 9300:9300 <br/>   environment: <br/>     ES_JAVA_OPTS: "-Xms1g -Xmx1g" <br/> kibana: <br/>   image: kibana:4.6.3 <br/>   ports: <br/>     - 5601:5601 <br/>   environment: <br/>     - ELASTICSEARCH_URL=http://elasticsearch:9200 <br/>   links: <br/>     - elasticsearch <br/> logstash: <br/>   image: logstash <br/>   command: -f /etc/logstash/conf.d/ <br/>   ports: <br/>     - 5000:5000 <br/>   volumes: <br/>     - ./logstash.conf:/etc/logstash/conf.d/logstash.conf <br/>   links: <br/>     - elasticsearch 
</pre>
<p>Elasticsearch is our datastore for our logging data, Kibana is the application we will use for querying this data, and Logstash is used for reading the data from your application logs and storing it in Elasticsearch. The only configuration, besides a few environment variables, is the logstash config:</p>
<pre>
input { <br/>   tcp { <br/>     port =&gt; 5000 <br/>     codec =&gt; "json" <br/>     type =&gt; "json" <br/>   } <br/> } <br/>  <br/>## Add your filters / logstash plugins configuration here <br/>output { <br/>  elasticsearch { <br/>    hosts =&gt; "elasticsearch:9200" <br/>  } <br/>} 
</pre>
<p>The input configuration allows us to send our logs direct over TCP to the Logstash server. This saves us the problem of writing to disk and then Logstash having to read these files. In general, TCP is probably going to be faster, disk I/O is not free, and the contention caused by writing a log file sequentially can slow down your application. Dependent upon your appetite for risk, you may choose to use UDP as transport for your logs. This will be faster than TCP; however, this speed comes at the expense that you will not get a confirmation that the data has been received and you may lose some logs.</p>
<div class="packt_quote">"I would tell you a joke about UDP, but you might not get it."</div>
<p>In general, this is not too much of a problem unless you need your logs for security auditing. In this instance, you could always configure multiple inputs for different log types. Logstash has the capability to grep many common output formats for logs and transform these into JSON format which can be indexed by Elasticsearch. Since our logs in our example application area are already in JSON format, we can set the type to JSON and Logstash will not apply any transformation. In the output section, we are defining our datastore; again, like the Prometheus configuration, we can use the link address provided by Docker for our URI:</p>
<p><a href="https://www.elastic.co/guide/en/logstash/current/configuration.html"><span class="URLPACKT">https://www.elastic.co/guide/en/logstash/current/configuration.html</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kibana</h1>
                </header>
            
            <article>
                
<p>Start your stack if it is not already running and send a little data to Elasticsearch:</p>
<pre>
<strong>curl $(docker-machine ip):8091/helloworld -d '{"name": "Nic"}'</strong>  
</pre>
<p>Now point your browser at <kbd>http://192.168.165.129:5601</kbd>. The first screen you should see if you are starting with a new setup is the one which prompts you to create a new index in Elasticsearch. Go ahead and create this using the defaults; you will now see the list of fields that Elasticsearch can index from your logs:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="329" width="685" class="image-border" src="assets/9bf720f8-bb47-48e3-8e1e-0acc1ad5dd08.png"/></div>
<p>If need be, you can change these settings. However, generally you will be fine with the defaults. The Kibana screen is relatively straightforward. If you switch to the <span class="packt_screen">Discover</span> tab, you will be able to see some of the logs that have been collected:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="454" width="679" class="image-border" src="assets/b6bc674f-ade7-43e6-9390-d6f4ed5d56f9.png"/></div>
<p>Expanding one of the entries will show the indexed fields in more detail:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="471" width="671" class="image-border" src="assets/d2e55101-6519-40a3-a6b1-1ea52d313d67.png"/></div>
<p>To filter the logs by one of these fields, you can enter the filter in the search bar at the top of the window. Search criteria must be written in Lucene format, so to filter our list by status code, we can enter the following query:</p>
<pre>
status: 200 
</pre>
<p>This filters the by <kbd>status</kbd> field containing the numeric value <kbd>200</kbd>. Whilst searching indexed fields is relatively straightforward, we have added the bulk of our data into the message field where it is stored as a JSON string:</p>
<pre>
status:200 and message:/.*"Method":"POST"/ 
</pre>
<p>To filter our list to only show <kbd>POST</kbd> actions, we can use a query containing a REGEX search:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="76" width="357" class="image-border" src="assets/5d691f4c-e193-45d8-9e1c-1ce5b9129285.png"/></div>
<p>REGEX search terms will be slower than indexed queries, as each item has to be evaluated. If we find that there is a particular field we are always referring to and would like to speed up these filters, then we have two options. The first and the most awkward is to add a <em>grok</em> section to our Logstash configuration:</p>
<p><a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html#plugins-filters-grok-add/_field"><span class="URLPACKT">https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html#plugins-filters-grok-add\_field</span></a></p>
<p>The other option is to specify these fields when we are preparing the data to log. If you look at the example code, you can see that we are extracting the method out and while this is also going into the <kbd>message</kbd> field, we are logging this using the <kbd>WithFields</kbd> method, which will allow Logstash to index this. If you take a look at line <strong>37</strong> of the <kbd>chandlers/helloworld.go</kbd> file, you can see this in action:</p>
<pre>
serializedRequest := serializeRequest(r) <br/>message, _ := json.Marshal(serializedRequest) <br/>h.logger.WithFields(logrus.Fields{ <br/>  "handler": "HelloWorld", <br/>  "status":  http.StatusOK, <br/>  "method":  serializedRequest.Method, <br/>}).Info(string(message)) 
</pre>
<p>In our example, we are using the Logrus logger. Logrus is a structured logger for Go which supports many different plugins. In our example, we are using the Logstash plugin which allows you to send our logs direct to the Logstash endpoint rather than writing them to a file and then having Logstash pick them up:</p>
<pre>
56 func createLogger(address string) (*logrus.Logger, error) { <br/>57  retryCount := 0 <br/>58 <br/>59  l := logrus.New() <br/>60  hostname, _ := os.Hostname() <br/>61  var err error <br/>62 <br/>63  // Retry connection to logstash incase the server has not yet come up <br/>64  for ; retryCount &lt; 10; retryCount++ { <br/>65    hook, err := logstash.NewHookWithFields( <br/>66     "tcp", <br/>67  address, <br/>68  "kittenserver", <br/>69  logrus.Fields{"hostname": hostname}, <br/>70    ) <br/>71 <br/>72    if err == nil { <br/>73      l.Hooks.Add(hook) <br/>74      return l, err <br/>75    } <br/>76 <br/>77    log.Println("Unable to connect to logstash, retrying") <br/>78    time.Sleep(1 * time.Second) <br/>79  } <br/>80 <br/>81  return nil, err <br/>82 } 
</pre>
<p>Adding plugins to Logrus is very simple. We define the hook which is in a separate package, specifying the connection protocol, address, application name, and a fields collection which is always sent to the logger:</p>
<pre>
func NewHookWithFields(protocol, address, appName string, alwaysSentFields logrus.Fields) (*Hook, error) 
</pre>
<p>We then register the plugin with the logger using the hooks method:</p>
<pre>
func AddHook(hook Hook) 
</pre>
<p>Logrus has many configurable options and the standard Log, Info, Debug, and Error logging levels will enable you to log any object. It will, however, use Go's built in <kbd>ToString</kbd> unless there is a particular implementation. To get around this and to be able to have more parsable data in our logfiles, I have added a simple serialization method which converts the relevant methods from the <kbd>http.Request</kbd> into a JSON object:</p>
<pre>
type SerialzableRequest struct { <br/>  *http.Request <br/>} <br/> <br/>func (sr *SerialzableRequest) ToJSON() string 
</pre>
<p>Full source code for this example can be found in the example code at <kbd>chapter7/httputil/request.go</kbd>. This is only a simple implementation at the moment but could be extended if required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exceptions</h1>
                </header>
            
            <article>
                
<p>One of the great things about Go is that the standard patterns are that you should always handle errors when they occur instead of bubbling them up to the top and presenting them to the user. Having said that, there is always a case when the unexpected happens. The secret to this is to know about it and to fix the problem when it occurs. There are many exception logging platforms on the market. However, the two techniques we have discussed are, in my opinion, more than sufficient for tracing the few errors that we hopefully will find in our web application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Panic and recover</h1>
                </header>
            
            <article>
                
<p>Golang has two great methods for handling unexpected errors:</p>
<ul>
<li>Panic</li>
<li>Recover</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Panic</h1>
                </header>
            
            <article>
                
<p>The built-in panic function stops the normal execution of the current goroutine. All the deferred functions are run in the normal way then the program is terminated:</p>
<pre>
func panic(v interface{}) 
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recover</h1>
                </header>
            
            <article>
                
<p>The recover function allows an application to manage the behavior of a panicking goroutine. When called inside a deferred function, recover stops the execution of the panic and returns the error passed to the call of panic:</p>
<pre>
func recover() interface{} 
</pre>
<p>If our handler panics for some reason, the HTTP server will recover this panic and write the output to the <kbd>std</kbd> error. While this is fine if we are running the application locally, it does not allow us to manage the errors when we have our application distributed across many remote servers. Since we have already logged into an ELK stack setup, we can write a simple handler which will wrap our main handler and allow the capture of any panic and forward it to the logger:</p>
<pre>
18 func (p *panicHandler) ServeHTTP(rw http.ResponseWriter, r *http.Request) { <br/>19  defer func() { <br/>20    if err := recover(); err != nil { <br/>21  p.logger.WithFields( <br/>22  logrus.Fields{ <br/>23      "handler": "panic", <br/>24      "status":  http.StatusInternalServerError, <br/>25      "method":  r.Method, <br/>26      "path":    r.URL.Path, <br/>27      "query":   r.URL.RawQuery, <br/>28      }, <br/>29  ).Error(fmt.Sprintf("Error: %v\n%s", err, debug.Stack())) <br/>30 <br/>31  rw.WriteHeader(http.StatusInternalServerError) <br/>32   } <br/>33  }() <br/>34 <br/>35  p.next.ServeHTTP(rw, r) <br/>36 }   
</pre>
<p>This is relatively straightforward. In line <strong>19</strong>, we are deferring the call to recover. When this runs, if we have an error message, that is, something has panicked, we want to log this. Like in the previous examples, we are adding fields to the log entry so that Elasicsearch will index these but instead of logging the request we are writing the error message. This message most likely will not have enough context for us to be able to debug the application, so to get the context, we make a call to <kbd>debug.Stack()</kbd>:</p>
<pre>
func Stack() []byte 
</pre>
<p>The stack is part of the runtime/debug package and returns a formatted stack trace of the goroutine that calls it. You can test this by running the example code of this chapter, and curl the <kbd>bang</kbd> endpoint:</p>
<pre>
<strong>curl -i [docker host ip]:8091/bang</strong>  
</pre>
<p>We are writing this along with the error message to Elasticsearch when we query Kibana. For this message, we will see the captured details which look something like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/32517d9d-5ab1-46c7-b2c3-539fc891ba74.png"/></div>
<p>Finally, we return the status code 500 to the client with no message body.</p>
<p>The message should give us enough context to understand where the problem area lies. The input which caused the exception will, however, be missing so if we are unable to replicate the error then it is probably time to add more instrumentation to our service and re-run.</p>
<p>As part of the application life cycle of your service, you should always endeavor to keep on top of exceptions. This will greatly enhance your ability to react when something goes wrong. More often than not, I see exception trackers which are so full of problems the teams lose all hope of ever cleaning them up and stop trying. Don't let your new services get this way when a new exception appears to fix it. This way you can set up alerting on your exceptions as you will be pretty confident there is a problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>That is it for this chapter. Logging and monitoring is a topic which you can tailor to your particular use case and environment, but I hope you have learned how easy it is to set up. Using software as a service, such as Datadog or Logmatic, is an excellent way to get up and running very quickly, and alerts integration with OpsGenie or PagerDuty will allow you to receive instant alerts whenever a problem may occur.</p>


            </article>

            
        </section>
    </body></html>