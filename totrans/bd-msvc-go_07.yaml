- en: Logging and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging and monitoring are not advanced topics. However, they are one of those
    things that you do not realize just how important they are until you do not have
    them. Useful data about your service is essential to understanding the load and
    environment that your service is operating in so that you can make sure that it
    is finely tuned to give the very best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example: when you first launch your service, you have an endpoint
    which returns a list of kittens. Initially, this service is responding promptly
    with a 20 ms response time; however, as people start to add items to the service,
    the speed slows to 200 ms. The first part of this problem is that you need to
    know about this slowdown. If you work in e-commerce, there is a direct correlation
    between the time it takes to process a request or a page to load and the likelihood
    that your customers will buy something.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the traditional methods for determining speed has always been to look
    at things from the edge; you use something such as Google Analytics, and you measure
    page load speed as experienced by the end user.
  prefs: []
  type: TYPE_NORMAL
- en: The first problem with this is that you have no idea where the slowdown originates.
    When we built monolithic applications, this was simple; the slowdown was either
    extra cruft which had been added to the HTML or it was the monolithic app server.
    So the app server may have some metrics output to a log file; however, due to
    the nature of the application only having one attached data store you did not
    have to look at many places before you found the source.
  prefs: []
  type: TYPE_NORMAL
- en: Everything changes with microservices; instead of one application, you may have
    1,000, instead of one data store, you may have 100, and dozens of other associated
    services such as cloud storage queues and message routers. You could take the
    same approach to guess and test, but you will end up with a deep-seated hatred
    for yourself and all your colleagues who built the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem number 2: using Google Analytics will not easily tell you if the site
    is slowing down when it is under load. How will you know when you experience an
    outage? If the page is not loading because the back end server is not responding,
    then the JavaScript which fires data to Google Analytics will not fire. Can you
    even set up an alert in Google Analytics to fire an alarm when the average load
    time drops below a threshold?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem number 3: you don''t have a website only an API; bye bye Google analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, I am not saying you should not use Google Analytics; what I am saying is
    that it should form part of a larger strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack traces and other application output which helps you diagnose a problem
    can be broken down into three categories::'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics**: These are things such as time series data (for example, transaction
    or individual component timings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-based logs**: Text-based records are your real old-fashioned logs which
    are spat out by things such as Nginx or a text log from your application software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exceptions**: Exceptions potentially could fall into the two previous categories;
    however, I like to break these out into a separate category since exceptions should
    be, well, exceptional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always the source code for this chapter is available on GitHub you can find
    it at [https://github.com/building-microservices-with-go/chapter7](https://github.com/building-microservices-with-go/chapter7)
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the free e-book, *The pragmatic logging handbook*, by Jon Gifford of Loggly
    (www.loggly.com), Jon proposes the following eight best practices to apply when
    determining your logging strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Treat application logging as an ongoing iterative process. Log at a high level
    and then add deeper instrumentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always instrument anything that goes out of the process because distributed
    system problems are not well behaved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always log unacceptable performance. Log anything outside the range in which
    you expect your system to perform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If possible, always log enough context for a complete picture of what happened
    from a single log event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View machines as your end consumer, not humans. Create records that your log
    management solution can interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trends tell the story better than data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumentation is NOT a substitute for profiling and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flying more slowly is better than flying blind. So the debate is not whether
    to instrument, just how much.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I think there is one of these points which need a little more explanation, that
    is "Instrumentation is NOT a substitute for profiling and vice versa." What Jon
    is referring to is that while your application may have high levels of logging
    and monitoring, you should still run through a pre-release process of profiling
    the application code. We looked at tools like Go's profiler, and we have also
    done some basic performance testing with the bench tool. However for a production
    service a more thorough approach should be taken, it is beyond the scope of this
    book to look at performance testing in depth, however, I would encourage you to
    read "Performance Testing with JMeter 3" by Bayo Erinle published by Packt for
    further information on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my opinion, metrics are the most useful form of logging for day-to-day operations.
    Metrics are useful because we have simple numeric data. We can plot this onto
    a time series dashboard and quite quickly set up alerting from the output as the
    data is incredibly cheap to process and collect.
  prefs: []
  type: TYPE_NORMAL
- en: No matter what you are storing, the superior efficiency of metrics is that you
    are storing numeric data in a time-series database using a unique key as an identifier.
    Numeric data allows the computation and comparison of the data to be incredibly
    efficient. It also allows the data store to reduce the resolution of the data
    as time progresses, enabling you to have granular data when you need it most at
    the right time and retain historical reference data without requiring petabytes
    of data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Types of data best represented by metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is quite simple: it is the data that is meaningful when expressed by simple
    numbers, such as request timings and counts. How granular you want to be with
    your metrics depends upon your requirements; generally, when I am building a microservice
    I start with top line metrics such as request timings, success, and failure counts
    for handlers, and if I am using a datastore, then I would include these too. As
    the service develops and I start performance testing things, I will start to add
    new items that help me diagnose performance problems with the service.'
  prefs: []
  type: TYPE_NORMAL
- en: Naming conventions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Defining a naming convention is incredibly important, as once you start to
    collect data, a point will come where you need to analyze it. The key thing for
    me is not to define a convention for your service but a convention that is useful
    for your entire estate. When you start to investigate issues with your service,
    more often than not, you will find that the problem is not necessarily with your
    service but could be due to a multitude of things:'
  prefs: []
  type: TYPE_NORMAL
- en: Exhausted CPU on host server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exhausted memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow data store queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency with downstream service caused by any of the preceding factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I recommend you break up the name of your service using dot notation such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`environment`: This is the working environment; for example: production, staging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host`: This is the hostname of the server running the application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service`: The name of your service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group`: This is the top level grouping; for an API, this might be handlers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segment`: The child level information for the group; this will typically be
    the name of the handler in the instance of an API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outcome`: This is something which denotes the result of the operation, in
    an API you may have called, success, or you may choose to use HTTP status codes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of how to use the following dot notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If your monitoring solution supports tags in addition to the event name, then
    I recommend you use tags for the environment and host, this will make querying
    the data store a little easier. For example, if I have a handler which lists kittens
    which are running on my production server then I may choose to add the following
    events to be emitted when the handler is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a pseudo code, but you can see that we are dispatching three events
    from this handler:'
  prefs: []
  type: TYPE_NORMAL
- en: The first event is that we are going so send some timing information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next, we are simply going to send an increment count which is simply
    going to state that the handler has been called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we are going to check if the operation has been successful. If not,
    we increment our handler-failed metric; if successful, we increment our success
    metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Naming our metrics in this way allows us to graph errors either on a granular
    level or makes it possible to write a query which is at a higher level. For example,
    we may be interested in the total number of failed requests for the entire service,
    not just this endpoint. Using this naming convention, we can query using wildcards;
    so to query all failures for this service, we could write a metric like the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were interested in all failed requests to handlers for all services,
    we could write the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Having a consistent naming convention for metrics is essential. Add this to
    your upfront design when building a service and implement this as a company-wide
    standard, not just on a team level. Let''s take a look at some example code to
    see just how easy it is to implement `statsD`. If we take a look at `chapter7/main.go`,
    we can see that on line **19**, we are initializing our `statsD` client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using an open source package by Alex Cesaro ([https://github.com/alexcesaro/statsd](https://github.com/alexcesaro/statsd)).
    This has a very simple interface; to create our client, we call the new function
    and pass it a list of options. In this instance, we are only passing through the
    address of the `statsD` server, which has been set by an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at line **27** in the file `cserver/handlers/helloworld.go`, we
    are deferring the sending of the timing data until the handler completes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The start time will be the time of execution of the defer statement so this
    should be the first line of your file; the end time will be once the deferred
    statement executes. If this handler is middleware and you are calling a downstream
    in a chain, then remember that the execution time of all the downstream calls
    will also be included in this metric. To exclude this, we can create a new `Timing`
    in line **27** and then call the send method manually just before we execute the
    next middleware in the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you take a look at line **35**, you will see we are calling the increment
    method when the request completes successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Increment` function will increase the count for the given bucket by one,
    and these are fascinating metrics to have in your application as they give you
    a really interesting picture of the health and status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `statsD` client does not work synchronously, sending each metric when you
    make a call to the client; instead, it buffers all the calls, and there is an
    internal goroutine which sends the data at a predetermined interval. This makes
    the operation highly efficient, and you should not have to worry about any application
    slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: Storage and querying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple options for storing and querying metric data; you have the
    possibility for self-hosting, or you can utilize a software as a service. How
    you manage this is dependent upon your company's scale and the security requirement
    for your data.
  prefs: []
  type: TYPE_NORMAL
- en: Software as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For software as a service (SaaS), I recommend looking at Datadog. To send metrics
    to Datadog, you have two options: one is to communicate with the API directly;
    the other is to run the Datadog collector as a container inside your cluster.
    The Datadog collector allows you to use `StatsD` as your data format and it supports
    a couple of nice extensions which standard `StatsD` does not, such as the ability
    to add additional tags or metadata to your metrics. Tagging allows you to categorize
    your data by user-defined tags, this allows you to keep your metric names specific
    to what they are monitoring without having to add environmental information.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-hosted
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it may be desirable to use a SaaS service for your production data, it
    is always useful to be able to run a server locally for local development. There
    are many options for backend data stores such as Graphite, Prometheus, InfluxDB,
    and ElasticSearch; however, when it comes to graphing, Grafana leads the way.
  prefs: []
  type: TYPE_NORMAL
- en: Let's spin up a Docker Compose stack for our list, kittenservice, so we can
    run through the simple steps of setting up Prometheus with Grafana with Docker
    Compose.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the Docker compose file, we can see that we have three entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`statsD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grafana`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatsD is not a `statsD` server as such but a `statsD` exporter; this exposes
    an endpoint which Prometheus can use to collect the statistics. Unlike Graphite,
    which you push metrics to, Prometheus pulls stats.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is the database server which is used for collecting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana is what we will use for graphing our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a look at the Docker Compose file `docker-compose.yml`, which is
    located at the root of our source repository, we will see that the Prometheus
    section requires some particular configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We are mounting a volume which contains the Prometheus configuration. Let''s
    take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The first part of this configuration sets the intervals for fetching the data
    from our sources and also the intervals upon which they will be evaluated. The
    default value for the scrape interval is one minute. We have reduced this for
    our example, as we are impatient and we would like to see our metrics update almost
    immediately after we have made a request to the server. However, in practice,
    we are not really interested in real-time data. A lag of a minute is OK. The next
    part is the scrape configs; these are the settings which define our data which
    we would like to import into Prometheus. The first element is our `statsD` collector;
    we point this to the collector defined in our `docker-compose` file. As we are
    using a link between our two containers, we can use the link name in the config.
    The next item is the configuration for Prometheus' performance metrics. We do
    not have to enable this; however, metrics are critical so it would make sense
    to monitor the health of our metrics database.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To display these metrics, we are going to use Grafana. If we start our stack
    by using the `make runserver` command and wait for a few moments for the server
    to start, we can then execute a few curls to the endpoint to start populating
    the system with data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s log into Grafana and have a look at some of the data we have collected.
    Point your browser at `[docker host ip]:3000` and you should be presented with
    a login screen. The default username and password is `admin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0845d43-63cd-4e9c-9631-915e01fb3197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you have logged in, the first thing we want to do is to configure our
    data source. Unfortunately, there seems to be no way to set up this automatically
    with configuration files. There is an API if you need to provision in an environment
    outside of your local machine; it should be pretty trivial to write something
    which syncs data using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.grafana.org/http_api/data_source/](http://docs.grafana.org/http_api/data_source/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuring the data source is relatively straightforward. All we need to do
    is to select Prometheus as our data type and then fill in the connection details.
    You need to ensure that you select proxy as opposed to direct. Proxy makes the
    calls for data from the Grafana server; direct will use your browser. Once we
    have done that, let''s add the default dashboard for the Prometheus server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83a732df-b4a7-44af-85c0-47849ec1c702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you click the dashboards tab, you will see that you have the ability to
    import a pre-created dashboard. This is useful but what we want to do is create
    our own dashboard from our server. To do this, hit the dashboards link and then
    choose new dashboard. This will take you to the dashboards creation page. We are
    going to add a graph of our requests. So let''s select the Graph option. In the
    bottom panel, we have the ability to add the metrics we would like to show; if
    we already know the name of the dashboard, then all we need to do is type the
    expression into the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79023681-8e3f-4c46-91d2-a12e1d36ea2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The metrics lookup allows us to search for metrics based on part of the name.
    If we type `kitten` into this box, then all the metrics from our simple API that
    have been tagged with kitten will show up in the box. For now, let''s select the
    validation success metric. By default, this metric is a count of all the times
    that the metric was reported for the given time interval. This is why you see
    the graph. While this may be useful in some instances, what we would like to see
    is a nice bar chart showing the successes for a given period. To do this, we can
    use one of the many expressions to group this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This expression will group the data into buckets of 30 seconds and will return
    the difference between the current and the previous bucket. In effect, what this
    gives us is a chart showing the number of successes every 30 seconds. To present
    the information, a bar graph would most likely be better, so we can change this
    option in the display tab. Changing the step setting to the same interval as the
    duration we set in our increase expression, will make the chart look a little
    more readable. Now add a second query for the timings of our hello world handler.
    This time we do not need to aggregate the data into buckets, as we are fine displaying
    it on the graph as it is. Timing metrics show three lines, the average (quartile,
    0.5), the top 10% (quartile, 0.9), and the top 1% (quartile, 0.99). In general,
    we would like to see these lines quite tightly grouped together, which indicates
    little variance in our service calls. We do not see this in our graph, even though
    we are performing the same operation time and time again, due to line 149 in the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Our handler was running just too fast to measure < 1 ms so I added a little
    random wait to make the graph more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7383b913-f642-4dc7-9eec-a028f330d4f8.png)'
  prefs: []
  type: TYPE_IMG
- en: That is the basics for simple metrics; for logging more detailed information,
    we need to fall back to trusty log files. The days of pulling data from servers
    are long gone, and in our highly distributed world this would be a nightmare.
    Thankfully, we have tools such as Elasticsearch and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with highly distributed containers, you may have 100 instances
    of your application running rather than one or two. This means that if you need
    to grep your log files, you will be doing this over hundreds of files instead
    of just a couple. In addition, Docker-based applications should be stateless and
    the scheduler may be moving them around on multiple hosts. This adds an extra
    layer of complexity to manage. To save the trouble, the best way to solve this
    problem is not to write the logs to disk in the first place. A distributed logging
    store, such as an ELK stack, or software as a service platform, such as Logmatic
    or Loggly, solve this problem for us and give us a fantastic insight into the
    health and operating condition of our system. Regarding the cost, you will most
    likely find that one of the SasS providers is cheaper than running and maintaining
    your ELK stack. However, your security needs may not always allow this. Retention
    is also an interesting problem while looking at logging. My personal preference
    is to only store log data for short periods of time, such as 30 days; this allows
    you to maintain diagnostic traces which could be useful for troubleshooting without
    the cost of maintaining historical data. For historical data, a metrics platform
    is best, as you can cheaply store this data over a period of years, which can
    be useful to compare current performance with that of a historic event.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing with Correlation IDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](b4c0c222-3d5d-458d-bd03-25a2a9082230.xhtml), *Designing a Great
    API*, we looked at the header `X-Request-ID` which allows us to mark all the service
    calls for an individual request with the same ID so that we can later query them.
    This is an incredibly important concept when it comes to debugging a request as
    it can dramatically help you understand why a service may be failing or misbehaving
    by looking at the tree of requests and the parameters passed to them. If you take
    a look at the file `handlers/correlation.go`, we can implement this quite simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The handler is implemented using the middleware pattern when we wish to use
    it all we need to do is wrap the actual handler like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now every time a request is made to the `/helloworld` endpoint, the header `X-Request-ID`
    will be appended to the request with a random UUID if it is not already present.
    This is a very simple method of adding distributed tracing into your application,
    depending upon your requirements you may like to check out Zipkin is a distributed
    tracing system designed to trouble shoot latency, which is becoming incredibly
    popular [http://zipkin.io.](http://zipkin.io) There are also tools from DataDog,
    NewRelic, and AWS X-Ray, it is too much to go into depth into these applications,
    however, please spend an hour and familiarize yourself with their capabilities
    as you never know when you are going to need them.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch, Logstash, and Kibana (ELK)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch, Logstash, and Kibana are pretty much the industry standard when
    it comes to logging verbose data. All of the output which would traditionally
    be streamed to a log file is stored in a central location which you can query
    with a graphical interface tool, Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at our Docker Compose file, you will see three entries for our ELK
    stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Elasticsearch is our datastore for our logging data, Kibana is the application
    we will use for querying this data, and Logstash is used for reading the data
    from your application logs and storing it in Elasticsearch. The only configuration,
    besides a few environment variables, is the logstash config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The input configuration allows us to send our logs direct over TCP to the Logstash
    server. This saves us the problem of writing to disk and then Logstash having
    to read these files. In general, TCP is probably going to be faster, disk I/O
    is not free, and the contention caused by writing a log file sequentially can
    slow down your application. Dependent upon your appetite for risk, you may choose
    to use UDP as transport for your logs. This will be faster than TCP; however,
    this speed comes at the expense that you will not get a confirmation that the
    data has been received and you may lose some logs.
  prefs: []
  type: TYPE_NORMAL
- en: '"I would tell you a joke about UDP, but you might not get it."'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, this is not too much of a problem unless you need your logs for
    security auditing. In this instance, you could always configure multiple inputs
    for different log types. Logstash has the capability to grep many common output
    formats for logs and transform these into JSON format which can be indexed by
    Elasticsearch. Since our logs in our example application area are already in JSON
    format, we can set the type to JSON and Logstash will not apply any transformation.
    In the output section, we are defining our datastore; again, like the Prometheus
    configuration, we can use the link address provided by Docker for our URI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.elastic.co/guide/en/logstash/current/configuration.html](https://www.elastic.co/guide/en/logstash/current/configuration.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start your stack if it is not already running and send a little data to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now point your browser at `http://192.168.165.129:5601`. The first screen you
    should see if you are starting with a new setup is the one which prompts you to
    create a new index in Elasticsearch. Go ahead and create this using the defaults;
    you will now see the list of fields that Elasticsearch can index from your logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bf720f8-bb47-48e3-8e1e-0acc1ad5dd08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If need be, you can change these settings. However, generally you will be fine
    with the defaults. The Kibana screen is relatively straightforward. If you switch
    to the Discover tab, you will be able to see some of the logs that have been collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6bc674f-ade7-43e6-9390-d6f4ed5d56f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expanding one of the entries will show the indexed fields in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2e55101-6519-40a3-a6b1-1ea52d313d67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To filter the logs by one of these fields, you can enter the filter in the
    search bar at the top of the window. Search criteria must be written in Lucene
    format, so to filter our list by status code, we can enter the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This filters the by `status` field containing the numeric value `200`. Whilst
    searching indexed fields is relatively straightforward, we have added the bulk
    of our data into the message field where it is stored as a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To filter our list to only show `POST` actions, we can use a query containing
    a REGEX search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d691f4c-e193-45d8-9e1c-1ce5b9129285.png)'
  prefs: []
  type: TYPE_IMG
- en: 'REGEX search terms will be slower than indexed queries, as each item has to
    be evaluated. If we find that there is a particular field we are always referring
    to and would like to speed up these filters, then we have two options. The first
    and the most awkward is to add a *grok* section to our Logstash configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html#plugins-filters-grok-add\_field](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html#plugins-filters-grok-add/_field)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other option is to specify these fields when we are preparing the data
    to log. If you look at the example code, you can see that we are extracting the
    method out and while this is also going into the `message` field, we are logging
    this using the `WithFields` method, which will allow Logstash to index this. If
    you take a look at line **37** of the `chandlers/helloworld.go` file, you can
    see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we are using the Logrus logger. Logrus is a structured logger
    for Go which supports many different plugins. In our example, we are using the
    Logstash plugin which allows you to send our logs direct to the Logstash endpoint
    rather than writing them to a file and then having Logstash pick them up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding plugins to Logrus is very simple. We define the hook which is in a separate
    package, specifying the connection protocol, address, application name, and a
    fields collection which is always sent to the logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We then register the plugin with the logger using the hooks method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Logrus has many configurable options and the standard Log, Info, Debug, and
    Error logging levels will enable you to log any object. It will, however, use
    Go''s built in `ToString` unless there is a particular implementation. To get
    around this and to be able to have more parsable data in our logfiles, I have
    added a simple serialization method which converts the relevant methods from the
    `http.Request` into a JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Full source code for this example can be found in the example code at `chapter7/httputil/request.go`.
    This is only a simple implementation at the moment but could be extended if required.
  prefs: []
  type: TYPE_NORMAL
- en: Exceptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the great things about Go is that the standard patterns are that you
    should always handle errors when they occur instead of bubbling them up to the
    top and presenting them to the user. Having said that, there is always a case
    when the unexpected happens. The secret to this is to know about it and to fix
    the problem when it occurs. There are many exception logging platforms on the
    market. However, the two techniques we have discussed are, in my opinion, more
    than sufficient for tracing the few errors that we hopefully will find in our
    web application.
  prefs: []
  type: TYPE_NORMAL
- en: Panic and recover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Golang has two great methods for handling unexpected errors:'
  prefs: []
  type: TYPE_NORMAL
- en: Panic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The built-in panic function stops the normal execution of the current goroutine.
    All the deferred functions are run in the normal way then the program is terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Recover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recover function allows an application to manage the behavior of a panicking
    goroutine. When called inside a deferred function, recover stops the execution
    of the panic and returns the error passed to the call of panic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If our handler panics for some reason, the HTTP server will recover this panic
    and write the output to the `std` error. While this is fine if we are running
    the application locally, it does not allow us to manage the errors when we have
    our application distributed across many remote servers. Since we have already
    logged into an ELK stack setup, we can write a simple handler which will wrap
    our main handler and allow the capture of any panic and forward it to the logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is relatively straightforward. In line **19**, we are deferring the call
    to recover. When this runs, if we have an error message, that is, something has
    panicked, we want to log this. Like in the previous examples, we are adding fields
    to the log entry so that Elasicsearch will index these but instead of logging
    the request we are writing the error message. This message most likely will not
    have enough context for us to be able to debug the application, so to get the
    context, we make a call to `debug.Stack()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The stack is part of the runtime/debug package and returns a formatted stack
    trace of the goroutine that calls it. You can test this by running the example
    code of this chapter, and curl the `bang` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We are writing this along with the error message to Elasticsearch when we query
    Kibana. For this message, we will see the captured details which look something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32517d9d-5ab1-46c7-b2c3-539fc891ba74.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we return the status code 500 to the client with no message body.
  prefs: []
  type: TYPE_NORMAL
- en: The message should give us enough context to understand where the problem area
    lies. The input which caused the exception will, however, be missing so if we
    are unable to replicate the error then it is probably time to add more instrumentation
    to our service and re-run.
  prefs: []
  type: TYPE_NORMAL
- en: As part of the application life cycle of your service, you should always endeavor
    to keep on top of exceptions. This will greatly enhance your ability to react
    when something goes wrong. More often than not, I see exception trackers which
    are so full of problems the teams lose all hope of ever cleaning them up and stop
    trying. Don't let your new services get this way when a new exception appears
    to fix it. This way you can set up alerting on your exceptions as you will be
    pretty confident there is a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That is it for this chapter. Logging and monitoring is a topic which you can
    tailor to your particular use case and environment, but I hope you have learned
    how easy it is to set up. Using software as a service, such as Datadog or Logmatic,
    is an excellent way to get up and running very quickly, and alerts integration
    with OpsGenie or PagerDuty will allow you to receive instant alerts whenever a
    problem may occur.
  prefs: []
  type: TYPE_NORMAL
