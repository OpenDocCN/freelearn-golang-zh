<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Event-Driven Architecture</h1>
                </header>
            
            <article>
                
<p>In the last few chapters, we have looked at issues around stability and performance, and some patterns you can employ in your code, which enable more stable systems. In this chapter, we are going to take a more in-depth look at event-driven architecture.</p>
<p>As your system grows, these patterns become more important; they allow you to loosely couple your microservices, and therefore you are not bound to the same dependencies of intertwined objects common in monolithic applications. We are going to learn that with the right amount of up-front design and effort that loosely coupling your systems with events need not be a painful process.</p>
<p>Before we begin, be sure to fetch the source code from <a href="https://github.com/building-microservices-with-go/chapter9">https://github.com/building-microservices-with-go/chapter9</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differences between synchronous and asynchronous processing</h1>
                </header>
            
            <article>
                
<p>If there is a choice between processing a message synchronously or asynchronously, then I would always choose synchronous as it always makes the application simpler with fewer components parts, the code is easier to understand, tests easier to write, and the system easier to debug.</p>
<p>Asynchronous processing should be a design decision that is driven by need, be that the requirement for decoupling, scale, batch processing, or time-based processing. <strong>Event-Driven Systems</strong> give an ability to scale at much higher things than monolithic systems and the reason for that is that because of the loose coupling the code scales horizontally with both greater granularity and effectiveness.</p>
<p>Another problem with asynchronous processing is the additional burden it adds to your operations. We need to create infrastructure for message queuing and message delivery, this infrastructure needs to be monitored and managed, even if you are using your cloud provider's functionality such as SNS/SQS or PubSub.</p>
<p>There is even a question about whether you should be implementing microservices or building a monolith, however, I think smaller chunks of code are invariably easier to deploy and test at the cost of increased duplication for setup of continuous integration and provisioning of hardware is a one-time hurdle and something that is worth learning. We will look at that in the next chapter when we examine continuous deployment and immutable infrastructure, but for now, let's stick with events.</p>
<p>Having got the warning out of the way, let's retake a look at the difference between the two styles of message processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synchronous processing</h1>
                </header>
            
            <article>
                
<p>With synchronous processing, all the communication to a downstream application happens in the process. A request is sent, and you wait for a reply using the same network connection and not using any callbacks. Synchronous processing is the simplest method of communication; while you are waiting for an answer the downstream service is processing the request. You have to manage the retry logic yourself, and it is typically best used only when you need an immediate reply. Let's take a look at the following diagram that depicts synchronous processing:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="361" width="314" class="image-border" src="assets/8d3ee88c-2980-4100-bc78-12202c4d5537.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous processing</h1>
                </header>
            
            <article>
                
<p>With asynchronous processing, all the communication to the downstream application happens out of process leveraging a queue or a message broker as an intermediary. Rather than communicating directly with the downstream service, messages dispatch to a queue such as <strong>AWS SQS/SNS</strong>, <strong>Google Cloud Pub/Sub</strong>, or <strong>NATS.io</strong>. Because there is no processing performed at this layer the only delay is the time it takes to deliver the message, which is very fast, also due to the design of these systems, acceptance, or not of a message is the only situation you must implement. Retry and connection handling logic is delegated to either the message broker or the downstream system as it is the storage of messages for archive or replay:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="410" width="664" class="image-border" src="assets/6147298f-87a1-4851-a930-06cf7ce3ac53.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of asynchronous messages</h1>
                </header>
            
            <article>
                
<p>Asynchronous processing often comes in two different forms, such as push and pull. The strategy that you implement is dependent upon your requirements, and often a single system implements both patterns. Let's take a look at the two different approaches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pull/queue messaging</h1>
                </header>
            
            <article>
                
<p>The pull pattern is an excellent design where you may have a worker process running, which for example is resizing images. The API would receive the request and then add this to a queue for background processing. The worker process or processes read from the queue retrieving the messages one by one, perform the required work, and then delete the message from the queue. Often there is also a queue commonly called a "dead letter queue" should the worker process fail for any reason then the message would be added to the dead letter queue. The dead letter queue allows the messages to be re-processed in the case of an incremental failure or for debugging purposes. Let's take a look the following diagram, which summarizes the whole process:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="402" width="667" class="image-border" src="assets/70b64a3a-ef18-4935-9a7f-912685e2a231.png"/></div>
<p>Implementing a queue-based service in Go is a relatively straightforward task, let's walk through the example in the source code that accompanies this book. This example uses Redis for storing the messages. Redis is an incredibly fast data store, and, while it is nice to be able to leverage a cloud providers queue rather than managing our infrastructure, this is not always possible. However, even if we are using cloud providers queue the pattern we are about to look at is easily replaceable with a different data store client. If we consider the following listing from the example code in <kbd>queue/queue.go</kbd>:</p>
<pre>
  7 // Message represents messages stored on the queue 
  8 type Message struct { 
  9   ID      string `json:"id"` 
 10   Name    string `json:"name"` 
 11   Payload string `json:"payload"` 
 12 } 
 13 
 14 // Queue defines the interface for a message queue 
 15 type Queue interface { 
 16   Add(messageName string, payload []byte) error 
 17   AddMessage(message Message) error 
 18   StartConsuming(size int, pollInterval time.Duration, callback func(Message) error) 
 19 } 
</pre>
<p>The first thing we are doing is defining a <kbd>Message</kbd> object that is used by the system and defines three simple parameters that are serializable to JSON. ID is never populated by the publisher directly instead this is a calculated ID that is unique for every message. Should the consumer need a simple mechanism to determine if a message has already been received and processed, then the ID can be used. The interface for <kbd>Queue</kbd> defines three simple methods as follows:</p>
<ul>
<li><kbd>Add(messageName string, payload []byte) error</kbd>: <kbd>Add</kbd> is a convenience method to publish a new message, the sender only needs to provide the name of the message and a slice of byte.</li>
<li><kbd>AddMessage(message Message) error</kbd>: <kbd>AddMessage</kbd> performs the same function as <kbd>Add</kbd> with the difference that the caller needs to construct a <kbd>Message</kbd> type and pass this to the method. The implementation of <kbd>AddMessage</kbd> automatically generates the <kbd>ID</kbd> field on <kbd>Message struct</kbd> and overwrites any initial <kbd>ID</kbd> value.</li>
<li><kbd>StartConsuming(size int, pollInterval time.Duration, callback func(Message) error)</kbd>: <kbd>StartConsuming</kbd> allows a subscriber to retrieve messages from the queue. The first parameter size relates to the batch size, which is returned in any one connection. The <kbd>pollInterval</kbd> parameter determines how often the client checks for messages on the queue. The <kbd>callback</kbd> function is executed when messages return from the queue. It has a return parameter of error which when not <kbd>nil</kbd> informs the client that processing has failed and the message should not be removed from the queue. One thing we need to note is that <kbd>StartConsuming</kbd> is not a blocking method, after it has registered the callback to the queue it immediately returns.</li>
</ul>
<p>The implementation at <kbd>queue/redis_queue.go</kbd> defines the <kbd>NewRedisQueue</kbd> function, which is a convenience function to create our queue. We are using the <kbd><span class="URLPACKT">github.com/adjust/rmq</span></kbd> library, which has an excellent implementation on top of Redis queues and in line <strong>27</strong>, we are opening a connection to our Redis data store:</p>
<pre>
 26 // NewRedisQueue creates a new RedisQueue 
 27 func NewRedisQueue(connectionString string, queueName string) (*RedisQueue, error) { 
 28   connection := rmq.OpenConnection("my service", "tcp", connectionString, 1) 
 29   taskQueue := connection.OpenQueue(queueName) 
 30 
 31   return &amp;RedisQueue{Queue: taskQueue, name: queueName}, nil 
 32 }  
</pre>
<p>Then on line <strong>29</strong> we need to open a connection to the queue that we are going to read and write from:</p>
<pre>
 42 // AddMessage to the queue, generating a unique ID for the message before dispatch 
 43 func (r *RedisQueue) AddMessage(message Message) error { 
 44   serialNumber, _ := rand.Int(rand.Reader, serialNumberLimit) 
 45   message.ID = strconv.Itoa(time.Now().Nanosecond()) + serialNumber.String() 
 46 
 47   payloadBytes, err := json.Marshal(message) 
 48   if err != nil { 
 49     // handle error 
 50     return err 
 51   } 
 52 
 53   fmt.Println("Add event to queue:", string(payloadBytes)) 
 54   if !r.Queue.PublishBytes(payloadBytes) { 
 55     return fmt.Errorf("unable to add message to the queue") 
 56   } 
 57 
 58   return nil 
 59 }  
</pre>
<p>The <kbd>Add</kbd> method, which is the implementation of our interface's <kbd>Add</kbd> method, is merely a convenience method that creates a message from the given parameters and then calls the <kbd>AddMessage</kbd> function. The <kbd>AddMessage</kbd> function first generates an ID for the message, in this simple implementation we are just generating a random number and appending it to the current time in nanoseconds, which should give us enough uniqueness without requiring a check to the queue. We then need to convert the message to its JSON representation as a slice of bytes before we are finally publishing the message to the queue on line <strong>54</strong>.</p>
<p>The final part of our implementation is the method that consumes messages from the queue:</p>
<pre>
 61 // StartConsuming consumes messages from the queue 
 62 func (r *RedisQueue) StartConsuming(size int, pollInterval time.Duration, callback func(Message) error) { 
 63   r.callback = callback 
 64   r.Queue.StartConsuming(size, pollInterval) 
 65   r.Queue.AddConsumer("RedisQueue_"+r.name, r) 
 66 } 
 67 
 68 // Consume is the internal callback for the message queue 
 69 func (r *RedisQueue) Consume(delivery rmq.Delivery) { 
 70   fmt.Println("Got event from queue:", delivery.Payload()) 
 71 
 72   message := Message{} 
 73 
 74   if err := json.Unmarshal([]byte(delivery.Payload()), &amp;message); err != nil { 
 75     fmt.Println("Error consuming event, unable to deserialise event") 
 76     // handle error 
 77     delivery.Reject() 
 78     return 
 79   } 
 80 
 81   if err := r.callback(message); err != nil { 
 82     delivery.Reject() 
 83     return 
 84   } 
 85 
 86   delivery.Ack() 
 87 } 
</pre>
<p>The <kbd>StartConsuming</kbd> method only has the responsibility for setting the callback to the queue instance; we then call the methods <kbd>StartConsuming</kbd> and <kbd>AddConsumer</kbd>, which are methods on the Redis package. On line <strong>65</strong>, we set the callback consumer to that the queue uses to self rather than the callback passed into the method. The delegate pattern assigned to an internal method allows us to abstract the implementation of the underlying queue from the implementing codebase. When a new message is detected on the queue, the <kbd>Consume</kbd> method is called passing an instance of <kbd>rmq.Delivery</kbd>, which is an interface defined in the <kbd>rmq</kbd> package:</p>
<pre>
type Delivery interface { 
  Payload() string 
  Ack() bool 
  Reject() bool 
  Push() bool 
} 
</pre>
<p>The first thing we need to do is unmarshal the message that is passed as a slice of byte into our <kbd>Message</kbd> structure. If this fails, then we call the <kbd>Reject</kbd> method on the <kbd>Delivery</kbd> interface, which pushes the message back onto the queue. Once we have the message in the format that our callback expects we can then execute the <kbd>callback</kbd> function, which is passed to the <kbd>StartConsuming</kbd> method. The type of callback is as follows:</p>
<pre>
func(Message) error 
</pre>
<p>It is the responsibility of the code, which implementing this method, to return an error should the processing of the message fail. Returning an error allows our consuming code to call <kbd>delivery.Reject()</kbd>, which would leave the message in the queue for later processing. When the message processes successfully, we pass a <kbd>nil</kbd> error and the consumer calls <kbd>delivery.Ack()</kbd>, which acknowledges that the message is successfully processed and removes it from the queue. These operations are process safe; they should not be available to other consumers so in the instance that we have many workers reading a queue, we can ensure that they are all working from distinct lists.</p>
<p>Let's take a look at the implementation of a service that would write messages to the queue, if we take a look at the example code file at <kbd>queue/writer/main.go</kbd> we can see that there is a very simple implementation. This is a too simple application for a production system and there is no message validation or security in the handler. However, this example is pared down to the bare minimum to highlight how messages are added to the queue:</p>
<pre>
   16 func main() { 
   17   q, err := queue.NewRedisQueue("redis:6379", "test_queue") 
   18   if err != nil { 
   19     log.Fatal(err) 
   20   } 
   21 
   22   http.HandleFunc("/", func(rw http.ResponseWriter, r <br/>        *http.Request) { 
   23     data, _ := ioutil.ReadAll(r.Body) 
   24     err := q.Add("new.product", data) 
   25     if err != nil { 
   26       log.Println(err) 
   27       rw.WriteHeader(http.StatusInternalServerError) 
   28       return                                                                                                                                      <br/>   29     } 
   30   }) 
   31 
   32   http.ListenAndServe(":8080", http.DefaultServeMux) 
   33 } 
</pre>
<p>We create an instance of <kbd>RedisQueue</kbd> and pass it the location of our Redis server and the name of the queue to which we would like to write messages. We then have a very simple implementation of <kbd>http.Handler</kbd>; this function reads the body of the request as a slice of bytes and calls the <kbd>Add</kbd> method with the name of the message and the payload. We then check the outcome of this operation before returning and closing the connection.</p>
<p>The consumer implementation is even simpler as this code implements a simple worker and does not implement any HTTP-based interface:</p>
<pre>
11 func main() { 
12   log.Println("Starting worker") 
13 
14   q, err := queue.NewRedisQueue("redis:6379", "test_queue") 
15   if err != nil { 
16     log.Fatal(err) 
17   } 
18 
19   q.StartConsuming(10, 100*time.Millisecond, func(message <br/>queue.Message) error { 
20     log.Printf("Received message: %v, %v, %v\n", message.ID, message.Name, message.Payload) 
21 
22     return nil // successfully processed message 
23   }) 
24 
25   runtime.Goexit() 
26 } 
</pre>
<p>Like in the client, we create an instance of our queue and then we call the <kbd>StartConsuming</kbd> method with our requested parameters and the <kbd>callback</kbd> function. The <kbd>callback</kbd> method executes for every message retrieved from the queue, and since we are returning batches of 10 potentially every 100 milliseconds this method could be called in quick succession, and every execution runs in its own <kbd>goroutine</kbd>, so when writing the implementation, we need to consider this detail. If for example, we were processing the messages and then writing them to a database then the number of connections to the database are not infinite. To determine an appropriate batch size we need to conduct initial testing and follow this up with constant monitoring, in order to tweak the application for optimum performance. These settings should be implemented as parameters so that they are easily changed as the hardware scales.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Push messaging</h1>
                </header>
            
            <article>
                
<p>Rather than using a queue, sometimes you want a service to act immediately on an event. Your service subscribes to receive messages from a broker such as NATS.io or SNS. When the broker receives a message, dispatched from another service, then the broker notifies all the registered services by making a call to the registered endpoint sending it a copy of the message. The receiver will generally disconnect once the message has been received and assumes that the message processes correctly. This pattern allows the message broker extreme throughput, in the case of NATS.io a single server instance can deliver millions of messages per second. Should the client be unable to process the message, then it must handle the logic to manage this failure. This logic could be to dispatch a notification to the broker or again the message could be added to a dead letter queue for later replay.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="422" width="665" class="image-border" src="assets/650704ea-e1b3-4c93-836b-cfd12dab6373.png"/></div>
<p>In this example, we are going to leverage the power of NATS.io to act as a message broker for our system, NATS is an incredibly lightweight application that is written in Go and provides such astounding performance and stability. Looking at <kbd>push/writer/main.go</kbd>, we can see that there is not very much code we need to write to implement NATS.io:</p>
<pre>
 24 func main() { 
 25   var err error 
 26   natsClient, err = nats.Connect("nats://" + *natsServer) 
 27   if err != nil { 
 28     log.Fatal(err) 
 29   } 
 30   defer natsClient.Close() 
 31 
 32   http.DefaultServeMux.HandleFunc("/product", productsHandler) 
 33 
 34   log.Println("Starting product write service on port 8080") 
 35   log.Fatal(http.ListenAndServe(":8080", http.DefaultServeMux)) 
 36 } 
</pre>
<p>The first thing we need to do when starting our application is to connect to the NATS server by calling the <kbd>Connect</kbd> function on the <kbd>nats</kbd> package:</p>
<pre>
func Connect(url string, options ...Option) (*Conn,error)
</pre>
<p>The <kbd>url</kbd> parameter, which is defined as a string, requires a little clarification. While you can pass a single URL such as <kbd>nats://server:port</kbd> you can also pass a comma separated list of servers. The reason for this is because of fault tolerance, NATS implements clustering, in our simple example we only have a single instance, however, when running in production you will have multiple instances for redundancy. We then define our <kbd>http.Handler</kbd> function and expose the <kbd>/product</kbd> endpoint:</p>
<pre>
 37 func productsHandler(rw http.ResponseWriter, r *http.Request) { 
 38   if r.Method == "POST" { 
 39     insertProduct(rw, r) 
 40   } 
 41 } 
 42 
 43 func insertProduct(rw http.ResponseWriter, r *http.Request) { 
 44   log.Println("/insert handler called") 
 45 
 46   data, err := ioutil.ReadAll(r.Body) 
 47   if err != nil { 
 48     rw.WriteHeader(http.StatusBadRequest) 
 49     return 
 50   } 
 51   defer r.Body.Close() 
 52 
 53   natsClient.Publish("product.inserted", data) 
 54 } 
</pre>
<p>The implementation of the handler is straightforward and we delegate the work to the <kbd>insertProduct</kbd> function. Again, in terms of implementation this is brief to highlight the use of publishing a message; in production there would be a higher level of implementation to manage security and validation.</p>
<p>On line <strong>53</strong>, we call the <kbd>Publish</kbd> method on our client; the method has an incredibly simple signature with the subject and the payload:</p>
<pre>
func (nc *Conn) Publish(subjstring, data []byte)error
</pre>
<p>Concerning the subject, we need to consider that this is the same name that the subscriber is going to use and that it must be unique otherwise it is possible that unintended recipients receive the messages and this is an incredibly difficult error to track down. The fully configurable options for NATS are in the GoDoc <a href="https://godoc.org/github.com/nats-io/go-nats"><span class="URLPACKT">https://godoc.org/github.com/nats-io/go-nats</span></a>, which is rather comprehensive.</p>
<p>Now we have seen how easy it is to publish messages to NATS, let's see how easy it is to consume them. If we take a look at the example code at <kbd>push/reader/main.go</kbd>, we can see that subscribing to messages is incredibly simple:</p>
<pre>
 25 func main() { 
 26   var err error 
 27   natsClient, err = nats.Connect("nats://" + *natsServer) 
 28   if err != nil { 
 29     log.Fatal(err) 
 30   } 
 31   defer natsClient.Close() 
 32 
 33   log.Println("Subscribing to events") 
 34   natsClient.Subscribe("product.inserted", handleMessage) 
 35 } 
 36 
 37 func handleMessage(m *nats.Msg) { 
 38   p := product{} 
 39   err := json.Unmarshal(m.Data, &amp;p) 
 40   if err != nil { 
 41     log.Println("Unable to unmarshal event object") 
 42     return 
 43   } 
 44 
 45   log.Printf("Received message: %v, %#v", m.Subject, p) 
 46 } 
</pre>
<p>Again, we make our connection to the NATS server, but to start receiving events we call the <kbd>Subscribe</kbd> method on the client:</p>
<pre>
func (nc *Conn) Subscribe(subjstring, cbMsgHandler) (*Subscription,error)
</pre>
<p>The <kbd>Subscribe</kbd> method will express interest in the given subject. The subject can have wildcards (partial: <kbd>*</kbd>, full: <kbd>&gt;</kbd>). Messages will be delivered to the associated <kbd>MsgHandler</kbd>.</p>
<p>If no <kbd>MsgHandler</kbd> is given, the subscription is a synchronous subscription, and it can be polled via <kbd>Subscription.NextMsg()</kbd>.</p>
<p>Unlike in our queue example, we are not polling the NATS server we are exposing an endpoint and registering that with NATS. When the NATS server receives a message, it attempts to forward that to all the registered endpoints. Using the implementation in the previous code sample, we obtain a copy of the message for every worker we have running on the system, which is not ideal. Rather than managing this ourselves, we can use a different method on the API, <kbd>QueueSubscribe</kbd>:</p>
<pre>
func (nc *Conn) QueueSubscribe(subj, queuestring, cbMsgHandler) (*Subscription,error)
</pre>
<p>The <kbd>QueueSubscribe</kbd> function creates an asynchronous queue subscriber on the given subject. All subscribers with the same queue name form the queue group and only one member of the group is selected to receive any given message asynchronously.</p>
<p>The signature is like the <kbd>Subscribe</kbd> method with the exception that we pass an additional parameter, which is the name of the queue or the unique cluster of subscribers who would like to register interest in the given subject.</p>
<p>Now we have defined the two main types of asynchronous messaging and looked at the simple implementation of each. Let's take a look at two common patterns that leverage this technique.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Command Query Responsibility Segregation (CQRS)</h1>
                </header>
            
            <article>
                
<p>CQRS is an abbreviation for Command Query Responsibility Segregation, a term attributed to Greg Young. The concept is that you use a different model to update information than the model used for reading information. The two main reasons for implementing CQRS are when the storage of a model differs dramatically from the presentation of the model, and when the concepts behind this approach are that attempting to create a model which is optimized for storage and a model which optimized for display might solve neither problem. For this reason, CQRS splits these models into a <strong>Query</strong> model used by the presentation logic and a <strong>Command</strong> model that is used for storage and validation. The other benefit is when we would like to separate load between reads and writes in high-performance applications. The CQRS pattern is not something that is hugely common and certainly should not be used everywhere as it does increase complexity; however, it is a very useful pattern to have in your arsenal.</p>
<p>Let's take a look at the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="379" width="259" class="image-border" src="assets/08eac447-b4d9-4d1f-a639-dbed2064a47f.png"/></div>
<p>In our example code, we again were leveraging NATS.io to broker the messages. However, this need not be the case. It is a legitimate setup to have a single service that has two separate models for reading and writing. Instead of the complexity of a message broker in process communication could be used just as effectively.</p>
<p>Take a look at the example code at <kbd>cCQRS/product_writer/main.go</kbd>:</p>
<pre>
 26 func init() { 
 27   flag.Parse() 
 28 
 29   schema = &amp;memdb.DBSchema{ 
 30     Tables: map[string]*memdb.TableSchema{ 
 31       "product": &amp;memdb.TableSchema{ 
 32         Name: "product", 
 33         Indexes: map[string]*memdb.IndexSchema{ 
 34           "id": &amp;memdb.IndexSchema{ 
 35             Name:    "id", 
 36             Unique:  true, 
 37             Indexer: &amp;memdb.StringFieldIndex{Field: "SKU"}, 
 38           }, 
 39         }, 
 40       }, 
 41     }, 
 42   } 
... 
 66   natsClient, err = nats.Connect("nats://" + *natsServer) 
 67   if err != nil { 
 68     log.Fatal(err) 
 69   } 
 70 } 
</pre>
<p>For simplicity, this example uses an in-memory database, <kbd>https://github.com/hashicorp/go-memdb</kbd>, written by HashiCorp and the bulk of the setup is configuring this data store. We will be separating our data stores for read and write and the reader service does not implement any methods to return the products to the caller. Instead, this responsibility is delegated to a second service that is running a separate database and even a different data model:</p>
<pre>
 84 func insertProduct(rw http.ResponseWriter, r *http.Request) { 
 85   log.Println("/insert handler called") 
 86 
 87   p := &amp;product{} 
 88 
 89   data, err := ioutil.ReadAll(r.Body) 
 90   if err != nil { 
 91     rw.WriteHeader(http.StatusBadRequest) 
 92     return 
 93   } 
 94   defer r.Body.Close() 
 95 
 96   err = json.Unmarshal(data, p) 
 97   if err != nil { 
 98     log.Println(err) 
 99     rw.WriteHeader(http.StatusBadRequest) 
100     return 
101   } 
102 
103   txn := db.Txn(true) 
104   if err := txn.Insert("product", p); err != nil { 
105     log.Println(err) 
106     rw.WriteHeader(http.StatusInternalServerError) 
107     return 
108   } 
109   txn.Commit() 
110 
111   natsClient.Publish("product.inserted", data) 
112 } 
</pre>
<p>Our handler first writes the model to the database and then like our push example we are publishing a message to NATS containing the payload of the message.</p>
<p>Looking at the reader server at <kbd>CQRS/product-read/main.go</kbd> again we are setting up our data store, however, the model is different from the read model:</p>
<ul>
<li><strong>Write model</strong>:</li>
</ul>
<pre>
        type product struct { 
           Name       string `json:"name"` 
           SKU        string `json:"sku"` 
           StockCount int    `json:"stock_count"`                                                                                                        <br/>        } 
</pre>
<ul>
<li><strong>Read model</strong>:</li>
</ul>
<pre>
        type product struct { 
          Name        string `json:"name"` 
          Code        string `json:"code"` 
          LastUpdated string `json:"last_updated"` 
        } 
</pre>
<p>We are also defining an event structure that contains the details for our event received from NATS. In this instance, this structure mirrors the write model; however, this does not always need to be the case:</p>
<pre>
type productInsertedEvent struct { 
  Name string `json:"name"` 
  SKU  string `json:"sku"` 
} 
</pre>
<p>Upon receipt of a message, we first decode the payload into the expected type <kbd>productInsertedEvent</kbd> and then we convert this to our product model that is stored in the database. Finally, we store the information in the database creating our copy in a format that our consumers wish to receive:</p>
<pre>
112 func productMessage(m *nats.Msg) { 
113   pie := productInsertedEvent{} 
114   err := json.Unmarshal(m.Data, &amp;pie) 
115   if err != nil { 
116     log.Println("Unable to unmarshal event object") 
117     return 
118   } 
119 
120   p := product{}.FromProductInsertedEvent(pie) 
121 
122   txn := db.Txn(true) 
123   if err := txn.Insert("product", p); err != nil { 
124     log.Println(err) 
125     return 
126   } 
127   txn.Commit() 
128 
129   log.Println("Saved product: ", p) 
130 } 
</pre>
<p>When a user calls the <kbd>/products</kbd> endpoint the data that they get back is that of the locally cached copy, not the master that is stored in a separate service. This process could cause issues with consistency as the two copies of data are eventually consistent and when we implement the CQRS pattern we need to consider this. If we were exposing the stock level, then it may not be desirable to have eventual consistency, however, we can make a design decision that when this information is required we sacrifice performance by making a synchronous call to the stock endpoint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Domain-Driven Design</h1>
                </header>
            
            <article>
                
<p>When implementing Event Driven Microservices, you need to have a good grasp of the way your system operates and the way data, and interactions flow from one service to the next. A useful technique for modeling any complex system is Domain-Driven Design.</p>
<p>When it comes to Domain-Driven Design, then there is Vernon Vaughn, whose two books, <em>Domain-Driven Design Distilled</em> and <em>Implementing Domain-Driven Design</em>, expand upon the seminal and for some slightly difficult to read work by Eric Evans. For newcomers to DDD, I recommend starting with DDD distilled and then moving to read Implementing Domain Driven Design. Reading DDD distilled first gives you a grounding of the terminology before you delve into what is a rather detailed book. DDD is most certainly an advanced topic and not something that can be covered comprehensively in one section of this book, nor do I profess to have the experience to write anything more detailed as DDD is a pattern that is learned by practice. DDD is also a tool for more complex large systems with many stakeholders and many moving parts. Even if you are not working on such a system, the concepts of aggregation and isolation are compelling and applicable to most systems. If nothing else, keep reading to become more proficient at buzzword bingo in your next architecture meeting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is DDD?</h1>
                </header>
            
            <article>
                
<p>To quote the words of VaughnVernon himself:</p>
<div class="packt_quote">"DDD is a set of tools that assist you in designing and implementing software that delivers high value, both strategically and tactically. Your organization can't be the best at everything, so it had better choose carefully at what it must excel. The DDD strategic development tools help you and your team make the competitively best software design choices and integration decisions for your business."</div>
<div class="packt_quote">-- Vaughn Vernon</div>
<p>That is quite an introduction; however, I think it highlights the fact that DDD is a tool for designing software and not a software framework. In the dark days, a couple of decades ago software architects and project managers would make the decisions for the design of a software system, often providing very detailed plans that were executed by the development teams. In my experience this was rarely an enjoyable way to work and neither did it produce good quality software and deliver on-time. The agile revolution proposed a different way of working and thankfully has improved the situation. We also now regard ourselves as software engineers rather than developers, I do not believe that this shift is a fashion, but it is driven by the change in the role that we have seen. Your role as someone who creates software is now one of a designer, negotiator of features, architect, mediator, and you are also required to have a full understanding of the materials at your disposal including the reactions to stress and strain. You now mirror the role of a traditional engineer rather than the assembly line worker role that software developers performed in the past.</p>
<p>Hopefully, that answers the question that may be in your head as to why you need to learn about DDD in a book about Go. Well, this book was never written to teach you the language, it was designed to show you how you can use it to build successful microservices.</p>
<p>I hear lots of noise surrounding DDD that it is a difficult technique and admittedly when I first read DDD I felt the same way, all this stuff about aggregates, ubiquitous language, domains, and subdomains. However, once I started to think about DDD to engineer separation and thought about many of the problems I have faced in the past with confused domain models then it slowly began to sink in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical debt</h1>
                </header>
            
            <article>
                
<p>If you have ever worked on a monolithic application, you are aware of the coupling and dependency that occurs between objects, this is predominately in the data layer, however, you also often find code that is not implementing correctly and is tightly bound to another object. The problems come when you want to change this system; a change in one area has an undesired impact in another and only one if you are lucky. An enormous effort happens in refactoring the system before changes are made. Often what happens is that the modification is shoehorned into the existing codebase without refactoring and to be brutally honest it would be kinder to the system to take it outside, around the back of the barn and unload two shotgun shells in the back of its head.</p>
<p>Don't fool yourself that you will ever get the opportunity to do this; if you have ever worked on a system of any real age, your job is like Lenin's embalmers. You spend an enormous amount of effort to keep a dead body presentable when you should just dig a hole in the ground and drop it in. DDD can help with understanding the monolith and slowly decoupling it; it is also a tool to prevent the unruly monolith from ever occurring. Let's take a quick look at the technical anatomy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Anatomy of DDD</h1>
                </header>
            
            <article>
                
<p>The primary part of the strategic design in DDD is to apply a concept called <strong>Bounded Contexts</strong>. Bounded Contexts are a method of segregating your domain into models. Using a technique called <strong>Context Mapping</strong> you can integrate multiple Bounded Contexts by defining both the team and technical relationships that exist between them.</p>
<p>The tactical design is where you refine the details of your domain model. In this phase, we learn how to aggregate entities and value objects together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategic design</h1>
                </header>
            
            <article>
                
<p>One of the phrases you will hear a lot when dealing with DDD is the term Bounded Contexts. A Bounded Contexts concept is a semantic contextual boundary that is the components inside each boundary has a specific meaning and does specific things. One of the most important of these Bounded Contexts is the <strong>Core Domain</strong>; this is the domain that distinguishes your organization competitively from all the others. We have already mentioned that you cannot do everything and by concentrating on your core domain this should be where you spend most of your time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tactical design</h1>
                </header>
            
            <article>
                
<p>From the base of strategic design is the tactical design, to quote Vaughn Vernon again:</p>
<div class="packt_quote">"Tactical design is like using a thin brush to paint the finer details of your domain model."</div>
<div class="packt_quote">--Vaughn Vernon</div>
<p>At this stage in the design, we need to start thinking about <strong>Aggregates</strong> and <strong>Domain Events</strong>. An aggregate is composed of entities and value objects. A value object models an immutable whole, it does not have a unique identity and equivalence is determined by comparing the attributes encapsulated by the value types. Domain events are published by an aggregate and subscribed to by interested parties. This subscription could be from the same Bounded Contexts, or it may come from a different source.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ubiquitous language</h1>
                </header>
            
            <article>
                
<p>The term ubiquitous language in DDD refers to a core language that everyone on the team understands about the software under development. It is entirely possible that a component in a different context and developed by a different team has a different meaning for the same terminology. In fact, they are probably talking about different components from your model.</p>
<p>How you develop your ubiquitous language is an activity that the team will form. You should not put too much emphasis onto using only nouns to describe your model, you should start to build up simple scenarios. Consider our example from the chapter on testing where we used BDD for our functional and integration testing. These are your scenarios; the language of which you write them is your team's ubiquitous language. You should write these scenarios so that they are meaningful to your team and not attempt to write something that is meaningful for the entire department.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bounded Contexts</h1>
                </header>
            
            <article>
                
<p>One of the main reasons for using a Bounded Context is that teams often do not know when to stop piling things into their software models. As the team adds more features, the model soon becomes difficult to manage and understand. Not only this, the language of the model starts to become blurred. When software becomes vast and convoluted with many unrelated interconnections, it starts to become what is known as a <em>Big Ball of Mud</em>. The big ball of mud is probably far worse than your traditional monolith. Monoliths are not inherently evil just because they are monolithic; monoliths are bad as within them exists a place where good coding standards are long forgotten. The other problem with a Bounded Context that is too large and owned by too many people is that it starts to be difficult to describe it using a ubiquitous language.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Context Mapping</h1>
                </header>
            
            <article>
                
<p>When two Bounded Contexts in DDD need to integrate the integration is known as Context Mapping. The importance of defining this Context Mapping is that a well-defined contract supports controlled changes over time. In the book <em>Domain-Driven Design Distilled</em>, Vaughn Vernon describes the following different kinds of mappings:</p>
<ul>
<li><strong>Partnership:</strong> The partnership mapping exists when two teams are each responsible for a Bounded Context and have a dependent set of goals.</li>
<li><strong>Shared kernel:</strong> A shared kernel is defined by an intersection of two separate Bounded Contexts and exists when two teams share a small but common model.</li>
<li><strong>Customer-supplier:</strong> A customer-supplier describes a relationship between two Bounded Contexts and their respective teams. The supplier is the upstream context, and the downstream is the customer. The supplier must provide what the customer needs, and the two teams must plan together to meet their expectations. This is a very typical and practical relationship between the teams as long as the supplier still considers the customers need.</li>
<li><strong>Conformist:</strong> A conformist relationship exists when there are upstream, and downstream teams and the upstream team has no motivation to support the specific needs of the downstream team. Rather than translate the upstream ubiquitous language to fit its own needs the downstream team adopts the language of the upstream.</li>
<li><strong>Anti-corruption layer:</strong> This is a standard and recommended model when you are connecting two systems together, the downstream team builds a translation layer between its ubiquitous language and that of the upstream thus isolating it from the upstream.</li>
<li><strong>Open Host Service:</strong> An Open Host Service defines a protocol or interface that gives access to your Bounded Contexts as a set of services. The services are offered via a well-documented API and are simple to consume.</li>
<li><strong>Published language:</strong> A published language is a well-documented information exchange language enabling easy consumption and translation. <strong>XML Schema</strong>, <strong>JSON Schema</strong>, and <strong>RPC</strong> based frameworks such as <strong>Protobufs</strong> are often used.</li>
<li><strong>Separate ways:</strong> In this situation, there is no significant payoff through the consumption of various ubiquitous languages and the team decides to produce their solution inside their Bounded Contexts.</li>
<li><strong>Big ball of mud:</strong> This should be pretty self-explanatory by now and not something a team should aim for; in fact, this is the very thing that DDD attempts to avoid.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software</h1>
                </header>
            
            <article>
                
<p>When we start working with DDD and event-oriented architectures in anger, we soon find that we need some help brokering our messages to ensure the at-least-once and at-most-once delivery that is required by the application. We could, of course, implement our strategy for this. However, there are many open source projects on the internet that handle this capability for us, and soon we find ourselves reaching out to leverage one of these.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka</h1>
                </header>
            
            <article>
                
<p>Kafka is a distributed streaming platform that allows you to publish and subscribe to streams of records. It lets your store streams of documents in a fault-tolerant way and process streams of records as they occur. It has been designed to be a fast and fault-tolerant system commonly running as a cluster of one or more servers to enable redundancy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NATS.io</h1>
                </header>
            
            <article>
                
<p>NATS.io is an open source messaging system written in Go, and it has the capability to perform two roles such as the at-most-once and at-least-once delivery, lets look at what this means:</p>
<ul>
<li><strong>at-most-once delivery</strong>: In the basic mode, NATS can act as a Pub/Sub router, where listening clients can subscribe to message topics and have new messages pushed to them. If a message has no subscriber, then it is sent to <kbd>/dev/null</kbd> and is not stored internally in the system.</li>
<li>
<p><strong>at-least-once delivery</strong>: When a higher level of service and more stringent delivery guarantees are required NATS can operate in at-least-once delivery mode. In this mode, NATS can no longer function as a standalone entity and needs to be backed by a storage device, which at present support is for file and in-memory. Now, there is no scaling and replication supported with NATS streaming, and this is where Kafka shines. However, we are not all building systems as big as Netflix, and the configuration and management of Kafka is a book in its own right, NATS can be understood very quickly.</p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AWS SNS/SQS</h1>
                </header>
            
            <article>
                
<p>Amazon's <strong>Simple Queue Service</strong> (<strong>SQS</strong>) is a queuing service that allows a publisher to add messages to a queue, which can later be consumed by clients. A message is read and then removed from the queue making it no longer available for other readers.</p>
<p>There are two different types of SQS, such as the standard mode, which allows maximum throughput at the expense that a message may be delivered more than once and SQS FIFO, which ensures that messages are only ever delivered once and in the order by which they are received. However, FIFO queues are subject to vastly reduced throughput, and therefore their use must be carefully considered.</p>
<p>Amazon's <strong>Simple Notification Service</strong> (<strong>SNS</strong>) is a service for coordinating and managing the delivery of queues of messages. SNS stands for Simple Notification Service; you configure a topic that you can publish messages to and then subscribers can register for notifications. SNS can deliver messages to the following different protocols:</p>
<ul>
<li>HTTP(S)</li>
<li>Email</li>
<li>Email-JSON</li>
<li>SMS</li>
<li>AWS Lambda</li>
<li>SQS</li>
</ul>
<p>You may wonder why you would want to add a message to a queue when you can just push a message to the recipient? One of the problems with SNS is that it can only deliver over HTTP to services that are publicly accessible. If your internal workers are not connected to the public internet and from reading <a href="d38f7017-1c2e-4a12-b1dc-5870121afd4e.xhtml">Chapter 8</a>, <em>Security</em>, I hope that they are not. Therefore, a pull-based approach may be your only option; reading from a queue is also potentially a better option for managing large streams of messages. You do not need to worry about the availability of SQS (most of the time), and you do not need to implement an HTTP interface for a simple application worker that can poll a queue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud Pub/Sub</h1>
                </header>
            
            <article>
                
<p>Google Cloud Pub/Sub is very like AWS SNS in that it is a messaging middleware, allowing the creation of topics with publishers and subscribers. At the time of writing, there is no formal product on Google Cloud such as SQS. However, it would be trivial to implement something using one of the many data storage options you have available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have looked at some of the main patterns for decoupling microservices using events, we have also had an introduction to a modern design methodology for building distributed systems, DDD. With the right tools and upfront design building highly scalable and maintainable systems should not be too challenging and you now have all the information you need to do this with Go. In the final chapter, we are going to look at automated building and deployment of your code, finalizing the information you need to be a successful microservices practitioner.</p>


            </article>

            
        </section>
    </body></html>