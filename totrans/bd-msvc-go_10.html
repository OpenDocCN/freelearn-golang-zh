<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Continuous Delivery</h1>
                </header>
            
            <article>
                
<p>We have covered a lot so far, including how to build resilient systems and how to keep them secure, but now we need to look at how to replace all the manual steps in our process and introduce continuous delivery.</p>
<p>In this chapter, we will discuss the following concepts:</p>
<ul>
<li>Continuous Delivery</li>
<li>Container orchestration</li>
<li>Immutable infrastructure</li>
<li>Terraform</li>
<li>Example Application</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is Continuous Delivery?</h1>
                </header>
            
            <article>
                
<p>Continuous delivery is the process of building and deploying code well, continuously. The aim is that we move code from development to production as efficiently and effectively as possible.</p>
<p>In a traditional or waterfall workflow, releases revolve around the completion of a major feature or update. It is not untypical for large enterprises to release once a quarter. When we look at the reason for this strategy, risk and effort are often cited. There is a risk to releasing as the confidence is weak in the software; there is effort involved in releasing because there needs to be a mostly manual process involved in quality assurance and the operational aspects of releasing the software. One part of this is something that we have covered in <a href="905299b5-e8d4-424c-827e-3fed5a58289e.xhtml" target="_blank">C</a><span class="MsoIntenseEmphasis">hapter 5, <em>Common Patterns</em>,</span> which is the concern with quality, and the possible absence of a satisfactory test suite or possibly the ability to run this automatically. The second element involves the physical deployment and post deployment testing process. We have not covered this aspect much in this book so far; we touched on it when we looked at <span class="MsoIntenseEmphasis">Docker</span> in <span class="MsoIntenseEmphasis"><a href="2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml" target="_blank">Chapter 4</a>, <em>Testing</em></span>.</p>
<p>If we could reduce the risk and effort involved in deploying code, would you do it more frequently? How about every time you complete a minor feature, or every time a bug is fixed, several times a day even? I would encourage you to do just that, and in this chapter, we will look at all the things we need to know and building on all the things we have previously learned to deliver continuously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manual deployment</h1>
                </header>
            
            <article>
                
<p>Manual deployment is at best problematic; even if you have an amazing team, things can and will go wrong. The larger the group, the more distributed the knowledge and the greater the need for comprehensive documentation. In a small team, the resources are constrained, and the time it takes for deployment can be a distraction from building great code. You also end up with a weak link; so, do you suspend deployment when the person who usually carries out the process is sick or on holiday?</p>
<p class="mce-root">The problems with manual deployment</p>
<ul>
<li>Issues can arise with the ordering and timing of the various deployment steps</li>
<li>The documentation needs to be comprehensive and always up to date</li>
<li>There is a significant reliance on manual testing</li>
<li>There are application servers with different states</li>
<li>There are constant problems with manual deployment due to the preceding points</li>
</ul>
<p>As a system grows in complexity, there are more moving parts, and the steps required to deploy the code increase with it. Since the steps to deploy need to be carried out in a set order, the process can fast become a burden. Consider deploying an update to an application, the application and its dependencies install on all instances of the application servers. Often a database schema needs to be updated, and there needs to be a clean switch over between the old and the new application. Even if you are leveraging the power of Docker, this process can be fraught with disaster. As the complexity of the application grows, so does the required documentation to deploy the application, and this is often a weak link. Documentation takes time to update and maintain, in my personal experience, this is the first area which suffers when deadlines are approaching. Once the application code is deployed, we need to test the function of the application. Assuming the application is manually deployed, it is often assumed that the application is also manually tested. A tester would need to run through a test plan (assuming there is a test plan) to check that the system is in a functioning state. If the system is not functioning, then either the process would need to be reversed to roll back to a previous state, or a decision would need to be made to hotfix the application and again run through the standard build and deploy cycle. When this process falls into a planned release, there is a little more safety as the whole team is around for the release. However, what happens when this process takes place in the middle of the night as a result of an incident? At best, what happens is that the fix is deployed, however, without updating any of the documentation or processes. At worst, the application ends up in a worse state than before the application code was attempted to be hot fixed. Out of hours, incidents are also often carried out by first line response, which is often the infrastructure team. I assume that if you are not running continuous deployment, then you will also not be following the practice of Developer on call. Also, what about the time it takes to do a deploy? What is the financial cost of the entire team taking time out to babysit a deployment? What about the motivational and mental productivity cost of this process? Have you ever felt the stress due to the uncertainty of deploying application code into production?</p>
<p>Continuous delivery removes these risks and problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The benefits of continuous delivery</h1>
                </header>
            
            <article>
                
<p>The concept of continuous delivery is that you plan for these problems and spend the up-front work to solve them. Automation of all the steps involved allows consistency of operation and is a self-documenting process. No longer do you have the requirement for specialized human knowledge, and the additional benefit of the removal of the human is that the quality improves due to automation of the process. Once we have the automation, improved the quality and speed of our deployments, we can then level this up and start to deploy continuously. The benefits of continuous delivery are:</p>
<ul>
<li>The releases are small and less complicated</li>
<li>The differences between master and feature branch are smaller</li>
<li>There are fewer areas to monitor post deployment</li>
<li>The rollbacks are potentially easier</li>
<li>They deliver business value sooner</li>
</ul>
<p>We start to deploy our code in smaller chunks, no longer waiting for the completion of a major feature but potentially after every commit. The primary benefit of this is that the differences between the master and the feature branches are smaller and less time is spent merging code between branches. Smaller changes also create fewer areas to monitor post deploy and because of this, should something go wrong it is easier to roll back the changes to a known working state. Most important of all, it gives you the capability to deliver business value sooner; whether this is in the form of a bug or a new feature, the capability is ready for your customers to use far earlier than would be available in a waterfall model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aspects of continuous delivery</h1>
                </header>
            
            <article>
                
<p>There are several important aspects to continuous delivery most of which are essential to the success of the process. In this section, we will look at what these aspects are before we look at how we can implement them to build our own pipeline.</p>
<p>Important aspects of continuous delivery:</p>
<ul>
<li>Reproducibility and easy setup</li>
<li>Artifact storage</li>
<li>Automation of tests</li>
<li>Automation of integration tests</li>
<li>Infrastructure as code</li>
<li>Security scanning</li>
<li>Static code analysis</li>
<li>Smoke testing</li>
<li>End 2 end testing</li>
<li>Monitoring - track deployments in metrics</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reproducibility and consistency</h1>
                </header>
            
            <article>
                
<p>I have a small doubt, at some point in your career you might have already seen this meme:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/2876a4cd-de1c-4d29-b7ab-e3492425c7c7.png"/></div>
<p>If you have not, don't worry, I am confident you are going to encounter it at some point. <em>Works on My Machine</em> why is this meme so popular? Could it be because there is a heavy element of truth in it? I certainly know that I have been there and many of you have too I am sure. If we are to deliver continuously and by this mean as often as possible, then we need to care about consistency and reproducibility.</p>
<p>Reproducibility is the ability of an entire analysis of an experiment or study to be duplicated, either by the same researcher or by someone else working independently. <em>Works on my machine</em> is not acceptable. If we are to deliver continuously, then we need to codify our build process and make sure that our dependencies for software and other elements are either minimized or managed.</p>
<p>The other thing that is important is the consistency of our builds. We cannot spend time fixing broken builds or manually deploying software, so we must treat them with the same regard that we treat our production code. If the build breaks, we need to stop the line and fix it immediately, understand why the build broke, and if necessary, introduce new safeguards or processes so that it does not occur again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Artifact storage</h1>
                </header>
            
            <article>
                
<p>When we implement any form of continuous integration, we produce various artifacts because of the build process. The artifacts can range from binaries to the output of tests. We need to consider how we are going to store this data; thankfully cloud computing has many answers to this problem. One solution is cloud storage such as AWS S3, which is available in abundance, and at a small cost. Many of the software as a service CI providers such as Travis and CircleCI also offer this capability built into the system; so for us to leverage it, there is very little we need to do. We can also leverage the same storage if, for example, we are using Jenkins. The existence of the cloud means we rarely need to worry about the management of CI artifacts anymore.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automation of tests</h1>
                </header>
            
            <article>
                
<p>Test automation is essential, and to ensure the integrity of the built application, we must run our unit tests on the CI platform. Test automation forces us to consider easy and reproducible setup, dependencies need minimizing, and we should only be checking the behavior and integrity of the code. In this step, we avoid integration tests, the tests should run without anything but the go test command.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automation of integration tests</h1>
                </header>
            
            <article>
                
<p>Of course, we do need to verify the integration between our code and any other dependencies such as a database or downstream service. It is easy to misconfigure something, especially when database statements are involved. The level of integration tests should be far less than the coverage of the unit tests, and again we need to be able to run these in a reproducible environment. Docker is an excellent ally in this situation; we can leverage the capability of Docker to run in multiple environments. This enables us to configure and debug our integration tests on our local environment before executing them on the build server. In the same way, that unit tests are a gate to a successful build, so are integration tests; failure of these tests should never result in a deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Infrastructure as code</h1>
                </header>
            
            <article>
                
<p>When we automate our build and deploy the process, this step is essential; ideally, we do not want to be deploying code to a dirty environment as this raises the risk of pollution such as an incorrectly vendored dependency. However, we also need to be able to rebuild the environment, if necessary, and this should be possible without enacting any of the problems we introduced earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Security scanning</h1>
                </header>
            
            <article>
                
<p>If possible, security scanning should be integrated into the pipeline; we need to be catching bugs early and often. Regardless of whether your service is external facing or not, scanning it can ensure that there is a limited attack vector for an attacker to misuse. We have looked at fuzzing in a previous chapter, and the time it can take to perform this task is quite considerable and possibly not suitable for inclusion inside of a pipeline. However, it is possible to include various aspects of security scanning into the pipeline without slowing down deployments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static code analysis</h1>
                </header>
            
            <article>
                
<p>Static code analysis is an incredibly effective tool to combat bugs and vulnerabilities in your applications, and often developers run tools such as <strong><span class="MsoIntenseEmphasis">govet</span></strong> and <strong><span class="MsoIntenseEmphasis">gofmt</span></strong> as part of their IDE. When the source is saved, the linter runs and identifies issues in the source code. It is important to run these applications inside of the pipeline as well as we cannot always guarantee that the change has come from an IDE which has it configured in this way. In addition to the save time linters, we can also run static code analysis to detect problems with SQL statements and code quality issues. These additional tools are often not included in the IDE's save workflow, and therefore it is imperative to run them on CI to detect any problems, which may have slipped through the net.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Smoke tests</h1>
                </header>
            
            <article>
                
<p>Smoke tests are our way of determining whether a deployment has gone successfully. We run a test, which can range from a simple curl to a more complex codified test to check various integration points within the running application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">End-to-end tests</h1>
                </header>
            
            <article>
                
<p>End-to-end tests are a complete check on the running system and typically follow the user flow testing the various parts. Often these tests are global to the application not local to the service and are automated using BDD-based tools such as cucumber. Determining whether you run E2E tests as a gate to deployment or a parallel process which is either triggered by a deployment or set as a continually running process is dependent upon your company's appetite for risk. If you are confident that your unit, integration, and smoke tests have adequate coverage to give you peace of mind or that the service in question is not essential to the core user journey, then you may decide to run these in parallel. If, however, the functionality in question is part of your core journey, then you may choose to run these tests sequentially as a gateway to deployment on a staging environment. Even when E2E tests run as a gateway, if any configuration changes are made such as the promotion of staging to production, it is advisable to again run the E2E tests before declaring a deployment successful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring</h1>
                </header>
            
            <article>
                
<p>Post deploy, we should not rely on our users to inform us when something has gone wrong, which is why we need application monitoring linked to an automated notification system such as <strong>PagerDuty</strong>. When a threshold of errors has exceeded, the monitor triggers and alerts you to the problem; this gives you the opportunity to roll back the last deploy or to fix the issue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous delivery process</h1>
                </header>
            
            <article>
                
<p>So far, we have talked about the problem, and why this is important for us. We have also looked at the constituent parts of a successful continuous delivery system, but how can we implement such a process for our application, and what does Go bring as a language which helps us with this? Now, let's look at the process:</p>
<ul>
<li>Build</li>
<li>Test</li>
<li>Package</li>
<li>Integration test</li>
<li>Benchmark test</li>
<li>Security test</li>
<li>Provision production</li>
<li>Smoke test</li>
<li>Monitor</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview</h1>
                </header>
            
            <article>
                
<p>The build process is mainly a focus for developers to get things up and running on their local machine but my recommendation is that we need to be thinking about cross-platform and cross-system builds from the beginning. What I mean by cross-system builds is that even if we are developing on a Macintosh, we may not be building a release product on a Mac. In fact, this behavior is quite common. We need our releases built by a third party and preferentially in a clean room environment, which is not going to be affected by pollution from other builds.</p>
<p>Every feature should have a branch and every branch should have a build. Every time your application code is pushed to the source repository, we should trigger the build even if this code is going nowhere near production. It is good practice never to leave a build in a broken state, and that includes branch builds. You should deal with the issues as and when they occur; delaying this action risks your ability to deploy, and while you may not plan to deploy to production until the end of the sprint, you must consider unforeseen issues which can occur such as the requirement to change the configuration or hotfix a bug. If the build process is in a broken state, then you will not be able to deal with the immediate issues, and you risk delaying a planned deployment.</p>
<p>The other important aspect other than automatically triggering a build whenever you push to a branch is to run a nightly build. Nightly builds for branches, should be rebased with the master branch before building and testing. The reason for this step is to give you early warning around potential merge conflicts. We want to catch these early rather than later; a failed nightly build should be the first task of the day.</p>
<p>We talked about Docker earlier on in <a href="2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml" target="_blank">Chapter 4</a>, <em>Testing</em>, and we should bring Docker into our build process. Docker through its immutability for a container gives us the clean room environment to ensure reproducibility. Because we start from scratch with every build, we cannot rely on a pre-existing state, which causes differences between the development environment and the built environment. Environmental pollution may seem like a trivial thing but the amount of time I have wasted over my career debugging a broken build because one application was using a dependency installed on a machine and another used a different version is immeasurable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is container orchestration?</h1>
                </header>
            
            <article>
                
<p>Simply container orchestration is the process of running one or more instances of an application. Think of the common understanding of an orchestra, a group of musicians who work together to produce a piece of music. The containers in your application are like the musicians in the orchestra; you may have specialist containers, of which there are low numbers of instances such as the percussionists, or you may have many instances such as the strings section. In an orchestra, the conductor keeps everything in time and ensures that the relevant musicians are playing the right music at the right time. In the world of containers, we have a scheduler; the scheduler is responsible for ensuring that the correct number of containers are running at any one time and that these containers are distributed correctly across the nodes in the cluster to ensure high availability. The scheduler, like a conductor, is also responsible for ensuring that the right instruments play at the right time. In addition to ensuring a constant suite of applications is constantly running, the scheduler also can start a container at a particular time or based on a particular condition to run ad hoc jobs. This capability is similar to what would be performed by <strong>cron</strong> on a Linux-based system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Options for container orchestration</h1>
                </header>
            
            <article>
                
<p>Thankfully, today there are many applications which provide an orchestration function, these are broken into two categories: Managed, such as PaaS solutions like AWS ECS, and Unmanaged, such as open source schedulers like Kubenetes, which need management of both servers and the scheduler application. Unfortunately, there is no one-fits-all solution. The option you choose is dependent on the scale you require and how complex your application is. If you are a startup or just starting to break out into the world of microservices, then the more managed end of the spectrum such as <strong>Elastic Beanstalk</strong> will more than suffice. If you are planning a large-scale migration, then you might be better looking at a fully fledged scheduler. One thing I am confident about is that by containerizing your applications using Docker, you have this flexibility, even if you are planning a large-scale migration, then start simple and work up to the complexity. We will examine how the concepts of orchestration and infrastructure-as-code enable us to complete this. We should never ignore the up-front-design and long term thinking, but we should not let this stop us from moving fast. Like code infrastructure can be refactored and upgraded, the important concepts are the patterns and the strong foundations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is immutable infrastructure?</h1>
                </header>
            
            <article>
                
<p>Immutability is the inability to be changed. We have already looked at Docker and how a Docker container is an immutable instance of an image. However, what about the hardware that the Docker server runs on? Immutable infrastructure gives us the same benefits--we have a known state and that state is consistent across our estate. Traditionally, the software would be upgraded on an application server, but this process was often problematic. The software update process would sometimes not go to plan, leaving in the operator with the arduous task of trying to roll this back. We would also experience situations where the application servers would be in a different state requiring different processes to upgrade each of them. The update process may be okay if you only have two application servers, but what if you have 200 of them? The cognitive load becomes too high to bear that the administration is distributed across a team or multiple teams, and then we need to start to maintain documentation to upgrade each of the applications. When we were dealing with bare metal servers, there was often no other way to deal with this; the time it would take to provision a machine was measured in days. With the virtualization, this time improved as it gave us the ability to create a base image, which contained a partial config and we could then provision new instances in tens of minutes. With the cloud, the level of abstraction became one layer greater; no longer did we even need to worry about the virtualization layer as we had the capability to spin up compute resource in seconds. So, the cloud solved the process of the hardware, but what about the process of provisioning the applications? Do we still need to write the documentation and keep it up to date? As it happens, we do not. Tooling has been created to allow us to codify our infrastructure and application provisioning. The code becomes the documentation and because it is code we can version it using the standard version control systems such as Git. There are many tools to choose from, such as Chef, Puppet, Ansible, and Terraform; however, in this chapter, we will take a look at Terraform because in my personal opinion, besides being the most modern of the tools and the easiest to use, it embodies all of the principles of immutability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Terraform</h1>
                </header>
            
            <article>
                
<p>Terraform (<a href="https://terraform.io"><span class="URLPACKT">https://terraform.io</span></a>) is an application by HashiCorp (<a href="https://hashicorp.com"><span class="URLPACKT">https://hashicorp.com</span></a>), which enables the provisioning of infrastructure for several applications and cloud providers.</p>
<p>It allows you to write codified infrastructure using the HCL language format. It enables the concepts of reproducibility and consistency that we have discussed are essential for continuous deployment.</p>
<p>Terraform as an application is a powerful tool and is a bigger topic than this book should cover; however, we will look at the basics of how it works to understand our demo application.</p>
<p>We will split our infrastructure into multiple chunks with the infrastructure code owned by each microservice located in the source code repository.</p>
<p>In this section, we will look closely at the shared infrastructure and services to get a deeper understanding of the Terraform concepts. Let's take a look at the example code in the following GitHub repository:</p>
<p><a href="https://github.com/building-microservices-with-go/chapter11-services-main"><span class="URLPACKT">https://github.com/building-microservices-with-go/chapter11-services-main</span></a></p>
<p>The shared infrastructure contains the following components:</p>
<div class="CDPAlignCenter CDPAlign"><img height="437" width="654" class="image-border" src="assets/bc503990-dab7-4129-a762-81a99c403866.png"/></div>
<ul>
<li><strong>VPC</strong>: This is the virtual cloud, it allows all of the applications connected to it to communicate without needing to go over the public internet</li>
<li><strong>S3 bucket</strong>: This is the remote storage for config and artifacts</li>
<li><strong>Elastic Beanstalk</strong>: This is the Elastic Beanstalk application which will run the NATS.io messaging system, we can split this over two availability zones which are the equivalent to a data center, hosting applications in multiple zones gives us redundancy in the instance that the zone suffers an outage</li>
<li><strong>Internal ALB:</strong> To communication with our NATS.io server when we add other applications to our VPC we need to use an internal application load balancer. An internal ALB has the same features as an external load balancer but it is only accessible to applications which are attached to the VPC, connections from the public internet are not allowed</li>
<li><strong>Internet Gateway:</strong> If we need our application to be able to make outbound calls to other internet services then we need to attach an internet gateway. For security, a VPC has no outbound connections by default</li>
</ul>
<p>Now we can understand the components which we need to create let's take a look at the Terraform configuration which can create them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Providers</h1>
                </header>
            
            <article>
                
<p>Terraform is broken up into providers. A provider is responsible for understanding the API interactions and exposing the resources for the chosen platform. In the first section, we will look at the provider configuration for AWS. In the following code, the <kbd>provider</kbd> block allows you to configure Terraform with your credentials and set an AWS region:</p>
<pre>
provider "aws" { 
    access_key = "XXXXXXXXXXX" 
    secret_key = "XXXXXXXXXXX" 
    region = "us-west-1" 
} 
</pre>
<p>Blocks in Terraform typically follow the previous pattern. HCL is not JSON; however, it is interoperable with JSON. The design of HCL is to find that balance between machine and human-readable format. In this particular provider, we can configure some different arguments; however, as a bare minimum, we must set up your <kbd><span class="MsoIntenseEmphasis">access_key</span></kbd>, <kbd><span class="MsoIntenseEmphasis">secret_key</span></kbd>, and <kbd><span class="MsoIntenseEmphasis">region</span></kbd>. These are explained as follows:</p>
<ul>
<li><kbd>access_key</kbd>: This is the AWS access key. This is a required argument; however, it may also be provided by setting the <kbd>AWS_ACCESS_KEY_ID</kbd> environment variable.</li>
<li><kbd>secret_key</kbd>: This is the AWS secret key. This is a required argument; however, it may also be provided by setting the <kbd>AWS_SECRET_ACCESS_KEY</kbd> environment variable.</li>
<li><kbd>region</kbd>: This is the AWS region. This is a required argument; however, it may also be provided by setting the <kbd>AWS_DEFAULT_REGION</kbd> environment variable.</li>
</ul>
<p>All of the required variables can be replaced with environment variables; we do not want to commit our AWS secrets to GitHub because if they leak we will most likely find that someone has kindly spun up lots of expensive resource to mine Bitcoin (<a href="http://www.securityweek.com/how-hackers-target-cloud-services-bitcoin-profit"><span class="URLPACKT">http://www.securityweek.com/how-hackers-target-cloud-services-bitcoin-profit</span></a>).</p>
<p>If we use environment variables, we can then securely inject these into our CI service where they are available for the job. Looking at our provider block <kbd><span class="MsoIntenseEmphasis">provider.tf</span></kbd>, we can see that it does not contain any of the settings:</p>
<pre>
provider "aws" { } 
</pre>
<p>Also, in this file, you will notice that there is a block by the name of <kbd><span class="MsoIntenseEmphasis">terraform</span></kbd>. This configuration block allows us to store the Terraform state in an S3 bucket:</p>
<pre>
terraform { 
  backend "s3" { 
    bucket = "nicjackson-terraform-state" 
    key    = "chapter11-main.tfstate" 
    region = "eu-west-1" 
  } 
} 
</pre>
<p>The state is what the <kbd>terraform</kbd> block uses to understand the resources, which have been created for a module. Every time you change your configuration and run either of the Terraform plans, Terraform will check the state files for differences to understand what it needs to delete, update, or create. A special note on remote state is that again it should never be checked into git. The remote state contains information about your infrastructure, including potentially secret information, not something you would ever want to leak. For this reason, we can use the remote state, rather than keep the state on your local disk; Terraform saves the state files to a remote backend such as <kbd>s3</kbd>. We can even implement locking with certain backends to ensure that only one run of the configuration takes place at any one time. In our config, we are using the AWS <kbd>s3</kbd> backend, which has the following attributes:</p>
<ul>
<li><kbd>bucket</kbd>: This is the name of the S3 bucket to store the state. S3 buckets are globally named and are not namespaced to your user account. So this value must not only be unique to you, but specific to AWS.</li>
<li><kbd>key</kbd>: This is the key of the bucket object which holds the state. This is unique to the bucket. You can use a bucket for multiple Terraform configs as long as this key is unique.</li>
<li><kbd>region</kbd>: This is the region for the S3 bucket.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Terraform config entry point</h1>
                </header>
            
            <article>
                
<p>The main entry point for our application is the <kbd><span class="MsoIntenseEmphasis">terraform.tf</span></kbd> file. There is no stipulation on this filename, Terraform is graph based. It recurses through all files which end in <kbd>.tf</kbd> in our directory and build up a dependency graph. It does this to understand the order to create resources.</p>
<p>If we look at this file, we see that it is made up of modules. Modules are a way for Terraform to create reusable sections of infrastructure code or just to logically separate things for readability. They are very similar to the concepts of packages in Go:</p>
<pre>
module "vpc" { 
  source = "./vpc" 
 
  namespace = "bog-chapter11" 
} 
 
module "s3" { 
  source = "./s3" 
 
  application_name = "chapter11" 
} 
 
module "nats" { 
  source = "./nats" 
 
  application_name        = "nats" 
  application_description = "Nats.io server" 
  application_environment = "dev" 
 
  deployment_bucket    = "${module.s3.deployment_bucket}" 
  deployment_bucket_id = "${module.s3.deployment_bucket_id}" 
 
  application_version = "1.1" 
  docker_image        = "nats" 
  docker_tag          = "latest" 
 
  elb_scheme   = "internal" 
  health_check = "/varz" 
 
  vpc_id  = "${module.vpc.id}" 
  subnets = ["${module.vpc.subnets}"] 
} 
</pre>
<p>Let's take a look at the VPC module in greater depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VPC module</h1>
                </header>
            
            <article>
                
<p>The VPC module creates our private network inside AWS; we do not want to or need to expose the NATS server to the outside world, so we can create a private network which only allows the resources attached to that network to access it, as shown in the following code:</p>
<pre>
module "vpc" { 
  source = "./vpc" 
 
  namespace = "bog-chapter11" 
} 
</pre>
<p>The <kbd>source</kbd> attribute is the location of the module; Terraform supports the following sources:</p>
<ul>
<li>Local file paths</li>
<li>GitHub</li>
<li>Bitbucket</li>
<li>Generic Git, Mercurial repositories</li>
<li>HTTP URLs</li>
<li>S3 buckets</li>
</ul>
<p>Following the <kbd>source</kbd> attribute, we can configure custom attributes, which correspond to the variables in the module. Variables are required placeholders for a module; when they are not present, Terraform complains when we try to run it.</p>
<p>The <kbd>vpc/variables.tf</kbd> file contains the following content:</p>
<pre>
variable "namespace" { 
  description = "The namespace for our module, will be prefixed to all resources."  
} 
 
variable "vpc_cidr_block" { 
  description = "The top-level CIDR block for the VPC." 
  default     = "10.1.0.0/16" 
} 
 
variable "cidr_blocks" { 
  description = "The CIDR blocks to create the workstations in." 
  default     = ["10.1.1.0/24", "10.1.2.0/24"] 
} 
</pre>
<p>The configuration for a variable is very much like that of the provider and it follows the following syntax:</p>
<pre>
variable "[name]" { 
  [config] 
} 
</pre>
<p>A variable has three possible configuration options:</p>
<ul>
<li><kbd>type</kbd>: This is an optional attribute which sets the type of the variable. The valid values are <kbd>string</kbd>, <kbd>list</kbd>, and <kbd>map</kbd>. If no value is given, then the type is assumed to be <kbd>string</kbd>.</li>
<li><kbd>default</kbd>: This is an optional attribute to set the default value for the variable.</li>
<li><kbd>description</kbd>: This is an optional attribute to assign a human-friendly description for the variable. The primary purpose of this attribute is for documentation of your Terraform configuration.</li>
</ul>
<p>Variables can be explicitly declared inside a <kbd><span class="MsoIntenseEmphasis">terraform.tfvars</span></kbd> file like the one in the root of our repository:</p>
<pre>
namespace = "chapter10-bog" 
</pre>
<p>We can also set an environment variable by prefixing <kbd><span class="MsoIntenseEmphasis">TF_VAR_</span></kbd> to the name of the variable:</p>
<pre>
export TF_VAR_namespace=chapter10-bog 
</pre>
<p>Alternatively, we can include the variable in the command when we run the <kbd>terraform</kbd> command:</p>
<pre>
<strong>terraform plan -var namespace=chapter10-bog </strong>
</pre>
<p>We are configuring the namespace of the application and the IP address block allocated to the network. If we look at the file, which contains the VPC blocks, we can see how this is used.</p>
<p>The <kbd>vpc/vpc.tf</kbd> file contains the following content:</p>
<pre>
# Create a VPC to launch our instances into 
resource "aws_vpc" "default" { 
  cidr_block           = "${var.vpc_cidr_block}" 
  enable_dns_hostnames = true 
 
  tags { 
    "Name" = "${var.namespace}" 
  } 
} 
</pre>
<p>A <kbd>resource</kbd> block is a Terraform syntax, which defines a resource in AWS and has the following syntax:</p>
<pre>
resource "[resource_type]" "[resource_id]" { 
    [config] 
} 
</pre>
<p>Resources in Terraform map to the objects needed for the API calls in the AWS SDK. If you look at the <span class="MsoIntenseEmphasis"><kbd>cidr_block</kbd> attribute</span>, you will see that we are referencing the variable using the Terraform interpolation syntax:</p>
<pre>
cidr_block = "${var.vpc_cidr_block}" 
</pre>
<p>Interpolation syntax is a metaprogramming language inside of Terraform. It allows you to manipulate variables and the output from resources and is defined using the <kbd><span class="MsoIntenseEmphasis">${[interpolation]}</span></kbd> syntax. We are using the variables collection, which is prefixed by <kbd><span class="MsoIntenseEmphasis">var</span></kbd> and references the <kbd><span class="MsoIntenseEmphasis">vpc_cidr_block</span></kbd> variable. When Terraform runs <kbd><span class="MsoIntenseEmphasis">${var.vpc_cidr_block}</span></kbd>, it will be replaced with the <kbd><span class="MsoIntenseEmphasis">10.1.0.0/16</span></kbd> value from our variable's file.</p>
<p>Creating a VPC which has external internet access in AWS requires four sections:</p>
<ul>
<li><kbd>aws_vpc</kbd>: This is a private network for our instances</li>
<li><kbd>aws_internet_gateway</kbd>: This is a gateway attached to our VPC to allow internet access</li>
<li><kbd>aws_route</kbd>: This is the routing table entry to map to the gateway</li>
<li><kbd>aws_subnet</kbd>: This is a subnet which our instances launch into--we create one subnet for each availability zone</li>
</ul>
<p>This complexity is not Terraform but AWS. The other cloud providers have very similar complexity, and unfortunately, it is unavoidable. It feels daunting at first, however, there are some amazing resources out there.</p>
<p>The next section of the VPC setup is to configure the internet gateway:</p>
<pre>
# Create an internet gateway to give our subnet access to the outside world 
resource "aws_internet_gateway" "default" { 
  vpc_id = "${aws_vpc.default.id}" 
 
  tags { 
    "Name" = "${var.namespace}" 
  } 
} 
</pre>
<p>Again, we have a similar format as the <kbd><span class="MsoIntenseEmphasis">aws_vpc</span></kbd> block; however, in this block, we need to set the <kbd><span class="MsoIntenseEmphasis">vpc_id</span></kbd> block, which needs to reference the VPC, which we created in the previous block. We can again use the Terraform interpolation syntax to find this reference even though it has not yet been created. The <kbd>aws_vpc.default.id</kbd> reference has the following form which is common across all resources in Terraform:</p>
<pre>
 [resource].[name].[attribute] 
</pre>
<p>When we reference another block in Terraform, it also tells the dependency graph that the referenced block needs to be created before this block. In this way, Terraform is capable of organizing which resources can be set up in parallel and those which have an exact order. We do not need to declare the order ourselves when the graph is created it automatically builds this for us.</p>
<p>The next block sets up the routing table for the VPC, <span>enabling the outbound</span> access to the public internet:</p>
<pre>
# Grant the VPC Internet access on its main route table 
resource "aws_route" "internet_access" { 
  route_table_id         = "${aws_vpc.default.main_route_table_id}" 
  destination_cidr_block = "0.0.0.0/0" 
  gateway_id             = "${aws_internet_gateway.default.id}" 
} 
</pre>
<p>Let's take a look at the attributes in this block in a little more detail:</p>
<ul>
<li><kbd>route_table_id</kbd>: This is the reference to the routing table we would like to create a new reference for. We can obtain this from the output attribute <kbd><span class="MsoIntenseEmphasis">main_route_table_id</span></kbd> from <span class="MsoIntenseEmphasis"><kbd>aws_vpc</kbd>.</span></li>
<li><kbd>destination_cidr_block</kbd>: This is the IP range of the instances which will be connected to the VPC who can send traffic to the gateway. We are using the block <span class="MsoIntenseEmphasis"><kbd>0.0.0.0/0</kbd>,</span> which allows all the connected instances. If required, we could only allow external access to certain IP ranges.</li>
<li><kbd>gateway_id</kbd>: This is a reference to the gateway block we previously created.</li>
</ul>
<p>The next block introduces a new concept for us data sources. Data sources allow data to be fetched or computed from information stored outside Terraform or stored in separate Terraform configuration. A data source may look up information in AWS, for example, you can query a list of existing EC2 instances, which may exist in your account. You can also query other providers, for instance, you have a DNS entry in CloudFlare, which you would like the details for or even the address of a load balancer in a different cloud provider, such as Google or Azure.</p>
<p>We will use it to retrieve the lists of availability zones in AWS. When we create the VPC, we need to create a subnet in each availability zone, because we are only configuring the region, we have not set the availability zones for that region. We could explicitly configure these in the variables section; however, that makes our config more brittle. The best way, whenever possible, is to use data blocks:</p>
<pre>
# Grab the list of availability zones 
data "aws_availability_zones" "available" {} 
</pre>
<p>The configuration is quite simple and again follows a common syntax:</p>
<pre>
data [resource] "[name]" 
</pre>
<p>We will make use of this information in the final part of the VPC setup, which is to configure the subnets; this also introduces another new Terraform feature <kbd>count</kbd>:</p>
<pre>
# Create a subnet to launch our instances into 
resource "aws_subnet" "default" { 
  count                   = "${length(var.cidr_blocks)}" 
  vpc_id                  = "${aws_vpc.default.id}" 
  availability_zone       = "${data.aws_availability_zones.available.names[count.index]}" 
  cidr_block              = "${var.cidr_blocks[count.index]}" 
  map_public_ip_on_launch = true 
 
  tags { 
    "Name" = "${var.namespace}" 
  } 
} 
</pre>
<p>Let's look closely at the <kbd>count</kbd> attribute; a <kbd>count</kbd> attribute is a special attribute, which when set creates <em><span class="MsoIntenseEmphasis">n</span></em> instances of the resource. The value of our attribute also expands on the interpolation syntax that we examined earlier to introduce the <kbd><span class="MsoIntenseEmphasis">length</span></kbd> function:</p>
<pre>
# cidr_blocks = ["10.1.1.0/24", 10.1.2.0/24"] 
${length(var.cidr_blocks)} 
</pre>
<p>The <kbd>cidr_blocks</kbd> is a Terraform list. In Go, this would be a slice and the length will return the number of elements inside a list. For comparison, let's look at how we would write this in Go:</p>
<pre>
cidrBlocks := []string {"10.1.1.0/24", "10.1.2.0/24"} 
elements := len(cidrBlocks) 
</pre>
<p>Interpolation syntax in Terraform is an amazing feature, allowing you to manipulate variables with many built-in functions. The documentation for the interpolation syntax can be found at the following location:</p>
<p><a href="https://www.terraform.io/docs/configuration/interpolation.html"><span class="URLPACKT">https://www.terraform.io/docs/configuration/interpolation.html</span></a></p>
<p>We also have the capability of using conditional statements. One of the best features of the <kbd>count</kbd> function is that if you set it to <kbd>0</kbd>, Terraform omits creation of a resource; as an example, it would allow us to write something like the following:</p>
<pre>
resource "aws_instance" "web" { 
  count = "${var.env == "production" ? 1 : 0}" 
} 
</pre>
<p>The syntax for conditionals uses the ternary operation, which is present in many languages:</p>
<pre>
CONDITION ? TRUEVAL : FALSEVAL 
</pre>
<p>When we use the <kbd>count</kbd> Terraform, it also provides us with an index, which we can use to obtain the correct element from a list. Consider how we are using this in the <kbd>availability_zone</kbd> attribute:</p>
<pre>
availability_zone = "${data.aws_availability_zones.available.names[count.index]}" 
</pre>
<p><span class="MsoIntenseEmphasis">The <kbd>count.index</kbd></span> will provide us with a <kbd>0</kbd> based index and because <kbd><span class="MsoIntenseEmphasis">data.aws_availability_zones.available.names</span></kbd> returns a list, we can use to access this like a slice. Let's take a look at the remaining attributes on our <span class="MsoIntenseEmphasis"><kbd>aws_subnet</kbd>:</span></p>
<ul>
<li><kbd>vpc_id</kbd>: This is the ID of the VPC, which we created in an earlier block and we would like to attach the subnet</li>
<li><kbd>availability_zone</kbd>: This is the name of the availability zone for the subnet</li>
<li><kbd>cidr_block</kbd>: This is the IP range of addresses, which will be given to an instance when we launch it in this particular VPC and availability zone</li>
<li><kbd>map_public_ip_on_launch</kbd>: Whether we should attach a public IP address to the instance when it is created, this is an optional parameter, and determines whether your instance should also have a public IP address in addition to the private one, which is allocated from the <kbd>cidr_block</kbd> attribute</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Output variables</h1>
                </header>
            
            <article>
                
<p>When we are building modules in Terraform, we often need to reference attributes from other modules. There is a clean separation between modules, which means that they cannot directly access another module resources. For example, in this module, we are creating a VPC, and later on, we would like to create an EC2 instance, which is attached to this VPC. We could not use the syntax as shown in the upcoming code.</p>
<p>The <kbd>module2/terraform.tf</kbd> file contains the following content:</p>
<pre>
resource "aws_instance" "web" { 
# ... 
    vpc_id = "${aws_vpc.default.id}" 
} 
</pre>
<p>The previous example would result in an error as we are trying to reference a variable which does not exist in this module even though it does exist in your global Terraform config. Consider these to be like Go packages. If we had the two following Go packages which contained non-exported variables:</p>
<p><kbd>a/main.go</kbd></p>
<pre>
package a 
 
var notExported = "Some Value" 
</pre>
<p><kbd>b/main.go</kbd></p>
<pre>
package b 
 
func doSomething() { 
    // invalid reference 
    if a.notExported == "Some Value { 
        //... 
    } 
} 
</pre>
<p>In Go we could, of course, have the variable exported by capitalizing the name of the variable <kbd><span class="MsoIntenseEmphasis">notExported</span></kbd> to <kbd><span class="MsoIntenseEmphasis">NotExported</span></kbd>. To achieve the same in Terraform, we use output variables:</p>
<pre>
output "id" { 
  value = "${aws_vpc.default.id}" 
} 
 
output "subnets" { 
  value = ["${aws_subnet.default.*.id}"] 
} 
 
output "subnet_names" { 
  value = ["${aws_subnet.default.*.arn}"] 
} 
</pre>
<p>The syntax should be starting to get familiar to you now:</p>
<pre>
output "[name]" { 
    value = "..." 
} 
</pre>
<p>We can then use the output of one module to be the input of another--an example found in the <span class="MsoIntenseEmphasis"><kbd>terraform.tf</kbd> file:</span></p>
<pre>
module "nats" { 
  source = "./nats" 
 
  application_name        = "nats" 
  application_description = "Nats.io server" 
  application_environment = "dev" 
 
  deployment_bucket    = "${module.s3.deployment_bucket}" 
  deployment_bucket_id = "${module.s3.deployment_bucket_id}" 
 
  application_version = "1.1" 
  docker_image        = "nats" 
  docker_tag          = "latest" 
 
  elb_scheme   = "internal" 
  health_check = "/varz" 
 
  vpc_id  = "${module.vpc.id}" 
  subnets = ["${module.vpc.subnets}"] 
} 
</pre>
<p>The <kbd><span class="MsoIntenseEmphasis">vpc_id</span></kbd> attribute is referencing an output from the <span class="MsoIntenseEmphasis"><kbd>vpc</kbd> module:</span></p>
<pre>
vpc_id  = "${module.vpc.id}" 
</pre>
<p>The syntax for the preceding statement is as follows:</p>
<pre>
module.[module name].[output variable] 
</pre>
<p>In addition to allowing us to keep our code dry and clean, output variables and module references allow Terraform to build its dependency graph. In this instance, Terraform knows that because there is a reference to the <kbd><span class="MsoIntenseEmphasis">vpc</span></kbd> module from the <kbd><span class="MsoIntenseEmphasis">nats</span></kbd> module, it needs to create the <kbd><span class="MsoIntenseEmphasis">vpc</span></kbd> module resources before the <kbd><span class="MsoIntenseEmphasis">nats</span></kbd> module. This might feel like a lot of information and it is. I did not say infrastructure as code was easy, but by the time we get to the end of this example, it will start to become clear. Applying these concepts to create other resources becomes quite straightforward with the only complexity being how the resource works, not the Terraform configuration which is needed to create it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the infrastructure</h1>
                </header>
            
            <article>
                
<p>To run Terraform and to create our infrastructure, we must first set some environment variables:</p>
<pre>
<strong>$ export AWS_SECRET_ID=[your aws secret id] 
$ export AWS_SECRET_ACCESS_KEY=[your aws access key] 
$ export AWS_DEFAULT_REGION=[aws region to create resource] </strong>
</pre>
<p>We then need to initialize Terraform to reference the modules and remote data store. We normally only need to perform this step whenever we first clone the repository or if we make changes to the modules:</p>
<pre>
<strong>$ terraform init</strong>  
</pre>
<p>The next step is to run a plan; we use the plan command in Terraform to understand which resources are created, updated, or deleted by the <kbd>apply</kbd> command. It will also syntax check our config without creating any resources:</p>
<pre>
<strong>$ terraform plan -out=main.terraform</strong>  
</pre>
<p>The <kbd><span class="MsoIntenseEmphasis">-out</span></kbd> argument saves the plan to <span class="MsoIntenseEmphasis"><kbd>main.terraform</kbd> file.</span> This is an optional step, but if we run <kbd>apply</kbd> with the output from the plan, we can ensure that nothing changes from when we inspected and approved the output of the <kbd>plan</kbd> command. To create the infrastructure, we can then run the <kbd><span class="MsoIntenseEmphasis">apply</span></kbd> command:</p>
<pre>
<strong>$ terraform apply main.terraform</strong>  
</pre>
<p>The first argument to the <kbd>apply</kbd> command is the <kbd>plan</kbd> output, which we created in the previous step. Terraform now creates your resources in AWS, this can take anything from a few seconds to 30 minutes depending upon the type of resource you are creating. Once the creation is complete, Terraform writes the output variables, which we defined in the <kbd><span class="MsoIntenseEmphasis">output.tf</span></kbd> file to <kbd>stdout</kbd>.</p>
<p>We have only covered one of the modules in our main infrastructure project. I recommend that you read through the remaining modules and familiarize yourself with both the Terraform code and the AWS resources it is creating. Excellent documentation is available on the Terraform website (<a href="https://terraform.io"><span class="URLPACKT">https://terraform.io</span></a>) and the AWS website.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example application</h1>
                </header>
            
            <article>
                
<p>Our sample application is a simple distributed system consisting of three services. The three main services, product, search and authentication, have a dependency on a database which they use to store their state. For simplicity, we are using MySQL; however, in a real production environment, you would want to choose the most appropriate data store for your use case. The three services are connected via the messaging system for which we are using NATS.io, which is a provider-agnostic system, which we looked at in <a href="2952a830-163e-4610-8554-67498ec77e1e.xhtml" target="_blank">C</a><span class="MsoIntenseEmphasis">hapter 9, <em>Event-Driven Architecture</em></span>.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="268" width="468" class=" image-border" src="assets/88b91630-a430-4c82-92c3-b6236f8d42fd.png"/></div>
<p>To provision this system, we have broken down the infrastructure and source code into four separate repositories:</p>
<ul>
<li><strong>Shared infrastructure and services</strong> (<a href="https://github.com/building-microservices-with-go/chapter11-services-main">https://github.com/building-microservices-with-go/chapter11-services-main</a>)</li>
<li><strong>Authentication service</strong> (<a href="https://github.com/building-microservices-with-go/chapter11-services-auth">https://github.com/building-microservices-with-go/chapter11-services-auth</a>)</li>
<li><strong>Product service</strong> (<a href="https://github.com/building-microservices-with-go/chapter11-services-product">https://github.com/building-microservices-with-go/chapter11-services-product</a>)</li>
<li><strong>Search service</strong> (<span class="URLPACKT"><a href="https://github.com/building-microservices-with-go/chapter11-services-search">https://github.com/building-microservices-with-go/chapter11-services-search</a>)</span></li>
</ul>
<p>The individual repositories enable us to separate our application in such a way that we only build and deploy the components that change. The shared infrastructure repository contains Terraform configuration to create a shared network and components to create the NATS.io server. The authentication service creates a JWT-based authentication microservice and contains separate Terraform configuration to deploy the service to Elastic Beanstalk. The product service and the search service repositories also each contain a microservice and Terraform infrastructure configuration. All the services are configured to build and deploy using Circle CI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous delivery workflow</h1>
                </header>
            
            <article>
                
<p>For the remainder of this chapter, we concentrate on the search service as the build pipeline is the most complex. In our example application, we have the following steps which to build a pipeline:</p>
<ul>
<li>Compile application</li>
<li>Unit test</li>
<li>Benchmark</li>
<li>Static code analysis</li>
<li>Integration test</li>
<li>Build Docker image</li>
<li>Deploy application</li>
<li>Smoke test</li>
</ul>
<p>Many of these steps are independent and can run in parallel, so when we compose the pipeline, it looks like the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="453" width="617" class=" image-border" src="assets/bd2372fa-3304-48e9-ad93-b0e66e5a71ab.png"/></div>
<p>Take a look at the example code at <a href="https://github.com/building-microservices-with-go/chapter11-services-auth"><span class="URLPACKT">https://github.com/building-microservices-with-go/chapter11-services-auth</span></a>. We are building this application with Circle CI; however, the concepts apply to whatever platform you use. If we look at the <span class="MsoIntenseEmphasis"><kbd>circleci/config.yml</kbd> file</span>, we see that we are first setting up the configuration for the process, which includes choosing the version of the Docker container within which the build executes and install some initial dependencies. We then compose the jobs, which are performed in the workflow and the various steps for each of the jobs:</p>
<pre>
defaults: &amp;defaults 
  docker: 
    # CircleCI Go images available at: https://hub.docker.com/r/circleci/golang/ 
    - image: circleci/golang:1.8 
 
  working_directory: /go/src/github.com/building-microservices-with-go/chapter11-services-search 
 
  environment: 
    TEST_RESULTS: /tmp/test-results 
 
version: 2 
jobs: 
  build: 
    &lt;&lt;: *defaults 
 
    steps: 
      - checkout 
 
      - run:  
          name: Install dependencies 
          command: | 
            go get github.com/Masterminds/glide 
            glide up 
       
      - run: 
          name: Build application for Linux  
          command: make build_linux 
           
      - persist_to_workspace: 
          root: /go/src/github.com/building-microservices-with-go/ 
          paths: 
            - chapter11-services-search 
       
# ... 
  
workflows: 
  version: 2 
  build_test_and_deploy: 
    jobs: 
      - build 
      - unit_test: 
          requires: 
            - build 
      - benchmark: 
          requires: 
            - build 
      - staticcheck: 
          requires: 
            - build 
      - integration_test: 
          requires: 
            - build 
            - unit_test 
            - benchmark 
            - staticcheck 
      - deploy: 
          requires: 
            - integration_test 
</pre>
<p>Finally, we will compose these jobs into a workflow or a pipeline. This workflow defines the relationship between the steps as there are obvious dependencies.</p>
<p>To isolate dependencies in our configuration and to ensure that the commands for building and testing are consistent across various processes, the commands have been placed into the <span class="MsoIntenseEmphasis">Makefile</span> in the root of the repository.</p>
<pre>
start_stack: 
    docker-compose up -d 
 
circleintegration: 
    docker build -t circletemp -f ./IntegrationDockerfile .     
    docker-compose up -d 
    docker run -network chapter11servicessearch_default -w /go/src/github.com/building-microservices-with-go/chapter11-services-search/features -e "MYSQL_CONNECTION=root:password@tcp(mysql:3306)/kittens" circletemp godog ./ 
    docker-compose stop 
    docker-compose rm -f 
 
integration: start_stack 
    cd features &amp;&amp; MYSQL_CONNECTION="root:password@tcp(${DOCKER_IP}:3306)/kittens" godog ./ 
    docker-compose stop 
    docker-compose rm -f 
 
unit: 
    go test -v -race $(shell go list ./... | grep -v /vendor/) 
 
staticcheck: 
    staticcheck $(shell go list ./... | grep -v /vendor/) 
 
safesql: 
    safesql github.com/building-microservices-with-go/chapter11-services-search 
 
benchmark: 
    go test -bench=. github.com/building-microservices-with-go/chapter11-services-search/handlers 
 
build_linux: 
    CGO_ENABLED=0 GOOS=linux go build -o ./search . 
 
build_docker: 
    docker build -t buildingmicroserviceswithgo/search . 
 
run: start_stack 
    go run main.go 
    docker-compose stop 
 
test: unit benchmark integration 
</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Build</h1>
                </header>
            
            <article>
                
<p>Let's take a closer look at the build process. Inside the build job configuration, we have three steps. The first is to <span class="MsoIntenseEmphasis">check out</span> the repository. The jobs themselves are broken up into steps, and the first notable one of these is to install the dependencies. Glide is our package manager for the repository, and we need this to be installed to fetch updates to our vendored packages. We also need a <kbd>go-junit-report</kbd> utility package. This application allows us to convert the Go test output into JUnit format, which Circle requires for presenting certain dashboard information. We then execute <kbd><span class="MsoIntenseEmphasis">glide up</span></kbd> to fetch any updates. In this example, I have checked in the <kbd><span class="MsoIntenseEmphasis">vendor</span></kbd> folder to the repository; however, I am not pinning the packages to a version. You should set a minimum package version rather than an exact package version, updating your packages frequently allows you to take advantage of regular releases in the open source community. You do of course run the risk that there will be a breaking change in a package and that change breaks the build, but as mentioned earlier, it is better to catch this as soon as possible rather than deal with the problem when you are under pressure to release.</p>
<p>Because we are building for production, we need to create a <span class="MsoIntenseEmphasis">Linux</span> binary, which is why we are setting the <kbd><span class="MsoIntenseEmphasis">GOOS=linux</span></kbd> environment variable before running the build. Setting the environment is redundant when we are running the build on Circle CI as we are already running in a Linux-based Docker container; however, to enable cross-platform builds from our developer machines, if they are not Linux-based, it is useful to have a common command.</p>
<p>Once we have built our application, we need to persist the workspace so that the other jobs can use this. In Circle CI, we use the special step <kbd><span class="MsoIntenseEmphasis">persist_to_workspace</span></kbd>; however, this capability is common to pipeline-based workflows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="306" width="697" class=" image-border" src="assets/93745586-831c-4ac4-a792-f60b688bd807.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>We also mentioned the fact that we need consistency and that, if we are deploying continuously, we need to have a good solid testing suite, which replaces almost all our manual testing. I am not saying there is no place for manual testing as there is always a use for exploratory testing but when we are deploying continuously, we need to automate all of this. Even if you are adding manual testing into your process, it will be most likely running as an asynchronous process complimentary to your build pipeline not as a gate to it.</p>
<p>The testing section of the configuration runs our unit tests as we saw in <a href="2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml" target="_blank">Chapter 4</a>, <em>Testing</em>. With the following configuration, we first need to attach the workspace that we created in the build step. The reason for this is that we do not need to check out the repository again.</p>
<pre>
unit_test: 
    &lt;&lt;: *defaults 
     
    steps: 
      - attach_workspace: 
          at: /go/src/github.com/building-microservices-with-go 
       
      - run: mkdir -p $TEST_RESULTS 
       
      - run:  
          name: Install dependencies 
          command:  go get github.com/jstemmer/go-junit-report 
           
      - run:  
          name: Run unit tests 
          command: | 
            trap "go-junit-report &lt;${TEST_RESULTS}/go-test.out &gt; ${TEST_RESULTS}/go-test-report.xml" EXIT 
            make unit | tee ${TEST_RESULTS}/go-test.out 
 
      - store_test_results: 
          path: /tmp/test-results 
</pre>
<p>The second thing we need to do is to install the dependencies, Circle CI requires the output of the tests to be in JUnit format for presentation. To enable this, we can fetch the <kbd><span class="MsoIntenseEmphasis">go-junit-report</span></kbd> package, which can take the output of our tests and convert them into JUnit format.</p>
<p>To run the tests, we have to do something slightly different, if we just ran our unit tests and piped them into the <kbd><span class="MsoIntenseEmphasis">go-junit-report</span></kbd> command, then we would lose the output. Reading the command in reverse order, we run our unit tests and output, <span class="MsoIntenseEmphasis"><kbd>make unit | tee ${TEST_RESULTS}/go-test.out</kbd>;</span> the <kbd><span class="MsoIntenseEmphasis">tee</span></kbd> command takes the input piped to it and writes to both the output file specified as well as to the <span class="MsoIntenseEmphasis"><kbd>stdout</kbd> file</span>. We can then use <span class="MsoIntenseEmphasis">trap,</span> which executes a command when an exit code is matched from another command. In our instance, if the unit tests exit with a status code 0 (normal condition), then we execute the <kbd><span class="MsoIntenseEmphasis">go-junit-report</span></kbd> command. Finally, we write our test results for Circle CI to be able to interpret them using the <kbd><span class="MsoIntenseEmphasis">store_test_results</span></kbd> step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benchmarking</h1>
                </header>
            
            <article>
                
<p>Benchmarking is an important feature for our CI pipeline; we need to understand when the performance of our application degrades. To do this, we are going to both run our benchmark tests and use the handy tool <strong><span class="MsoIntenseEmphasis">benchcmp</span></strong>, which compares two runs of tests. The standard version of <span class="MsoIntenseEmphasis">benchcmp</span> only outputs the difference between two test runs. While this is fine for comparison, it does not give us the capability to fail our CI job should this difference be within a certain threshold. To enable this capability, I have forked the benchcmp tool and added <kbd>flag<span class="MsoIntenseEmphasis">-tollerance=[FLOAT]</span></kbd>. If any of the benchmarks change +/- the given tolerance, then <span class="MsoIntenseEmphasis">benchcmp</span> exits with status code 1, allowing us to fail the job and investigate why this change has taken place. For this to work, we need to keep the previous benchmark data available for comparison, so we can use the caching feature to store the last run data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static code analysis</h1>
                </header>
            
            <article>
                
<p>Static code analysis is a fast and efficient way to check for any problems in our source code automatically. In our example, we will run two different static code analysis tools, the first is <strong><span class="MsoIntenseEmphasis">megacheck</span></strong> by Dominik Honnef, which examines the code for common problems such as misuse of the standard library, concurrency issues, and many more problems.</p>
<p>Second is <strong>S<span class="MsoIntenseEmphasis">afeSQL</span></strong> from the Stripe team. S<span class="MsoIntenseEmphasis">afeSQL</span> runs through our code and looks for uses of the SQL package. It then examines the ones looking for vulnerabilities such as incorrectly constructed queries, which may be open to SQL injection.</p>
<p>Lastly, we will be checking our code, including the tests for unhandled errors, for example, you have the following function:</p>
<pre>
func DoSomething() (*Object, error) 
</pre>
<p>When invoking a method like this, the error object can be thrown away and not handled:</p>
<pre>
obj, _ := DoSomething() 
</pre>
<p>Unhandled errors are more often found in tests rather than the main body of code; however, even in tests, this could introduce a bug due to unhandled behavior, <kbd><span class="MsoIntenseEmphasis">errcheck</span></kbd> runs through the code looking for instances like this and reports an error when found and fails the build:</p>
<pre>
staticcheck: 
    &lt;&lt;: *defaults 
 
    steps: 
      - attach_workspace: 
          at: /go/src/github.com/building-microservices-with-go 
 
      - run: 
          name: Install dependencies 
          command: | 
            go get honnef.co/go/tools/cmd/staticcheck 
            go get github.com/stripe/safesql 
 
      - run: 
          name: Static language checks 
          command: make staticcheck 
 
      - run: 
          name: Safe SQL checks 
          command: make safesql 
 
        - run: 
          name: Check for unhandled errors 
          command: make errcheck 
</pre>
<p>Static check invokes the <span class="MsoIntenseEmphasis">megacheck</span> linter which runs <kbd><span class="MsoIntenseEmphasis">staticcheck</span></kbd> a static code analysis tool which helps to detect bugs, G<span class="MsoIntenseEmphasis">o simple</span> which identifies areas of the source code which should be improved by re-writing in a simpler way and unused which identifies unused constants, types, and functions. The first checker is designed to spot bugs; however, the remaining three are concerned with your application life cycle management.</p>
<p>Clean code is essential to bug-free code; the easier and the simpler your code is the more the reduced likelihood that you have logic bugs. Why? Because the code is easier to understand and since you spend more of your time reading code than writing, it makes sense to optimize for readability. Static code analysis should not be a replacement for the code review. However, these tools allow you to focus on logic flaws rather than semantics. Integrating this into your continuous integration pipeline acts as a gatekeeper to the sanity of your codebase, the checks run incredibly quickly and in my humble opinion are an essential step.</p>
<p><a href="https://github.com/dominikh/go-tools/tree/master/cmd/megacheck"><span class="URLPACKT">https://github.com/dominikh/go-tools/tree/master/cmd/megacheck</span></a></p>
<p>SafeSQL from the Stripe team is a static code analysis tool which protects against SQL injections. It attempts to find problems with usage of the <span class="MsoIntenseEmphasis"><kbd>database/sql</kbd> package.</span></p>
<p><a href="https://github.com/stripe/safesql"><span class="URLPACKT">https://github.com/stripe/safesql</span></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration tests</h1>
                </header>
            
            <article>
                
<p>Then, there are integration tests. In this example, we are again using GoDog BDD; however, when we are running on Circle CI, we need to modify our setup a little because of the way that Circle deals with security for Docker. The first steps are again to attach the workspace, including the binary that we built in a previous step; then we can get the dependencies which are only the GoDog application. The <kbd><span class="MsoIntenseEmphasis">setup_remote_docker</span></kbd> command requests a Docker instance from Circle CI. The current build is running in a Docker container; however, because of the security configuration, we cannot access the Docker host, which is currently running the current build.</p>
<pre>
circleintegration: 
    docker build -t circletemp -f ./IntegrationDockerfile .     
    docker-compose up -d 
    docker run -network chapter11servicessearch_default -w /go/src/github.com/building-microservices-with-go/chapter11-services-search/features -e "MYSQL_CONNECTION=root:password@tcp(mysql:3306)/kittens" circletemp godog ./ 
    docker-compose stop 
    docker-compose rm -f 
</pre>
<p>The section of the Makefile for running on CI is quite a bit more complex than when we run it on our local machine. We need this modification because we need to copy the source code and install the <kbd>godog</kbd> command to a container, which will be running on the same network as the stack we start with Docker compose. When we are running locally, this is not necessary as we have the capability to connect to the network. This access is forbidden on Circle CI and most likely other shared continuous integration environments.</p>
<pre>
FROM golang:1.8 
 
COPY . /go/src/github.com/building-microservices-with-go/chapter11-services-search 
RUN go get github.com/DATA-DOG/godog/cmd/godog 
</pre>
<p>We build our temporary container, which contains the current directory and adds the <kbd>godog</kbd> dependency. We can then start the stack as normal by running <kbd><span class="MsoIntenseEmphasis">docker-compose up</span></kbd> and then the <span class="MsoIntenseEmphasis"><kbd>godog</kbd> command</span>.</p>
<p>Integration tests on continuous delivery are an essential gate before we deploy to production. We also want to be able to test our Docker image to ensure that the startup process is functioning correctly and that we have tested all our assets. When we looked at integration tests in <span class="MsoIntenseEmphasis"><a href="2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml" target="_blank">Chapter 4</a>, <em>Testing</em>,</span> we were only running the application, which is fine for our development process--it gives us the happy medium of quality and speed. When it comes to building our production images, however, this compromise is not acceptable, and therefore, we need to make some modifications to the development process to ensure that we include the production image in our test plan.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Since we have all our application code build tested and packaged, it is time to think about deploying this into production. We need to start thinking about our infrastructure as immutable, that is, we will not make changes to the infrastructure but replace it. The level with which this occurs can be multiple. For example, we have our container scheduler, which only runs the containers. When we deploy an update to our application binary, we are replacing a container on the scheduler not refreshing the application in it. Containers give us one level of immutability, the other is the scheduler itself. To operate successful continuous delivery, the setup of this facet also needs to be automated, we need to think of our infrastructure as code.</p>
<p>For our application, we are splitting the infrastructure up into separate parts. We have a main infrastructure repository, which creates the VPC, S3 buckets used by deployments and creates an Elastic Beanstalk instance for our messaging platform NATS.io. We also have Terraform config for each of the services. We could create one massive Terraform config as Terraform replaces or destroys infrastructure, which has changed; however, there are several reasons why we would not want this. The first is that we want to be able to break down our infrastructure code into small parts in the same way we break up our application code; the second is due to the way Terraform works. To ensure the consistency of the state, we can only run a single operation against the infrastructure code at any one time. Terraform obtains a lock when it runs to ensure that you cannot run it multiple times at once. If we consider a situation where there are many microservices and that these services are being continuously deployed, then having a single deployment which is single threaded becomes a terrible thing. When we decompose the infrastructure configuration and localize it with each service, then this no longer becomes a problem. One problem with this distributed configuration is that we still need a method of accessing resource information in the master repository. In our case, we are creating the main VPC in this repository, and we need the details to be able to connect our microservices to it. Thankfully, Terraform manages rather pleasantly using the concept of remote state.</p>
<pre>
terraform { 
  backend "s3" { 
    bucket = "nicjackson-terraform-state" 
    key    = "chapter11-main.tfstate" 
    region = "eu-west-1" 
  } 
} 
</pre>
<p>We can configure our master Terraform config to use remote state, which we can then access from the search Terraform config using the remote state data element:</p>
<pre>
data "terraform_remote_state" "main" { 
  backend = "s3" 
 
  config { 
    bucket = "nicjackson-terraform-state" 
    key    = "chapter11-main.tfstate" 
    region = "eu-west-1" 
  } 
} 
</pre>
<p>When all the previous steps in the build process complete, we deploy this to AWS automatically. This way we always deploy every time a new instance of the master branch builds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Smoke tests</h1>
                </header>
            
            <article>
                
<p>Smoke testing the application post deploy is an essential step in continuous delivery we need to ensure that the application is functioning correctly and that nothing has gone wrong in the build and deploy steps. In our example, we are simply checking that we can reach the health endpoint. However, a smoke test can be as simple or as complex as required. Many organizations run more detail checks, which confirm that the core integration to the deployed system is correct and functioning. The smoke tests are conducted as either a codified test re-using many of the steps in the GoDog integration tests or a specialized test. In our example, we are simply checking the health endpoint for the search service.</p>
<pre>
- run: 
          name: Smoke test 
          command: | 
            cd terraform 
            curl $(terraform output search_alb)/health 
</pre>
<p>In our application, we can run this test because the endpoint is public. When an endpoint is not public, testing becomes more complicated, and we need to check the integration by calling through a public endpoint.</p>
<p>One of the considerations for end-to-end testing is that you need to be careful of polluting the data inside the production database. A complimentary or even alternative approach is to ensure that your system has extensive logging and monitoring. We can set up dashboards and alerts, which actively check for user errors. When an issue occurs post deploy, we can investigate the problem, and if necessary, rollback to a previous version of the build with a known good state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring/alerting</h1>
                </header>
            
            <article>
                
<p>When the application is running, we need to be sure of the health and status of the application. Monitoring is an incredibly important facet of the continuous deployment life cycle. If we are deploying automatically, we need to understand how our application is performing and how this differs from the previous release. We have seen how we can use StatsD to emit data about our service to a backend such as Prometheus or a managed application such as Datadog. Should our recent deploy exhibit anomalous behavior, we are alerted about this and from there we can act to help identify the source of the problem, intermittently rolling back if necessary or modifying our alerts as our server may just be doing more work.</p>
<pre>
# Create a new Datadog timeboard 
resource "datadog_timeboard" "search" { 
  title       = "Search service Timeboard (created via Terraform)" 
  description = "created using the Datadog provider in Terraform" 
  read_only   = true 
 
  graph { 
    title = "Authentication" 
    viz   = "timeseries" 
 
    request { 
      q    = "sum:chapter11.auth.jwt.badrequest{*}" 
      type = "bars" 
 
      style { 
        palette = "warm" 
      } 
    } 
 
    request { 
      q    = "sum:chapter11.auth.jwt.success{*}" 
      type = "bars" 
    } 
  } 
 
  graph { 
    title = "Health Check" 
    viz   = "timeseries" 
 
    request { 
      q    = "sum:chapter11.auth.health.success{*}" 
      type = "bars" 
    } 
  } 
} 
</pre>
<p>Again, using the concepts of infrastructure as code, we can provision these monitors at build time using Terraform. While errors are useful for monitoring, it is also important to not forget timing data. An error tells you that something is going wrong; however, with the clever use of timing information in the service, we can learn that something is about to go wrong.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Complete workflow</h1>
                </header>
            
            <article>
                
<p>Assuming all is functioning well, we should have a successful build, and the UI in our build environment should show all steps passing. Remember our warning from the beginning of this chapter--when your build fails, you need to make it your primary objective to fix it; you never know when you are going to need it.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/345f20fb-eaf2-4666-9081-45a2eb830e63.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned that it need not be an arduous task to set up continuous integration and deployment for your application, and in fact, this is essential to the health and success of your application. We have built on all the concepts covered in the previous chapters, and while the final example is somewhat simple, it has all the constituent parts for you to build into your applications to ensure that you spend your time developing new features and not fixing production issues or wasting time repetitively and riskily deploying application code. Like all aspects of our development, we should practice and test this process. Before releasing continuous delivery to your production workflow, you need to ensure that you can deal with problems such as hot fixing and rolling back a release. This activity should be completed across teams and depending on your process for out-of-hours support should also involve any support staff. A well-practiced and functioning deployment process gives you the confidence that when an issue occurs, and it most likely will, you can comfortably and confidently deal with it.</p>
<p>I hope that by working through this book, you now have a greater understanding of most of the things you need to build microservices with go successfully. The one thing I cannot teach is the experience that you need to find out for yourself by getting out there and performing. I wish you luck on this journey, and the one thing that I have learned from my career is that you never regret putting in the time and effort to learn these techniques. I am sure you will be hugely successful.</p>


            </article>

            
        </section>
    </body></html>