- en: Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot so far, including how to build resilient systems and how
    to keep them secure, but now we need to look at how to replace all the manual
    steps in our process and introduce continuous delivery.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immutable infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example Application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Continuous Delivery?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous delivery is the process of building and deploying code well, continuously.
    The aim is that we move code from development to production as efficiently and
    effectively as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional or waterfall workflow, releases revolve around the completion
    of a major feature or update. It is not untypical for large enterprises to release
    once a quarter. When we look at the reason for this strategy, risk and effort
    are often cited. There is a risk to releasing as the confidence is weak in the
    software; there is effort involved in releasing because there needs to be a mostly
    manual process involved in quality assurance and the operational aspects of releasing
    the software. One part of this is something that we have covered in [C](905299b5-e8d4-424c-827e-3fed5a58289e.xhtml)hapter
    5, *Common Patterns*, which is the concern with quality, and the possible absence
    of a satisfactory test suite or possibly the ability to run this automatically.
    The second element involves the physical deployment and post deployment testing
    process. We have not covered this aspect much in this book so far; we touched
    on it when we looked at Docker in [Chapter 4](2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml),
    *Testing*.
  prefs: []
  type: TYPE_NORMAL
- en: If we could reduce the risk and effort involved in deploying code, would you
    do it more frequently? How about every time you complete a minor feature, or every
    time a bug is fixed, several times a day even? I would encourage you to do just
    that, and in this chapter, we will look at all the things we need to know and
    building on all the things we have previously learned to deliver continuously.
  prefs: []
  type: TYPE_NORMAL
- en: Manual deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manual deployment is at best problematic; even if you have an amazing team,
    things can and will go wrong. The larger the group, the more distributed the knowledge
    and the greater the need for comprehensive documentation. In a small team, the
    resources are constrained, and the time it takes for deployment can be a distraction
    from building great code. You also end up with a weak link; so, do you suspend
    deployment when the person who usually carries out the process is sick or on holiday?
  prefs: []
  type: TYPE_NORMAL
- en: The problems with manual deployment
  prefs: []
  type: TYPE_NORMAL
- en: Issues can arise with the ordering and timing of the various deployment steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation needs to be comprehensive and always up to date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a significant reliance on manual testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are application servers with different states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are constant problems with manual deployment due to the preceding points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a system grows in complexity, there are more moving parts, and the steps
    required to deploy the code increase with it. Since the steps to deploy need to
    be carried out in a set order, the process can fast become a burden. Consider
    deploying an update to an application, the application and its dependencies install
    on all instances of the application servers. Often a database schema needs to
    be updated, and there needs to be a clean switch over between the old and the
    new application. Even if you are leveraging the power of Docker, this process
    can be fraught with disaster. As the complexity of the application grows, so does
    the required documentation to deploy the application, and this is often a weak
    link. Documentation takes time to update and maintain, in my personal experience,
    this is the first area which suffers when deadlines are approaching. Once the
    application code is deployed, we need to test the function of the application.
    Assuming the application is manually deployed, it is often assumed that the application
    is also manually tested. A tester would need to run through a test plan (assuming
    there is a test plan) to check that the system is in a functioning state. If the
    system is not functioning, then either the process would need to be reversed to
    roll back to a previous state, or a decision would need to be made to hotfix the
    application and again run through the standard build and deploy cycle. When this
    process falls into a planned release, there is a little more safety as the whole
    team is around for the release. However, what happens when this process takes
    place in the middle of the night as a result of an incident? At best, what happens
    is that the fix is deployed, however, without updating any of the documentation
    or processes. At worst, the application ends up in a worse state than before the
    application code was attempted to be hot fixed. Out of hours, incidents are also
    often carried out by first line response, which is often the infrastructure team.
    I assume that if you are not running continuous deployment, then you will also
    not be following the practice of Developer on call. Also, what about the time
    it takes to do a deploy? What is the financial cost of the entire team taking
    time out to babysit a deployment? What about the motivational and mental productivity
    cost of this process? Have you ever felt the stress due to the uncertainty of
    deploying application code into production?
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery removes these risks and problems.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of continuous delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of continuous delivery is that you plan for these problems and
    spend the up-front work to solve them. Automation of all the steps involved allows
    consistency of operation and is a self-documenting process. No longer do you have
    the requirement for specialized human knowledge, and the additional benefit of
    the removal of the human is that the quality improves due to automation of the
    process. Once we have the automation, improved the quality and speed of our deployments,
    we can then level this up and start to deploy continuously. The benefits of continuous
    delivery are:'
  prefs: []
  type: TYPE_NORMAL
- en: The releases are small and less complicated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between master and feature branch are smaller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are fewer areas to monitor post deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rollbacks are potentially easier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They deliver business value sooner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start to deploy our code in smaller chunks, no longer waiting for the completion
    of a major feature but potentially after every commit. The primary benefit of
    this is that the differences between the master and the feature branches are smaller
    and less time is spent merging code between branches. Smaller changes also create
    fewer areas to monitor post deploy and because of this, should something go wrong
    it is easier to roll back the changes to a known working state. Most important
    of all, it gives you the capability to deliver business value sooner; whether
    this is in the form of a bug or a new feature, the capability is ready for your
    customers to use far earlier than would be available in a waterfall model.
  prefs: []
  type: TYPE_NORMAL
- en: Aspects of continuous delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several important aspects to continuous delivery most of which are
    essential to the success of the process. In this section, we will look at what
    these aspects are before we look at how we can implement them to build our own
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important aspects of continuous delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility and easy setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation of tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation of integration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure as code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security scanning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static code analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoke testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End 2 end testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring - track deployments in metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility and consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have a small doubt, at some point in your career you might have already seen
    this meme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2876a4cd-de1c-4d29-b7ab-e3492425c7c7.png)'
  prefs: []
  type: TYPE_IMG
- en: If you have not, don't worry, I am confident you are going to encounter it at
    some point. *Works on My Machine* why is this meme so popular? Could it be because
    there is a heavy element of truth in it? I certainly know that I have been there
    and many of you have too I am sure. If we are to deliver continuously and by this
    mean as often as possible, then we need to care about consistency and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility is the ability of an entire analysis of an experiment or study
    to be duplicated, either by the same researcher or by someone else working independently.
    *Works on my machine* is not acceptable. If we are to deliver continuously, then
    we need to codify our build process and make sure that our dependencies for software
    and other elements are either minimized or managed.
  prefs: []
  type: TYPE_NORMAL
- en: The other thing that is important is the consistency of our builds. We cannot
    spend time fixing broken builds or manually deploying software, so we must treat
    them with the same regard that we treat our production code. If the build breaks,
    we need to stop the line and fix it immediately, understand why the build broke,
    and if necessary, introduce new safeguards or processes so that it does not occur
    again.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we implement any form of continuous integration, we produce various artifacts
    because of the build process. The artifacts can range from binaries to the output
    of tests. We need to consider how we are going to store this data; thankfully
    cloud computing has many answers to this problem. One solution is cloud storage
    such as AWS S3, which is available in abundance, and at a small cost. Many of
    the software as a service CI providers such as Travis and CircleCI also offer
    this capability built into the system; so for us to leverage it, there is very
    little we need to do. We can also leverage the same storage if, for example, we
    are using Jenkins. The existence of the cloud means we rarely need to worry about
    the management of CI artifacts anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Automation of tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test automation is essential, and to ensure the integrity of the built application,
    we must run our unit tests on the CI platform. Test automation forces us to consider
    easy and reproducible setup, dependencies need minimizing, and we should only
    be checking the behavior and integrity of the code. In this step, we avoid integration
    tests, the tests should run without anything but the go test command.
  prefs: []
  type: TYPE_NORMAL
- en: Automation of integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, we do need to verify the integration between our code and any other
    dependencies such as a database or downstream service. It is easy to misconfigure
    something, especially when database statements are involved. The level of integration
    tests should be far less than the coverage of the unit tests, and again we need
    to be able to run these in a reproducible environment. Docker is an excellent
    ally in this situation; we can leverage the capability of Docker to run in multiple
    environments. This enables us to configure and debug our integration tests on
    our local environment before executing them on the build server. In the same way,
    that unit tests are a gate to a successful build, so are integration tests; failure
    of these tests should never result in a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure as code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we automate our build and deploy the process, this step is essential; ideally,
    we do not want to be deploying code to a dirty environment as this raises the
    risk of pollution such as an incorrectly vendored dependency. However, we also
    need to be able to rebuild the environment, if necessary, and this should be possible
    without enacting any of the problems we introduced earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Security scanning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If possible, security scanning should be integrated into the pipeline; we need
    to be catching bugs early and often. Regardless of whether your service is external
    facing or not, scanning it can ensure that there is a limited attack vector for
    an attacker to misuse. We have looked at fuzzing in a previous chapter, and the
    time it can take to perform this task is quite considerable and possibly not suitable
    for inclusion inside of a pipeline. However, it is possible to include various
    aspects of security scanning into the pipeline without slowing down deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Static code analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static code analysis is an incredibly effective tool to combat bugs and vulnerabilities
    in your applications, and often developers run tools such as **govet** and **gofmt**
    as part of their IDE. When the source is saved, the linter runs and identifies
    issues in the source code. It is important to run these applications inside of
    the pipeline as well as we cannot always guarantee that the change has come from
    an IDE which has it configured in this way. In addition to the save time linters,
    we can also run static code analysis to detect problems with SQL statements and
    code quality issues. These additional tools are often not included in the IDE's
    save workflow, and therefore it is imperative to run them on CI to detect any
    problems, which may have slipped through the net.
  prefs: []
  type: TYPE_NORMAL
- en: Smoke tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Smoke tests are our way of determining whether a deployment has gone successfully.
    We run a test, which can range from a simple curl to a more complex codified test
    to check various integration points within the running application.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: End-to-end tests are a complete check on the running system and typically follow
    the user flow testing the various parts. Often these tests are global to the application
    not local to the service and are automated using BDD-based tools such as cucumber.
    Determining whether you run E2E tests as a gate to deployment or a parallel process
    which is either triggered by a deployment or set as a continually running process
    is dependent upon your company's appetite for risk. If you are confident that
    your unit, integration, and smoke tests have adequate coverage to give you peace
    of mind or that the service in question is not essential to the core user journey,
    then you may decide to run these in parallel. If, however, the functionality in
    question is part of your core journey, then you may choose to run these tests
    sequentially as a gateway to deployment on a staging environment. Even when E2E
    tests run as a gateway, if any configuration changes are made such as the promotion
    of staging to production, it is advisable to again run the E2E tests before declaring
    a deployment successful.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Post deploy, we should not rely on our users to inform us when something has
    gone wrong, which is why we need application monitoring linked to an automated
    notification system such as **PagerDuty**. When a threshold of errors has exceeded,
    the monitor triggers and alerts you to the problem; this gives you the opportunity
    to roll back the last deploy or to fix the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have talked about the problem, and why this is important for us.
    We have also looked at the constituent parts of a successful continuous delivery
    system, but how can we implement such a process for our application, and what
    does Go bring as a language which helps us with this? Now, let''s look at the
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: Build
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoke test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The build process is mainly a focus for developers to get things up and running
    on their local machine but my recommendation is that we need to be thinking about
    cross-platform and cross-system builds from the beginning. What I mean by cross-system
    builds is that even if we are developing on a Macintosh, we may not be building
    a release product on a Mac. In fact, this behavior is quite common. We need our
    releases built by a third party and preferentially in a clean room environment,
    which is not going to be affected by pollution from other builds.
  prefs: []
  type: TYPE_NORMAL
- en: Every feature should have a branch and every branch should have a build. Every
    time your application code is pushed to the source repository, we should trigger
    the build even if this code is going nowhere near production. It is good practice
    never to leave a build in a broken state, and that includes branch builds. You
    should deal with the issues as and when they occur; delaying this action risks
    your ability to deploy, and while you may not plan to deploy to production until
    the end of the sprint, you must consider unforeseen issues which can occur such
    as the requirement to change the configuration or hotfix a bug. If the build process
    is in a broken state, then you will not be able to deal with the immediate issues,
    and you risk delaying a planned deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The other important aspect other than automatically triggering a build whenever
    you push to a branch is to run a nightly build. Nightly builds for branches, should
    be rebased with the master branch before building and testing. The reason for
    this step is to give you early warning around potential merge conflicts. We want
    to catch these early rather than later; a failed nightly build should be the first
    task of the day.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about Docker earlier on in [Chapter 4](2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml),
    *Testing*, and we should bring Docker into our build process. Docker through its
    immutability for a container gives us the clean room environment to ensure reproducibility.
    Because we start from scratch with every build, we cannot rely on a pre-existing
    state, which causes differences between the development environment and the built
    environment. Environmental pollution may seem like a trivial thing but the amount
    of time I have wasted over my career debugging a broken build because one application
    was using a dependency installed on a machine and another used a different version
    is immeasurable.
  prefs: []
  type: TYPE_NORMAL
- en: What is container orchestration?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply container orchestration is the process of running one or more instances
    of an application. Think of the common understanding of an orchestra, a group
    of musicians who work together to produce a piece of music. The containers in
    your application are like the musicians in the orchestra; you may have specialist
    containers, of which there are low numbers of instances such as the percussionists,
    or you may have many instances such as the strings section. In an orchestra, the
    conductor keeps everything in time and ensures that the relevant musicians are
    playing the right music at the right time. In the world of containers, we have
    a scheduler; the scheduler is responsible for ensuring that the correct number
    of containers are running at any one time and that these containers are distributed
    correctly across the nodes in the cluster to ensure high availability. The scheduler,
    like a conductor, is also responsible for ensuring that the right instruments
    play at the right time. In addition to ensuring a constant suite of applications
    is constantly running, the scheduler also can start a container at a particular
    time or based on a particular condition to run ad hoc jobs. This capability is
    similar to what would be performed by **cron** on a Linux-based system.
  prefs: []
  type: TYPE_NORMAL
- en: Options for container orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thankfully, today there are many applications which provide an orchestration
    function, these are broken into two categories: Managed, such as PaaS solutions
    like AWS ECS, and Unmanaged, such as open source schedulers like Kubenetes, which
    need management of both servers and the scheduler application. Unfortunately,
    there is no one-fits-all solution. The option you choose is dependent on the scale
    you require and how complex your application is. If you are a startup or just
    starting to break out into the world of microservices, then the more managed end
    of the spectrum such as **Elastic Beanstalk** will more than suffice. If you are
    planning a large-scale migration, then you might be better looking at a fully
    fledged scheduler. One thing I am confident about is that by containerizing your
    applications using Docker, you have this flexibility, even if you are planning
    a large-scale migration, then start simple and work up to the complexity. We will
    examine how the concepts of orchestration and infrastructure-as-code enable us
    to complete this. We should never ignore the up-front-design and long term thinking,
    but we should not let this stop us from moving fast. Like code infrastructure
    can be refactored and upgraded, the important concepts are the patterns and the
    strong foundations.'
  prefs: []
  type: TYPE_NORMAL
- en: What is immutable infrastructure?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Immutability is the inability to be changed. We have already looked at Docker
    and how a Docker container is an immutable instance of an image. However, what
    about the hardware that the Docker server runs on? Immutable infrastructure gives
    us the same benefits--we have a known state and that state is consistent across
    our estate. Traditionally, the software would be upgraded on an application server,
    but this process was often problematic. The software update process would sometimes
    not go to plan, leaving in the operator with the arduous task of trying to roll
    this back. We would also experience situations where the application servers would
    be in a different state requiring different processes to upgrade each of them.
    The update process may be okay if you only have two application servers, but what
    if you have 200 of them? The cognitive load becomes too high to bear that the
    administration is distributed across a team or multiple teams, and then we need
    to start to maintain documentation to upgrade each of the applications. When we
    were dealing with bare metal servers, there was often no other way to deal with
    this; the time it would take to provision a machine was measured in days. With
    the virtualization, this time improved as it gave us the ability to create a base
    image, which contained a partial config and we could then provision new instances
    in tens of minutes. With the cloud, the level of abstraction became one layer
    greater; no longer did we even need to worry about the virtualization layer as
    we had the capability to spin up compute resource in seconds. So, the cloud solved
    the process of the hardware, but what about the process of provisioning the applications?
    Do we still need to write the documentation and keep it up to date? As it happens,
    we do not. Tooling has been created to allow us to codify our infrastructure and
    application provisioning. The code becomes the documentation and because it is
    code we can version it using the standard version control systems such as Git.
    There are many tools to choose from, such as Chef, Puppet, Ansible, and Terraform;
    however, in this chapter, we will take a look at Terraform because in my personal
    opinion, besides being the most modern of the tools and the easiest to use, it
    embodies all of the principles of immutability.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Terraform ([https://terraform.io](https://terraform.io)) is an application by
    HashiCorp ([https://hashicorp.com](https://hashicorp.com)), which enables the
    provisioning of infrastructure for several applications and cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: It allows you to write codified infrastructure using the HCL language format.
    It enables the concepts of reproducibility and consistency that we have discussed
    are essential for continuous deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform as an application is a powerful tool and is a bigger topic than this
    book should cover; however, we will look at the basics of how it works to understand
    our demo application.
  prefs: []
  type: TYPE_NORMAL
- en: We will split our infrastructure into multiple chunks with the infrastructure
    code owned by each microservice located in the source code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will look closely at the shared infrastructure and services
    to get a deeper understanding of the Terraform concepts. Let''s take a look at
    the example code in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/building-microservices-with-go/chapter11-services-main](https://github.com/building-microservices-with-go/chapter11-services-main)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The shared infrastructure contains the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc503990-dab7-4129-a762-81a99c403866.png)'
  prefs: []
  type: TYPE_IMG
- en: '**VPC**: This is the virtual cloud, it allows all of the applications connected
    to it to communicate without needing to go over the public internet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S3 bucket**: This is the remote storage for config and artifacts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Beanstalk**: This is the Elastic Beanstalk application which will
    run the NATS.io messaging system, we can split this over two availability zones
    which are the equivalent to a data center, hosting applications in multiple zones
    gives us redundancy in the instance that the zone suffers an outage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal ALB:** To communication with our NATS.io server when we add other
    applications to our VPC we need to use an internal application load balancer.
    An internal ALB has the same features as an external load balancer but it is only
    accessible to applications which are attached to the VPC, connections from the
    public internet are not allowed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet Gateway:** If we need our application to be able to make outbound
    calls to other internet services then we need to attach an internet gateway. For
    security, a VPC has no outbound connections by default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we can understand the components which we need to create let's take a look
    at the Terraform configuration which can create them.
  prefs: []
  type: TYPE_NORMAL
- en: Providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Terraform is broken up into providers. A provider is responsible for understanding
    the API interactions and exposing the resources for the chosen platform. In the
    first section, we will look at the provider configuration for AWS. In the following
    code, the `provider` block allows you to configure Terraform with your credentials
    and set an AWS region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Blocks in Terraform typically follow the previous pattern. HCL is not JSON;
    however, it is interoperable with JSON. The design of HCL is to find that balance
    between machine and human-readable format. In this particular provider, we can
    configure some different arguments; however, as a bare minimum, we must set up
    your `access_key`, `secret_key`, and `region`. These are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`access_key`: This is the AWS access key. This is a required argument; however,
    it may also be provided by setting the `AWS_ACCESS_KEY_ID` environment variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`secret_key`: This is the AWS secret key. This is a required argument; however,
    it may also be provided by setting the `AWS_SECRET_ACCESS_KEY` environment variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`region`: This is the AWS region. This is a required argument; however, it
    may also be provided by setting the `AWS_DEFAULT_REGION` environment variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the required variables can be replaced with environment variables; we
    do not want to commit our AWS secrets to GitHub because if they leak we will most
    likely find that someone has kindly spun up lots of expensive resource to mine
    Bitcoin ([http://www.securityweek.com/how-hackers-target-cloud-services-bitcoin-profit](http://www.securityweek.com/how-hackers-target-cloud-services-bitcoin-profit)).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use environment variables, we can then securely inject these into our
    CI service where they are available for the job. Looking at our provider block
    `provider.tf`, we can see that it does not contain any of the settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, in this file, you will notice that there is a block by the name of `terraform`.
    This configuration block allows us to store the Terraform state in an S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The state is what the `terraform` block uses to understand the resources, which
    have been created for a module. Every time you change your configuration and run
    either of the Terraform plans, Terraform will check the state files for differences
    to understand what it needs to delete, update, or create. A special note on remote
    state is that again it should never be checked into git. The remote state contains
    information about your infrastructure, including potentially secret information,
    not something you would ever want to leak. For this reason, we can use the remote
    state, rather than keep the state on your local disk; Terraform saves the state
    files to a remote backend such as `s3`. We can even implement locking with certain
    backends to ensure that only one run of the configuration takes place at any one
    time. In our config, we are using the AWS `s3` backend, which has the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bucket`: This is the name of the S3 bucket to store the state. S3 buckets
    are globally named and are not namespaced to your user account. So this value
    must not only be unique to you, but specific to AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key`: This is the key of the bucket object which holds the state. This is
    unique to the bucket. You can use a bucket for multiple Terraform configs as long
    as this key is unique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`region`: This is the region for the S3 bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terraform config entry point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main entry point for our application is the `terraform.tf` file. There is
    no stipulation on this filename, Terraform is graph based. It recurses through
    all files which end in `.tf` in our directory and build up a dependency graph.
    It does this to understand the order to create resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at this file, we see that it is made up of modules. Modules are
    a way for Terraform to create reusable sections of infrastructure code or just
    to logically separate things for readability. They are very similar to the concepts
    of packages in Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at the VPC module in greater depth.
  prefs: []
  type: TYPE_NORMAL
- en: VPC module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VPC module creates our private network inside AWS; we do not want to or
    need to expose the NATS server to the outside world, so we can create a private
    network which only allows the resources attached to that network to access it,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `source` attribute is the location of the module; Terraform supports the
    following sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Local file paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitbucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic Git, Mercurial repositories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP URLs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3 buckets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the `source` attribute, we can configure custom attributes, which
    correspond to the variables in the module. Variables are required placeholders
    for a module; when they are not present, Terraform complains when we try to run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vpc/variables.tf` file contains the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration for a variable is very much like that of the provider and
    it follows the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A variable has three possible configuration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`type`: This is an optional attribute which sets the type of the variable.
    The valid values are `string`, `list`, and `map`. If no value is given, then the
    type is assumed to be `string`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default`: This is an optional attribute to set the default value for the variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`description`: This is an optional attribute to assign a human-friendly description
    for the variable. The primary purpose of this attribute is for documentation of
    your Terraform configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variables can be explicitly declared inside a `terraform.tfvars` file like
    the one in the root of our repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also set an environment variable by prefixing `TF_VAR_` to the name
    of the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can include the variable in the command when we run the `terraform`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are configuring the namespace of the application and the IP address block
    allocated to the network. If we look at the file, which contains the VPC blocks,
    we can see how this is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vpc/vpc.tf` file contains the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A `resource` block is a Terraform syntax, which defines a resource in AWS and
    has the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Resources in Terraform map to the objects needed for the API calls in the AWS
    SDK. If you look at the `cidr_block` attribute, you will see that we are referencing
    the variable using the Terraform interpolation syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Interpolation syntax is a metaprogramming language inside of Terraform. It allows
    you to manipulate variables and the output from resources and is defined using
    the `${[interpolation]}` syntax. We are using the variables collection, which
    is prefixed by `var` and references the `vpc_cidr_block` variable. When Terraform
    runs `${var.vpc_cidr_block}`, it will be replaced with the `10.1.0.0/16` value
    from our variable's file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a VPC which has external internet access in AWS requires four sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aws_vpc`: This is a private network for our instances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws_internet_gateway`: This is a gateway attached to our VPC to allow internet
    access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws_route`: This is the routing table entry to map to the gateway'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws_subnet`: This is a subnet which our instances launch into--we create one
    subnet for each availability zone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This complexity is not Terraform but AWS. The other cloud providers have very
    similar complexity, and unfortunately, it is unavoidable. It feels daunting at
    first, however, there are some amazing resources out there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section of the VPC setup is to configure the internet gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we have a similar format as the `aws_vpc` block; however, in this block,
    we need to set the `vpc_id` block, which needs to reference the VPC, which we
    created in the previous block. We can again use the Terraform interpolation syntax
    to find this reference even though it has not yet been created. The `aws_vpc.default.id`
    reference has the following form which is common across all resources in Terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When we reference another block in Terraform, it also tells the dependency graph
    that the referenced block needs to be created before this block. In this way,
    Terraform is capable of organizing which resources can be set up in parallel and
    those which have an exact order. We do not need to declare the order ourselves
    when the graph is created it automatically builds this for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block sets up the routing table for the VPC, enabling the outbound
    access to the public internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the attributes in this block in a little more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`route_table_id`: This is the reference to the routing table we would like
    to create a new reference for. We can obtain this from the output attribute `main_route_table_id`
    from `aws_vpc`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`destination_cidr_block`: This is the IP range of the instances which will
    be connected to the VPC who can send traffic to the gateway. We are using the
    block `0.0.0.0/0`, which allows all the connected instances. If required, we could
    only allow external access to certain IP ranges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gateway_id`: This is a reference to the gateway block we previously created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next block introduces a new concept for us data sources. Data sources allow
    data to be fetched or computed from information stored outside Terraform or stored
    in separate Terraform configuration. A data source may look up information in
    AWS, for example, you can query a list of existing EC2 instances, which may exist
    in your account. You can also query other providers, for instance, you have a
    DNS entry in CloudFlare, which you would like the details for or even the address
    of a load balancer in a different cloud provider, such as Google or Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use it to retrieve the lists of availability zones in AWS. When we
    create the VPC, we need to create a subnet in each availability zone, because
    we are only configuring the region, we have not set the availability zones for
    that region. We could explicitly configure these in the variables section; however,
    that makes our config more brittle. The best way, whenever possible, is to use
    data blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration is quite simple and again follows a common syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will make use of this information in the final part of the VPC setup, which
    is to configure the subnets; this also introduces another new Terraform feature
    `count`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look closely at the `count` attribute; a `count` attribute is a special
    attribute, which when set creates *n* instances of the resource. The value of
    our attribute also expands on the interpolation syntax that we examined earlier
    to introduce the `length` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cidr_blocks` is a Terraform list. In Go, this would be a slice and the
    length will return the number of elements inside a list. For comparison, let''s
    look at how we would write this in Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Interpolation syntax in Terraform is an amazing feature, allowing you to manipulate
    variables with many built-in functions. The documentation for the interpolation
    syntax can be found at the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.terraform.io/docs/configuration/interpolation.html](https://www.terraform.io/docs/configuration/interpolation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have the capability of using conditional statements. One of the best
    features of the `count` function is that if you set it to `0`, Terraform omits
    creation of a resource; as an example, it would allow us to write something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax for conditionals uses the ternary operation, which is present in
    many languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When we use the `count` Terraform, it also provides us with an index, which
    we can use to obtain the correct element from a list. Consider how we are using
    this in the `availability_zone` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `count.index` will provide us with a `0` based index and because `data.aws_availability_zones.available.names`
    returns a list, we can use to access this like a slice. Let''s take a look at
    the remaining attributes on our `aws_subnet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vpc_id`: This is the ID of the VPC, which we created in an earlier block and
    we would like to attach the subnet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`availability_zone`: This is the name of the availability zone for the subnet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cidr_block`: This is the IP range of addresses, which will be given to an
    instance when we launch it in this particular VPC and availability zone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map_public_ip_on_launch`: Whether we should attach a public IP address to
    the instance when it is created, this is an optional parameter, and determines
    whether your instance should also have a public IP address in addition to the
    private one, which is allocated from the `cidr_block` attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are building modules in Terraform, we often need to reference attributes
    from other modules. There is a clean separation between modules, which means that
    they cannot directly access another module resources. For example, in this module,
    we are creating a VPC, and later on, we would like to create an EC2 instance,
    which is attached to this VPC. We could not use the syntax as shown in the upcoming
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `module2/terraform.tf` file contains the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example would result in an error as we are trying to reference
    a variable which does not exist in this module even though it does exist in your
    global Terraform config. Consider these to be like Go packages. If we had the
    two following Go packages which contained non-exported variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a/main.go`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`b/main.go`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In Go we could, of course, have the variable exported by capitalizing the name
    of the variable `notExported` to `NotExported`. To achieve the same in Terraform,
    we use output variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax should be starting to get familiar to you now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the output of one module to be the input of another--an example
    found in the `terraform.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vpc_id` attribute is referencing an output from the `vpc` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax for the preceding statement is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In addition to allowing us to keep our code dry and clean, output variables
    and module references allow Terraform to build its dependency graph. In this instance,
    Terraform knows that because there is a reference to the `vpc` module from the
    `nats` module, it needs to create the `vpc` module resources before the `nats`
    module. This might feel like a lot of information and it is. I did not say infrastructure
    as code was easy, but by the time we get to the end of this example, it will start
    to become clear. Applying these concepts to create other resources becomes quite
    straightforward with the only complexity being how the resource works, not the
    Terraform configuration which is needed to create it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run Terraform and to create our infrastructure, we must first set some environment
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to initialize Terraform to reference the modules and remote data
    store. We normally only need to perform this step whenever we first clone the
    repository or if we make changes to the modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to run a plan; we use the plan command in Terraform to understand
    which resources are created, updated, or deleted by the `apply` command. It will
    also syntax check our config without creating any resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-out` argument saves the plan to `main.terraform` file. This is an optional
    step, but if we run `apply` with the output from the plan, we can ensure that
    nothing changes from when we inspected and approved the output of the `plan` command.
    To create the infrastructure, we can then run the `apply` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The first argument to the `apply` command is the `plan` output, which we created
    in the previous step. Terraform now creates your resources in AWS, this can take
    anything from a few seconds to 30 minutes depending upon the type of resource
    you are creating. Once the creation is complete, Terraform writes the output variables,
    which we defined in the `output.tf` file to `stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: We have only covered one of the modules in our main infrastructure project.
    I recommend that you read through the remaining modules and familiarize yourself
    with both the Terraform code and the AWS resources it is creating. Excellent documentation
    is available on the Terraform website ([https://terraform.io](https://terraform.io))
    and the AWS website.
  prefs: []
  type: TYPE_NORMAL
- en: Example application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our sample application is a simple distributed system consisting of three services.
    The three main services, product, search and authentication, have a dependency
    on a database which they use to store their state. For simplicity, we are using
    MySQL; however, in a real production environment, you would want to choose the
    most appropriate data store for your use case. The three services are connected
    via the messaging system for which we are using NATS.io, which is a provider-agnostic
    system, which we looked at in [C](2952a830-163e-4610-8554-67498ec77e1e.xhtml)hapter
    9, *Event-Driven Architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88b91630-a430-4c82-92c3-b6236f8d42fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To provision this system, we have broken down the infrastructure and source
    code into four separate repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared infrastructure and services** ([https://github.com/building-microservices-with-go/chapter11-services-main](https://github.com/building-microservices-with-go/chapter11-services-main))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication service** ([https://github.com/building-microservices-with-go/chapter11-services-auth](https://github.com/building-microservices-with-go/chapter11-services-auth))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product service** ([https://github.com/building-microservices-with-go/chapter11-services-product](https://github.com/building-microservices-with-go/chapter11-services-product))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search service** ([https://github.com/building-microservices-with-go/chapter11-services-search](https://github.com/building-microservices-with-go/chapter11-services-search))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individual repositories enable us to separate our application in such a
    way that we only build and deploy the components that change. The shared infrastructure
    repository contains Terraform configuration to create a shared network and components
    to create the NATS.io server. The authentication service creates a JWT-based authentication
    microservice and contains separate Terraform configuration to deploy the service
    to Elastic Beanstalk. The product service and the search service repositories
    also each contain a microservice and Terraform infrastructure configuration. All
    the services are configured to build and deploy using Circle CI.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the remainder of this chapter, we concentrate on the search service as
    the build pipeline is the most complex. In our example application, we have the
    following steps which to build a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Compile application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static code analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build Docker image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoke test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these steps are independent and can run in parallel, so when we compose
    the pipeline, it looks like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd2372fa-3304-48e9-ad93-b0e66e5a71ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the example code at [https://github.com/building-microservices-with-go/chapter11-services-auth](https://github.com/building-microservices-with-go/chapter11-services-auth).
    We are building this application with Circle CI; however, the concepts apply to
    whatever platform you use. If we look at the `circleci/config.yml` file, we see
    that we are first setting up the configuration for the process, which includes
    choosing the version of the Docker container within which the build executes and
    install some initial dependencies. We then compose the jobs, which are performed
    in the workflow and the various steps for each of the jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will compose these jobs into a workflow or a pipeline. This workflow
    defines the relationship between the steps as there are obvious dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: To isolate dependencies in our configuration and to ensure that the commands
    for building and testing are consistent across various processes, the commands
    have been placed into the Makefile in the root of the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a closer look at the build process. Inside the build job configuration,
    we have three steps. The first is to check out the repository. The jobs themselves
    are broken up into steps, and the first notable one of these is to install the
    dependencies. Glide is our package manager for the repository, and we need this
    to be installed to fetch updates to our vendored packages. We also need a `go-junit-report`
    utility package. This application allows us to convert the Go test output into
    JUnit format, which Circle requires for presenting certain dashboard information.
    We then execute `glide up` to fetch any updates. In this example, I have checked
    in the `vendor` folder to the repository; however, I am not pinning the packages
    to a version. You should set a minimum package version rather than an exact package
    version, updating your packages frequently allows you to take advantage of regular
    releases in the open source community. You do of course run the risk that there
    will be a breaking change in a package and that change breaks the build, but as
    mentioned earlier, it is better to catch this as soon as possible rather than
    deal with the problem when you are under pressure to release.
  prefs: []
  type: TYPE_NORMAL
- en: Because we are building for production, we need to create a Linux binary, which
    is why we are setting the `GOOS=linux` environment variable before running the
    build. Setting the environment is redundant when we are running the build on Circle
    CI as we are already running in a Linux-based Docker container; however, to enable
    cross-platform builds from our developer machines, if they are not Linux-based,
    it is useful to have a common command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have built our application, we need to persist the workspace so that
    the other jobs can use this. In Circle CI, we use the special step `persist_to_workspace`;
    however, this capability is common to pipeline-based workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93745586-831c-4ac4-a792-f60b688bd807.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also mentioned the fact that we need consistency and that, if we are deploying
    continuously, we need to have a good solid testing suite, which replaces almost
    all our manual testing. I am not saying there is no place for manual testing as
    there is always a use for exploratory testing but when we are deploying continuously,
    we need to automate all of this. Even if you are adding manual testing into your
    process, it will be most likely running as an asynchronous process complimentary
    to your build pipeline not as a gate to it.
  prefs: []
  type: TYPE_NORMAL
- en: The testing section of the configuration runs our unit tests as we saw in [Chapter
    4](2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml), *Testing*. With the following
    configuration, we first need to attach the workspace that we created in the build
    step. The reason for this is that we do not need to check out the repository again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The second thing we need to do is to install the dependencies, Circle CI requires
    the output of the tests to be in JUnit format for presentation. To enable this,
    we can fetch the `go-junit-report` package, which can take the output of our tests
    and convert them into JUnit format.
  prefs: []
  type: TYPE_NORMAL
- en: To run the tests, we have to do something slightly different, if we just ran
    our unit tests and piped them into the `go-junit-report` command, then we would
    lose the output. Reading the command in reverse order, we run our unit tests and
    output, `make unit | tee ${TEST_RESULTS}/go-test.out`; the `tee` command takes
    the input piped to it and writes to both the output file specified as well as
    to the `stdout` file. We can then use trap, which executes a command when an exit
    code is matched from another command. In our instance, if the unit tests exit
    with a status code 0 (normal condition), then we execute the `go-junit-report`
    command. Finally, we write our test results for Circle CI to be able to interpret
    them using the `store_test_results` step.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarking is an important feature for our CI pipeline; we need to understand
    when the performance of our application degrades. To do this, we are going to
    both run our benchmark tests and use the handy tool **benchcmp**, which compares
    two runs of tests. The standard version of benchcmp only outputs the difference
    between two test runs. While this is fine for comparison, it does not give us
    the capability to fail our CI job should this difference be within a certain threshold.
    To enable this capability, I have forked the benchcmp tool and added `flag-tollerance=[FLOAT]`.
    If any of the benchmarks change +/- the given tolerance, then benchcmp exits with
    status code 1, allowing us to fail the job and investigate why this change has
    taken place. For this to work, we need to keep the previous benchmark data available
    for comparison, so we can use the caching feature to store the last run data.
  prefs: []
  type: TYPE_NORMAL
- en: Static code analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static code analysis is a fast and efficient way to check for any problems in
    our source code automatically. In our example, we will run two different static
    code analysis tools, the first is **megacheck** by Dominik Honnef, which examines
    the code for common problems such as misuse of the standard library, concurrency
    issues, and many more problems.
  prefs: []
  type: TYPE_NORMAL
- en: Second is **SafeSQL** from the Stripe team. SafeSQL runs through our code and
    looks for uses of the SQL package. It then examines the ones looking for vulnerabilities
    such as incorrectly constructed queries, which may be open to SQL injection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will be checking our code, including the tests for unhandled errors,
    for example, you have the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'When invoking a method like this, the error object can be thrown away and not
    handled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Unhandled errors are more often found in tests rather than the main body of
    code; however, even in tests, this could introduce a bug due to unhandled behavior,
    `errcheck` runs through the code looking for instances like this and reports an
    error when found and fails the build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Static check invokes the megacheck linter which runs `staticcheck` a static
    code analysis tool which helps to detect bugs, Go simple which identifies areas
    of the source code which should be improved by re-writing in a simpler way and
    unused which identifies unused constants, types, and functions. The first checker
    is designed to spot bugs; however, the remaining three are concerned with your
    application life cycle management.
  prefs: []
  type: TYPE_NORMAL
- en: Clean code is essential to bug-free code; the easier and the simpler your code
    is the more the reduced likelihood that you have logic bugs. Why? Because the
    code is easier to understand and since you spend more of your time reading code
    than writing, it makes sense to optimize for readability. Static code analysis
    should not be a replacement for the code review. However, these tools allow you
    to focus on logic flaws rather than semantics. Integrating this into your continuous
    integration pipeline acts as a gatekeeper to the sanity of your codebase, the
    checks run incredibly quickly and in my humble opinion are an essential step.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/dominikh/go-tools/tree/master/cmd/megacheck](https://github.com/dominikh/go-tools/tree/master/cmd/megacheck)'
  prefs: []
  type: TYPE_NORMAL
- en: SafeSQL from the Stripe team is a static code analysis tool which protects against
    SQL injections. It attempts to find problems with usage of the `database/sql`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/stripe/safesql](https://github.com/stripe/safesql)'
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Then, there are integration tests. In this example, we are again using GoDog
    BDD; however, when we are running on Circle CI, we need to modify our setup a
    little because of the way that Circle deals with security for Docker. The first
    steps are again to attach the workspace, including the binary that we built in
    a previous step; then we can get the dependencies which are only the GoDog application.
    The `setup_remote_docker` command requests a Docker instance from Circle CI. The
    current build is running in a Docker container; however, because of the security
    configuration, we cannot access the Docker host, which is currently running the
    current build.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The section of the Makefile for running on CI is quite a bit more complex than
    when we run it on our local machine. We need this modification because we need
    to copy the source code and install the `godog` command to a container, which
    will be running on the same network as the stack we start with Docker compose.
    When we are running locally, this is not necessary as we have the capability to
    connect to the network. This access is forbidden on Circle CI and most likely
    other shared continuous integration environments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We build our temporary container, which contains the current directory and adds
    the `godog` dependency. We can then start the stack as normal by running `docker-compose
    up` and then the `godog` command.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests on continuous delivery are an essential gate before we deploy
    to production. We also want to be able to test our Docker image to ensure that
    the startup process is functioning correctly and that we have tested all our assets.
    When we looked at integration tests in [Chapter 4](2baaa0cf-170d-4d7f-8449-b26f20a9bbab.xhtml),
    *Testing*, we were only running the application, which is fine for our development
    process--it gives us the happy medium of quality and speed. When it comes to building
    our production images, however, this compromise is not acceptable, and therefore,
    we need to make some modifications to the development process to ensure that we
    include the production image in our test plan.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have all our application code build tested and packaged, it is time
    to think about deploying this into production. We need to start thinking about
    our infrastructure as immutable, that is, we will not make changes to the infrastructure
    but replace it. The level with which this occurs can be multiple. For example,
    we have our container scheduler, which only runs the containers. When we deploy
    an update to our application binary, we are replacing a container on the scheduler
    not refreshing the application in it. Containers give us one level of immutability,
    the other is the scheduler itself. To operate successful continuous delivery,
    the setup of this facet also needs to be automated, we need to think of our infrastructure
    as code.
  prefs: []
  type: TYPE_NORMAL
- en: For our application, we are splitting the infrastructure up into separate parts.
    We have a main infrastructure repository, which creates the VPC, S3 buckets used
    by deployments and creates an Elastic Beanstalk instance for our messaging platform
    NATS.io. We also have Terraform config for each of the services. We could create
    one massive Terraform config as Terraform replaces or destroys infrastructure,
    which has changed; however, there are several reasons why we would not want this.
    The first is that we want to be able to break down our infrastructure code into
    small parts in the same way we break up our application code; the second is due
    to the way Terraform works. To ensure the consistency of the state, we can only
    run a single operation against the infrastructure code at any one time. Terraform
    obtains a lock when it runs to ensure that you cannot run it multiple times at
    once. If we consider a situation where there are many microservices and that these
    services are being continuously deployed, then having a single deployment which
    is single threaded becomes a terrible thing. When we decompose the infrastructure
    configuration and localize it with each service, then this no longer becomes a
    problem. One problem with this distributed configuration is that we still need
    a method of accessing resource information in the master repository. In our case,
    we are creating the main VPC in this repository, and we need the details to be
    able to connect our microservices to it. Thankfully, Terraform manages rather
    pleasantly using the concept of remote state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can configure our master Terraform config to use remote state, which we
    can then access from the search Terraform config using the remote state data element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: When all the previous steps in the build process complete, we deploy this to
    AWS automatically. This way we always deploy every time a new instance of the
    master branch builds.
  prefs: []
  type: TYPE_NORMAL
- en: Smoke tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Smoke testing the application post deploy is an essential step in continuous
    delivery we need to ensure that the application is functioning correctly and that
    nothing has gone wrong in the build and deploy steps. In our example, we are simply
    checking that we can reach the health endpoint. However, a smoke test can be as
    simple or as complex as required. Many organizations run more detail checks, which
    confirm that the core integration to the deployed system is correct and functioning.
    The smoke tests are conducted as either a codified test re-using many of the steps
    in the GoDog integration tests or a specialized test. In our example, we are simply
    checking the health endpoint for the search service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In our application, we can run this test because the endpoint is public. When
    an endpoint is not public, testing becomes more complicated, and we need to check
    the integration by calling through a public endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: One of the considerations for end-to-end testing is that you need to be careful
    of polluting the data inside the production database. A complimentary or even
    alternative approach is to ensure that your system has extensive logging and monitoring.
    We can set up dashboards and alerts, which actively check for user errors. When
    an issue occurs post deploy, we can investigate the problem, and if necessary,
    rollback to a previous version of the build with a known good state.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring/alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the application is running, we need to be sure of the health and status
    of the application. Monitoring is an incredibly important facet of the continuous
    deployment life cycle. If we are deploying automatically, we need to understand
    how our application is performing and how this differs from the previous release.
    We have seen how we can use StatsD to emit data about our service to a backend
    such as Prometheus or a managed application such as Datadog. Should our recent
    deploy exhibit anomalous behavior, we are alerted about this and from there we
    can act to help identify the source of the problem, intermittently rolling back
    if necessary or modifying our alerts as our server may just be doing more work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Again, using the concepts of infrastructure as code, we can provision these
    monitors at build time using Terraform. While errors are useful for monitoring,
    it is also important to not forget timing data. An error tells you that something
    is going wrong; however, with the clever use of timing information in the service,
    we can learn that something is about to go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Complete workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assuming all is functioning well, we should have a successful build, and the
    UI in our build environment should show all steps passing. Remember our warning
    from the beginning of this chapter--when your build fails, you need to make it
    your primary objective to fix it; you never know when you are going to need it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/345f20fb-eaf2-4666-9081-45a2eb830e63.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned that it need not be an arduous task to set
    up continuous integration and deployment for your application, and in fact, this
    is essential to the health and success of your application. We have built on all
    the concepts covered in the previous chapters, and while the final example is
    somewhat simple, it has all the constituent parts for you to build into your applications
    to ensure that you spend your time developing new features and not fixing production
    issues or wasting time repetitively and riskily deploying application code. Like
    all aspects of our development, we should practice and test this process. Before
    releasing continuous delivery to your production workflow, you need to ensure
    that you can deal with problems such as hot fixing and rolling back a release.
    This activity should be completed across teams and depending on your process for
    out-of-hours support should also involve any support staff. A well-practiced and
    functioning deployment process gives you the confidence that when an issue occurs,
    and it most likely will, you can comfortably and confidently deal with it.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that by working through this book, you now have a greater understanding
    of most of the things you need to build microservices with go successfully. The
    one thing I cannot teach is the experience that you need to find out for yourself
    by getting out there and performing. I wish you luck on this journey, and the
    one thing that I have learned from my career is that you never regret putting
    in the time and effort to learn these techniques. I am sure you will be hugely
    successful.
  prefs: []
  type: TYPE_NORMAL
