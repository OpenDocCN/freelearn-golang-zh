<html><head></head><body>
		<div id="_idContainer036">
			<h1 id="_idParaDest-147" class="chapter-number"><a id="_idTextAnchor147"/>10</h1>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Concurrency and Functional Programming</h1>
			<p>Concurrency is all around us, both in the real world as well as the virtual one. Humans can easily multitask (although we might not do a good job at either task). It’s entirely possible to drink a cup of coffee while you are reading this chapter or to run while listening to a podcast. For machines, concurrency is a complex undertaking, although a lot of that complexity can be hidden away by the programming language <span class="No-Break">we choose.</span></p>
			<p>Go was built to be a language with all the necessary tools a modern-day software engineer needs. As we are now in a world where CPU power is abundant for most intents and purposes, it’s only natural that concurrency was a main concern when developing the language, rather than having to bolt it on later. In this chapter, we are going to take a look at how functional programming can help with concurrency and, conversely, how concurrency can help with <span class="No-Break">functional programming.</span></p>
			<p>In this chapter, we are going to cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Why functional programming helps us write <span class="No-Break">concurrent code</span></li>
				<li>How to create concurrent functions (Filter, Map, and <span class="No-Break">so on)</span></li>
				<li>How to chain functions together concurrently using the <span class="No-Break">pipeline pattern</span></li>
			</ul>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>Technical requirements</h1>
			<p>For this chapter, you can use any version of Go at or above version 1.18. All the code for this chapter can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Functional-Programming-in-Go./tree/main/Chapter10"><span class="No-Break">https://github.com/PacktPublishing/Functional-Programming-in-Go./tree/main/Chapter10</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor150"/>Functional programming and concurrency</h1>
			<p>We have already hinted<a id="_idIndexMarker472"/> at it throughout<a id="_idIndexMarker473"/> this book, but the ideas behind functional programming can help us write concurrent code. Typically, thinking about concurrency is a bit of a headache, even when a language has modern tools to support it, such as goroutines and channels. Before we dive too deep into this material, let’s first take a small detour as a refresher on what exactly we mean when we talk about concurrent code, and how it compares to parallelism and <span class="No-Break">distributed computing.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>Concurrency, parallelism, and distributed computing</h2>
			<p>The terms <em class="italic">concurrency</em>, <em class="italic">parallelism</em>, and <em class="italic">distributed computing</em> are, at times, used interchangeably. And while<a id="_idIndexMarker474"/> they are related, they are not exactly the same thing. Let’s just point out what we mean by concurrency first. <strong class="bold">Concurrency</strong> is what happens when our program can execute multiple tasks at the same time. For example, when we are playing a video game, typically a thread is playing audio, another one is processing input from the player, and another one is taking care of the internal game logic, updating the game state and performing the main <span class="No-Break">game loop.</span></p>
			<p>Video games have been around for a long time, and a game such as <em class="italic">DOOM</em> works in this way. It’s also safe to say that people were not playing this on a computer with multiple cores available back in 1995. In other words, it’s possible for a single core to manage the execution of these distinct tasks and give the appearance of executing them at the same time. Exactly how this is done is beyond the scope of this book, but as a takeaway, just remember that the concurrency that we will mainly focus on is concurrency as defined previously – not the simultaneous execution of code, but the concurrent execution of code. One thing to note, though, is that concurrency can happen across multiple cores, or pipelines, as well. However, to keep things simple, we can imagine concurrency using a <span class="No-Break">single core.</span></p>
			<p>This brings<a id="_idIndexMarker475"/> us to the second term, <strong class="bold">parallelism</strong>. When we talk about a program executing in parallel, this means that multiple cores are performing a task simultaneously. You can not have parallelism without a physical means to run two tasks at the same time. The native Go mechanisms of channels and goroutines are focused on concurrency and not parallelism. This is an important distinction between the two. However, Go still lends itself to building out <span class="No-Break">parallel algorithms.</span></p>
			<p>To get an idea of what this looks like, there are a few packages available for Go that offer parallel solutions, such as the ExaScience Pargo package: <a href="https://github.com/ExaScience/pargo">https://github.com/ExaScience/pargo</a>. At the time of writing, this package is written in a pre-generics fashion, so do bear that in mind when looking through the code. In <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em>, the difference between concurrency and parallelism is highlighted by how the tasks get executed. Notably, the two tasks in the concurrent model are broken into multiple chunks, and each<a id="_idIndexMarker476"/> gets assigned<a id="_idIndexMarker477"/> CPU time<a id="_idIndexMarker478"/> in an <span class="No-Break">alternating fashion.</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/Figure_10.1_B18771.jpg" alt="Figure 10.1: Concurrent (above) versus parallel (below) execution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Concurrent (above) versus parallel (below) execution</p>
			<p>Finally, we have <strong class="bold">distributed computing</strong>. While concurrency is part of distributed<a id="_idIndexMarker479"/> computing, it is not the only requirement for this. Distributed computation does imply spreading out computational tasks over multiple machines, in which sense it is concurrent, but there’s more overhead than with typically concurrent or <span class="No-Break">parallel applications.</span></p>
			<p>In distributed systems, you need to have mechanisms for fault tolerance (what if one node in the network becomes unavailable?) and mechanisms for dealing with the network (unreliable or insecure networks). So, while people might talk about distributed computation as an example of concurrency, concurrency only gives you the bare minimum required. The physical infrastructure and myriad of difficulties in making a distributed system work are beyond the scope of this book. One thing to take away is that Go is a language that can be used to write distributed systems. In fact, the use of goroutines and channels might help you build out the underlying infrastructure needed for distributed systems, but you’ll need more than the basic functionality of the language. If you want to learn more about distributed computing with Go, the book <em class="italic">Distributed Computing with Go</em> is a good place to <span class="No-Break">start: </span><a href="https://www.packtpub.com/product/distributed-computing-with-go/9781787125384?_ga=2.217817046.1391922680.1675144438-1944326834.1674539572"><span class="No-Break">https://www.packtpub.com/product/distributed-computing-with-go/9781787125384?_ga=2.217817046.1391922680.1675144438-1944326834.1674539572</span></a><span class="No-Break">.</span></p>
			<p>In this chapter, we will focus on concurrency<a id="_idIndexMarker480"/> only, and we won’t zoom in on parallelism or distributed computing. However, why do we want our code to be concurrent? There are a few clear advantages<a id="_idIndexMarker481"/> that this <span class="No-Break">can bring:</span></p>
			<ul>
				<li><strong class="bold">Higher responsiveness</strong>: A program does not need to wait for a single long-running task to complete before starting <span class="No-Break">another one</span></li>
				<li><strong class="bold">Higher performance</strong>: If we can chunk out a heavy workload and perform this over multiple threads (and Go might schedule these across multiple cores to get a form of parallelism as well), this will reduce the time it takes to complete <span class="No-Break">the operation</span></li>
			</ul>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>Functional programming and concurrency</h2>
			<p>I’ve made the claim<a id="_idIndexMarker482"/> before in this book<a id="_idIndexMarker483"/> that functional programming makes it easier to write concurrent code, but this claim needs to be tailored a little bit further. When talking about how functional programming makes concurrency easier, we are talking about the stricter subset of functional programming called “pure” functional programming. Pure functional programming<a id="_idIndexMarker484"/> gives us a few key features that make reasoning about concurrent execution easier and our code less error-prone. These are the main features responsible <span class="No-Break">for this:</span></p>
			<ul>
				<li>Immutable variables <span class="No-Break">and state</span></li>
				<li>Pure functions (no <span class="No-Break">side effects)</span></li>
				<li><span class="No-Break">Referential transparency</span></li>
				<li><span class="No-Break">Lazy evaluation</span></li>
				<li><span class="No-Break">Composability</span></li>
			</ul>
			<p>For the rest of this chapter, when talking about functional programming, the assumption can be made that we’re talking strictly about<a id="_idIndexMarker485"/> pure functional programming. Let’s focus on each of these features and explain why they make for safer concurrent code, or make our code at least easier to reason about. The result is that when our code is easier to understand, it should help us reduce the number of bugs <span class="No-Break">in it.</span></p>
			<h3>Immutable variables and state</h3>
			<p>When working in an object-oriented<a id="_idIndexMarker486"/> model, objects typically hold an internal state. If this state is allowed to mutate, then the state that two threads are working on might diverge. By not allowing the state to change, even if operating on the same data sources (or, rather, copies of the same data), our functions can execute independently of each other without ever messing with the <span class="No-Break">shared memory.</span></p>
			<p>In Go, if we do want to use structs, there are some pitfalls, which we discussed in earlier chapters. By avoiding the use of pointers, we can avoid the main causes of mutation in structs. When writing pure functional code, each individual component of our code needs to be immutable. When each component is immutable, we can more safely execute <span class="No-Break">functions concurrently.</span></p>
			<p>Another issue we avoid by having immutable variables and states is that of resource contention. If we have a single true resource (a singleton in an object-oriented model), then this resource might be locked by thread A, causing thread B to wait until the resource is freed up before it can be used. Typically, this is implemented through a resource-locking mechanism (thread A locks the resource , <em class="italic">X</em>, performs operations while other threads wait for resource <em class="italic">X</em>, and then finally removes the lock when it is done operating). In a purely functional world, we would not need such singleton operations, partly due to our immutable state and partly due to the other benefits, such as <span class="No-Break">pure functions.</span></p>
			<h3>Pure functions</h3>
			<p>As we saw in <a href="B18771_04.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, a function<a id="_idIndexMarker487"/> is considered pure when it does not produce any side effects and does not interact with the outside world. In this book, we implemented many functions that are common to functional programming. All of these were written in the pure functional style (although remember that pure functional is a subset of functional programming and not strictly required). The benefits here relate to the immutable state but extend beyond it as well. If our functions do not depend on the program state, then anything modifying the state of our program cannot disrupt <span class="No-Break">our function.</span></p>
			<p>Beyond this, it also eliminates another class of problems. If our functions were allowed to mutate state, or our system, the order of operations would matter. For example, imagine that we were to write a concurrent function that appends content to a file. Writing to a file is a clear case of a side effect, but in a concurrent application, the content of our file would now depend on the order in which our threads are executed. This breaks the determinism of our application and, furthermore, would likely lead to a file that is not exactly what we desired. In an object-oriented model, this is again resolved through locking. In a purely<a id="_idIndexMarker488"/> functional language, the “impure” functions would be handled by monads. Go is not purely functional, but later in this chapter, we will look at the pipeline pattern through which we can model the data flow and control the <span class="No-Break">side effects.</span></p>
			<h3>Referential transparency</h3>
			<p><strong class="bold">Referential transparency</strong> means that we can replace a function<a id="_idIndexMarker489"/> call with its result, without changing<a id="_idIndexMarker490"/> the result of our computation. We covered this in more detail in <a href="B18771_02.xhtml#_idTextAnchor028"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, but for concurrency, the important aspect is that if all our calls are referentially transparent, it does not matter when exactly a call is resolved (ahead of time or just in time). This means that when we chunk our code out into concurrent functions, it is safe to resolve certain function calls ahead of time in a <span class="No-Break">concurrent fashion.</span></p>
			<h3>Lazy evaluation</h3>
			<p><strong class="bold">Lazy evaluation</strong> is a common approach when writing concurrent<a id="_idIndexMarker491"/> code. An example we are all too familiar<a id="_idIndexMarker492"/> with is the idea of <em class="italic">callbacks</em>. These are functions that can be passed to an asynchronous call, but they are only executed once they become relevant. It’s also entirely possible for a function to never get called. For example, let’s write a piece of code that performs an asynchronous <strong class="source-inline">GET</strong> request to a URL. We will use two callbacks, which will be lazily evaluated. The first callback will be resolved only if the <strong class="source-inline">GET</strong> request completed successfully, while the second callback will be resolved if the <strong class="source-inline">GET</strong> request failed. Note that here we mean the <strong class="source-inline">GET</strong> request itself did work, but we received a response code that is not in the <span class="No-Break"><strong class="source-inline">200</strong></span><span class="No-Break"> range:</span></p>
			<pre class="source-code">
import (
        "fmt"
        "io/ioutil"
        "net/http"
)
type ResponseFunc func(*http.Response)
func getURL(url string, onSuccess, onFailure ResponseFunc)
    {
        resp, err := http.Get(url)
        if err != nil {
                panic(err)
        }
        if resp.StatusCode &gt;= 200 &amp;&amp; resp.StatusCode &lt; 300 {
                onSuccess(resp)
        } else {
                onFailure(resp)
        }
}</pre>
			<p>In the preceding<a id="_idIndexMarker493"/> code, we can see that <strong class="source-inline">getURL</strong> requires <a id="_idIndexMarker494"/>a string representing a URL to resolve, as well as two functions. Both functions have the same <strong class="source-inline">ResponseFunc</strong> type, which is a function with the <span class="No-Break"><strong class="source-inline">func(*http.Response)</strong></span><span class="No-Break"> signature.</span></p>
			<p>Next, we can write a <strong class="source-inline">main</strong> function in which we call <strong class="source-inline">getURL</strong> and provide <span class="No-Break">two callbacks:</span></p>
			<ul>
				<li>The first callback, <strong class="source-inline">onSuccess</strong>, will be executed if our <strong class="source-inline">GET</strong> request returns a status code in the <strong class="source-inline">200</strong> range; this function will simply print out the content of the <span class="No-Break">response body.</span></li>
				<li>The second callback, <strong class="source-inline">onFailure</strong>, will simply print an error message along with the corresponding<a id="_idIndexMarker495"/> status code that our response<a id="_idIndexMarker496"/> received. We’ll call <strong class="source-inline">getURL</strong> twice, once with a valid URL and once with an invalid URL. However, instead of running this code synchronously, we will make the calls to <strong class="source-inline">getURL</strong> on separate goroutines by prefixing each call with <strong class="source-inline">go</strong>. This means we don’t know which call will complete first, but as we are using lazy functions (a type of continuation-passing style programming), we don’t have to orchestrate the control flow of our program. The correct callback will execute when its time comes. The callback, which is not necessary, will never be evaluated, so we avoid potentially expensive computation when it is <span class="No-Break">not necessary:</span><pre class="source-code">
func main() {
        success := func(response *http.Response) {
                fmt.Println("success")
                content, err := ioutil.ReadAll
                    (response.Body)
                if err != nil {
                        panic(err)
                }
                fmt.Printf("%v\n", string(content))
        }
        failure := func(response *http.Response) {
                fmt.Printf("something went wrong,
                  received: %d\n", response
                    .StatusCode)
        }
        go getURL("https://news.ycombinator.com",
          success, failure)
        go getURL("https://news.ycombinator.com/
          ThisPageDoesNotExist", success, failure)
        done := make(chan bool)
        &lt;-done // keep main alive
}</pre></li>
			</ul>
			<p>In the preceding example, our <strong class="source-inline">GET</strong> requests complete asynchronously and then call the corresponding callback, as defined in the <strong class="source-inline">getURL</strong> function. One interesting bit of code is near the end of our main snippet. We have created a <strong class="source-inline">bool</strong> channel, and then we are reading from this channel without ever writing to it. This essentially keeps our application alive. If we didn’t have these two statements, our <strong class="source-inline">main</strong> function would likely exit and thus terminate our program, before our goroutines completed their computation. In a real-world application, you could also keep waiting for the threads to resolve using <strong class="source-inline">waitgroup</strong>. If you are stuck after running this from a terminal, press <em class="italic">Ctrl</em> + <em class="italic">C</em> to kill <span class="No-Break">the process.</span></p>
			<p>Lazy evaluation<a id="_idIndexMarker497"/> will show up again later in this chapter<a id="_idIndexMarker498"/> when we take a look at implementing functional pipes. However, we’ll be looking at it more through a direct lens of concurrent applications, rather than the callback mechanisms that we <span class="No-Break">saw here.</span></p>
			<p class="callout-heading">Threads versus goroutines</p>
			<p class="callout">While the terms <em class="italic">thread</em> and <em class="italic">goroutine</em> are often used<a id="_idIndexMarker499"/> interchangeably, they are distinct things. Goroutines<a id="_idIndexMarker500"/> are a construct in Go, built to leverage executing tasks concurrently. They are managed by the Go runtime, are lightweight and fast to start and execute, and have a built-in communication medium (channels). Threads, on the other hand, are implemented at the hardware level and are managed by the operating system. They are slower to spin up, have no communication medium built in, and <span class="No-Break">are hardware-dependent.</span></p>
			<h3>Composability</h3>
			<p>Functions are composable<a id="_idIndexMarker501"/> in a myriad of ways. This allows us to define the building blocks of our application and then chain them together to solve our concrete problem. As each block is independent of one another, we can build concurrency<a id="_idIndexMarker502"/> layers in between them. This will be the focus in the last part of this chapter when we will create functional pipes that can run concurrently. However, before we get there, let’s take a look at making our functions <span class="No-Break">internally concurrent.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor153"/>Creating concurrent functions</h1>
			<p>Broadly speaking, there are two types<a id="_idIndexMarker503"/> of concurrency that we will be looking at in this chapter. We can call them <strong class="bold">intra-concurrency</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">extra-concurrency</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><em class="italic">Intra-concurrency</em> is about creating<a id="_idIndexMarker504"/> functions that are implemented concurrently internal to each function. For example, in <a href="B18771_06.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, we saw various functions such as <strong class="source-inline">Filter</strong>, <strong class="source-inline">Map</strong>, and <strong class="source-inline">FMap</strong> that lend themselves to a concurrent implementation. That will be the focus of this section. Notably, they can be used in conjunction with each other so that we achieve concurrency at multiple steps in our algorithm, and we can even decide on the level of concurrency required for each <span class="No-Break">step individually.</span></li>
				<li><em class="italic">Extra-concurrency</em> is about chaining together functions using Go's built-in concurrency features: channels and goroutines. This is explored later in <span class="No-Break">the chapter.</span></li>
			</ul>
			<p>Why are many of the fundamental building blocks of functional programming good candidates for concurrency? Well, first and foremost, it is because a purely functional implementation lends itself to a concurrent implementation without too many headaches. As we saw in the preceding chapter, concepts such as an immutable state and the elimination of side effects make it easy to take our functions and concurrently rewrite them. There should not be interference from other functions, no outside state to deal with, and no I/O to contend with. However, just because we <em class="italic">can</em> does not mean that we <em class="italic">should</em>. In this chapter, I will make the assumption that a concurrent implementation is going to be the right choice for the problems that we are solving. In the real world, concurrency is not a zero-cost implementation. There is real overhead associated with writing a concurrent application, as the threaded execution needs to be managed by our system (or, in Go’s case, <span class="No-Break">our runtime).</span></p>
			<p>Although in Go we are not responsible for managing the goroutines ourselves, under the hood of the Go runtime, context switching is not a zero-cost implementation. This means that just adding concurrent calls does not guarantee a performance improvement and can, in fact, harm performance. Ultimately, as with anything done for performance, the key to understanding the benefit that can be achieved is obtained through profiling your application. Profiling itself is beyond the scope of this section; the only comment to make on it is that Go has built-in benchmarking<a id="_idIndexMarker505"/> tools, which we saw in earlier chapters. These can also be used to determine the cost benefit of concurrent versus <span class="No-Break">sequential functions.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>Concurrent filter implementation</h2>
			<p>As we started with sequential filter implementation<a id="_idIndexMarker506"/> in earlier chapters and have become more familiar with it throughout the book, let’s start with this function and turn it into a concurrent implementation. Keep in mind that our initial function was a pure function, and as such, refactoring it into a concurrent one can be done without causing too much of a headache. There are a few steps to making <span class="No-Break">this concurrent:</span></p>
			<ol>
				<li>Split the input <span class="No-Break">into batches.</span></li>
				<li>Start a process to filter <span class="No-Break">each batch.</span></li>
				<li>Aggregate the result of <span class="No-Break">each batch.</span></li>
				<li>Return the <span class="No-Break">aggregated output.</span></li>
			</ol>
			<p>To achieve this, we do need to refactor the initial <strong class="source-inline">Filter</strong> implementation. We will leverage some of Go’s built-in concurrency features to implement this, and the first thing we’ll want to leverage are channels and goroutines. In our initial <strong class="source-inline">Filter</strong> function, we iterated over each element, appended it to an output slice if it matched the predicate, and finally, we returned the output slice. In this version, rather than returning an output slice, we’ll write the result onto <span class="No-Break">a channel:</span></p>
			<pre class="source-code">
type Predicate[A any] func(A) bool
func Filter[A any](input []A, p Predicate[A], out chan []A)
  {
        output := []A{}
        for _, element := range input {
                if p(element) {
                        output = append(output, element)
                }
        }
        out &lt;- output
}</pre>
			<p>Writing to a channel allows<a id="_idIndexMarker507"/> us to call this function in a traditional concurrent fashion within Go. However, before we get there, we’ll have to establish a wrapper function around <strong class="source-inline">Filter</strong>, which we will call <strong class="source-inline">ConcurrentFilter</strong>. This function does a few things, including allowing us to configure the batch size. Playing around with the batch sizes can help us tweak the performance to get it where we want it (if there are too few batches, there’s little benefit to running concurrently; too many, and the overhead caused by managing goroutines similarly reduces our benefit). Apart from batching our input, we’ll also need to call the <strong class="source-inline">Filter</strong> function prepended with the <strong class="source-inline">go</strong> keyword so that it spins up a new goroutine. Finally, this function will read the results for each of the goroutines that we started and aggregate this result in to a single <span class="No-Break">output slice:</span></p>
			<pre class="source-code">
func ConcurrentFilter[A any](input []A, p Predicate[A],
	batchSize int) []A {
	output := []A{}
out := make(chan []A)
	threadCount := int(math.Ceil(float64(len(input)) /
		float64(batchSize)))
	fmt.Printf("goroutines: %d\n", threadCount)
	for i := 0; i &lt; threadCount; i++ {
		fmt.Println("spun up thread")
		if ((i + 1) * batchSize) &lt; len(input) {
			go Filter(input[i*batchSize:(i+1)*batchSize],  	                     p, out)
		} else {
			go Filter(input[i*batchSize:], p, out)
		}
	}
	for i := 0; i &lt; threadCount; i++ {
		filtered := &lt;-out
		fmt.Printf("got data: %v\n", filtered)
		output = append(output, filtered...)
	}
	close(out)
	return output
}</pre>
			<p>In the preceding code snippet, we keep the print statements so we can see what execution looks like when running<a id="_idIndexMarker508"/> this. Let’s create a simple <strong class="source-inline">main</strong> function that will filter a slice of integers in this fashion and look at the <span class="No-Break">corresponding output:</span></p>
			<pre class="source-code">
func main() {
        ints := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        output := ConcurrentFilter(ints, func(i int) bool {
           return i%2 == 0 }, 3)
        fmt.Printf("%v\n", output)
}</pre>
			<p>Running this function gives us the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
goroutines: 4
spun up thread
spun up thread
spun up thread
spun up thread
got data: [10]
got data: [2]
got data: [4 6]
got data: [8]
[10 2 4 6 8]</pre>
			<p>In this output, we can see that <strong class="source-inline">4</strong> goroutines had to be spun up to process our input with a batch size of <strong class="source-inline">3</strong>. This has sharded our input data into the <span class="No-Break">following segments:</span></p>
			<pre class="source-code">
[]int{1,2,3}
[]int{4,5,6}
[]int{7,8,9}
[]int{10}</pre>
			<p>Next, we can see in which order the threads completed and returned their output. As you can tell from the output, we get the output back in random order. This is visible both in the <strong class="source-inline">got data</strong> output as well as in the final <span class="No-Break">aggregated result.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">An important callout here is that by sharding our data and running our functions concurrently, we no longer have a predictable sequence order in the output list. If we want to restore the ordering of our data, we should implement a <strong class="source-inline">Sort</strong> function after concurrently calling <span class="No-Break">our functions.</span></p>
			<p>This <strong class="source-inline">Filter</strong> implementation is a good template<a id="_idIndexMarker509"/> to start from when we want to make our functions run concurrently. Let’s take a look at a concurrent implementation for both the <strong class="source-inline">Map</strong> and <span class="No-Break"><strong class="source-inline">FMap</strong></span><span class="No-Break"> functions.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Concurrent Map and FMap implementation</h2>
			<p>Implementing the <strong class="source-inline">Map</strong> and <strong class="source-inline">FMap</strong> functions concurrently<a id="_idIndexMarker510"/> requires the same steps<a id="_idIndexMarker511"/> as for the concurrent <strong class="source-inline">Filter</strong> implementation, <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Split the input <span class="No-Break">into batches.</span></li>
				<li>Start a process to filter <span class="No-Break">each batch.</span></li>
				<li>Aggregate the result of <span class="No-Break">each batch.</span></li>
				<li>Return the <span class="No-Break">aggregated output.</span></li>
			</ol>
			<p>As such, we won’t go over each step in detail for these implementations. The explanation behind each step and how we implement it is pretty much identical to the <strong class="source-inline">Filter</strong> implementation. We are showing these here for completeness and to showcase the general pattern of refactoring these functions to <span class="No-Break">operate concurrently.</span></p>
			<h3>Concurrent Map</h3>
			<p>To implement our <strong class="source-inline">Map</strong> function<a id="_idIndexMarker512"/> concurrently, we first refactor the <strong class="source-inline">Map</strong> function that we created in <a href="B18771_06.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Here, again, we are removing the explicit return, and we’ll use channels to communicate the output of mapping <span class="No-Break">each element:</span></p>
			<pre class="source-code">
type MapFunc[A any] func(A) A
func Map[A any](input []A, m MapFunc[A], out chan []A) {
        output := make([]A, len(input))
        for i, element := range input {
                output[i] = m(element)
        }
        out &lt;- output
}</pre>
			<p>Next, we will implement the <strong class="source-inline">ConcurrentMap</strong> function, batching the output as we did with the <span class="No-Break"><strong class="source-inline">ConcurrentFilter</strong></span><span class="No-Break"> implementation:</span></p>
			<pre class="source-code">
func ConcurrentMap[A any](input []A, mapFn MapFunc[A],
    batchSize int) []A {
        output := make([]A, 0, len(input))
        out := make(chan []A)
        threadCount := int(math.Ceil(float64(len(input)) /
            float64(batchSize)))
        fmt.Printf("goroutines: %d\n", threadCount)
        for i := 0; i &lt; threadCount; i++ {
                fmt.Println("spun up thread")
                if ((i + 1) * batchSize) &lt; len(input) {
                        go Map(input[i*batchSize:(i+1)
                           *batchSize], mapFn, out)
                } else {
                        go Map(input[i*batchSize:],
                            mapFn, out)
                }
        }
        for i := 0; i &lt; threadCount; i++ {
                mapped := &lt;-out
                fmt.Printf("got data: %v\n", mapped)
                output = append(output, mapped...)
        }
        close(output)
        return output
}</pre>
			<p>Note that both the <strong class="source-inline">ConcurrentFilter</strong> and <strong class="source-inline">ConcurrentMap</strong> implementations require <strong class="source-inline">batchSize</strong> to be passed as input to the function. This means that we can process each step with a different<a id="_idIndexMarker513"/> number of goroutines, and tweak each <span class="No-Break">function individually:</span></p>
			<pre class="source-code">
func main() {
        ints := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        output := ConcurrentFilter(ints, func(i int) bool {
            return i%2 == 0 }, 3)
        fmt.Printf("%v\n", output)
        output = ConcurrentMap(output, func(i int) int {
            return i * 2 }, 2)
        fmt.Printf("%v\n", output)
}</pre>
			<p>In this example, we are using a batch size of <strong class="source-inline">3</strong> for filtering but only a batch size of <strong class="source-inline">2</strong> for mapping. The output of this <strong class="source-inline">main</strong> function looks <span class="No-Break">like this:</span></p>
			<pre class="source-code">
goroutines: 4
spun up thread
spun up thread
spun up thread
spun up thread
got data: [10]
got data: [2]
got data: [4 6]
got data: [8]
[10 2 4 6 8]
{next statements are the output for the map function}
goroutines: 3
spun up thread
spun up thread
spun up thread
got data: [16]
got data: [20 4]
got data: [8 12]
[16 20 4 8 12]</pre>
			<h3>Concurrent FMap implementation</h3>
			<p>This implementation<a id="_idIndexMarker514"/> is pretty similar to the <strong class="source-inline">Map</strong> implementation. The main difference is that our channel has changed type. Rather than having the entire function signature operate on the same <strong class="source-inline">A</strong> type, we’ll now have a mix of <strong class="source-inline">A</strong> and <strong class="source-inline">B</strong>. This is a minor change and does not affect the implementation details beyond having to create the right type for <span class="No-Break">the channels:</span></p>
			<pre class="source-code">
func FMap[A, B any](input []A, m func(A) B, out chan []B) {
        output := make([]B, len(input))
        for i, element := range input {
                output[i] = m(element)
        }
        out &lt;- output
}
func ConcurrentFMap[A, B any](input []A, fMapFn ,
    batchSize int) []B {
        output := make([]B, 0, len(input)
        out := make(chan []B)
        threadCount := int(math.Ceil(float64(len(input)) /
            float64(batchSize)))
        fmt.Printf("goroutines: %d\n", threadCount)
        for i := 0; i &lt; threadCount; i++ {
                fmt.Println("spun up thread")
                if ((i + 1) * batchSize) &lt; len(input) {
                        go FMap(input[i*batchSize:
                           (i+1)*batchSize], fMapFn, out)
                } else {
                        go FMap(input[i*batchSize:],
                            fMapFn, out)
                }
        }
        for i := 0; i &lt; threadCount; i++ {
                mapped := &lt;-out
                fmt.Printf("got data: %v\n", mapped)
                output = append(output, mapped...)
        }
        return output
}</pre>
			<p>I hope that this serves as an illustration<a id="_idIndexMarker515"/> of how easy it is to create concurrent implementations for functions that are written in the purely functional style. There is one limitation posed by Go that makes this a bit more verbose than it would be in other languages. As Go is a strictly typed language (which is a good thing in general), our function signatures need to match exactly when using higher-order functions. Otherwise, we could template out the recursive part of our function and call a higher-order function for the actual implementation on each node. In pseudo-code, we would get something like <span class="No-Break">the following:</span></p>
			<pre class="source-code">
func ConcurrentRunner(input []Input, fn func(), batchSize
  int) []Output {
     // set up channels and batch logic
     for batch in batches {
         go Run(fn(batch))
     }
     // collect output and return
}</pre>
			<p>Either way, we saw that leveraging concurrency<a id="_idIndexMarker516"/> in our functions is relatively headache-free and can be achieved with only a bit of refactoring. Let’s move on to the final topic of this chapter, which is using concurrency mechanisms to chain <span class="No-Break">functions together.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor156"/>The pipeline pattern</h1>
			<p>In the previous sections, we concerned<a id="_idIndexMarker517"/> ourselves with organizing concurrency inside the functions themselves. However, we have chained them together pretty much as we would normally, by calling them in sequential order in the main function. In this section, we are going to look at the pipeline pattern, which will allow us to leverage goroutines and channels to chain function calls together. First, let’s discuss what a pipeline is exactly. In 1964, Doug McIlroy wrote <span class="No-Break">the following:</span></p>
			<p class="author-quote">We should have some ways of coupling programs like garden hose – screw in another segment when it becomes necessary to massage data in another way.</p>
			<p>This quote neatly expresses the Unix philosophy of composing programs. Many of us are familiar with the concept of Unix pipes, denoted by the <strong class="source-inline">|</strong> symbol. By using pipes, we can chain Unix programs together. The output of one program becomes the input of the next. For example, we can use <strong class="source-inline">cat</strong> to read a file, and we can use <strong class="source-inline">wc</strong> to get the word count of that file. To join this together, we would write <strong class="source-inline">cat file.txt | wc</strong>. In Unix’s modular program approach, the idea is that programs each serve a single purpose but can be joined together to create complex programs. This philosophy can be ported over to the functional programming paradigm. We want to chain simple functions together, where each function only has a single purpose, to create a complex program. Take the following example; each function serves a single purpose, and we chain them together using the pipe (<strong class="source-inline">|</strong>) <span class="No-Break">character:</span></p>
			<pre class="source-code">
cat main.go | grep "func" | wc -l | awk '{print "lines: "
  $1}'</pre>
			<p>In this example, we first read the <strong class="source-inline">main.go</strong> file using <strong class="source-inline">cat</strong>. We send the content of that file to <strong class="source-inline">grep</strong>, which searches<a id="_idIndexMarker518"/> that content for the <strong class="source-inline">func</strong> keyword. Then, we send each line that matches this search to the <strong class="source-inline">wc</strong> program and count the lines in the output (the <strong class="source-inline">-l</strong> flag counts newlines). And finally, we send this to <strong class="source-inline">awk</strong> and print the result. What follows is a similar way of chaining Go functions together, rather than <span class="No-Break">Unix commands.</span></p>
			<h3>Chaining functions with channels</h3>
			<p>Go ships with all the tools necessary<a id="_idIndexMarker519"/> to create such building programs, namely<a id="_idIndexMarker520"/> channels. Channels are a way to send messages (data) from one function to another; thus, we can use channels as an alternative to the <span class="No-Break">Unix pipes.</span></p>
			<p>The first step in creating our pipeline starts by changing how our functions get their input and output. For the rest of this chapter, we will mainly be focusing on two functions, <strong class="source-inline">Filter</strong> and <strong class="source-inline">Map</strong>, but this can be extended to any other functions. The core idea is to use channels for input and output data communication. First, let’s take a look at the <strong class="source-inline">Filter</strong> function and how this needs to be changed to follow our channels-in/channels-out approach. We’ll name our new function <strong class="source-inline">FilterNode</strong>. We’ll get back to this naming convention later, but each function can be thought of as a node in our chain of functions. Instead of accepting a slice as input, we’ll have a channel as input, from which we can read incoming data. We’ll still have <strong class="source-inline">predicate</strong> as expected, and finally, we’ll return a channel rather than a slice of data <span class="No-Break">as well:</span></p>
			<pre class="source-code">
func FilterNode[A any](in &lt;-chan A, predicate Predicate[A])
  &lt;-chan A {
        out := make(chan A)
        go func() {
                for n := range in {
                        if predicate(n) {
.                                out &lt;- n
                        }
                }
                close(out)
        }()
        return out
}</pre>
			<p>In the preceding function, the main algorithm for filtering elements remains unchanged. We’ll test each value against a predicate; if the predicate returns <strong class="source-inline">true</strong>, we’ll keep the value (by sending it to the output channel); otherwise, we'll discard it. Pay attention to the use of the <strong class="source-inline">go</strong> keyword here. This function, although it gets executed immediately, is launched on its own goroutine. The function immediately returns the <strong class="source-inline">out</strong> channel, although the evaluation on the goroutine has not necessarily finished <span class="No-Break">the computation.</span></p>
			<p>The next function<a id="_idIndexMarker521"/> that we will refactor<a id="_idIndexMarker522"/> similarly is the <strong class="source-inline">Map</strong> function. It’s an analogous change to the <strong class="source-inline">Filter</strong> function. We’ll use a channel to receive input for the function, a channel to return the output, and run the actual mapping logic inside a goroutine, which we start before returning the channel from <span class="No-Break">our function:</span></p>
			<pre class="source-code">
func MapNode[A any](in &lt;-chan A, mapf MapFunc[A]) &lt;-chan A
  {
        out := make(chan A)
        go func() {
                for n := range in {
                        out &lt;- mapf(n)
                }
                close(out)
        }()
        return out
}</pre>
			<p>So far, so good – we’ve refactored<a id="_idIndexMarker523"/> two of our functions to fit in with<a id="_idIndexMarker524"/> this new design. Next, let’s tackle the question of receiving input to these functions. From the function signature, we can tell that we need to receive data on a channel of type <strong class="source-inline">A</strong>. Thus, any function that can provide this can be used as the input for our function. We’ll call these types of functions <em class="italic">generators</em>. The first generator that we will create takes a variadic input of type <strong class="source-inline">A</strong> and pushes each of these values onto <span class="No-Break">a channel:</span></p>
			<pre class="source-code">
func Generator[A any](input ...A) &lt;-chan A {
        out := make(chan A)
        go func() {
                for _, element := range input {
                        out &lt;- element
                }
                close(out)
        }()
        return out
}</pre>
			<p>As you can tell, the main logic still resembles that of the previous <strong class="source-inline">Filter</strong> and <strong class="source-inline">Map</strong> implementations. The main difference is that we’re no longer receiving values over a channel but, rather, through some other input data structure (in this case, variadic input parameters). This could also be a function that reads a file and places each line on the channel. It’s similar to how <strong class="source-inline">cat</strong> worked in our earlier <span class="No-Break">Unix example:</span></p>
			<pre class="source-code">
func Cat(filepath string) &lt;-chan string {
        out := make(chan string)
        f, err := ioutil.ReadFile(filepath)
        if err != nil {
                panic(err)
        }
        go func() {
                lines := strings.Split(string(f), "\n")
                for _, line := range lines {
                        out &lt;- line
                }
                close(out)
        }()
        return out
}</pre>
			<p>The key point is that our function<a id="_idIndexMarker525"/> places values on a channel and returns<a id="_idIndexMarker526"/> this channel. How it gets to those values doesn’t matter too much for building our pipeline. Before we can test this implementation end to end, we still have one hurdle to cross. Each node in this setup writes data to a channel, but to collect the output at the end, we’ll want to store it in a more common data structure. Slices are the perfect structure for this, at least in our examples. We can call this last type of function a <em class="italic">collector</em>. A collector takes a channel as input and returns a slice of the elements as output. Essentially, it’s performing the reverse operation of <span class="No-Break">the </span><span class="No-Break"><em class="italic">generator</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
func Collector[A any](in &lt;-chan A) []A {
        output := []A{}
        for n := range in {
                output = append(output, n)
        }
        return output
}</pre>
			<p>With this in place, we can tie all of them together into a single pipeline. To demonstrate this, in our <strong class="source-inline">main</strong> function, we will push some numbers to a channel using <strong class="source-inline">Generator</strong>. We’ll then filter these numbers to only retain the even ones using <strong class="source-inline">FilterNode</strong>. These numbers then get squared using <strong class="source-inline">MapNode</strong>, and finally, we collect the output in a slice using the <span class="No-Break"><strong class="source-inline">Collector</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
func main(){
        generated := Generator(1, 2, 3, 4)
        filtered := FilterNode(generated, func(i int) bool
            { return i%2 == 0 })
        mapped := MapNode(filtered, func(i int) int {
            return i * 2 })
        collected := Collector(mapped)
        fmt.Printf("%v\n", collected)
}</pre>
			<p>The output of running this is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
[4 8]</pre>
			<p>The preceding<a id="_idIndexMarker527"/> is a good first step toward chaining our functions<a id="_idIndexMarker528"/> together. However, we can make it cleaner. We can build a <strong class="source-inline">ChainPipes</strong> function that will tie together the various functions and take care of managing the channels <span class="No-Break">for us.</span></p>
			<h3>Improved function chaining</h3>
			<p>The initial approach of chaining<a id="_idIndexMarker529"/> the functions together was a workable<a id="_idIndexMarker530"/> solution, but it required some overhead, as we had to manage passing around the right channels to each subsequent function. What we want to achieve is for the engineers using our setup only needing to concern themselves with the functions to call, and which order to call them in. We don’t want them to be concerned about how the channels operate underneath; we can consider that an implementation detail. What we will work toward in this section will allow us to compose the functions <span class="No-Break">like so:</span></p>
			<pre class="source-code">
        out := pkg.ChainPipes(generated,
                pkg.CurriedFilterNode(func(i int) bool { return i%2 == 0 }),
                pkg.CurriedMapNode(func(i int) int { return
                    i * i }))</pre>
			<p>This snippet gives us a bit of a teaser of what’s to come. In order to chain functions like this, we will need to take advantage of function currying. Let’s get there step by step. What we want to achieve is function composition by passing in functions to <strong class="source-inline">ChainPipes</strong>, as we saw in the preceding snippet. Go has a strict type system, so to make this function work nicely, we want to define a custom type for such functions, which will allow us to use it in the function signature and get the compiler to type-check <span class="No-Break">for us.</span></p>
			<p>The first thing we will do is define<a id="_idIndexMarker531"/> our own types for the main functions<a id="_idIndexMarker532"/> representing an operation on our data. We’ll call these <strong class="source-inline">Nodes</strong>. There are three distinct types of nodes that we can define, based on the previous discussion – nodes that generate a channel, nodes that take a channel and return a new channel, and finally, nodes that take a channel and return a concrete data structure such as <span class="No-Break">a slice:</span></p>
			<pre class="source-code">
type (
        Node[A any]          func(&lt;-chan A) &lt;-chan A
        GeneratorNode[A any] func() &lt;-chan A
        CollectorNode[A any] func(&lt;-chan A) []A
)</pre>
			<p>These type definitions make up the bread and butter of the function types that can be used to chain together our applications. With this in place, we can define the <strong class="source-inline">ChainPipes</strong> function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
func ChainPipes[A any](in &lt;-chan A, nodes ...Node[A]) []A {
        for _, node := range nodes {
                in = node(in)
        }
        return Collector(in)
}</pre>
			<p>The preceding snippet creates a <strong class="source-inline">ChainPipes</strong> function that takes a channel as input and a series of nodes. Finally, it will call the default collector and return the data in a slice of type <strong class="source-inline">[]A</strong>. Do note that one limitation is that we are assuming that each node has a compatible type (<strong class="source-inline">A</strong>) throughout <span class="No-Break">the chain.</span></p>
			<p>To make the type system work, each node needs to have the same function signature. In our initial setup, that was difficult, as we already had two distinct function signatures for <strong class="source-inline">Filter</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">Map</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
func FilterNode[A any](in &lt;-chan A, predicate Predicate[A])
  &lt;-chan A
func MapNode[A any](in &lt;-chan A, mapf MapFunc[A]) &lt;-chan A</pre>
			<p>More functions<a id="_idIndexMarker533"/> would mean more distinct function signatures. Therefore, what<a id="_idIndexMarker534"/> is needed is refactoring these functions so that they adhere to the same type signature. We have already learned how to do that through function currying. We need to create two new functions that each <strong class="bold">return</strong> a function of type <strong class="source-inline">Node</strong>. Each function will have the original functionality of <strong class="source-inline">Filter</strong> and <strong class="source-inline">Map</strong> baked in but returns a new function that takes a channel as the input (hence the function is <span class="No-Break">partially applied):</span></p>
			<pre class="source-code">
func CurriedFilterNode[A any](p Predicate[A]) Node[A] {
        return func(in &lt;-chan A) &lt;-chan A {
                out := make(chan A)
                go func() {
                        for n := range in {
                                if p(n) {
                                        out &lt;- n
                                }
                        }
                        close(out)
                }()
                return out
        }
}
func CurriedMapNode[A any](mapFn MapFunc[A]) Node[A] {
        return func(in &lt;-chan A) &lt;-chan A {
                out := make(chan A)
                go func() {
                        for n := range in {
                                out &lt;- mapFn(n)
                        }
                        close(out)
                }()
                return out
        }
}</pre>
			<p>We can tell in the preceding example that the core logic of each function has remained the same. However, rather than being instantly applied when the function is called, a new function is returned that expects to receive a channel as input and returns a channel as output. Inside this anonymous function, we have coded the <strong class="source-inline">Filter</strong> and <strong class="source-inline">Map</strong> <span class="No-Break">logic respectively.</span></p>
			<p>As the return type is <strong class="source-inline">Node</strong>, that means<a id="_idIndexMarker535"/> that when<a id="_idIndexMarker536"/> we call the <strong class="source-inline">CurriedFilterNode</strong> function, we are not receiving a result, but we are receiving another function that needs to be called at a later stage to actually compute the filtered list <span class="No-Break">of values:</span></p>
			<pre class="source-code">
pkg.CurriedFilterNode(func(i int) bool { return i%2 == 0 }}</pre>
			<p>This is the crucial part of making our pipeline builder work. If we look at <strong class="source-inline">ChainPipes</strong> again, the main loop is calling the nodes (functions) that were supplied to it with the channel as input and reassigning the output to the same channel that was used <span class="No-Break">as input:</span></p>
			<pre class="source-code">
        for _, node := range nodes {
                in = node(in)
        }</pre>
			<p>We could go one step further and also abstract away the generator from the <span class="No-Break"><strong class="source-inline">ChainPipes</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
func ChainPipes[A any](gn GeneratorNode[A], nodes
  ...Node[A]) []A {
        in := gn()
        for _, node := range nodes {
                in = node(in)
        }
        return Collector(in)
}</pre>
			<p>With this change in place, it does imply<a id="_idIndexMarker537"/> that when calling the function, we need <a id="_idIndexMarker538"/>another curried function to supply the generator. This can be done in-line, but for clarity, the following example is a separate function existing at the package level. In this case, we will use the <strong class="source-inline">Cat</strong> function that we introduced earlier and return the <span class="No-Break">curried version:</span></p>
			<pre class="source-code">
func CurriedCat(filepath string) func() &lt;-chan string {
        return func() &lt;-chan string {
                out := make(chan string)
                f, err := ioutil.ReadFile(filepath)
                if err != nil {
                        panic(err)
                }
                go func() {
                        lines := strings.Split(string(f),
                            "\n")
                        for _, line := range lines {
                                out &lt;- line
                        }
                        close(out)
                }()
                return out
        }
}</pre>
			<p>Once again, this curried version<a id="_idIndexMarker539"/> of the function operates in the same way<a id="_idIndexMarker540"/> as the non-curried version. However, through currying, we can make it adhere to the type signature indicated by the <strong class="source-inline">ChainPipes</strong> function. We can now pass both the generator as well as the nodes to <span class="No-Break">this function:</span></p>
			<pre class="source-code">
func main() {
        out := ChainPipes[string](CurriedCat("./main.go"),
                CurriedFilterNode(func(s string) bool {
                    return strings.Contains(s, "func") }),
                CurriedMapNode(func(i string) string {
                    return "line contains func: " + i }))
        fmt.Printf("%v\n", out2)
}</pre>
			<p>Notice that in the preceding example, we did have to give a type hint to <strong class="source-inline">ChainPipes</strong> to indicate the resulting type of the <strong class="source-inline">CurriedCat</strong> function. What we saw in the preceding section is that by using channels, the Go type system, higher-order functions, and more specifically, function currying, we can build programs by chaining together functions in the right way. Using this method of function composition, it’s also easier to refactor our application. If we want<a id="_idIndexMarker541"/> to apply a map before filtering, we just need<a id="_idIndexMarker542"/> to change the order in which the node is passed <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">ChainPipes</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, we took a look at how Go’s concurrency model can be used when writing code in the functional paradigm. We started the chapter with a brief discussion on the difference between concurrency, parallelism, and distributed computing to delineate exactly what <span class="No-Break">concurrency is.</span></p>
			<p>Once we established that concurrency is the ability to do multiple tasks at once (although not necessarily simultaneously), we looked at how we can refactor the functions from <a href="B18771_06.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> into a concurrent implementation, leveraging channels and goroutines. We concluded this chapter by looking at pipelines, a way to create programs by composing functions together and orchestrating the flow of data with the use of channels. We also looked at how we can create a higher-order function to compose functions (<strong class="source-inline">ChainPipes</strong>) and have observed how, through the use of function currying, we can create functions that adhere to our type system without giving up <span class="No-Break">type safety.</span></p>
			<p>In the next and final chapter, we will take a look at programming libraries that we can leverage to create Go programs, following some of the functional programming principles that we explored in <span class="No-Break">this book.</span></p>
		</div>
		<div>
			<div id="_idContainer037" class="IMG---Figure">
			</div>
		</div>
	</body></html>