- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and Functional Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is all around us, both in the real world as well as the virtual
    one. Humans can easily multitask (although we might not do a good job at either
    task). It’s entirely possible to drink a cup of coffee while you are reading this
    chapter or to run while listening to a podcast. For machines, concurrency is a
    complex undertaking, although a lot of that complexity can be hidden away by the
    programming language we choose.
  prefs: []
  type: TYPE_NORMAL
- en: Go was built to be a language with all the necessary tools a modern-day software
    engineer needs. As we are now in a world where CPU power is abundant for most
    intents and purposes, it’s only natural that concurrency was a main concern when
    developing the language, rather than having to bolt it on later. In this chapter,
    we are going to take a look at how functional programming can help with concurrency
    and, conversely, how concurrency can help with functional programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why functional programming helps us write concurrent code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create concurrent functions (Filter, Map, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to chain functions together concurrently using the pipeline pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you can use any version of Go at or above version 1.18\. All
    the code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Functional-Programming-in-Go./tree/main/Chapter10](https://github.com/PacktPublishing/Functional-Programming-in-Go./tree/main/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming and concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already hinted at it throughout this book, but the ideas behind functional
    programming can help us write concurrent code. Typically, thinking about concurrency
    is a bit of a headache, even when a language has modern tools to support it, such
    as goroutines and channels. Before we dive too deep into this material, let’s
    first take a small detour as a refresher on what exactly we mean when we talk
    about concurrent code, and how it compares to parallelism and distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency, parallelism, and distributed computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The terms *concurrency*, *parallelism*, and *distributed computing* are, at
    times, used interchangeably. And while they are related, they are not exactly
    the same thing. Let’s just point out what we mean by concurrency first. **Concurrency**
    is what happens when our program can execute multiple tasks at the same time.
    For example, when we are playing a video game, typically a thread is playing audio,
    another one is processing input from the player, and another one is taking care
    of the internal game logic, updating the game state and performing the main game
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: Video games have been around for a long time, and a game such as *DOOM* works
    in this way. It’s also safe to say that people were not playing this on a computer
    with multiple cores available back in 1995\. In other words, it’s possible for
    a single core to manage the execution of these distinct tasks and give the appearance
    of executing them at the same time. Exactly how this is done is beyond the scope
    of this book, but as a takeaway, just remember that the concurrency that we will
    mainly focus on is concurrency as defined previously – not the simultaneous execution
    of code, but the concurrent execution of code. One thing to note, though, is that
    concurrency can happen across multiple cores, or pipelines, as well. However,
    to keep things simple, we can imagine concurrency using a single core.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the second term, **parallelism**. When we talk about a program
    executing in parallel, this means that multiple cores are performing a task simultaneously.
    You can not have parallelism without a physical means to run two tasks at the
    same time. The native Go mechanisms of channels and goroutines are focused on
    concurrency and not parallelism. This is an important distinction between the
    two. However, Go still lends itself to building out parallel algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an idea of what this looks like, there are a few packages available
    for Go that offer parallel solutions, such as the ExaScience Pargo package: [https://github.com/ExaScience/pargo](https://github.com/ExaScience/pargo).
    At the time of writing, this package is written in a pre-generics fashion, so
    do bear that in mind when looking through the code. In *Figure 10**.1*, the difference
    between concurrency and parallelism is highlighted by how the tasks get executed.
    Notably, the two tasks in the concurrent model are broken into multiple chunks,
    and each gets assigned CPU time in an alternating fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Concurrent (above) versus parallel (below) execution](img/Figure_10.1_B18771.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Concurrent (above) versus parallel (below) execution'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have **distributed computing**. While concurrency is part of distributed
    computing, it is not the only requirement for this. Distributed computation does
    imply spreading out computational tasks over multiple machines, in which sense
    it is concurrent, but there’s more overhead than with typically concurrent or
    parallel applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In distributed systems, you need to have mechanisms for fault tolerance (what
    if one node in the network becomes unavailable?) and mechanisms for dealing with
    the network (unreliable or insecure networks). So, while people might talk about
    distributed computation as an example of concurrency, concurrency only gives you
    the bare minimum required. The physical infrastructure and myriad of difficulties
    in making a distributed system work are beyond the scope of this book. One thing
    to take away is that Go is a language that can be used to write distributed systems.
    In fact, the use of goroutines and channels might help you build out the underlying
    infrastructure needed for distributed systems, but you’ll need more than the basic
    functionality of the language. If you want to learn more about distributed computing
    with Go, the book *Distributed Computing with Go* is a good place to start: [https://www.packtpub.com/product/distributed-computing-with-go/9781787125384?_ga=2.217817046.1391922680.1675144438-1944326834.1674539572](https://www.packtpub.com/product/distributed-computing-with-go/9781787125384?_ga=2.217817046.1391922680.1675144438-1944326834.1674539572).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on concurrency only, and we won’t zoom in on
    parallelism or distributed computing. However, why do we want our code to be concurrent?
    There are a few clear advantages that this can bring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Higher responsiveness**: A program does not need to wait for a single long-running
    task to complete before starting another one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Higher performance**: If we can chunk out a heavy workload and perform this
    over multiple threads (and Go might schedule these across multiple cores to get
    a form of parallelism as well), this will reduce the time it takes to complete
    the operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming and concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ve made the claim before in this book that functional programming makes it
    easier to write concurrent code, but this claim needs to be tailored a little
    bit further. When talking about how functional programming makes concurrency easier,
    we are talking about the stricter subset of functional programming called “pure”
    functional programming. Pure functional programming gives us a few key features
    that make reasoning about concurrent execution easier and our code less error-prone.
    These are the main features responsible for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Immutable variables and state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pure functions (no side effects)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Referential transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lazy evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Composability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the rest of this chapter, when talking about functional programming, the
    assumption can be made that we’re talking strictly about pure functional programming.
    Let’s focus on each of these features and explain why they make for safer concurrent
    code, or make our code at least easier to reason about. The result is that when
    our code is easier to understand, it should help us reduce the number of bugs
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable variables and state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working in an object-oriented model, objects typically hold an internal
    state. If this state is allowed to mutate, then the state that two threads are
    working on might diverge. By not allowing the state to change, even if operating
    on the same data sources (or, rather, copies of the same data), our functions
    can execute independently of each other without ever messing with the shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: In Go, if we do want to use structs, there are some pitfalls, which we discussed
    in earlier chapters. By avoiding the use of pointers, we can avoid the main causes
    of mutation in structs. When writing pure functional code, each individual component
    of our code needs to be immutable. When each component is immutable, we can more
    safely execute functions concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue we avoid by having immutable variables and states is that of resource
    contention. If we have a single true resource (a singleton in an object-oriented
    model), then this resource might be locked by thread A, causing thread B to wait
    until the resource is freed up before it can be used. Typically, this is implemented
    through a resource-locking mechanism (thread A locks the resource , *X*, performs
    operations while other threads wait for resource *X*, and then finally removes
    the lock when it is done operating). In a purely functional world, we would not
    need such singleton operations, partly due to our immutable state and partly due
    to the other benefits, such as pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 4*](B18771_04.xhtml#_idTextAnchor060), a function is
    considered pure when it does not produce any side effects and does not interact
    with the outside world. In this book, we implemented many functions that are common
    to functional programming. All of these were written in the pure functional style
    (although remember that pure functional is a subset of functional programming
    and not strictly required). The benefits here relate to the immutable state but
    extend beyond it as well. If our functions do not depend on the program state,
    then anything modifying the state of our program cannot disrupt our function.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond this, it also eliminates another class of problems. If our functions
    were allowed to mutate state, or our system, the order of operations would matter.
    For example, imagine that we were to write a concurrent function that appends
    content to a file. Writing to a file is a clear case of a side effect, but in
    a concurrent application, the content of our file would now depend on the order
    in which our threads are executed. This breaks the determinism of our application
    and, furthermore, would likely lead to a file that is not exactly what we desired.
    In an object-oriented model, this is again resolved through locking. In a purely
    functional language, the “impure” functions would be handled by monads. Go is
    not purely functional, but later in this chapter, we will look at the pipeline
    pattern through which we can model the data flow and control the side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Referential transparency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Referential transparency** means that we can replace a function call with
    its result, without changing the result of our computation. We covered this in
    more detail in [*Chapter 2*](B18771_02.xhtml#_idTextAnchor028), but for concurrency,
    the important aspect is that if all our calls are referentially transparent, it
    does not matter when exactly a call is resolved (ahead of time or just in time).
    This means that when we chunk our code out into concurrent functions, it is safe
    to resolve certain function calls ahead of time in a concurrent fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`GET` request to a URL. We will use two callbacks, which will be lazily evaluated.
    The first callback will be resolved only if the `GET` request completed successfully,
    while the second callback will be resolved if the `GET` request failed. Note that
    here we mean the `GET` request itself did work, but we received a response code
    that is not in the `200` range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can see that `getURL` requires a string representing
    a URL to resolve, as well as two functions. Both functions have the same `ResponseFunc`
    type, which is a function with the `func(*http.Response)` signature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can write a `main` function in which we call `getURL` and provide
    two callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: The first callback, `onSuccess`, will be executed if our `GET` request returns
    a status code in the `200` range; this function will simply print out the content
    of the response body.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second callback, `onFailure`, will simply print an error message along
    with the corresponding status code that our response received. We’ll call `getURL`
    twice, once with a valid URL and once with an invalid URL. However, instead of
    running this code synchronously, we will make the calls to `getURL` on separate
    goroutines by prefixing each call with `go`. This means we don’t know which call
    will complete first, but as we are using lazy functions (a type of continuation-passing
    style programming), we don’t have to orchestrate the control flow of our program.
    The correct callback will execute when its time comes. The callback, which is
    not necessary, will never be evaluated, so we avoid potentially expensive computation
    when it is not necessary:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding example, our `GET` requests complete asynchronously and then
    call the corresponding callback, as defined in the `getURL` function. One interesting
    bit of code is near the end of our main snippet. We have created a `bool` channel,
    and then we are reading from this channel without ever writing to it. This essentially
    keeps our application alive. If we didn’t have these two statements, our `main`
    function would likely exit and thus terminate our program, before our goroutines
    completed their computation. In a real-world application, you could also keep
    waiting for the threads to resolve using `waitgroup`. If you are stuck after running
    this from a terminal, press *Ctrl* + *C* to kill the process.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation will show up again later in this chapter when we take a look
    at implementing functional pipes. However, we’ll be looking at it more through
    a direct lens of concurrent applications, rather than the callback mechanisms
    that we saw here.
  prefs: []
  type: TYPE_NORMAL
- en: Threads versus goroutines
  prefs: []
  type: TYPE_NORMAL
- en: While the terms *thread* and *goroutine* are often used interchangeably, they
    are distinct things. Goroutines are a construct in Go, built to leverage executing
    tasks concurrently. They are managed by the Go runtime, are lightweight and fast
    to start and execute, and have a built-in communication medium (channels). Threads,
    on the other hand, are implemented at the hardware level and are managed by the
    operating system. They are slower to spin up, have no communication medium built
    in, and are hardware-dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Composability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functions are composable in a myriad of ways. This allows us to define the building
    blocks of our application and then chain them together to solve our concrete problem.
    As each block is independent of one another, we can build concurrency layers in
    between them. This will be the focus in the last part of this chapter when we
    will create functional pipes that can run concurrently. However, before we get
    there, let’s take a look at making our functions internally concurrent.
  prefs: []
  type: TYPE_NORMAL
- en: Creating concurrent functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Broadly speaking, there are two types of concurrency that we will be looking
    at in this chapter. We can call them **intra-concurrency** and **extra-concurrency**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Intra-concurrency* is about creating functions that are implemented concurrently
    internal to each function. For example, in [*Chapter 6*](B18771_06.xhtml#_idTextAnchor101),
    we saw various functions such as `Filter`, `Map`, and `FMap` that lend themselves
    to a concurrent implementation. That will be the focus of this section. Notably,
    they can be used in conjunction with each other so that we achieve concurrency
    at multiple steps in our algorithm, and we can even decide on the level of concurrency
    required for each step individually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extra-concurrency* is about chaining together functions using Go''s built-in
    concurrency features: channels and goroutines. This is explored later in the chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are many of the fundamental building blocks of functional programming good
    candidates for concurrency? Well, first and foremost, it is because a purely functional
    implementation lends itself to a concurrent implementation without too many headaches.
    As we saw in the preceding chapter, concepts such as an immutable state and the
    elimination of side effects make it easy to take our functions and concurrently
    rewrite them. There should not be interference from other functions, no outside
    state to deal with, and no I/O to contend with. However, just because we *can*
    does not mean that we *should*. In this chapter, I will make the assumption that
    a concurrent implementation is going to be the right choice for the problems that
    we are solving. In the real world, concurrency is not a zero-cost implementation.
    There is real overhead associated with writing a concurrent application, as the
    threaded execution needs to be managed by our system (or, in Go’s case, our runtime).
  prefs: []
  type: TYPE_NORMAL
- en: Although in Go we are not responsible for managing the goroutines ourselves,
    under the hood of the Go runtime, context switching is not a zero-cost implementation.
    This means that just adding concurrent calls does not guarantee a performance
    improvement and can, in fact, harm performance. Ultimately, as with anything done
    for performance, the key to understanding the benefit that can be achieved is
    obtained through profiling your application. Profiling itself is beyond the scope
    of this section; the only comment to make on it is that Go has built-in benchmarking
    tools, which we saw in earlier chapters. These can also be used to determine the
    cost benefit of concurrent versus sequential functions.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent filter implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we started with sequential filter implementation in earlier chapters and
    have become more familiar with it throughout the book, let’s start with this function
    and turn it into a concurrent implementation. Keep in mind that our initial function
    was a pure function, and as such, refactoring it into a concurrent one can be
    done without causing too much of a headache. There are a few steps to making this
    concurrent:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the input into batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a process to filter each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the result of each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the aggregated output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To achieve this, we do need to refactor the initial `Filter` implementation.
    We will leverage some of Go’s built-in concurrency features to implement this,
    and the first thing we’ll want to leverage are channels and goroutines. In our
    initial `Filter` function, we iterated over each element, appended it to an output
    slice if it matched the predicate, and finally, we returned the output slice.
    In this version, rather than returning an output slice, we’ll write the result
    onto a channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing to a channel allows us to call this function in a traditional concurrent
    fashion within Go. However, before we get there, we’ll have to establish a wrapper
    function around `Filter`, which we will call `ConcurrentFilter`. This function
    does a few things, including allowing us to configure the batch size. Playing
    around with the batch sizes can help us tweak the performance to get it where
    we want it (if there are too few batches, there’s little benefit to running concurrently;
    too many, and the overhead caused by managing goroutines similarly reduces our
    benefit). Apart from batching our input, we’ll also need to call the `Filter`
    function prepended with the `go` keyword so that it spins up a new goroutine.
    Finally, this function will read the results for each of the goroutines that we
    started and aggregate this result in to a single output slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we keep the print statements so we can see what
    execution looks like when running this. Let’s create a simple `main` function
    that will filter a slice of integers in this fashion and look at the corresponding
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this function gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this output, we can see that `4` goroutines had to be spun up to process
    our input with a batch size of `3`. This has sharded our input data into the following
    segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can see in which order the threads completed and returned their output.
    As you can tell from the output, we get the output back in random order. This
    is visible both in the `got data` output as well as in the final aggregated result.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: An important callout here is that by sharding our data and running our functions
    concurrently, we no longer have a predictable sequence order in the output list.
    If we want to restore the ordering of our data, we should implement a `Sort` function
    after concurrently calling our functions.
  prefs: []
  type: TYPE_NORMAL
- en: This `Filter` implementation is a good template to start from when we want to
    make our functions run concurrently. Let’s take a look at a concurrent implementation
    for both the `Map` and `FMap` functions.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent Map and FMap implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing the `Map` and `FMap` functions concurrently requires the same
    steps as for the concurrent `Filter` implementation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the input into batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a process to filter each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the result of each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the aggregated output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As such, we won’t go over each step in detail for these implementations. The
    explanation behind each step and how we implement it is pretty much identical
    to the `Filter` implementation. We are showing these here for completeness and
    to showcase the general pattern of refactoring these functions to operate concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent Map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To implement our `Map` function concurrently, we first refactor the `Map` function
    that we created in [*Chapter 6*](B18771_06.xhtml#_idTextAnchor101). Here, again,
    we are removing the explicit return, and we’ll use channels to communicate the
    output of mapping each element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will implement the `ConcurrentMap` function, batching the output as
    we did with the `ConcurrentFilter` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that both the `ConcurrentFilter` and `ConcurrentMap` implementations require
    `batchSize` to be passed as input to the function. This means that we can process
    each step with a different number of goroutines, and tweak each function individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we are using a batch size of `3` for filtering but only a
    batch size of `2` for mapping. The output of this `main` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Concurrent FMap implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This implementation is pretty similar to the `Map` implementation. The main
    difference is that our channel has changed type. Rather than having the entire
    function signature operate on the same `A` type, we’ll now have a mix of `A` and
    `B`. This is a minor change and does not affect the implementation details beyond
    having to create the right type for the channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'I hope that this serves as an illustration of how easy it is to create concurrent
    implementations for functions that are written in the purely functional style.
    There is one limitation posed by Go that makes this a bit more verbose than it
    would be in other languages. As Go is a strictly typed language (which is a good
    thing in general), our function signatures need to match exactly when using higher-order
    functions. Otherwise, we could template out the recursive part of our function
    and call a higher-order function for the actual implementation on each node. In
    pseudo-code, we would get something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Either way, we saw that leveraging concurrency in our functions is relatively
    headache-free and can be achieved with only a bit of refactoring. Let’s move on
    to the final topic of this chapter, which is using concurrency mechanisms to chain
    functions together.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we concerned ourselves with organizing concurrency
    inside the functions themselves. However, we have chained them together pretty
    much as we would normally, by calling them in sequential order in the main function.
    In this section, we are going to look at the pipeline pattern, which will allow
    us to leverage goroutines and channels to chain function calls together. First,
    let’s discuss what a pipeline is exactly. In 1964, Doug McIlroy wrote the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We should have some ways of coupling programs like garden hose – screw in another
    segment when it becomes necessary to massage data in another way.
  prefs: []
  type: TYPE_NORMAL
- en: 'This quote neatly expresses the Unix philosophy of composing programs. Many
    of us are familiar with the concept of Unix pipes, denoted by the `|` symbol.
    By using pipes, we can chain Unix programs together. The output of one program
    becomes the input of the next. For example, we can use `cat` to read a file, and
    we can use `wc` to get the word count of that file. To join this together, we
    would write `cat file.txt | wc`. In Unix’s modular program approach, the idea
    is that programs each serve a single purpose but can be joined together to create
    complex programs. This philosophy can be ported over to the functional programming
    paradigm. We want to chain simple functions together, where each function only
    has a single purpose, to create a complex program. Take the following example;
    each function serves a single purpose, and we chain them together using the pipe
    (`|`) character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first read the `main.go` file using `cat`. We send the content
    of that file to `grep`, which searches that content for the `func` keyword. Then,
    we send each line that matches this search to the `wc` program and count the lines
    in the output (the `-l` flag counts newlines). And finally, we send this to `awk`
    and print the result. What follows is a similar way of chaining Go functions together,
    rather than Unix commands.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining functions with channels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Go ships with all the tools necessary to create such building programs, namely
    channels. Channels are a way to send messages (data) from one function to another;
    thus, we can use channels as an alternative to the Unix pipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in creating our pipeline starts by changing how our functions
    get their input and output. For the rest of this chapter, we will mainly be focusing
    on two functions, `Filter` and `Map`, but this can be extended to any other functions.
    The core idea is to use channels for input and output data communication. First,
    let’s take a look at the `Filter` function and how this needs to be changed to
    follow our channels-in/channels-out approach. We’ll name our new function `FilterNode`.
    We’ll get back to this naming convention later, but each function can be thought
    of as a node in our chain of functions. Instead of accepting a slice as input,
    we’ll have a channel as input, from which we can read incoming data. We’ll still
    have `predicate` as expected, and finally, we’ll return a channel rather than
    a slice of data as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, the main algorithm for filtering elements remains
    unchanged. We’ll test each value against a predicate; if the predicate returns
    `true`, we’ll keep the value (by sending it to the output channel); otherwise,
    we'll discard it. Pay attention to the use of the `go` keyword here. This function,
    although it gets executed immediately, is launched on its own goroutine. The function
    immediately returns the `out` channel, although the evaluation on the goroutine
    has not necessarily finished the computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function that we will refactor similarly is the `Map` function. It’s
    an analogous change to the `Filter` function. We’ll use a channel to receive input
    for the function, a channel to return the output, and run the actual mapping logic
    inside a goroutine, which we start before returning the channel from our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, so good – we’ve refactored two of our functions to fit in with this
    new design. Next, let’s tackle the question of receiving input to these functions.
    From the function signature, we can tell that we need to receive data on a channel
    of type `A`. Thus, any function that can provide this can be used as the input
    for our function. We’ll call these types of functions *generators*. The first
    generator that we will create takes a variadic input of type `A` and pushes each
    of these values onto a channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can tell, the main logic still resembles that of the previous `Filter`
    and `Map` implementations. The main difference is that we’re no longer receiving
    values over a channel but, rather, through some other input data structure (in
    this case, variadic input parameters). This could also be a function that reads
    a file and places each line on the channel. It’s similar to how `cat` worked in
    our earlier Unix example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The key point is that our function places values on a channel and returns this
    channel. How it gets to those values doesn’t matter too much for building our
    pipeline. Before we can test this implementation end to end, we still have one
    hurdle to cross. Each node in this setup writes data to a channel, but to collect
    the output at the end, we’ll want to store it in a more common data structure.
    Slices are the perfect structure for this, at least in our examples. We can call
    this last type of function a *collector*. A collector takes a channel as input
    and returns a slice of the elements as output. Essentially, it’s performing the
    reverse operation of the *generator*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, we can tie all of them together into a single pipeline.
    To demonstrate this, in our `main` function, we will push some numbers to a channel
    using `Generator`. We’ll then filter these numbers to only retain the even ones
    using `FilterNode`. These numbers then get squared using `MapNode`, and finally,
    we collect the output in a slice using the `Collector` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding is a good first step toward chaining our functions together. However,
    we can make it cleaner. We can build a `ChainPipes` function that will tie together
    the various functions and take care of managing the channels for us.
  prefs: []
  type: TYPE_NORMAL
- en: Improved function chaining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The initial approach of chaining the functions together was a workable solution,
    but it required some overhead, as we had to manage passing around the right channels
    to each subsequent function. What we want to achieve is for the engineers using
    our setup only needing to concern themselves with the functions to call, and which
    order to call them in. We don’t want them to be concerned about how the channels
    operate underneath; we can consider that an implementation detail. What we will
    work toward in this section will allow us to compose the functions like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This snippet gives us a bit of a teaser of what’s to come. In order to chain
    functions like this, we will need to take advantage of function currying. Let’s
    get there step by step. What we want to achieve is function composition by passing
    in functions to `ChainPipes`, as we saw in the preceding snippet. Go has a strict
    type system, so to make this function work nicely, we want to define a custom
    type for such functions, which will allow us to use it in the function signature
    and get the compiler to type-check for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we will do is define our own types for the main functions representing
    an operation on our data. We’ll call these `Nodes`. There are three distinct types
    of nodes that we can define, based on the previous discussion – nodes that generate
    a channel, nodes that take a channel and return a new channel, and finally, nodes
    that take a channel and return a concrete data structure such as a slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'These type definitions make up the bread and butter of the function types that
    can be used to chain together our applications. With this in place, we can define
    the `ChainPipes` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet creates a `ChainPipes` function that takes a channel as
    input and a series of nodes. Finally, it will call the default collector and return
    the data in a slice of type `[]A`. Do note that one limitation is that we are
    assuming that each node has a compatible type (`A`) throughout the chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the type system work, each node needs to have the same function signature.
    In our initial setup, that was difficult, as we already had two distinct function
    signatures for `Filter` and `Map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'More functions would mean more distinct function signatures. Therefore, what
    is needed is refactoring these functions so that they adhere to the same type
    signature. We have already learned how to do that through function currying. We
    need to create two new functions that each `Node`. Each function will have the
    original functionality of `Filter` and `Map` baked in but returns a new function
    that takes a channel as the input (hence the function is partially applied):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We can tell in the preceding example that the core logic of each function has
    remained the same. However, rather than being instantly applied when the function
    is called, a new function is returned that expects to receive a channel as input
    and returns a channel as output. Inside this anonymous function, we have coded
    the `Filter` and `Map` logic respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the return type is `Node`, that means that when we call the `CurriedFilterNode`
    function, we are not receiving a result, but we are receiving another function
    that needs to be called at a later stage to actually compute the filtered list
    of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the crucial part of making our pipeline builder work. If we look at
    `ChainPipes` again, the main loop is calling the nodes (functions) that were supplied
    to it with the channel as input and reassigning the output to the same channel
    that was used as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We could go one step further and also abstract away the generator from the
    `ChainPipes` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With this change in place, it does imply that when calling the function, we
    need another curried function to supply the generator. This can be done in-line,
    but for clarity, the following example is a separate function existing at the
    package level. In this case, we will use the `Cat` function that we introduced
    earlier and return the curried version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, this curried version of the function operates in the same way as
    the non-curried version. However, through currying, we can make it adhere to the
    type signature indicated by the `ChainPipes` function. We can now pass both the
    generator as well as the nodes to this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Notice that in the preceding example, we did have to give a type hint to `ChainPipes`
    to indicate the resulting type of the `CurriedCat` function. What we saw in the
    preceding section is that by using channels, the Go type system, higher-order
    functions, and more specifically, function currying, we can build programs by
    chaining together functions in the right way. Using this method of function composition,
    it’s also easier to refactor our application. If we want to apply a map before
    filtering, we just need to change the order in which the node is passed to `ChainPipes`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a look at how Go’s concurrency model can be used when
    writing code in the functional paradigm. We started the chapter with a brief discussion
    on the difference between concurrency, parallelism, and distributed computing
    to delineate exactly what concurrency is.
  prefs: []
  type: TYPE_NORMAL
- en: Once we established that concurrency is the ability to do multiple tasks at
    once (although not necessarily simultaneously), we looked at how we can refactor
    the functions from [*Chapter 6*](B18771_06.xhtml#_idTextAnchor101) into a concurrent
    implementation, leveraging channels and goroutines. We concluded this chapter
    by looking at pipelines, a way to create programs by composing functions together
    and orchestrating the flow of data with the use of channels. We also looked at
    how we can create a higher-order function to compose functions (`ChainPipes`)
    and have observed how, through the use of function currying, we can create functions
    that adhere to our type system without giving up type safety.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will take a look at programming libraries
    that we can leverage to create Go programs, following some of the functional programming
    principles that we explored in this book.
  prefs: []
  type: TYPE_NORMAL
