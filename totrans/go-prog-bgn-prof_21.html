<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer232">
			<h1 id="_idParaDest-518" class="chapter-number"><a id="_idTextAnchor2172"/>21</h1>
			<h1 id="_idParaDest-519">Go in the Cloud<a id="_idTextAnchor2173"/><a id="_idTextAnchor2174"/></h1>
			<p class="callout-heading"><a id="_idTextAnchor2175"/><a id="_idTextAnchor2176"/>Overview</p>
			<p class="callout">This chapter will show you how to take your Go application to the next level of readiness for deployment. It will cover the considerations you have to make your Go application run reliably once deployed to your server or cloud infrastructure by demonstrating how to add monitoring capabilities<a id="_idIndexMarker1361"/> to the system through an open source monitoring and alerting toolkit known as <strong class="bold">Prometheus</strong>. The chapter will also discuss how to run your application using an orchestrator and all of the benefits you get out of the box. Lastly, the chapter will cover insights that <strong class="bold">OpenTelemetry</strong> allows, as well as best practices for containerizing your Go <span class="No-Break">application code.</span></p>
			<p class="callout">By the end of this chapter, you will be empowered to deploy your Go application reliably, and with valuable insights into the system to ensure <span class="No-Break">its success.</span></p>
			<h1 id="_idParaDest-520"><a id="_idTextAnchor2177"/>Technical requirements</h1>
			<p>For this chapter, you'll require Go version 1.21 or higher. The code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Go-Programming-From-Beginner-to-Professional-Second-Edition-/tree/main/Chapter21"><span class="No-Break">https://github.com/PacktPublishing/Go-Programming-From-Beginner-to-Professional-Second-Edition-/tree/main/Chapter21</span></a><span class="No-Break">.</span><a id="_idTextAnchor2178"/><a id="_idTextAnchor2179"/></p>
			<h1 id="_idParaDest-521"><a id="_idTextAnchor2180"/>Introduction</h1>
			<p>In the previous chapter, you learned about<a id="_idIndexMarker1362"/> the various Go tools that enable developers to write better code and be more productive. We covered the Go tools to compile and run your Go code using the <strong class="source-inline">go build</strong> and <strong class="source-inline">go run</strong> commands. We then looked at how to format Go code using <strong class="source-inline">gofmt</strong>. We also saw the power of working with the Go ecosystem through its dependencies with the <strong class="source-inline">goimports</strong> and <strong class="source-inline">go get</strong> command-line tools. After having functional dependencies in our code, we can see issues that might be present using the <strong class="source-inline">Go vet</strong> tool and the <strong class="source-inline">Go race</strong> detector. Lastly, with any good code comes a well-rounded project through proper documentation using the <strong class="source-inline">Go doc</strong> tool. The previous chapter empowered you with the tools right at your fingertips in the <span class="No-Break">Go ecosystem.</span></p>
			<p>In this chapter, we focus on the fact that at some point in a project, your application development journey will lead you to the final frontier: deploying the application. But, before you hit the deploy button or run the final command, there are essential considerations to ensure your application runs reliably and efficiently in its <span class="No-Break">destination environment.</span></p>
			<p>Where your Go application<a id="_idIndexMarker1363"/> will be deployed depends on numerous factors. This can be a stakeholder and leadership-led decision, based on existing infrastructure, or even based on the specifications of your project or customer base. No matter the destination, your Go code will be able to be packaged up and shipped to it. However, it is on you to ensure your project is ready <span class="No-Break">for deployment.</span></p>
			<p>This chapter will teach you about running your application in the cloud successfully and some of the considerations you might make before deploying it into the cloud, or wherever you choose to deploy it to really. We will cover topics such as monitoring, orchestration, tracing, and containerization, equipping you with the knowledge and tools to navigate the complexities of cloud <span class="No-Break">infrastructure effectively.</span></p>
			<p>First and foremost, we’ll discuss the importance of monitoring your application’s performance and health in a cloud-native environment. We’ll explore how to integrate monitoring systems such as Prometheus into your Go application, enabling you to gather vital metrics and gain insights into its <span class="No-Break">ongoing behavior.</span></p>
			<p>Next, we’ll delve into the realm of distributed tracing and logs with OpenTracing. By implementing tracing and logging into your Go application, you’ll gain visibility into the flow of requests and responses across microservices. This will provide you with additional insights to make debugging issues a breeze – hopefully – and provide you with insights to potentially make performance optimizations in <span class="No-Break">the future.</span></p>
			<p>Finally, we’ll cover essential containerization practices for Go applications, including image optimization, dependency management, and security considerations. You’ll learn how to build robust container images for your Go application, ready for deployment in any environment. That will allow for a seamless transition to where we’ll tackle the challenge of orchestrating your application using platforms such as Kubernetes. Orchestrators allow for scalability, resiliency, and ease of management for your application at a <span class="No-Break">greater scale.</span></p>
			<p>By the end of the chapter, you will be well equipped to deploy<a id="_idIndexMarker1364"/> your Go application confidently to the cloud, armed with the knowledge and tools to ensure its reliability, scalability, performance, and visibility into production environments. Let’s <span class="No-Break">dive in!</span></p>
			<h1 id="_idParaDest-522"><a id="_idTextAnchor2181"/>Making your app monitorable by systems such as Promet<a id="_idTextAnchor2182"/>heus</h1>
			<p>Monitoring is a critical aspect<a id="_idIndexMarker1365"/> of maintaining the health<a id="_idIndexMarker1366"/> and performance of any application, no matter the language. Monitoring is especially important in a cloud-native environment where resources are dynamic and distributed. There are certain nuances as to the differences between monitoring and observability in <span class="No-Break">software engineering.</span></p>
			<p>The monitoring aspects abide more by collecting data through predefined metrics and thresholds to detect and alert upon issues to define the overall health of the system, whereas observability is much more investigative and goes into a more comprehensive understanding of system behavior and performance to enable effective debugging and troubleshooting in complex environments. To focus on enabling monitoring capabilities and insights into the health of our application, we will focus on monitoring instead of observability in this <span class="No-Break">book chapter.</span></p>
			<p>Prometheus is a powerful tool when it comes to enabling monitoring capabilities on an application. It operates on a pull-based model, where it scrapes metrics from instrumented applications at regular intervals. These metrics are then stored in a time-series database, allowing developers to query, visualize, and alert them in real time. As a Go developer, integrating Prometheus into an application enables you to gain valuable insights into its performance <span class="No-Break">and behavior.</span></p>
			<p>To make your Go application monitorable with Prometheus, you need to instrument it with metrics that capture relevant information about its internal state and performance. This involves adding instrumentation code to your application’s code base to expose metrics endpoints that Prometheus can <span class="No-Break">then scrape.</span></p>
			<p>The Prometheus Go client library provides a convenient way to instrument your Go application with metrics. It offers a range of metric types that allow you to capture different aspects of your <span class="No-Break">application’s behavior:</span></p>
			<ul>
				<li><strong class="bold">Counters</strong>: Monotonically increasing values used to track the number of occurrences of an event over time. They reset to zero when the application restarts and are useful for measuring event frequencies, such as the number of requests <span class="No-Break">or errors.</span></li>
				<li><strong class="bold">Gauges</strong>: Instantaneous measurements of a particular value at a specific point in time. They can increase or decrease and represent the current state of a system, such as CPU usage, memory consumption, or the number of <span class="No-Break">active connections.</span></li>
				<li><strong class="bold">Histograms</strong>: A means to track the distribution of values over time, allowing you to understand the variability and spread of data. They collect observations into configurable buckets and provide metrics such as percentiles, median, and average, which are useful for understanding response times, latencies, and <span class="No-Break">request durations.</span></li>
				<li><strong class="bold">Summaries</strong>: Similar to histograms, summaries provide a more accurate representation of the data distribution, especially for high-cardinality datasets. They calculate quantiles and percentiles dynamically, allowing you to analyze data distribution with precision and granularity, making them suitable for measuring latency, duration, and <span class="No-Break">response-time distributions.</span></li>
			</ul>
			<p>Once you’ve instrumented your application<a id="_idIndexMarker1367"/> using the aforementioned metric<a id="_idIndexMarker1368"/> types that are appropriate to the metrics you desire and use case, you then need to expose metrics endpoints for Prometheus to scrape. These endpoints typically serve metrics in a format compatible with the Prometheus exposition format, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">/metrics</strong></span><span class="No-Break">.</span></p>
			<p>Prometheus uses configuration<a id="_idIndexMarker1369"/> files called scrape configs to define targets it should scrape for metrics. You’ll need to configure Prometheus to scrape your application’s metrics endpoint(s) and specify the scrape interval to collect data at <span class="No-Break">regular intervals.</span></p>
			<p>With Prometheus collecting metrics from your Go application, you can now visualize them using tools such as Grafana<a id="_idIndexMarker1370"/> and set up alerts based on predefined<a id="_idIndexMarker1371"/> thresholds or conditions. This allows you to proactively monitor your application’s health and performance and take corrective action <span class="No-Break">when <a id="_idTextAnchor2183"/><a id="_idTextAnchor2184"/>necessary.</span></p>
			<h2 id="_idParaDest-523"><a id="_idTextAnchor2185"/>Exercise 21.01 – Creating an app with a /healthz endpoint</h2>
			<p>We’ve just walked through an overview<a id="_idIndexMarker1372"/> of how monitoring works, a powerful tool<a id="_idIndexMarker1373"/> you can utilize to capture metrics, and how you can then visualize those metrics and use them for the betterment of your project. We will now take a look at what this looks like <span class="No-Break">in code:</span></p>
			<ol>
				<li>Create a new directory called <strong class="source-inline">Exercise21.01</strong>. Within that directory, create a new file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">main.go</strong></span><span class="No-Break">.</span></li>
				<li>Run the following two commands to create a <strong class="source-inline">go</strong> module for <span class="No-Break">the exercise:</span><pre class="source-code">
go mod init
go mod tidy</pre></li>				<li>Add the following code to the file to create a simple application we <span class="No-Break">can monitor:</span><pre class="source-code">
package main
import (
  "fmt"
  "net/http"
  "time"
  "github.com/prometheus/client_golang/prometheus"
  "github.com/prometheus/client_golang/prometheus/promhttp"
)</pre></li>				<li>Add a counter metric we will use<a id="_idIndexMarker1374"/> to monitor the count<a id="_idIndexMarker1375"/> of calls to <span class="No-Break">the endpoint:</span><pre class="source-code">
var (
  healthzCounter = prometheus.NewCounter(prometheus.CounterOpts{
    Name: "healthz_calls_total",
    Help: "Total number of calls to the healthz endpoint.",
  })
)</pre></li>				<li>Register the metric <span class="No-Break">with Prometheus:</span><pre class="source-code">
func init() {
  prometheus.MustRegister(healthzCounter)
}</pre></li>				<li>Define a handler for the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">healthz</strong></span><span class="No-Break"> endpoint:</span><pre class="source-code">
func main() {
  http.HandleFunc("/healthz", func(w http.ResponseWriter, r *http.Request) {
    healthzCounter.Inc()
    w.WriteHeader(http.StatusOK)
    fmt.Println("Monitoring endpoint invoked! Counter was incremented!")
  })</pre></li>				<li>Define a handler for viewing <span class="No-Break">the metrics:</span><pre class="source-code">
  http.Handle("/metrics", promhttp.Handler())</pre></li>				<li>Define and start the server, and then close <span class="No-Break">the function:</span><pre class="source-code">
  server := &amp;http.Server{
    Addr: ":8080",
    ReadTimeout: 10 * time.Second,
    WriteTimeout: 10 * time.Second,
  }
  fmt.Println("Server listening on port 8080...")
  if err := server.ListenAndServe(); err != nil {
    fmt.Printf("Error starting server: %s\n", err)
  }
}</pre></li>				<li>To run the program, you need<a id="_idIndexMarker1376"/> to open your terminal<a id="_idIndexMarker1377"/> and navigate to the directory that you created the <strong class="source-inline">main.go</strong> file in. Then, run the <strong class="source-inline">go build</strong> tool by writing <span class="No-Break">the following:</span><pre class="source-code">
go build -o monitored_app main.go</pre></li>				<li>This will create an executable called <strong class="source-inline">monitored_app</strong> that you can execute the binary in by running it on the <span class="No-Break">command line:</span><pre class="source-code">
./monitored_app</pre></li>			</ol>
			<p>The output will look <span class="No-Break">as follows:</span></p>
			<pre class="console">
Server listening on port 8080...</pre>			<p>The server is now listening for requests. You can now navigate to the <strong class="source-inline">/healthz</strong> endpoint in your web browser, or through a <strong class="source-inline">curl</strong> command to perform the HTTP request. Navigate to the web browser at the endpoint, and reload the page a few <span class="No-Break">times: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080/healthz</strong></span><span class="No-Break">.</span></p>
			<p>If you return to your terminal, you will see that the counter was incremented with each request or, in other words, each time you refreshed the <span class="No-Break">web page:</span></p>
			<pre class="console">
Monitoring endpoint invoked! Counter was incremented!
Monitoring endpoint invoked! Counter was incremented!
Monitoring endpoint invoked! Counter was incremented!</pre>			<p>You will see the same number <a id="_idIndexMarker1378"/>of lines of that output as the number<a id="_idIndexMarker1379"/> of times you made a request to the web server for that endpoint. Now that we’ve made a few requests to the server, we’ve generated some data on our monitored application. We can view the available Prometheus metrics <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">http://localhost:8080/metrics</strong></span><span class="No-Break">.</span></p>
			<p>If you go to the web browser at the <strong class="source-inline">/metrics</strong> endpoint, you will see the metric we created, among a bunch of other metrics that have been abbreviated with three dots, as there are too many to list nicely on <span class="No-Break">a page:</span></p>
			<pre class="console">
...
# HELP healthz_calls_total Total number of calls to the healthz endpoint.
# TYPE healthz_calls_total counter
healthz_calls_total 3
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0</pre>			<p>You can see our <span class="No-Break">custom metric:</span></p>
			<pre class="console">
healthz_calls_total 3</pre>			<p>We invoked our endpoint three times; therefore, we see a counter value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">3</strong></span><span class="No-Break">.</span></p>
			<p>The additional metrics you are seeing are provided by the Prometheus client library itself and are related to Go runtime metrics, including memory allocation, garbage collection, goroutines, and other runtime statistics. These metrics are automatically exposed by the Prometheus client library when you import and use it in your <span class="No-Break">Go application.</span></p>
			<p>In this exercise, you defined a counter metric for an endpoint on an HTTP server using Prometheus. By instrumenting your Go application with Prometheus and following best practices for monitoring, you can gain valuable insights into its behavior and performance in a cloud-native environment. With Prometheus, we saw how you can use it to be well equipped to define the monitoring capabilities of <span class="No-Break">an application.</span></p>
			<p>Expanding the example by adding additional custom<a id="_idIndexMarker1380"/> metrics allows teams<a id="_idIndexMarker1381"/> to detect issues early on, effectively troubleshoot their applications, and ensure the reliability and scalability of Go applications in production-level environments. Prometheus also enables alerting upon metrics upon certain criteria, proving it to be quite a powerful tool when gaining insights in<a id="_idTextAnchor2186"/>to <span class="No-Break">your application.</span></p>
			<h1 id="_idParaDest-524"><a id="_idTextAnchor2187"/>Enabling deep insights through OpenTelemetry</h1>
			<p>In today’s complex distributed systems<a id="_idIndexMarker1382"/> landscape, understanding how our applications behave and perform is crucial for maintaining reliability and performance. We will now take a look at another useful monitoring tool readily available. <strong class="bold">OpenTelemetry</strong> is a pivotal tool for gaining profound insights into the functionality and performance of distributed systems. OpenTelemetry, often referred to as OTel, provides a standardized approach to collect and correlate data across various components of <span class="No-Break">the system.</span></p>
			<p>By incorporating OpenTelemetry into your Go applications, you can seamlessly capture telemetry data, including traces, metrics, and logs, to gain a holistic understanding of your system’s operation. Let’s take a look at the three main pillars <span class="No-Break">OpenTelemetry encompasses:</span></p>
			<ul>
				<li><strong class="bold">Tracing</strong> allows us to track the flow of requests<a id="_idIndexMarker1383"/> as they travel through different services and components, providing invaluable insights into latency, dependencies, and error propagation. For tracing, we create and propagate trace contexts across service boundaries in order to achieve end-to-end visibility into request flows. This enables us to visualize requests as they flow through the system, identify performance bottlenecks, diagnose errors, and optimize <span class="No-Break">resource utilization.</span></li>
				<li><strong class="bold">Metrics</strong> offer a quantitative view of our<a id="_idIndexMarker1384"/> system’s health and performance, enabling us to monitor key indicators and identify potential bottlenecks or anomalies. OpenTelemetry provides a means of collecting metrics similar to providing insights into the health and performance of <span class="No-Break">our applications.</span></li>
				<li><strong class="bold">Logs</strong> provide a narrative of events<a id="_idIndexMarker1385"/> and actions within our applications, aiding in troubleshooting and debugging efforts. This also allows us to trace the flow of information across our distributed systems and capture logs as events occur internal to <span class="No-Break">the application.</span></li>
			</ul>
			<p>To harness the power of OpenTelemetry<a id="_idIndexMarker1386"/> in your application, you must first instrument the application with the necessary instrumentation libraries and <strong class="bold">software development kits</strong>, or <strong class="bold">SDKs</strong>. This is similar to how we saw you must instrument the application for Prometheus in the previous section of this chapter. For OpenTelemetry, it is a similar process of integrating the OpenTelemetry SDKs into the code base and configuring instrumentation for tracing, metrics, <span class="No-Break">and logging.</span></p>
			<p>Let’s see what some of thi<a id="_idTextAnchor2188"/><a id="_idTextAnchor2189"/>s looks like <span class="No-Break">in practice.</span></p>
			<h2 id="_idParaDest-525"><a id="_idTextAnchor2190"/>Exercise 21.02 – Using OpenTelemetry for queryable logs and tracing</h2>
			<p>We now understand<a id="_idIndexMarker1387"/> the monitoring<a id="_idIndexMarker1388"/> capabilities that<a id="_idIndexMarker1389"/> OpenTelemetry<a id="_idIndexMarker1390"/> allows developers. We will now see how it helps to make structured logs that enable developers to more easily query their logs later on, as well as see what tracing is like <span class="No-Break">with OpenTelemetry.</span></p>
			<p>Create a new directory<a id="_idIndexMarker1391"/> called <strong class="source-inline">Exercise21.02</strong>. Within that directory, create<a id="_idIndexMarker1392"/> a new Go file<a id="_idIndexMarker1393"/> called <strong class="source-inline">main.go</strong>, then do<a id="_idIndexMarker1394"/> <span class="No-Break">the following:</span></p>
			<ol>
				<li>Run the following two commands to create a <strong class="source-inline">go</strong> module for <span class="No-Break">the exercise:</span><pre class="source-code">
go mod init
go mod tidy</pre></li>				<li>Add the following code to the file, including all of the imports necessary for our <span class="No-Break">OpenTelemetry monitoring:</span><pre class="source-code">
package main
import (
  "context"
  "fmt"
  "log"
  "net/http"
  "time"
  "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
  "go.opentelemetry.io/otel"
  "go.opentelemetry.io/otel/exporters/otlp/otlptrace"
  "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
  "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
  "go.opentelemetry.io/otel/exporters/stdout/stdouttrace"
  sdktrace "go.opentelemetry.io/otel/sdk/trace"
  "go.opentelemetry.io/otel/trace"
  "go.uber.org/zap"
)</pre></li>				<li>Create a function to initialize a <span class="No-Break">trace exporter:</span><pre class="source-code">
func initTraceExporter(ctx context.Context) *otlptrace.Exporter {
  traceExporter, err := otlptracegrpc.New(
    ctx,
    otlptracegrpc.WithEndpoint("http://localhost:4317),
  )
  if err != nil {
    log.Fatalf("failed to create trace exporter: %v", err)
  }
  return traceExporter
}</pre></li>				<li>Create a<a id="_idIndexMarker1395"/> function<a id="_idIndexMarker1396"/> to initialize<a id="_idIndexMarker1397"/> a <span class="No-Break">log</span><span class="No-Break"><a id="_idIndexMarker1398"/></span><span class="No-Break"> exporter:</span><pre class="source-code">
func initLogExporter(ctx context.Context) *otlptrace.Exporter {
  logExporter, err := otlptracehttp.New(
    ctx,otlptracehttp.WithEndpoint("http://localhost:4318/v1/logs"),
  )
  if err != nil {
    log.Fatalf("failed to create log exporter: %v", err)
  }
  return logExporter
}</pre></li>				<li>Create a<a id="_idIndexMarker1399"/> function<a id="_idIndexMarker1400"/> to initialize<a id="_idIndexMarker1401"/> a <span class="No-Break">structured</span><span class="No-Break"><a id="_idIndexMarker1402"/></span><span class="No-Break"> logger:</span><pre class="source-code">
func initLogger() *zap.Logger {
  logger, err := zap.NewProduction()
  if err != nil {
    log.Fatalf("failed to create logger: %v", err)
  }
  return logger
}</pre></li>				<li>Create a function to initialize the <span class="No-Break">tracing provider:</span><pre class="source-code">
func initTracerProvider(traceExporter *otlptrace.Exporter) *sdktrace.TracerProvider {
  exp, err := stdouttrace.New(stdouttrace.WithPrettyPrint())
  if err != nil {
    log.Println("failed to initialize stdouttrace exporter:", err)
  }
  bsp := sdktrace.NewBatchSpanProcessor(exp)
  tp := sdktrace.NewTracerProvider(
    sdktrace.WithBatcher(traceExporter),
    sdktrace.WithSpanProcessor(bsp),
  )
  return tp
}</pre><p class="list-inset">Then, define an HTTP handler<a id="_idIndexMarker1403"/> that will handle the incoming <a id="_idIndexMarker1404"/>monitored request and capture log<a id="_idIndexMarker1405"/> information, as well as start<a id="_idIndexMarker1406"/> the span for the <span class="No-Break">incoming request:</span></p><pre class="source-code">func handler(w http.ResponseWriter, r *http.Request){
  ctx := r.Context()
  span := trace.SpanFromContext(ctx)
  defer span.End()
  logger := zap.NewExample().Sugar()
  logger.Infow("Received request",
    "service", "exercise22.02",
    "httpMethod", r.Method,
    "httpURL", r.URL.String(),
    "remoteAddr", r.RemoteAddr,
  )
  w.WriteHeader(http.StatusOK)
  fmt.Fprintf(w, "Monitoring endpoint invoked!")
}</pre></li>				<li>Last, define a <strong class="source-inline">main()</strong> function where you will call all of the initialization helper functions we <span class="No-Break">just defined:</span><pre class="source-code">
func main() {
  ctx := context.Background()
  traceExporter := initTraceExporter(ctx)
  defer traceExporter.Shutdown(context.Background())
  logExporter := initLogExporter(ctx)
  defer logExporter.Shutdown(context.Background())
  tp := initTracerProvider(traceExporter)
  otel.SetTracerProvider(tp)
  logger := initLogger()
  defer logger.Sync()</pre></li>				<li>Wrap the HTTP handler<a id="_idIndexMarker1407"/> with OpenTelemetry <a id="_idIndexMarker1408"/>instrumentation, start<a id="_idIndexMarker1409"/> the HTTP<a id="_idIndexMarker1410"/> server, and close the <span class="No-Break"><strong class="source-inline">main()</strong></span><span class="No-Break"> function:</span><pre class="source-code">
  httpHandler := otelhttp.NewHandler(http.HandlerFunc(handler), "HTTPServer")
  http.Handle("/", httpHandler)
  server := &amp;http.Server{
    Addr: ":8080",
    ReadTimeout: 10 * time.Second,
    WriteTimeout: 10 * time.Second,
  }
  fmt.Println("Server listening on port 8080...")
  if err := server.ListenAndServe(); err != nil {
    fmt.Printf("Error starting server: %s\n", err)
  }
}</pre></li>				<li>To run the program, you need to open your terminal and navigate to the directory that you created the <strong class="source-inline">main.go</strong> file in. Then, run the <strong class="source-inline">go build</strong> tool by writing <span class="No-Break">the following:</span><pre class="source-code">
go build -o monitored_app main.go</pre></li>				<li>This will create an executable called <strong class="source-inline">monitored_app</strong> that you can execute the binary in by running it on the <span class="No-Break">command line:</span><pre class="source-code">
./monitored_app</pre></li>			</ol>
			<p>The output will look <span class="No-Break">as follows:</span></p>
			<pre class="console">
Server listening on port 8080...</pre>			<p>The server is now listening<a id="_idIndexMarker1411"/> for requests. You can now navigate to the <strong class="source-inline">/healthz</strong> endpoint<a id="_idIndexMarker1412"/> in your web browser, or through a <strong class="source-inline">curl</strong> command<a id="_idIndexMarker1413"/> to perform the HTTP<a id="_idIndexMarker1414"/> request. Navigate to the web browser at the endpoint, and reload the page a few <span class="No-Break">times: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080/healthz</strong></span><span class="No-Break">.</span></p>
			<p>The web page will now show <span class="No-Break">the following:</span></p>
			<pre class="console">
Monitoring endpoint invoked!</pre>			<p>If you return to your terminal, you will see the structured log <span class="No-Break">we defined:</span></p>
			<pre class="console">
{"level":"info","msg":"Received request","service":"exercise22.02","httpMethod":"GET","httpURL":"/healthz","remoteAddr":"[::1]:51082"}</pre>			<p>You will also see the results of the tracing information exported to standard output, so we can see the trace in the terminal. Here, you can see part of the output, shortened to allow ease <span class="No-Break">of visibility:</span></p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B18621_21_1.jpg" alt="Figure 21.1: OpenTelemetry tracing output – this image is meant to show the output and text; readability is not essential" width="527" height="1100"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 21.1: OpenTelemetry tracing output – this image is meant to show the output and text; readability is not essential</p>
			<p>In this exercise, you worked<a id="_idIndexMarker1415"/> with OpenTelemetry to gain valuable<a id="_idIndexMarker1416"/> monitoring insights into the<a id="_idIndexMarker1417"/> application, including structured logging <a id="_idIndexMarker1418"/>and tracing information on requests. The logs help to provide information on what occurred, and we saw how you can structure the logs to include information relevant to your use case and project. From there, you can use different aspects of the logs to query. For example, our logs included the service name, HTTP method used, and endpoint invoked. We could easily create queries based on all service requests or all requests to a specific endpoint. This could provide valuable insights to enable teams to practice on their projects. We also saw tracing information using OpenTelemetry. This information is useful for timing insights and execution flow if there are sub-requests made. We can also visualize these results with different exporters or UI tools<a id="_idIndexMarker1419"/> to more easily see what is going<a id="_idIndexMarker1420"/> on in our request<a id="_idIndexMarker1421"/> flows in more complex<a id="_idIndexMarker1422"/> <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-526"><a id="_idTextAnchor2191"/>Best practices for putting your Go application in a container</h1>
			<p>In recent years, containerization<a id="_idIndexMarker1423"/> has revolutionized the way software engineers deploy and manage software applications. By encapsulating an application along with its dependencies into a lightweight, portable container, containerization provides numerous benefits, including consistency, scalability, and portability for our applications. This approach has gained widespread adoption across industries and is considered a standard practice for modern software development and <span class="No-Break">deployment workflows.</span></p>
			<p>Containerization is essential to software nowadays as it ensures consistency by packaging the application and its dependencies into a single unit, eliminating the infamous and dreaded “it works on my machine” problem This consistency extends to different environments, including production, reducing the risk of configuration drift. It also allows scalability on demand, as it is efficient to add or remove instances of the application when it is lightweight and fast to spin<a id="_idIndexMarker1424"/> up in a container. Lastly, containers can be run on-premises, in <strong class="bold">cloud service providers</strong> (<strong class="bold">CSPs</strong>), or even in hybrid environments. Therefore, it is essential to understand how to package up your Go application dependencies into a container to run your Go code to be consistent, scalable, <span class="No-Break">and portable.</span></p>
			<p>Docker is a big player in the containerization ecosystem, serving as one of the most widely used containerization platforms. Docker provides a containerization engine, image management, container orchestration, and a widely integrated ecosystem. It provides tools, workflows, and infrastructure for creating, deploying, and managing <span class="No-Break">containers effectively.</span></p>
			<p>There are a few best practices<a id="_idIndexMarker1425"/> to keep in mind when containerizing your <span class="No-Break">Go application:</span></p>
			<ul>
				<li><strong class="bold">Leverage Go modules for dependency management</strong>: Go modules provide a convenient way to manage dependencies for your Go applications. When containerizing your Go application, ensure that you are using Go modules to manage dependencies effectively. Go modules were covered early on in the book in <a href="B18621_09.xhtml#_idTextAnchor1367"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Using Go Modules to Define </em><span class="No-Break"><em class="italic">a Project</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Keep containers lightweight</strong>: One of the fundamental principles of containerization is to keep containers lightweight. This means minimizing the size of your container images to reduce deployment times and resource usage. When building container images for Go applications, use multi-stage builds to compile your application binary and copy only the necessary files into the final image. Additionally, leverage Alpine-based or scratch images as base images to further reduce <span class="No-Break">image size.</span></li>
				<li><strong class="bold">Optimize Dockerfile instructions</strong>: When writing Dockerfiles for your Go applications, optimize Dockerfile instructions to improve build performance and reduce image size. Use multi-stage builds to separate the build environment from the final production image, minimizing the size of the final image. Additionally, leverage Docker’s layer-caching mechanism by ordering your Dockerfile instructions from least frequently changing to most frequently changing, ensuring that only necessary steps are executed when rebuilding <span class="No-Break">the image.</span></li>
				<li><strong class="bold">Secure your container environment</strong>: Security should be a top priority when containerizing your Go applications. Follow security best practices such as using minimal and trusted base images, scanning<a id="_idIndexMarker1426"/> container images for vulnerabilities using tools such as <strong class="bold">Trivy</strong>, and applying least privilege principles by running containers with non-root users whenever possible. Additionally, ensure that sensitive information such as credentials or API keys are not hardcoded into your container images but instead provided as environment variables or mounted as secrets at runtime. Lastly, consider leveraging Chainguard images for your Dockerfiles to enhance the security of container images by relying upon their enhanced <span class="No-Break">security measures.</span></li>
				<li><strong class="bold">Implement health checks and logging</strong>: Implement health checks and logging in your Go applications to improve observability and reliability in a containerized environment. Define health check endpoints to allow container orchestration platforms such as Kubernetes to monitor the health of your application and automatically restart unhealthy containers. Additionally, use structured logging to provide valuable insights into the behavior of your application, making it easier to troubleshoot issues and debug problems <span class="No-Break">in production.</span></li>
				<li><strong class="bold">Use container orchestration platforms</strong>: We will discuss why this is important in the next section<a id="_idIndexMarker1427"/> of <span class="No-Break">this chapter.</span></li>
			</ul>
			<p>Now that we understand why it is crucial to know how to containerize<a id="_idTextAnchor2192"/><a id="_idTextAnchor2193"/> our Go applications, let’s see what this looks like <span class="No-Break">in practice.</span></p>
			<h2 id="_idParaDest-527"><a id="_idTextAnchor2194"/>Exercise 21.03 – Creating a Dockerfile for a Go application</h2>
			<p>To containerize your Go<a id="_idIndexMarker1428"/> application, you’ll need<a id="_idIndexMarker1429"/> to create a Dockerfile, which is a text document that contains instructions for Docker on how to build your application’s image. Let’s walk through the process of creating a Dockerfile for a simple Go application and then see how to build and run the container. We will use the code from earlier on in the chapter, in the <span class="No-Break"><strong class="source-inline">Exercise21.01</strong></span><span class="No-Break"> directory:</span></p>
			<ol>
				<li>Create a new directory called <strong class="source-inline">Exercise21.03</strong>. Within that directory, create a new file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">main.go</strong></span><span class="No-Break">.</span></li>
				<li>Copy the contents of <strong class="source-inline">Exercise21.03/main.go</strong>, <strong class="source-inline">Exercise21.03/go.mod</strong>, and <strong class="source-inline">Exercise21.03/go.sum</strong> into the <span class="No-Break">new directory.</span></li>
				<li>Create a new file called <strong class="source-inline">Dockerfile</strong> that <span class="No-Break">contains instructions.</span></li>
				<li>Start with the official Go image as the <span class="No-Break">base image:</span><pre class="source-code">
FROM golang:latest AS builder</pre></li>				<li>Ensure the Go compiler builds a statically linked binary, including all necessary libraries within <span class="No-Break">the binary:</span><pre class="source-code">
ENV CGO_ENABLED=0</pre></li>				<li>Set the working directory inside of <span class="No-Break">the container:</span><pre class="source-code">
WORKDIR /app</pre></li>				<li>Copy over the Go modules<a id="_idIndexMarker1430"/> files and our code for the <span class="No-Break">monitored</span><span class="No-Break"><a id="_idIndexMarker1431"/></span><span class="No-Break"> application:</span><pre class="source-code">
COPY go.mod go.sum ./
COPY main.go ./
RUN go mod download</pre></li>				<li>Build our <span class="No-Break">Go binary:</span><pre class="source-code">
RUN go build -o monitored_app .</pre></li>				<li>Start a new stage to create a minimal <span class="No-Break">final image:</span><pre class="source-code">
FROM scratch</pre></li>				<li>Copy over the binary to our <span class="No-Break">final stage:</span><pre class="source-code">
COPY --from=builder /app/monitored_app /.</pre></li>				<li>Expose the port we will use to interact with <span class="No-Break">our application:</span><pre class="source-code">
EXPOSE 8080</pre></li>				<li>Run our <span class="No-Break">monitored application:</span><pre class="source-code">
CMD ["./monitored_app"]</pre></li>				<li>Now that we have filled the contents of our <strong class="source-inline">Dockerfile</strong> file, we can build our Docker image by running the following command in <span class="No-Break">the terminal:</span><pre class="source-code">
docker build -t monitored-app .</pre></li>				<li>We can then run our Docker container using the following command, which will start a container based on our monitored application image and map port <strong class="source-inline">8080</strong> on our host machine to port 8080 of <span class="No-Break">the container:</span><pre class="source-code">
docker run -p 8080:8080 monitored-app</pre></li>				<li>We can now access our application at the same URL we’ve been <span class="No-Break">hitting: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080/healthz</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>We still see the same output as we did before with <span class="No-Break">the application:</span></p>
			<pre class="console">
Monitoring endpoint invoked! Counter was incremented!</pre>			<p>We’ve now seen how to take<a id="_idIndexMarker1432"/> our Go application<a id="_idIndexMarker1433"/> into a lightweight, ephemeral container and run it using Docker commands. Docker is a platform that enables us to build, ship, and run our application in a Docker container by packaging up our Go application dependencies into a portable container that can be deployed across <span class="No-Break">different environments.</span></p>
			<p>Let’s now expand on this idea of portability and <span class="No-Break">container orchestration.</span></p>
			<h1 id="_idParaDest-528"><a id="_idTextAnchor2195"/>Making your app ready to work with orchestrators such as Kubernetes</h1>
			<p><strong class="bold">Kubernetes</strong>, often abbreviated as <strong class="bold">K8s</strong>, has emerged as the de facto<a id="_idIndexMarker1434"/> standard for container orchestration<a id="_idIndexMarker1435"/> and management. It provides the capabilities to automate the deployment, scaling, and management of containerized applications. At its core, Kubernetes abstracts away the complexities of managing individual containers and offers a unified API and control plane for orchestrating containerized workloads across a cluster of machines. Orchestrators such as Kubernetes are what you turn to when you want to streamline the deployment and management of modern, <span class="No-Break">cloud-native applications.</span></p>
			<p>In today’s dynamic and rapidly evolving software landscape, where microservices architectures and containerization have become mainstream, Kubernetes offers a scalable and resilient platform for deploying and operating these distributed applications. However, it is not without its complexities and <span class="No-Break">learning curve.</span></p>
			<p>There are a few things<a id="_idIndexMarker1436"/> to do in order for your application<a id="_idIndexMarker1437"/> to work with orchestrators such <span class="No-Break">as Kubernetes:</span></p>
			<ul>
				<li><strong class="bold">Containerize your application</strong>: Package your Go application and its dependencies into a Docker container, as we saw in the <span class="No-Break">previous section.</span></li>
				<li><strong class="bold">Deploy your containerized application</strong>: Once you’ve built your container image, you need to deploy it into your Kubernetes cluster. This typically involves pushing your container<a id="_idIndexMarker1438"/> image to a container registry (such as Docker Hub, <strong class="bold">Google Container Registry</strong> (<strong class="bold">GCR</strong>), or <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">Amazon ECR</strong>), and then deploying it into your Kubernetes<a id="_idIndexMarker1439"/> cluster using Kubernetes <span class="No-Break">deployment manifests.</span></li>
				<li><strong class="bold">Define Kubernetes resources</strong>: In Kubernetes, you define the desired state of your application using Kubernetes resources such as Deployments, Services, ConfigMaps, and Secrets. You will need to create Kubernetes manifests (YAML files) that describe these resources and specify how Kubernetes should manage your <span class="No-Break">Go application.</span></li>
				<li><strong class="bold">Handle application life cycle</strong>: Kubernetes manages the life cycle of your application, including scaling, rolling updates, and monitoring. Ensure that your application is designed to work well with Kubernetes by implementing features such as health checks, readiness probes, graceful shutdowns, and <span class="No-Break">logging/metrics instrumentation.</span></li>
				<li><strong class="bold">Service discovery and load balancing</strong>: Use Kubernetes services to expose your application internally within the cluster and to external clients. This allows other parts of your application to discover and communicate with your Go application and enables Kubernetes to load-balance traffic to multiple instances of <span class="No-Break">your application.</span></li>
				<li><strong class="bold">Monitoring and logging</strong>: Instrument your Go application for monitoring and logging using tools such as Prometheus, Grafana, Fluentd, OpenTelemetry, and so on. Emit metrics, logs, and trace information in a structured format so that Kubernetes can collect and analyze them. This allows you to gain visibility into the health and performance of your application running <span class="No-Break">in Kubernetes.</span></li>
			</ul>
			<p>By following these steps, you can successfully deploy and run your Go application using an orchestrator, such as a Kubernetes<a id="_idIndexMarker1440"/> environment. It’s important to familiarize yourself with Kubernetes concepts and best<a id="_idIndexMarker1441"/> practices to ensure that your application runs smoothly and efficiently in production. You should also acknowledge a much more complex environme<a id="_idTextAnchor2196"/><a id="_idTextAnchor2197"/>nt and learning curve to bring yourself up to speed on working <span class="No-Break">with Kubernetes.</span></p>
			<h1 id="_idParaDest-529"><a id="_idTextAnchor2198"/>Summary</h1>
			<p>This chapter was an exciting one that expanded our understanding of where we are running the Go applications that we write. We learned how to run our Go code in the cloud, all packaged up nicely and providing us with the monitoring insights that we need to ensure success for <span class="No-Break">our services.</span></p>
			<p>We started with understanding why and how to make our Go application code instrumented with monitoring using Prometheus. That was a nice segue into gaining even richer insights into our application using OpenTelemetry. We then demonstrated how to containerize our application using Docker, and then looked at how to run that containerized application in an orchestrated environment, such as in Kubernetes. We’ve covered a lot of ground in this chapter and in this book in <span class="No-Break">its entirety.</span></p>
			<p>Over the course of the book, we have covered the basics of Go with variables and various type declarations. We moved into control flow and data rules with Go, to include some of the newest features of working with complex types such as using generics and interfaces. We covered good software engineering practices as they apply to Go through code reuse, error handling, and how to work with large-scale projects through Go modules and packages. We even touched on time and files and systems. </p>
			<p>The book transformed our skills to a professional level demonstrating debugging best practices with Go, crafting state-of-the-art CLI applications, and how to perform application development by connecting to databases, and working with web servers and clients. We tied everything up with a nice bow ending with covering the concurrency Go primitives, strong testing practices, and even highlighting the best of the Go ecosystem with the tools it offers. Lastly, we saw how to run our Go code in the cloud and gain insights into how our application is performing. This book should provide you with the tools and knowledge to transform your Go knowledge into a professional <span class="No-Break">Go developer!</span></p>
		</div>
	</div></div></body></html>