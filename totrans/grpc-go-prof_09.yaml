- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Production-Grade APIs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量产级API
- en: Up until now, we’ve focused on the features provided by gRPC and those added
    by community projects. That was an important topic but it wasn’t the whole story.
    We now need to think about how to test, debug, and deploy our gRPC server.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们一直关注gRPC提供的功能和社区项目添加的功能。这是一个重要的主题，但并不是全部。我们现在需要考虑如何测试、调试和部署我们的gRPC服务器。
- en: 'In this chapter, we are going to see how to unit and load test our services.
    Then, we are going to see how we can manually interact with our API to debug it.
    Finally, we are going to see how we can containerize and deploy our services.
    This chapter is divided into the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何对服务进行单元和负载测试。然后，我们将看到如何手动与我们的API交互以调试它。最后，我们将看到如何容器化和部署我们的服务。本章分为以下主要主题：
- en: Testing APIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试API
- en: Debugging using server reflection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用服务器反射进行调试
- en: Deploying gRPC services on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署gRPC服务
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for this chapter in the folder called chapter5 in the
    companion repo for this book at [https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9](https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9).
    In this chapter, I will be using three main tools: `ghz`, `grpcurl`, and Wireshark.
    You should already have Wireshark installed from [*Chapter 1*](B19664_01.xhtml#_idTextAnchor014),
    but if this is not the case, you can find it at [https://www.wireshark.org/](https://www.wireshark.org/).
    ghz is a tool that will let us load test our API. You can get it by visiting [https://ghz.sh/](https://ghz.sh/).
    Finally, we will use grpcurl to interact with our API from the terminal. You should
    be able to get it from [https://github.com/fullstorydev/grpcurl](https://github.com/fullstorydev/grpcurl).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书配套仓库中名为chapter5的文件夹中找到本章的代码，该仓库的网址为[https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9](https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9)。在本章中，我将使用三个主要工具：`ghz`、`grpcurl`和Wireshark。您应该已经从[*第1章*](B19664_01.xhtml#_idTextAnchor014)安装了Wireshark，但如果没有，您可以在[https://www.wireshark.org/](https://www.wireshark.org/)找到它。ghz是一个让我们能够进行API负载测试的工具。您可以通过访问[https://ghz.sh/](https://ghz.sh/)来获取它。最后，我们将使用grpcurl从终端与我们的API交互。您应该能够从[https://github.com/fullstorydev/grpcurl](https://github.com/fullstorydev/grpcurl)获取它。
- en: Testing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: Developing production-grade APIs begins with writing comprehensive tests to
    ensure that the business requirements are met while also verifying the API’s consistency
    and performance. The first part is mostly handled in unit and integration tests
    and the second part with load testing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开发量产级API始于编写全面的测试，以确保满足业务需求，同时验证API的一致性和性能。第一部分主要在单元和集成测试中处理，第二部分通过负载测试处理。
- en: In the first part of this section, we are going to focus on unit testing the
    server. We are going to do one test per API type to understand how you can introduce
    more in the future. In the second part, we are going to introduce ghz, which is
    a tool for load testing gRPC APIs. We are going to introduce the different options
    that the tool has and how to load test an API with credentials, an auth token
    as a header, and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的第一个部分，我们将重点关注服务器的单元测试。我们将针对每种API类型进行一次测试，以了解如何在未来引入更多测试。在第二部分，我们将介绍ghz，这是一个用于负载测试gRPC
    API的工具。我们将介绍该工具的不同选项以及如何使用凭据、作为头部的认证令牌等进行API负载测试。
- en: Unit testing
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试
- en: As mentioned, we are going to focus on unit testing the server. Before beginning,
    it is important to know that the tests presented here are not all the possible
    tests that we could do. To keep this book readable, I will be presenting how to
    write unit tests for each API type, and you can find an example of other tests
    in the `server/impl_test.go` file.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将重点关注服务器的单元测试。在开始之前，重要的是要知道这里展示的测试并不是我们可能做的所有测试。为了使本书易于阅读，我将展示如何为每种API类型编写单元测试，您可以在`server/impl_test.go`文件中找到其他测试的示例。
- en: Before writing any tests, we need to do some setup. We are going to write some
    boilerplate for the different tests to share the same server and connection. This
    is mostly to avoid creating new servers and connections each time we are running
    a test. However, note that these are non-hermetic tests. This means that an unexpected
    state could be shared across multiple tests and make the tests flaky. We are going
    to introduce ways to deal with this and make sure we clear the states.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写任何测试之前，我们需要做一些设置。我们将为不同的测试编写一些样板代码，以便它们可以共享相同的服务器和连接。这主要是为了避免每次运行测试时都创建新的服务器和连接。然而，请注意，这些是非密封测试。这意味着意外的状态可能会在多个测试之间共享，从而使测试变得不可靠。我们将介绍处理这种情况的方法，并确保我们清除状态。
- en: The first thing that we can do is create a fake database. This is like what
    we did with `inMemoryDb`, and in fact, `FakeDb` is a wrapper around `inMemoryDb`,
    but we are also going to test problems due to connectivity with the database.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先可以做的事情是创建一个假数据库。这就像我们处理 `inMemoryDb` 一样，实际上，`FakeDb` 是 `inMemoryDb` 的包装器，但我们还将测试由于数据库连接问题引起的问题。
- en: 'To do so, we are going to use the same pattern as `grpc.ServerOption`. `grpc.ServerOption`
    is a function applying a value to a private struct. An example of this is `grpc.Creds`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用与 `grpc.ServerOption` 相同的模式。`grpc.ServerOption` 是一个将值应用到私有结构体的函数。这个模式的例子是
    `grpc.Creds`：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It returns a function that, once called, will set the value of `c` to the `creds`
    property in `serverOptions`. Note that `serverOptions` is different from `ServerOption`.
    This is a private struct.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个函数，一旦调用，就会将 `c` 的值设置为 `serverOptions` 中的 `creds` 属性。请注意，`serverOptions`
    与 `ServerOption` 不同。这是一个私有结构体。
- en: 'We are going to create a function that tells us whether the database is available
    or not. Later, we are going to enable the option to return an error if it is not.
    In `test_options.go`, we will have the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个函数，告诉我们数据库是否可用。稍后，我们将启用选项，如果不可用则返回错误。在 `test_options.go` 中，我们将有以下内容：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I’ll leave it up to you to check the rest of the content of `test_options.go`.
    The functions and structs there simply create some utilities and variables in
    order to be able to write the `IsAvailable` function and get default values for
    `isAvailable`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给你们去检查 `test_options.go` 的其余内容。那里的函数和结构体只是创建一些工具和变量，以便能够编写 `IsAvailable`
    函数并获取 `isAvailable` 的默认值。
- en: 'Now, we can create `FakeDb`. As mentioned, this is a wrapper around `inMemoryDb`,
    and it has some options. In `fake_db.go`, we can have the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建 `FakeDb`。如前所述，这是一个 `inMemoryDb` 的包装器，它有一些选项。在 `fake_db.go` 中，我们可以有以下内容：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now create a `FakeDb` in multiple ways:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过多种方式创建一个 `FakeDb`：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also override the `inMemoryDb` functions so that our `FakeDb` implements
    the `db` interface and so that we can instantiate a server with this database.
    Each function of `FakeDb` follows the same pattern. We check whether the database
    is available or not; if it is not, we return an error, and if it is, we return
    the result of `inMemoryDb`. An example of this is `addTask` (in `fake_db.go`):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还重写了 `inMemoryDb` 的函数，以便我们的 `FakeDb` 实现了 `db` 接口，并且我们可以使用这个数据库实例化一个服务器。`FakeDb`
    的每个函数都遵循相同的模式。我们检查数据库是否可用；如果不，我们返回一个错误，如果可用，我们返回 `inMemoryDb` 的结果。一个例子是 `addTask`（在
    `fake_db.go` 中）：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have that, we can move one step closer to writing an actual unit
    test. We now need to create a server. However, we do not want this server to actually
    use ports on our computer. Using an actual port could make our tests flaky because
    if the port is already in use, the test would directly return an error saying
    that it could not create the instance of the server.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了这个，我们就可以再向前迈进一步，编写实际的单元测试。我们现在需要创建一个服务器。然而，我们不想这个服务器实际上使用我们电脑上的端口。使用实际端口可能会使我们的测试变得不可靠，因为如果端口已经被占用，测试会直接返回一个错误，表示无法创建服务器的实例。
- en: 'To solve that, gRPC has a package called `bufconn` (`grpc/test/bufconn`). It
    lets us create a buffered connection and thus does not need to use ports. `bufconn.Listen`
    will create a listener and we will be able to use this listener to server requests.
    In `server_test.go`, we will share the listener and database as global variables.
    This will let us dispose of the listener after all tests and add/clear tasks in
    the database from within a test. On top of that, we will create a function that
    returns a `net.Conn` connection so that we can use it within the test to create
    a client:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，gRPC有一个名为`bufconn`的包（`grpc/test/bufconn`）。它允许我们创建一个缓冲连接，因此不需要使用端口。`bufconn.Listen`将创建一个监听器，我们将能够使用这个监听器来处理请求。在`server_test.go`中，我们将监听器和数据库作为全局变量共享。这将允许我们在所有测试完成后销毁监听器，并在测试中从数据库中添加/清除任务。此外，我们将创建一个返回`net.Conn`连接的函数，这样我们就可以在测试中使用它来创建一个客户端：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first thing to notice is that we are using the Go `init()` function to do
    this setup before the tests are started. Then, notice that we create an instance
    of our server and register the implementation of our `TodoService`. Finally, the
    server is serving in a goroutine. So, we need to make sure that the goroutine
    is canceled.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，我们使用Go的`init()`函数在测试开始之前进行此设置。然后，注意我们创建了我们服务器的实例并注册了我们的`TodoService`实现的实例。最后，服务器在一个goroutine中提供服务。所以，我们需要确保goroutine被取消。
- en: 'We are almost done with the boilerplate. We need to create a client that uses
    the `bufDialer` function to connect to the server through the buffered connection.
    In `impl_test.go`, we are going to create a function that returns `TodoServiceClient`
    and `grpc.ClientConn`. The first is obviously to call our endpoints but the second
    one is for us to close the client connection at the end of each test:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了样板代码。我们需要创建一个使用`bufDialer`函数通过缓冲连接连接到服务器的客户端。在`impl_test.go`中，我们将创建一个返回`TodoServiceClient`和`grpc.ClientConn`的函数。第一个显然是用来调用我们的端点，但第二个是为了我们在每个测试结束时关闭客户端连接：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One important thing to understand here is that we are not testing the whole
    server that we wrote in `main.go`. We are simply testing our endpoints implementation.
    This is why we can connect to the server with insecure credentials. The interceptors,
    encryption, and so on should be tested in integration tests.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要理解的一个重要的事情是，我们不是在测试我们在`main.go`中编写的整个服务器。我们只是在测试我们的端点实现。这就是为什么我们可以使用不安全的凭据连接到服务器。拦截器、加密等应该在集成测试中进行测试。
- en: 'Finally, we can create a small utility function that checks that an error is
    a grpc error and that it has an expected message:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以创建一个小的实用函数来检查一个错误是否是gRPC错误，并且它有一个预期的消息：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are now ready to write some unit tests. We are going to create a function
    that will run all the unit tests and dispose of the listener when all the subtests
    are finished:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备编写一些单元测试。我们将创建一个函数，该函数将运行所有单元测试，并在所有子测试完成后销毁监听器：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We are now able to populate the `TestRunAll` function with subtests, like so:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们能够用子测试填充`TestRunAll`函数，如下所示：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let us now write the `testAddTaskEmptyDescription` function, which checks that
    we get an error when we send a request with an empty description. We will create
    a new instance of a client, create an empty request, send it to `AddTask`, and
    finally, check that our error has an unknown code (returned by `protoc-gen-validate`)
    and that the message is `invalid AddTaskRequest.Description: value length must
    be at least 1 runes` (also from `protoc-gen-validate`):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们编写`testAddTaskEmptyDescription`函数，该函数检查当我们发送一个带有空描述的请求时是否会得到一个错误。我们将创建一个客户端的新实例，创建一个空请求，将其发送到`AddTask`，最后检查我们的错误是否有一个未知代码（由`protoc-gen-validate`返回）以及消息是`invalid
    AddTaskRequest.Description: value length must be at least 1 runes`（也来自`protoc-gen-validate`）：'
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can then add it to our `TestRunAll` function, like so:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将其添加到我们的`TestRunAll`函数中，如下所示：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To run this test, we can run the following command in the root folder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个测试，我们可以在根目录中运行以下命令：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, before moving on to looking at how to test streams, let us see how we
    can test with an unavailable database. This is almost the same as what we did
    in `testAddTaskEmptyDescription`, but we are going to override the database. Finally,
    we are going to check that we get an internal error and reset the database (to
    clear the options):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在继续查看如何测试流之前，让我们看看我们如何测试一个不可用的数据库。这几乎与我们在`testAddTaskEmptyDescription`中做的是一样的，但我们将要覆盖数据库。最后，我们将检查我们得到一个内部错误并重置数据库（以清除选项）：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can see that it is easy to test a database failure. That is all for unary
    RPC. I will let you add `testAddTaskUnavailableDb` to `TestRunAll` and look at
    the other tests for `AddTasks` in `impl_test.go`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到测试数据库故障很容易。这就是一元RPC的全部内容。我将让你将`testAddTaskUnavailableDb`添加到`TestRunAll`中，并查看`impl_test.go`中`AddTasks`的其他测试。
- en: 'We are now going to test `ListTasks`. We will add some tasks to our fake database,
    call `ListTasks`, make sure that there is no error, and check that `ListTasks`
    iterated through all the tasks:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将测试`ListTasks`。我们将在我们的模拟数据库中添加一些任务，调用`ListTasks`，确保没有错误，并检查`ListTasks`是否遍历了所有任务：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There is nothing new in terms of calling the API. We already know all of this
    from when we wrote the client. However, the main difference here, for this test,
    is we do not look at the values; we simply assert the time we looped. Of course,
    you could create more sophisticated tests out of this, but I wanted to show you
    a simple test on a server streaming API so that you can build upon it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用API方面没有新的内容。我们已经在编写客户端时知道了所有这些。然而，对于这次测试，主要的不同之处在于我们不看值；我们只是断言我们循环的时间。当然，你可以从这个基础上创建更复杂的测试，但我只想展示一个简单的服务器流式API测试，这样你就可以在此基础上构建。
- en: 'Next, let us test the client streaming API endpoint. As we are working with
    the `UpdateTasks` endpoint, we will need to set data in our database. After that,
    we will basically create an array of `UpdateTasksRequest` in order to change all
    the items in the database, send the requests, and check that all the updates ran
    without error:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们测试客户端流式API端点。由于我们正在使用`UpdateTasks`端点，我们需要在我们的数据库中设置数据。之后，我们将基本上创建一个`UpdateTasksRequest`数组，以便更改数据库中的所有项目，发送请求，并检查所有更新是否运行无误：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is similar to the previous test. We used a counter to check that all updates
    were “applied.” In an integration test, you would have to check that the value
    actually changed in the database; however, because we are in unit tests and we
    have an in-memory database, checking the actual values would not mean much.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的测试类似。我们使用计数器来检查所有更新都被“应用”。在一个集成测试中，你将不得不检查数据库中的值是否实际上发生了变化；然而，因为我们是在单元测试中，并且我们有一个内存数据库，检查实际值并没有什么意义。
- en: 'Finally, we will test the bidirectional streaming API. This is a little bit
    more complex in the testing context, but we are going to tackle the problem step
    by step. Previously, in the client, when an error happened in a goroutine, we
    simply ran `log.Fatalf` to exit. However, here, because we want to keep track
    of errors and we cannot call `t.Fatalf` in a different goroutine from the one
    of the tests, we are going to use a channel of `struct` called `countAndError`.
    As its name suggests, this is a structure containing a counter and an optional
    error:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将测试双向流式API。在测试环境中，这稍微复杂一些，但我们将一步一步地解决这个问题。之前，在客户端，当一个goroutine中发生错误时，我们简单地运行`log.Fatalf`来退出。然而，在这里，因为我们想跟踪错误，并且我们不能从测试的另一个goroutine中调用`t.Fatalf`，我们将使用一个名为`countAndError`的`struct`通道。正如其名所示，这是一个包含计数器和可选错误的结构：
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is useful because now, we will be able to wait for the goroutine to finish
    and get a result of the channel. First, let us create the function that sends
    all the requests. This function is called `sendRequestsOverStream` and it will
    be called in a separate goroutine:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用，因为现在我们将能够等待goroutine完成并获取通道的结果。首先，让我们创建一个发送所有请求的函数。这个函数被命名为`sendRequestsOverStream`，它也将在一个单独的goroutine中被调用：
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If an error occurs, we will close the waiting channel with an error set in the
    `countAndError` structure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生错误，我们将使用在`countAndError`结构中设置错误来关闭等待的通道。
- en: 'Then, we can create the function that reads responses. This function is called
    `readResponsesOverStream` and will also be called in a separate goroutine:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个读取响应的函数。这个函数被命名为`readResponsesOverStream`，它将在一个单独的goroutine中被调用：
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This time, if everything goes well, the channel will get a `countAndError` with
    a count set. This count is the same as what we did in previous tests. It checks
    the number of responses that were collected without error.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，如果一切顺利，通道将获得一个设置计数的`countAndError`。这个计数与我们在之前的测试中所做的是相同的。它检查收集到的没有错误的响应数量。
- en: 'Now that we have these two functions, we are ready to write the actual test
    for our bidirectional streaming API. This is similar to what we did for `ListTasks`
    and `UpdateTasks`; however, this time, we launch two goroutines, wait for the
    result, and check that we have no error and a count equal to the number of requests:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这两个函数，我们就可以为我们的双向流API编写实际的测试了。这与我们为 `ListTasks` 和 `UpdateTasks` 做的事情类似；然而，这次，我们启动了两个goroutines，等待结果，并检查我们没有错误并且计数等于请求数量：
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With that, we have finally finished testing all the different types of gRPC
    APIs. Once again, there are more tests that can be done, and other examples are
    available in `impl_test.go`. I strongly encourage you to take a look there so
    you can get more ideas.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们终于完成了所有不同类型gRPC API的测试。再次强调，还有更多可以进行的测试，其他示例可以在 `impl_test.go` 中找到。我强烈建议你看看那里，以便获得更多灵感。
- en: 'After adding all these tests to `TestRunAll`, you should be able to run them
    like so:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在将所有这些测试添加到 `TestRunAll` 之后，你应该能够像这样运行它们：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you want a more detailed output of what test ran, you can add the `–v` option.
    This will return something like the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看测试运行了什么更详细的信息，可以添加 `–v` 选项。这将返回类似以下的内容：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Bazel
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bazel
- en: 'In order to run tests with Bazel, you can run Gazelle to generate the `//``server:server_test`
    target:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 Bazel 运行测试，你可以运行 Gazelle 生成 `//server:server_test` 目标：
- en: '[PRE22]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will then have the target available in `server/BUILD.bazel`, and you should
    be able to run the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将在 `server/BUILD.bazel` 中有这个目标，你应该能够运行以下命令：
- en: '[PRE23]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you want to get a more verbose output for your tests, you can use the `–test_arg`
    option and set it to `-test.v`. It will return something like the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想为你的测试获取更详细的输出，可以使用 `–test_arg` 选项并将其设置为 `-test.v`。它将返回类似以下的内容：
- en: '[PRE24]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: To conclude, we saw how to test unary, server streaming, client streaming, and
    bidirectional streaming APIs. We saw that we do not need to use a port on the
    machine running the test when using `bufconn`. This makes our tests less reliant
    on the environment it runs on. Finally, we also saw that we can use fakes in order
    to test our system dependencies. This is a bit out of the scope of this book,
    but it was important to me to show you that you can write normal tests even if
    you are using gRPC.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结，我们看到了如何测试一元、服务器端流、客户端流和双向流API。我们看到了在使用 `bufconn` 时，我们不需要在运行测试的机器上使用端口。这使得我们的测试对运行环境的依赖性更小。最后，我们还看到了我们可以使用模拟来测试我们的系统依赖。这超出了本书的范围，但对我来说，展示你可以即使使用gRPC也能编写正常的测试是很重要的。
- en: Load testing
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压力测试
- en: Another important step when testing your services is to make sure that they
    are efficient and can handle a specific load. For this, we use load-testing tools
    that will concurrently send requests to our service. ghz is a tool that does just
    that. In this section, we are going to see how to use the tool and some options
    that we need to set in order to test our API.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 测试你的服务时，确保它们高效并能处理特定负载是另一个重要步骤。为此，我们使用并发向我们的服务发送请求的负载测试工具。ghz 就是这样一个工具。在本节中，我们将看到如何使用这个工具以及我们需要设置的一些选项，以便测试我们的API。
- en: 'ghz is a tool that is highly configurable. Run the following command to see
    and understand the output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ghz 是一个高度可配置的工具。运行以下命令以查看和理解输出：
- en: '[PRE25]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Obviously, we are not going to use all these options but we will examine the
    most common ones and the ones that we need to use in some specific cases. Let
    us start by trying to make a simple call.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们不会使用所有这些选项，但我们将检查最常用的选项以及在某些特定情况下我们需要使用的选项。让我们先尝试进行一个简单的调用。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In order to run the following load test, you will need to deactivate the rate-limiting
    middleware in the `server/main.go` file. You can do so by commenting `ratelimit.UnaryServerInterceptor`
    and `ratelimit.StreamServerInterceptor`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行以下负载测试，你需要在 `server/main.go` 文件中禁用速率限制中间件。你可以通过注释掉 `ratelimit.UnaryServerInterceptor`
    和 `ratelimit.StreamServerInterceptor` 来做到这一点。
- en: 'We first run our server:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先运行我们的服务器：
- en: '[PRE26]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The first four options that we are going to talk about are the most common
    ones. We need to be able to name the service and method that we want to call (`--call`),
    indicate in which proto file the service is defined (`--proto`) and where to find
    the imports (`--import_paths`), and finally, specify the data to be sent as a
    request. In our case, a basic command, run from the `chapter9` folder, will look
    like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的前四个选项是最常见的。我们需要能够命名我们想要调用的服务和方法（`--call`），指出服务定义在哪个proto文件中（`--proto`）以及在哪里可以找到导入（`--import_paths`），最后，指定作为请求发送的数据。在我们的例子中，一个基本的命令，从`chapter9`文件夹运行，看起来像这样：
- en: '[PRE27]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'However, if you try to run this command, you will end up having an error message
    like the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试运行这个命令，你最终会得到一个类似以下错误的消息：
- en: '[PRE28]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can surely guess from the message, this is because we set up our server
    to only accept secure connections. To solve this problem, we will use the `--cacert`
    option, which lets us specify a path to where the CA certificate is. If you remember,
    this is exactly what we did in the code for our client. `ghz` also needs that
    information:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从消息中可以肯定猜到的，这是因为我们设置了我们的服务器只接受安全连接。为了解决这个问题，我们将使用`--cacert`选项，它允许我们指定CA证书的路径。如果你记得，这正是我们在客户端代码中做的。`ghz`也需要这个信息：
- en: '[PRE29]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you run this command, you will get the same error as previously. This is
    because a certificate has a domain name associated with it. This means that only
    requests from a certain domain name will be accepted. However, because we are
    working from localhost, this simply does not meet that requirement and fails.
    To solve that, we are going to use the `--cname` option to override the domain
    name from which we are sending to comply with the certificate:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个命令，你会得到与之前相同的错误。这是因为证书与一个域名相关联。这意味着只有来自特定域名的请求才会被接受。然而，因为我们是从本地主机工作的，这根本不符合那个要求，所以失败了。为了解决这个问题，我们将使用`--cname`选项来覆盖我们发送的域名，以符合证书：
- en: '[PRE30]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we used `check.test.example.com` because the generated certificate that
    we downloaded from [https://github.com/grpc/grpc-go/tree/master/examples/data/x509](https://github.com/grpc/grpc-go/tree/master/examples/data/x509)
    was generated with the DNS name `*.test.example.com` (see `openssl.cnf`). Also,
    note that this `--cacert` and `--cname` are only useful for self-signed certificates.
    In general, except for specific cases, these certificates are used for testing
    and non-production environments.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`check.test.example.com`，因为我们从[https://github.com/grpc/grpc-go/tree/master/examples/data/x509](https://github.com/grpc/grpc-go/tree/master/examples/data/x509)下载的生成证书是以DNS名称`*.test.example.com`生成的（见`openssl.cnf`）。此外，请注意，这个`--cacert`和`--cname`选项仅对自签名证书有用。通常，除了特定情况外，这些证书用于测试和非生产环境。
- en: 'Now, if you run the previous command, you should get the following error:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你运行前面的命令，你应该会得到以下错误：
- en: '[PRE31]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'That should ring a bell. This is the error we are sending in our auth interceptor
    when a client does not provide `auth_token` metadata. In order to send that metadata,
    we are going to use the `--metadata` option, which takes a JSON string for keys
    and values:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会让你想起什么。这是我们发送到认证拦截器中的错误，当客户端没有提供`auth_token`元数据时。为了发送这些元数据，我们将使用`--metadata`选项，它接受一个JSON字符串作为键和值：
- en: '[PRE32]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After running with all these options, we should be able to run our first load
    test (the results might be different for you):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行了所有这些选项之后，我们应该能够运行我们的第一次负载测试（你的结果可能会有所不同）：
- en: '[PRE33]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There is a lot to say and to look at in this summary. However, let us focus
    on some interesting points. The first one is the number of requests made. We can
    see that we made 200 of them in this test. This is the default number of requests.
    We can change that by using the `--total` option and setting another number (e.g.,
    500).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个总结中有很多东西可以讨论和观察。然而，让我们关注一些有趣的观点。第一个是请求的数量。我们可以看到在这个测试中我们发出了200个请求。这是默认的请求数量。我们可以通过使用`--total`选项并设置另一个数字（例如，500）来改变它。
- en: Then, in the response time histogram, we can see that 111 out of 200 requests
    were executed in ~2.29 ms. Another interesting thing to see here is that we have
    some commands (50) running in more than 13 ms. If we were in production, we might
    want to dig deeper into this in order to find the cause of these “high” execution
    times. This depends a lot on the use case and requirements. In our case, this
    is almost certainly due to the inefficient “database” that we use, or more precisely,
    the `append` that we repeatedly call in `inMemoryDb.addTask`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在响应时间直方图中，我们可以看到 200 个请求中有 111 个在约 2.29 毫秒内执行。这里还有一个有趣的现象，我们有一些命令（50 个）运行时间超过
    13 毫秒。如果我们处于生产环境，我们可能需要深入挖掘以找出这些“高”执行时间的原因。这很大程度上取决于用例和需求。在我们的案例中，这几乎肯定是由于我们使用的“数据库”效率低下，或者更确切地说，是我们在
    `inMemoryDb.addTask` 中反复调用的 `append`。
- en: After that, we have the distribution of our execution time. We can see that
    75% of our requests execute in under 2.39 ms. In fact, this is a similar information
    as presented previously. If we take the number of requests under 3.504 ms, add
    them up, and calculate the percentage, we get (1 + 111 + 38) * 100 / 200 = 75%.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们来看我们的执行时间分布。我们可以看到，75% 的请求在 2.39 毫秒以下执行。实际上，这与之前展示的信息类似。如果我们把 3.504 毫秒以下的请求数量加起来，并计算百分比，我们得到
    (1 + 111 + 38) * 100 / 200 = 75%。
- en: 'Then, we have the status code distribution. In our case, all 200 requests succeeded.
    However, in a production scenario, you might have something that looks more like
    this (from the ghz documentation):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们来看状态码分布。在我们的案例中，所有 200 个请求都成功了。然而，在生产环境中，你可能会遇到类似以下情况（来自 ghz 文档）：
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, one last thing that we cannot see here (because we do not have any)
    is the error distribution. This is the distribution of the error messages. Once
    again, in production, you might have something like the following (from the ghz
    documentation):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们无法看到的是错误分布。这是错误消息的分布。同样，在生产环境中，你可能会看到类似以下情况（来自 ghz 文档）：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There is obviously a lot more that we could do with this tool. As mentioned,
    it is highly configurable, and it is even possible to link the results in Grafana
    (`https://ghz.sh/docs/extras`) for visualization. However, this is out of the
    scope of this book. I will leave it up to you to try the different options and
    call ghz on our other API endpoints to see how they perform.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们可以用这个工具做更多的事情。如前所述，它是高度可配置的，甚至可以将结果链接到 Grafana（`https://ghz.sh/docs/extras`）进行可视化。然而，这超出了本书的范围。我将把它留给你去尝试不同的选项，并在我们的其他
    API 端点调用 ghz 来查看它们的性能。
- en: To conclude, we saw how can load test our service with ghz. We only saw how
    to use it for our unary API, but it is also useful for testing all the other streaming
    APIs. After executing the ghz command, we saw that we can get information about
    latency, error codes, error message distribution, and the fastest and slowest
    running times. All of this is useful, but it is important to understand that it
    can be even more powerful when linked with visualization tools such as Grafana.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们看到了如何使用 ghz 对我们的服务进行负载测试。我们只展示了如何用于我们的单一 API，但它也适用于测试其他所有流式 API。执行 ghz
    命令后，我们看到了可以获取有关延迟、错误代码、错误消息分布以及最快和最慢运行时间的信息。所有这些都很有用，但重要的是要理解，当与可视化工具如 Grafana
    链接时，它可能更加强大。
- en: Debugging
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试
- en: No matter how well we unit test our services, we are humans and humans make
    mistakes. At some point, we are going to need to debug a service. In this section,
    we are going to see how to approach debugging. We are first going to enable server
    reflection, which will let us call our service simply from the command line. After
    that, we will use Wireshark to inspect data on the wire. Finally, because the
    error might not always come directly from our code, we will see how we can take
    a look at the gRPC logs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们如何对服务进行单元测试，我们都是人，人都会犯错误。在某个时刻，我们都需要调试一个服务。在本节中，我们将探讨如何进行调试。我们首先将启用服务器反射，这将使我们能够从命令行简单地调用我们的服务。之后，我们将使用
    Wireshark 检查线上的数据。最后，因为错误可能并不总是直接来自我们的代码，我们将了解如何查看 gRPC 日志。
- en: Server reflection
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务器反射
- en: Server reflection is an interesting feature when it comes to exposing the API
    to external clients. This is because it lets the server describe itself. In other
    words, the server knows all the services registered and the message definition.
    If a client asks for more information, the server, through reflection, can list
    all the services, messages, and so on. With that, the client does not even need
    to have a copy of the proto file. Now, this is not only useful for exposing the
    API to external clients. It is also useful for manual testing/debugging. It lets
    developers/testers focus only on debugging the API and not getting the whole environment
    to work (copy the proto files, and so on).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器反射是向外部客户端暴露 API 时一个有趣的功能。这是因为它让服务器描述自己。换句话说，服务器知道所有注册的服务和消息定义。如果客户端请求更多信息，服务器通过反射可以列出所有服务、消息等。有了这个，客户端甚至不需要有
    proto 文件的副本。现在，这不仅对向外部客户端暴露 API 有用。它对手动测试/调试也很有用。它让开发者/测试者只需专注于调试 API，而不必让整个环境都工作（复制
    proto 文件等）。
- en: 'Enabling server reflection is an easy thing in gRPC Go. We only need two lines
    of code: an `import` statement and a call to the `reflection.Register` function
    to register the reflection service on our server. It looks like this (`server/main.go`):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 gRPC Go 中启用服务器反射是一件简单的事情。我们只需要两行代码：一个 `import` 语句和对 `reflection.Register`
    函数的调用，以在我们的服务器上注册反射服务。它看起来像这样（`server/main.go`）：
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: However, even though this is enough to expose information, we will need to get
    a client that contacts the server and understand the information it is getting.
    There are multiple such tools out there. [The most popular one is `grpcurl` (`https`](https://github.com/fullstorydev/grpcurl)`://github.com/fullstorydev/grpcurl`).
    If you are familiar with cURL, this is basically a similar tool, but one that
    understands the gRPC protocol. Even though we are going to use this tool to explore
    server reflection, know that it can also make other normal requests. If you are
    interested in such a tool, the repository’s README is full of examples of how
    to use it for other tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管这足以暴露信息，我们仍需要获取一个联系服务器并理解其获取信息的客户端。市面上有多个这样的工具。[最受欢迎的一个是 `grpcurl` (`https://github.com/fullstorydev/grpcurl`)](https://github.com/fullstorydev/grpcurl)。如果你熟悉
    cURL，这个工具基本上是类似的，但它理解 gRPC 协议。尽管我们将使用这个工具来探索服务器反射，但要知道它也可以发出其他正常请求。如果你对这样的工具感兴趣，仓库的
    README 中充满了如何使用它进行其他任务的示例。
- en: 'Let us try to create a simple command with `grpcurl` first. We are going to
    use options that are similar to the ones we used in ghz. We are going to use the
    CA certificate and override the domain name with `–cacert` and `-authority`. Then,
    we are going to add an `auth_token` header for reflection with `-reflect-header`,
    and finally, we will use the list verb in order to list the services present on
    the server:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试使用 `grpcurl` 创建一个简单的命令。我们将使用与我们在 ghz 中使用的类似选项。我们将使用 CA 证书，并用 `–cacert`
    和 `-authority` 覆盖域名。然后，我们将为反射添加一个 `auth_token` 标头，最后，我们将使用列表动词来列出服务器上存在的服务：
- en: '[PRE37]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once we run this command, we should get the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行这个命令，我们应该得到以下输出：
- en: '[PRE38]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can see that we have both our `TodoService` and the `ServerReflection` service
    that we registered earlier. With that, we can describe a service to get all the
    RPC endpoints it contains. We do that with the `describe` verb followed by the
    service name:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们既有我们的 `TodoService`，也有我们之前注册的 `ServerReflection` 服务。有了这些，我们可以描述一个获取它包含的所有
    RPC 端点服务的服务。我们通过在服务名称后跟 `describe` 动词来实现这一点：
- en: '[PRE39]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Running this command will show the definition of the service:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令将显示服务的定义：
- en: '[PRE40]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also take a look at the message content by replacing the service’s name
    after `describe` with the name message. An example for `AddTaskRequest` is as
    follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将 `describe` 后面的服务名称替换为消息名称来查看消息内容。以下是对 `AddTaskRequest` 的一个示例：
- en: '[PRE41]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, as we are talking about debugging in this section, we want to be able
    to call these RPC endpoints and test them with different data. This is easy because
    we do not even need to have the proto file with us. The server reflection will
    help `grpcurl` figure everything out for us. Let us call the `AddTask` endpoint
    with an invalid request:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们在本节中讨论调试，我们希望能够调用这些 RPC 端点并使用不同的数据对其进行测试。这很简单，因为我们甚至不需要携带 proto 文件。服务器反射将帮助
    `grpcurl` 为我们解决所有问题。让我们用无效请求调用 `AddTask` 端点：
- en: '[PRE42]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Notice that we use other options here. We use the `-d` option to set the data
    that we want to send as `AddTaskRequest`. We use the `–use-reflection` option
    so that `grpcurl` can verify that the data is valid (we are going to see that
    soon) and the `–rpc-header` on top of `–reflect-header` because `–reflect-header`
    only sends the header to the `ServerReflection` service, and we also need to send
    the header to `TodoService`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在这里使用了其他选项。我们使用 `-d` 选项来设置我们想要发送的数据作为 `AddTaskRequest`。我们使用 `–use-reflection`
    选项，以便 `grpcurl` 可以验证数据的有效性（我们很快就会看到这一点），并且使用 `–rpc-header` 在 `–reflect-header`
    之上，因为 `–reflect-header` 只会将头部发送到 `ServerReflection` 服务，而我们还需要将头部发送到 `TodoService`。
- en: 'As expected, the previous command returns the following error:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，之前的命令返回以下错误：
- en: '[PRE43]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, as mentioned, grpcurl does not let us execute commands without adding
    guardrails. The use of reflection is useful here because it does not let us send
    data that cannot be deserialized in the request message. An example is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如提到的，grpcurl 不允许我们在不添加防护措施的情况下执行命令。在这里使用反射是有用的，因为它不允许我们发送无法在请求消息中反序列化的数据。以下是一个示例：
- en: '[PRE44]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Finally, as we also have a non-unary RPC endpoint that we would want to test,
    we can use an interactive terminal. This will let us send and receive multiple
    messages. To do that, we will set the data to `@` and end the command with `<<EOF`,
    where `EOF` stands for end of file (you can use any suffix really). This will
    let us type data interactively, and when we are finished, we write `EOF` to let
    grpcurl know.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为我们还有一个我们想要测试的非一元 RPC 端点，我们可以使用交互式终端。这将使我们能够发送和接收多个消息。为此，我们将数据设置为 `@` 并以
    `<<EOF` 结束命令，其中 `EOF` 代表文件结束（你可以使用任何后缀）。这将使我们能够交互式地输入数据，当我们完成时，我们写入 `EOF` 来让 grpcurl
    知道。
- en: 'Let us start by adding two new tasks to our server:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先在我们的服务器上添加两个新的任务：
- en: '[PRE45]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we can use `ListTasks` to show the tasks:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `ListTasks` 来显示任务：
- en: '[PRE46]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Can you spot any bugs here? If not, do not worry; we are going to come back
    to it shortly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在这里找到任何 bug 吗？如果没有，请不要担心；我们很快就会回到这个问题上。
- en: 'Then, to call our client streaming API (`UpdateTasks`), we can use the following
    command on Linux/Mac (press *Enter* after the last `EOF`):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了调用我们的客户端流式 API (`UpdateTasks`)，我们可以在 Linux/Mac 上使用以下命令（在最后一个 `EOF` 后按 *Enter*）：
- en: '[PRE47]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Windows (PowerShell) users should use the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Windows（PowerShell）用户应使用以下命令：
- en: '[PRE48]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: After that, another call to `ListTasks` should show the data with the new descriptions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，再次调用 `ListTasks` 应该会显示带有新描述的数据。
- en: Now, while executing these functions, you might have noticed a bug. If you did
    not, there is nothing to worry about; we are going to solve the problem together.
    The problem is that we can send an empty `DueDate`, which then gets transformed
    to the `0` value in Unix Time (`1970-01-01`). This bug comes from the fact that
    `protoc-gen-validate` checks `DueDate` only if it is set.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在执行这些函数时，你可能已经注意到了一个 bug。如果你没有，请不要担心；我们将一起解决这个问题。问题是我们可以发送一个空的 `DueDate`，然后它被转换为
    Unix 时间（`1970-01-01`）中的 `0` 值。这个 bug 来自于 `protoc-gen-validate` 只在设置时检查 `DueDate`。
- en: To solve that, we can add another validation rule for `due_date` to our `todo.proto`
    file. This rule is `required`. It will make it impossible for the field to not
    be set. You might argue that a task does not need a due date, but we could make
    a different endpoint for `Adding Notes` and say that `Tasks` should have a due
    date, but not `Note`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以在 `todo.proto` 文件中为 `due_date` 添加另一个验证规则。这个规则是 `required`。这将使得字段无法不设置。你可能认为任务不需要截止日期，但我们可以为
    `Adding Notes` 创建一个不同的端点，并说 `Tasks` 应该有截止日期，但 `Note` 不需要。
- en: '`due_date` will now be defined as such:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`due_date` 将被定义为以下形式：'
- en: '[PRE49]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can rerun the generation of the `validate` plugin (in `chapter9`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新运行 `validate` 插件的生成（在 `chapter9` 中）：
- en: '[PRE50]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we shut our server down and relaunch it:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们关闭并重新启动我们的服务器：
- en: '[PRE51]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If we rerun one of the previous `AddTask` commands, it should fail:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新运行之前的 `AddTask` 命令之一，它应该会失败：
- en: '[PRE52]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We solved a bug! How great is that?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决了 bug！这真是太棒了？
- en: 'If you want to now send a request with a `due_data` value you will have to
    specify a date in `RFC3339` format as a string. An example using a `due_date`
    value of 500 years from the day of writing this is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在想发送一个带有 `due_data` 值的请求，你必须指定一个日期，格式为 `RFC3339` 的字符串。以下是一个示例，使用从写作此文档之日起
    500 年的 `due_date` 值：
- en: '[PRE53]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Bazel
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bazel
- en: 'In order to run the server with Bazel, you will have to update the dependencies.
    You can run Gazelle to update `//server:server`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 Bazel 运行服务器，你必须更新依赖项。你可以运行 Gazelle 来更新 `//server:server`：
- en: '[PRE54]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, you will be able to run the server normally:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将能够正常运行服务器：
- en: '[PRE55]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: To conclude, we saw that we can turn server reflection on in order to get information
    and interact with a server for debugging purposes. We saw that we can list services
    and describe both services and messages. We also saw that we can call unary RPC
    endpoints, and finally, we saw that we can also call streaming APIs with interactive
    terminals.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们看到了可以通过开启服务器反射来获取信息和与服务器交互以进行调试。我们看到了可以列出服务并描述服务和消息。我们还看到了可以调用单一 RPC
    端点，最后，我们看到了也可以使用交互式终端调用流式 API。
- en: Using Wireshark
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Wireshark
- en: Sometimes, we need to be able to inspect the data that is going through the
    wire. This lets us get a sense of how heavy the payloads are, know if we execute
    too many requests, and so on. In this section, we are going to see how we can
    use Wireshark to analyze payloads and requests.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要能够检查通过线缆传输的数据。这让我们能够了解有效载荷的重量，知道我们是否执行了过多的请求等等。在本节中，我们将看到如何使用 Wireshark
    分析有效载荷和请求。
- en: The first thing that we need in order to get access to readable information
    is to disable the encryption via TLS that we enabled in `Chapter 7`. Note that
    this should be fine because we are in development mode, but you will need to make
    sure that encryption is on when you push back to production.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取可读信息，我们首先需要禁用我们在 `第7章` 中启用的 TLS 加密。请注意，这应该没问题，因为我们处于开发模式，但当你将代码推回生产环境时，你需要确保加密是开启的。
- en: To disable the encryption, we are going to create a switch variable. With this
    variable, the TLS will be disabled by setting the `ENABLE_TLS` environment variable
    to `false`. Obviously, as we want to make TLS the default, we are going to check
    whether the environment variable value is different from `false`, so that if there
    is a typo in the value or the value is not set, TLS will be enabled.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用加密，我们将创建一个开关变量。使用这个变量，通过将 `ENABLE_TLS` 环境变量设置为 `false` 来禁用 TLS。显然，因为我们想将
    TLS 设置为默认值，所以我们将检查环境变量值是否与 `false` 不同，这样如果值有误或未设置，TLS 将被启用。
- en: 'In `server/main.go`, we can have the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `server/main.go` 中，我们可以有如下代码：
- en: '[PRE56]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We now need to do something similar on the client side (`client/main.go`):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要在客户端进行类似操作（`client/main.go`）：
- en: '[PRE57]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With that, we can now enable/disable TLS easily. To run the server on Linux
    or Mac and without TLS, we can now run the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们现在可以轻松地启用/禁用 TLS。要在 Linux 或 Mac 上运行服务器而不使用 TLS，现在可以运行以下命令：
- en: '[PRE58]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For Windows (PowerShell), we can run the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows（PowerShell），我们可以运行以下命令：
- en: '[PRE59]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Similarly, for the client, we can run the following (Linux/Mac):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于客户端，我们可以运行以下命令（Linux/Mac）：
- en: '[PRE60]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For Windows (PowerShell), we can run the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows（PowerShell），可以运行以下命令：
- en: '[PRE61]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We are now ready to start inspecting the data sent over the wire. In Wireshark,
    we will first check the network interface on which we want to intercept the payloads:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始检查通过线缆发送的数据。在 Wireshark 中，我们首先检查我们想要拦截有效载荷的网络接口：
- en: "![Figure 9.1 – \uFEFFSelecting the network interface](img/B19664_09_01.jpg)"
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 选择网络接口](img/B19664_09_01.jpg)'
- en: Figure 9.1 – Selecting the network interface
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 选择网络接口
- en: 'The loopback interface is the one that we are working on: localhost. By double-clicking
    on it, we will enter the recording interface. But before doing that, we want to
    tell Wireshark where to find our proto files. Without them, it will show you only
    the field tags and the value. It would be better if we could have the field names
    too.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 回环接口是我们正在工作的接口：localhost。通过双击它，我们将进入记录界面。但在做之前，我们希望告诉 Wireshark 哪里可以找到我们的 proto
    文件。如果没有它们，它只会显示字段标签和值。如果能同时看到字段名会更好。
- en: 'To do that, we will go to `chapter9/proto` folder and the folder needed to
    access the Well-Known Types. The last path depends on how you installed `protoc`.
    Here are the most common paths:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们将进入 `chapter9/proto` 文件夹以及访问已知类型的所需文件夹。最后一个路径取决于你如何安装 `protoc`。以下是最常见的路径：
- en: If installed through GitHub releases and you moved the `include` folder to `/usr/local`,
    then the second path is `/usr/local/include`.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果通过 GitHub 发布版安装并且将 `include` 文件夹移动到 `/usr/local`，则第二个路径是 `/usr/local/include`。
- en: If installed through `brew`, you should be able to get the path on which protobuf
    is installed with the `brew --prefix protobuf` command. This will give you a path;
    simply append `/include` to the path.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果通过 `brew` 安装，你应该可以使用 `brew --prefix protobuf` 命令获取 protobuf 安装的路径。这将给出一个路径；只需将
    `/include` 添加到路径中。
- en: If installed through Chocolatey, you should run the `choco list --local-only
    --exact protoc --trace` command. This will list a path finishing with `.files`.
    Open the path in a tool such as Notepad, find a path containing `include/google/protobuf`,
    and select it up until the `include` folder – for example, `C:\ProgramData\chocolatey\lib\protoc\tools\include`.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果通过 Chocolatey 安装，你应该运行 `choco list --local-only --exact protoc --trace` 命令。这将列出以
    `.files` 结尾的路径。在记事本等工具中打开路径，找到包含 `include/google/protobuf` 的路径，并选择它直到 `include`
    文件夹 – 例如，`C:\ProgramData\chocolatey\lib\protoc\tools\include`。
- en: "![Figure 9.2 – \uFEFFAdding path to proto files](img/B19664_09_02.jpg)"
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 将路径添加到 proto 文件中](img/B19664_09_02.jpg)'
- en: Figure 9.2 – Adding path to proto files
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 将路径添加到 proto 文件中
- en: Once this is done, we can go back to our loopback interface and double-click
    on it. We should now have the following recording interface.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们可以回到我们的回环接口，并双击它。现在，我们应该有以下记录接口。
- en: We will then enter a filter to only show the requests on port `50051` and only
    the requests related to gRPC and Protobuf. Make sure you click on the arrow just
    next to the filter area; otherwise, you will get all the requests made on the
    interface.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将输入一个过滤器，只显示端口 `50051` 上的请求，以及与 gRPC 和 Protobuf 相关的请求。确保你点击过滤器区域旁边的箭头；否则，你将看到接口上发出的所有请求。
- en: '![Figure 9.3 – Entering a filter](img/B19664_09_03.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 输入过滤器](img/B19664_09_03.jpg)'
- en: Figure 9.3 – Entering a filter
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 输入过滤器
- en: 'After that, we can go ahead and run the server and the client. Once the client
    is done executing, you will have some logs appearing in Wireshark. This should
    look like the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以继续运行服务器和客户端。一旦客户端执行完毕，你将在 Wireshark 中看到一些日志出现。这应该看起来像以下这样：
- en: '![Figure 9.4 – Logs appearing in Wireshark](img/B19664_09_04.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – Wireshark 中出现的日志](img/B19664_09_04.jpg)'
- en: Figure 9.4 – Logs appearing in Wireshark
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – Wireshark 中出现的日志
- en: 'We can now understand what was sent over the network. If we are looking at
    the payloads, will should be looking at the `DATA (GRPC) (PROTOBUF)` frames. An
    example is the `DATA` frame for `AddTask`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以理解通过网络发送了什么。如果我们查看有效载荷，我们应该查看 `DATA (GRPC) (PROTOBUF)` 帧。例如，`AddTask`
    的 `DATA` 帧如下：
- en: '[PRE62]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Finally, if we are looking at gRPC-related recordings, we can take a look at
    the `HEADERS` and `DATA (GRPC)` frame. These can tell you when half-closes and
    trailers are sent and their size. An example of a half-close for `ListTasks` is
    as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们正在查看与 gRPC 相关的记录，我们可以查看 `HEADERS` 和 `DATA (GRPC)` 帧。这些可以告诉你何时发送半关闭和跟踪器以及它们的大小。`ListTasks`
    的一个半关闭示例如下：
- en: '[PRE63]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'An example trailer for `DeleteTasks` is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`DeleteTasks` 的一个示例跟踪器如下：'
- en: '[PRE64]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: For the sake of keeping this book readable, we will have to end this section
    here. However, there is a lot more to look at and discover. We saw that we can
    use Wireshark to intercept messages over the wire. We made a switch variable to
    be able to disable TLS temporarily to not read encrypted data. We loaded protobuf
    messages into Wireshark to let it know how to deserialize messages. Finally, we
    saw that we can look at messages, as well as lower-level parts of the HTTP2 protocol.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这本书易于阅读，我们不得不在这里结束这一节。然而，还有很多东西可以查看和发现。我们看到了可以使用 Wireshark 截获线上的消息。我们创建了一个开关变量，以便暂时禁用
    TLS，以便不读取加密数据。我们将 protobuf 消息加载到 Wireshark 中，以便它知道如何反序列化消息。最后，我们看到了我们可以查看消息，以及
    HTTP2 协议的更低级别的部分。
- en: Turning gRPC logs on
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打开 gRPC 日志
- en: Finally, if you are ready to go to an even lower level than Wireshark to debug
    gRPC applications, gRPC provides two important environment variables to get logs
    from the framework.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你准备比 Wireshark 更低级别地调试 gRPC 应用程序，gRPC 提供了两个重要的环境变量来获取框架的日志。
- en: The first environment variable is `GRPC_GO_LOG_SEVERITY_LEVEL`. It will give
    you the logs written by gRPC depending on certain severity levels (`debug`, `info`,
    or `error`). To enable this, you can simply execute your binary or Go command
    with `GRPC_GO_LOG_SEVERITY_LEVEL` set in front of it. We did something similar
    with our custom `ENABLE_TLS` variable.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个环境变量是 `GRPC_GO_LOG_SEVERITY_LEVEL`。它将根据某些严重级别（`debug`、`info` 或 `error`）提供
    gRPC 编写的日志。要启用此功能，你可以简单地使用 `GRPC_GO_LOG_SEVERITY_LEVEL` 在二进制文件或 Go 命令之前执行。我们用自定义的
    `ENABLE_TLS` 变量做了类似的事情。
- en: 'An example of running `GRPC_GO_LOG_SEVERITY_LEVEL` set with `info` while spinning
    up the server and closing it is as follows (for Linux/Mac):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动服务器和关闭服务器时设置 `GRPC_GO_LOG_SEVERITY_LEVEL` 为 `info` 的示例如下（对于 Linux/Mac）：
- en: '[PRE65]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'For Windows (PowerShell), we have the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows（PowerShell），我们有以下内容：
- en: '[PRE66]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'On top of the severity level, you can also set the verbosity of those logs
    with `GRPC_GO_LOG_VERBOSITY_LEVEL`, which takes a number between 2 and 99, where
    the bigger the number, the more verbose it will be. This will not be in a short-term
    runtime like the one we have right now. This will be more useful on long-term
    runs, which we normally have for servers. To enable it, we add `GRPC_GO_LOG_VERBOSITY_LEVEL`
    just after our `GRPC_GO_LOG_SEVERITY_LEVEL`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在严重程度级别之上，您还可以使用 `GRPC_GO_LOG_VERBOSITY_LEVEL` 设置这些日志的详细程度，它接受一个介于2到99之间的数字，数字越大，日志越详细。这不会在短期运行时出现，比如我们现在所拥有的。这在长期运行中更有用，我们通常为服务器进行长期运行。要启用它，我们在
    `GRPC_GO_LOG_SEVERITY_LEVEL` 之后添加 `GRPC_GO_LOG_VERBOSITY_LEVEL`：
- en: '[PRE67]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, I know that I said there are two important environment variables but
    there is another that deserves a mention. This one is important if you are planning
    to parse logs. You can set the formatter for the logs. As of right now, we have
    the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我知道我说过有两个重要的环境变量，但还有一个值得提及。如果您计划解析日志，这个变量很重要。您可以设置日志的格式化器。到目前为止，我们有以下设置：
- en: '[PRE68]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'But we can set the formatter to JSON in order to get the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以将格式设置为 JSON 以获取以下内容：
- en: '[PRE69]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: You will now be able to deserialize the JSON and implement all the kinds of
    tools you need to monitor and report errors.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将能够反序列化 JSON 并实现所有需要的工具来监控和报告错误。
- en: 'To conclude, in this brief section, we saw that we can get information for
    code that we do not write: the gRPC framework. I am aware that the examples presented
    in this section are superficial, but generally, these flags are set when something
    goes really wrong or if you are involved in the development of gRPC Go itself.
    I still think it is important to know about their existence and I encourage you
    to try getting more interesting messages out of it.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本节中，我们看到了我们可以获取我们未编写的代码的信息：gRPC 框架。我清楚本节中展示的例子是表面的，但通常，这些标志是在真正出错或您参与
    gRPC Go 本身开发时设置的。我仍然认为了解它们的存续很重要，并鼓励您尝试从中获取更有趣的消息。
- en: There are as many ways to debug as there are requirements and settings. As such,
    we cannot cover everything here, but at least you have the basic skills and tools
    to get started with hacking. In this section, we saw that we can enable server
    reflection to get information from the server and interact with it with grpcurl.
    We also saw that we can intercept messages with Wireshark to get a sense of the
    requests made and their size. Finally, we saw that we can turn on a certain flag
    to get logs from gRPC. Before going on to the next section, I wanted to mention
    that there is another tool that you might find useful and that we did not cover
    here. This tool is called Channelz ([https://grpc.io/blog/a-short-introduction-to-channelz/](https://grpc.io/blog/a-short-introduction-to-channelz/)).
    Its purpose is to debug networking issues. You might want to take a look at it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 调试的方法有无数种，取决于需求和设置。因此，我们无法在此涵盖所有内容，但至少您有了基本的技能和工具来开始破解。在本节中，我们看到了我们可以启用服务器反射以从服务器获取信息，并使用
    grpcurl 与其交互。我们还看到了我们可以使用 Wireshark 截获消息以了解请求及其大小。最后，我们看到了我们可以打开一个特定的标志来获取 gRPC
    的日志。在进入下一节之前，我想提到还有一个可能对您有用的工具，我们没有在此涵盖。这个工具叫做 Channelz ([https://grpc.io/blog/a-short-introduction-to-channelz/](https://grpc.io/blog/a-short-introduction-to-channelz/))。它的目的是调试网络问题。您可能想看看它。
- en: Deploying
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Another crucial step of production-grade APIs is deploying the services online.
    In this section, we will see how we can create a Docker image for gRPC Go, deploy
    it to Kubernetes, and finally deploy the Envoy proxy to let clients make requests
    from outside the cluster to a server inside it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级 API 的另一个关键步骤是将服务在线部署。在本节中，我们将看到如何为 gRPC Go 创建 Docker 镜像，将其部署到 Kubernetes，最后部署
    Envoy 代理以允许客户端从集群外部向集群内部的服务器发送请求。
- en: Docker
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: This first step in deploying is often containerizing your application with Docker.
    If we did not do so, we would have to deal with errors depending on the server
    architecture, tools not being available on it, and so on. By containerizing our
    application, we can build our image once and run it everywhere where Docker is
    available.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的第一步通常是使用 Docker 容器化您的应用程序。如果我们没有这样做，我们就必须处理与服务器架构相关的错误，工具不可用等问题。通过容器化我们的应用程序，我们只需构建一次镜像，就可以在任何有
    Docker 的地方运行。
- en: We are going to focus on containerizing our server. This makes much more sense
    than working on the client because we will later deploy our gRPC server as microservices
    in Kubernetes and we will make the client, which is outside, make requests to
    them.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于容器化我们的服务器。与客户端相比，这更有意义，因为我们将在 Kubernetes 中将我们的 gRPC 服务器作为微服务部署，并且我们将使客户端（外部）向它们发出请求。
- en: 'The first thing that we can think about is what all the steps needed to build
    our application are. We ran it quite a few times, but we need to remember all
    the tools that we set up in the first place. This includes the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先可以想到的是构建我们应用程序所需的步骤。我们已经运行了它很多次，但我们需要记住最初设置的所有的工具。这包括以下内容：
- en: protoc to compile our proto files
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 protoc 编译我们的 proto 文件
- en: Proto Go, gRPC, and the `validate` plugin to generate Go code out of proto files
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proto Go、gRPC 和 `validate` 插件，用于从 proto 文件生成 Go 代码
- en: Obviously, Golang
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显然，是 Golang
- en: 'Let us start with getting protoc. For that, we are going to create a first
    stage based on Alpine, which will use `wget` to get the protoc ZIP file and unzip
    it inside `/usr/local`. If you are impatient, you can find the whole Dockerfile
    in `server/Dockerfile`, but we are going to explain it step by step:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从获取 protoc 开始。为此，我们将基于 Alpine 创建一个第一阶段，使用 `wget` 获取 protoc ZIP 文件并在 `/usr/local`
    内解压它。如果你不耐烦，可以在 `server/Dockerfile` 中找到整个 Dockerfile，但我们将会一步一步地解释它：
- en: '[PRE70]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: There are quite a few things happening here. First notice that we are using
    the Docker BuildKit engine. This lets us use defined variables such as `BUILDPLATFORM`,
    `TARGETOS`, and `TARGETARCH`. We do that because even though we are containerizing
    our application to avoid dealing with architecture, running a container with the
    same architecture as the host (virtualization) is much more efficient than emulation.
    Furthermore, as you can see, we need to specify the architecture and OS in the
    URL to download protoc.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情。首先请注意，我们正在使用 Docker BuildKit 引擎。这让我们可以使用定义的变量，如 `BUILDPLATFORM`、`TARGETOS`
    和 `TARGETARCH`。我们这样做是因为尽管我们正在容器化我们的应用程序以避免处理架构，但运行与主机（虚拟化）具有相同架构的容器（容器）比仿真要高效得多。此外，正如你所见，我们需要在
    URL 中指定架构和操作系统以下载 protoc。
- en: Then, we define some variables that are important for building up the download
    URL. We set the version of protoc (here, 23.0). Then, we set the architecture
    we want to work on. This is based on the result of `uname –m`, which gives information
    about the machine. Notice that we use a little trick to replace `aarch64` with
    `aarch_64`. This is because if you take a look at the releases on the Protobuf
    repository (`https://github.com/protocolbuffers/protobuf/releases`), they use
    `aarch_64` in their ZIP filenames.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一些对于构建下载 URL 重要的变量。我们设置 protoc 的版本（这里为 23.0）。然后，我们设置我们想要工作的架构。这是基于 `uname
    –m` 的结果，它提供了有关机器的信息。请注意，我们使用了一个小技巧将 `aarch64` 替换为 `aarch_64`。这是因为如果你查看 Protobuf
    存储库的发布版（`https://github.com/protocolbuffers/protobuf/releases`），它们在 ZIP 文件名中使用
    `aarch_64`。
- en: After that, we use the `TARGETOS` variable to define which OS we want to deal
    with. Notice, once again, the similar trick to replace `darwin` with `linux`.
    This is simply because protoc does not have a binary specific to macOS. You can
    simply use a Linux one.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们使用 `TARGETOS` 变量来定义我们想要处理的操作系统。请注意，再次使用类似的小技巧将 `darwin` 替换为 `linux`。这仅仅是因为
    protoc 没有针对 macOS 的特定二进制文件。你可以简单地使用 Linux 的。
- en: Then, we do the actual downloading of the file by concatenating all the variables
    that we defined previously, and we unzip the file into `/usr/local`. Notice that
    we are extracting both the protoc binary (`/bin/protoc`) and the `/include` folder
    because the first one is the compiler that we are going to use and the second
    one is all the files needed to include Well-Known Types.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过连接之前定义的所有变量来实际下载文件，并将文件解压到 `/usr/local`。请注意，我们正在提取 protoc 二进制文件（`/bin/protoc`）和
    `/include` 文件夹，因为前者是我们将要使用的编译器，后者是包含 Well-Known Types 所需的所有文件。
- en: 'Now that this is done, we can create another stage for building the application
    with Go. Here, we are going to copy protoc from the previous stage, download the
    protoc plugins, compile the proto files, and compile the Go project. We are going
    to use an Alpine-based image for that:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经完成，我们可以为使用 Go 构建应用程序创建另一个阶段。在这里，我们将从上一个阶段复制 protoc，下载 protoc 插件，编译 proto
    文件，并编译 Go 项目。我们将使用基于 Alpine 的镜像来完成这项工作：
- en: '[PRE71]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: At this point, none of this should be confusing. This is exactly what we have
    done earlier in the book. However, I want to mention some non-trivial things that
    are happening here. We are once again taking BuildKit-defined parameters. This
    lets us use the `GOOS` and `GOARCH` environment variables to build a Go binary
    for this specific setting.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这一切都不应该让人感到困惑。这正是我们在本书前面所做过的。然而，我想提及一些在这里发生的不平凡的事情。我们再次使用BuildKit定义的参数。这使得我们可以使用`GOOS`和`GOARCH`环境变量来为这个特定设置构建Go二进制文件。
- en: Also, notice that we are copying both protoc and the `include` folder. As mentioned,
    the second one is the directory containing Well-Known Types and we use some of
    them in our proto files, so this is necessary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，我们正在复制protoc和`include`文件夹。正如所述，后者是包含已知类型的目录，我们在我们的proto文件中使用了一些类型，因此这是必要的。
- en: Finally, I am using two linker flags. The `-s` flag is here to disable the generation
    of the Go symbol table. While I will not dive into what this means, this is sometimes
    used when creating smaller binaries to remove some information that should not
    impact the runtime capabilities. `-w` removes debug information. As these are
    not needed for production, we can just get rid of them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用了两个链接器标志。`-s`标志在这里是为了禁用Go符号表的生成。虽然我不会深入探讨这意味着什么，但有时在创建较小的二进制文件时，会使用它来删除不应影响运行时功能的一些信息。`-w`移除调试信息。由于这些信息在生产环境中不是必需的，我们可以直接删除它们。
- en: 'Finally, we will build our last stage, which will be based on a scratch image.
    This is an image that does not have any OS and that we use for hosting binaries
    and making our images really small. In there, we will copy our certificates into
    a `certs` directory, copy the binary we created with `go build`, and launch the
    application with the parameters that we usually set:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建我们的最后一个阶段，它将基于一个scratch镜像。这是一个没有操作系统的镜像，我们用它来托管二进制文件并使我们的镜像非常小。在那里，我们将我们的证书复制到`certs`目录中，复制我们使用`go
    build`创建的二进制文件，并使用我们通常设置的参数启动应用程序：
- en: '[PRE72]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'With that, we are ready to build our first image of the server. The first thing
    that we can create is a Docker Builder. As described in the Docker documentation:
    “Builder instances are isolated environments where builds can be invoked.” This
    is basically an environment that we need to launch the build of our images. To
    create that, we can run the following command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就准备好构建我们的第一个服务器镜像。我们可以创建的第一件事是一个Docker Builder。正如Docker文档所述：“Builder实例是构建可以调用的隔离环境。”这基本上是我们需要启动镜像构建的环境。为了创建它，我们可以运行以下命令：
- en: Important note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You need to make sure that Docker is running. This is as simple as making sure
    that Docker Desktop is running. Finally, you might need to prepend all the following
    Docker commands with `sudo` if you are on Linux/Mac and you did not create a Docker
    group and add your user to it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保Docker正在运行。这就像确保Docker Desktop正在运行一样简单。最后，如果你在Linux/Mac上，并且你没有创建Docker组并将你的用户添加到其中，你可能需要将所有以下Docker命令的前缀设置为`sudo`。
- en: '[PRE73]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Notice that we give this build environment the name `mybuild` and that we use
    the `docker-container` driver. This driver will let us generate multi-platform
    images. We are going to see that later.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们给这个构建环境命名为`mybuild`，并且我们使用的是`docker-container`驱动程序。这个驱动程序将允许我们生成多平台镜像。我们将在稍后看到这一点。
- en: 'Once we have executed the command, we will be able to use this Builder in another
    Docker command: `docker buildx build`. With this command, we are going to generate
    the image. We will give it a tag (a name), specify where to find the Dockerfile,
    specify the architecture we want to build on, and load the image into Docker.
    To build an image for `arm64` (you can try `amd64`), we run the following (from
    `chapter9`):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完命令后，我们将能够在另一个Docker命令中使用这个Builder：`docker buildx build`。使用这个命令，我们将生成镜像。我们将给它一个标签（一个名称），指定Dockerfile的位置，指定我们想要构建的架构，并将镜像加载到Docker中。要为`arm64`（你可以尝试`amd64`）构建镜像，我们从`chapter9`运行以下命令：
- en: '[PRE74]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'After everything is built, we should be able to see the image by executing
    the following command:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一切构建完成后，我们可以通过执行以下命令来查看镜像：
- en: '[PRE75]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, let us try to run the server image and make requests to it. We are
    going to run the image we just created and expose the ports we used for the server
    (`50051` and `50052`) to the same ports on the host:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试运行服务器镜像并向其发送请求。我们将运行我们刚刚创建的镜像，并将我们用于服务器的端口（`50051`和`50052`）暴露在主机上的相同端口：
- en: '[PRE76]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now, if we run our client normally, we should be able to get all the logs we
    had previously:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们正常运行我们的客户端，我们应该能够获取我们之前所有的日志：
- en: '[PRE77]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: To conclude, we saw that we can create slim images around our gRPC applications.
    We used a multi-stage Dockerfile in which we first downloaded protoc and the Protobuf
    Well-Known Types. We then downloaded all the Golang dependencies and built a binary,
    and finally, we copied the binary into a scratch image to create a thin wrapper
    around it.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们看到了我们可以在我们的gRPC应用程序周围创建瘦镜像。我们使用了一个多阶段Dockerfile，首先下载了protoc和Protobuf
    Well-Known Types。然后下载了所有Golang依赖项并构建了一个二进制文件，最后将二进制文件复制到一个scratch镜像中，以创建一个围绕它的薄包装。
- en: Kubernetes
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Now that we have our server image, we can deploy multiple instances of our service
    and we will have created our to-do microservice. In this section, we are going
    to focus mostly on how to deploy our gRPC service. This means that we are going
    to write a Kubernetes configuration. If you are not familiar with Kubernetes,
    there is nothing to be afraid of. Our configuration is simple and I will explain
    all the blocks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了我们的服务器镜像，我们可以部署我们服务的多个实例，这样我们就已经创建了我们的待办微服务。在本节中，我们将主要关注如何部署我们的gRPC服务。这意味着我们将编写一个Kubernetes配置。如果你不熟悉Kubernetes，没有必要害怕。我们的配置很简单，我会解释所有的块。
- en: 'The first thing that we need to think about is how our service will be accessed.
    We have two major ways of exposing our services: making them accessible only from
    inside the cluster or accessible from outside the cluster. In most cases, we do
    not want our services to be accessed directly. We want to go through a proxy that
    will redirect and load balance the requests to multiple instances of our service.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要考虑的是我们的服务如何被访问。我们有两种主要方式来公开我们的服务：只从集群内部访问或从集群外部访问。在大多数情况下，我们不希望我们的服务被直接访问。我们希望通过一个代理来访问，该代理将重定向并将请求负载均衡到我们服务的多个实例。
- en: As such, we will create a Kubernetes Service, which will itself assign a DNS
    A record for all the instances of our service. This basically means that each
    of our services will have its own internal address in the cluster. This will let
    our proxy resolve all the addresses and load balance across all of them.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将创建一个Kubernetes服务，它将为我们的服务所有实例分配一个DNS A记录。这基本上意味着我们的每个服务都将有一个集群内部的独立地址。这将让我们的代理解析所有地址并在它们之间进行负载均衡。
- en: 'Such a service is called a headless service. In Kubernetes, this is a service
    with the `clusterIp` property set to `None`. Here is the service definition (`k8s/server.yaml`):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的服务被称为无头服务。在Kubernetes中，这是一个将`clusterIp`属性设置为`None`的服务。以下是服务定义（`k8s/server.yaml`）：
- en: '[PRE78]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Note that we create a port called `grpc` with the value `50051`. This is because
    we want to be able to access all the services on port `50051`. Then, notice that
    we are creating a selector to specify which app this service will handle. In our
    case, we call it `todo-server` and this will be the name for our deployments.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们创建了一个名为`grpc`的端口，其值为`50051`。这是因为我们希望能够访问端口`50051`上的所有服务。然后，请注意我们正在创建一个选择器来指定这个服务将处理哪个应用程序。在我们的例子中，我们称之为`todo-server`，这将是我们的部署名称。
- en: 'Now, we can think about creating instances of our service. We are going to
    do that with a Kubernetes Deployment. This will let us specify how many instances
    we want, which image to use, and which container port to use. This looks like
    the following (`k8s/server.yaml`):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以考虑创建我们服务的实例。我们将使用Kubernetes Deployment来完成这项工作。这将让我们指定我们想要多少实例，使用哪个镜像，以及使用哪个容器端口。这看起来如下（`k8s/server.yaml`）：
- en: '[PRE79]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Here, we specify that the name of the Pods will match `todo-server`. This makes
    them be handled by the service. We then specify that we want to use the image
    that we created earlier. However, notice here that we are setting `imagePullPolicy`
    to `Always`. This means that each time we create the Pods, they will pull a new
    image from the image registry. This makes sure that we always get the newest image
    on the registry. However, note that this might be inefficient if the images do
    not change often and if you have local copies of the images that are not outdated.
    I would recommend you check, depending on your Kubernetes environment, what to
    use as a value of `imagePullPolicy`. Finally, we use port `50051`. There is nothing
    more to it than specifying on which port our service is exposing the API.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定Pod的名称将与`todo-server`匹配。这使得它们由服务处理。然后，我们指定我们想要使用我们之前创建的镜像。然而，请注意，我们在这里将`imagePullPolicy`设置为`Always`。这意味着每次我们创建Pod时，它们都会从镜像仓库拉取一个新的镜像。这确保了我们总是获取仓库上的最新镜像。然而，请注意，如果镜像不经常更改，并且你有本地副本的镜像且这些镜像不是过时的，这可能会效率低下。我建议你根据你的Kubernetes环境，检查`imagePullPolicy`的值应该使用什么。最后，我们使用端口`50051`。这并没有比指定我们的服务在哪个端口上暴露API更多。
- en: Important note
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the remainder of this chapter, I expect that you already have a Kubernetes
    cluster. If you have one in the cloud, this is perfect and you can continue. If
    you do not have one, you can refer to Kind ([https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/)),
    and once installed, you can create a simple cluster with the configuration provided
    in `k8s/kind.yaml`. Simply run `kind create cluster --``config k8s/kind.yaml`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我期望你已经有一个Kubernetes集群。如果你在云中有，这很完美，你可以继续。如果你没有，你可以参考Kind ([https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/))，一旦安装，你可以使用在`k8s/kind.yaml`中提供的配置创建一个简单的集群。只需运行`kind
    create cluster --config k8s/kind.yaml`。
- en: 'With that, we can now deploy our three services. We will run the following
    command from the `chapter9` folder:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以部署我们的三个服务。我们将从`chapter9`文件夹运行以下命令：
- en: '[PRE80]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We are going to execute the following command to look at the Pod being created:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行以下命令来查看正在创建的Pod：
- en: '[PRE81]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, since we do not have a proxy, we will simply use the `port-forward` command
    from Kubernetes to access one server and see whether it works. This is purely
    for testing purposes, and we are going to see later how to hide the services behind
    a proxy. So, we run the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们没有代理，我们将简单地使用Kubernetes的`port-forward`命令来访问一个服务器并查看它是否工作。这纯粹是为了测试目的，我们将在稍后看到如何通过代理隐藏服务。所以，我们运行以下命令：
- en: '[PRE82]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Then, we should be able to use our client normally on `localhost:50051`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应该能够在`localhost:50051`上正常使用我们的客户端：
- en: '[PRE83]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: To conclude, we saw that we can use a headless service to create a DNS A record
    for each of the Pods in the Deployment. We then deployed three Pods and saw that
    we can test whether they are working or not by using the `port-forward` command
    in `kubectl`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们看到了我们可以使用无头服务为部署中的每个Pod创建一个DNS A记录。然后我们部署了三个Pod，并看到我们可以通过在`kubectl`中使用`port-forward`命令来测试它们是否工作。
- en: Envoy proxy
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Envoy代理
- en: Now that we have our microservices created, we need to add a proxy that will
    balance the load between all of them. This proxy is Envoy. This is one of the
    few proxies that can interact with gRPC services. We are going to see how to set
    up Envoy to redirect traffic to our services, load balance with the round robin
    algorithm, and enable TLS.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的微服务，我们需要添加一个代理来在它们之间平衡负载。这个代理是Envoy。这是少数几个可以与gRPC服务交互的代理之一。我们将看到如何设置Envoy来将流量重定向到我们的服务，使用轮询算法进行负载均衡，并启用TLS。
- en: 'Let us first focus on writing a listener. This is an entity that specifies
    the address and port on which to listen and defines some filters. These filters,
    at least in our case, will let us route the requests for `todo.v2.TodoService`
    to an Envoy cluster. A cluster is the entity that will let us define the actual
    endpoints and shows us how to load balance. We can first write our listener (`envoy/envoy.yaml`):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先专注于编写一个监听器。这是一个指定要监听地址和端口的实体，并定义了一些过滤器。这些过滤器，至少在我们的案例中，将允许我们将`todo.v2.TodoService`的请求路由到Envoy集群。集群是允许我们定义实际端点并展示如何进行负载均衡的实体。我们首先编写我们的监听器（`envoy/envoy.yaml`）：
- en: '[PRE84]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The most important things to note are that we defined a route matching all the
    gRPC requests from any domain names and matching the `/todo.v2.TodoService` prefix.
    Then, all these requests will be redirected to `grpc_cluster`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的最重要的事情是，我们定义了一个匹配来自任何域名且匹配`/todo.v2.TodoService`前缀的所有gRPC请求的路由。然后，所有这些请求都将被重定向到`grpc_cluster`。
- en: 'After that, let us define our cluster. We are going to use the `STRICT_DNS`
    resolution to detect all the gRPC services by DNS A record. Then, we will specify
    that we are only accepting HTTP/2 requests. This is because, as you know, gRPC
    is based on HTTP/2\. After that, we will set the load balancing policy to use
    round robin. Finally, we will specify the address and port of the endpoint:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，让我们定义我们的集群。我们将使用`STRICT_DNS`解析通过DNS A记录检测所有gRPC服务。然后，我们将指定我们只接受HTTP/2请求。这是因为，正如你所知，gRPC基于HTTP/2。之后，我们将设置负载均衡策略为轮询。最后，我们将指定端点的地址和端口：
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Notice that we use the address generated by Kubernetes. This is of the form
    `$SERVICE_NAME-$NAMESPACE-svc-cluster.local`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们使用的是Kubernetes生成的地址。其形式为`$SERVICE_NAME-$NAMESPACE-svc-cluster.local`。
- en: 'In order to test our configuration, we can run everything locally first. We
    will temporarily make the `listener_0` port equal to `50050` so that it does not
    conflict with our server port:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的配置，我们首先可以在本地运行一切。我们将临时将`listener_0`端口设置为`50050`，以免与我们的服务器端口冲突：
- en: '[PRE86]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We will also have to set the endpoint address to localhost to access the server
    running locally:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须将端点地址设置为localhost，以便访问本地运行的服务器：
- en: '[PRE87]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Then, we will run our server:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将运行我们的服务器：
- en: '[PRE88]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We can now run our envoy instance with `func-e`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`func-e`运行我们的envoy实例：
- en: '[PRE89]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Finally, we can run our client on port `50050`, not `50051`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在端口`50050`而不是`50051`上运行我们的客户端：
- en: '[PRE90]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: As you can guess, this is because Envoy is somehow breaking the TLS connection
    between the server and the client. To solve that, we are going to specify that
    the upstream of our cluster uses TLS and that the downstream of our listener also
    uses TLS.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，这是因为Envoy在某种程度上破坏了服务器和客户端之间的TLS连接。为了解决这个问题，我们将指定我们的集群上游使用TLS，并且我们的监听器下游也使用TLS。
- en: 'In the filters, we will tell Envoy where to find our self-signed certificates:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤器中，我们将告诉Envoy在哪里找到我们的自签名证书：
- en: '[PRE91]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Note that this is probably not what you would do in production. You would use
    a tool such as Let’s Encrypt to automatically generate your certificates and link
    them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这很可能不是你会在生产中做的事情。你将使用像Let’s Encrypt这样的工具来自动生成你的证书并将它们链接起来。
- en: 'Now, we will tell the cluster that the upstream is also using TLS:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将告诉集群上游也使用TLS：
- en: '[PRE92]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Obviously, this is not going to work directly. On our local computer, we do
    not have the `/etc/envoy/certs/server_cert.pem` and `/etc/envoy/certs/server_key.pem`
    files. But we have them in the `chapter9` `certs` folder. We will replace them
    temporarily:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这不会直接工作。在我们的本地计算机上，我们没有`/etc/envoy/certs/server_cert.pem`和`/etc/envoy/certs/server_key.pem`文件。但我们在`chapter9`的`certs`文件夹中有它们。我们将临时替换它们：
- en: '[PRE93]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Let us now kill the previous instance of Envoy and rerun it:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们杀死之前的Envoy实例并重新运行它：
- en: '[PRE94]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Finally, we should be able to run our client and receive responses from our
    server:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该能够运行我们的客户端并从我们的服务器接收响应：
- en: '[PRE95]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: We are now certain that our requests go through Envoy and are redirected to
    our gRPC server. The next step will be reverting all the temporary changes that
    we made for testing (listener port to `50051`, endpoint address to `todo-server.default.svc.cluster.local`,
    and `certs` path to `/etc/envoy`) and creating a Docker image that we will use
    for deploying Envoy in our Kubernetes cluster.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以确定我们的请求是通过Envoy并且被重定向到我们的gRPC服务器。下一步将是撤销我们为测试所做的所有临时更改（监听器端口到`50051`，端点地址到`todo-server.default.svc.cluster.local`，以及`certs`路径到`/etc/envoy`），并创建一个我们将用于在Kubernetes集群中部署Envoy的Docker镜像。
- en: 'To build such an image, we will copy the certificates to `/etc/envoy/certs`
    (once again, this is not recommended in production) and the configuration (`envoy.yaml`)
    to `/etc/envoy`. Finally, this image will run the `envoy` command with the `--config-path`
    flag, which will point to the `/etc/envoy/envoy.yaml` path. In `envoy/Dockerfile`,
    we have the following:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这样的镜像，我们将证书复制到`/etc/envoy/certs`（再次强调，在生产环境中不推荐这样做）和配置（`envoy.yaml`）到`/etc/envoy`。最后，这个镜像将使用带有`--config-path`标志的`envoy`命令运行，该标志将指向`/etc/envoy/envoy.yaml`路径。在`envoy/Dockerfile`中，我们有以下内容：
- en: '[PRE96]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We can now build the image for `arm64` (you can use `amd64`) like so:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建`arm64`（你可以使用`amd64`）的镜像，如下所示：
- en: '[PRE97]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'That is it! We are ready to deploy Envoy in front of our TODO microservices.
    We need a headless service for Envoy. This is for the same reasons that we had
    when creating a headless service for our microservices. In production, there will
    potentially be more than one instance of Envoy and you need to make sure they
    are all addressable. In `envoy/service.yaml`, we have the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们准备好在TODO微服务前部署Envoy了。我们需要一个无头服务来为Envoy服务。这与我们为微服务创建无头服务时的原因相同。在生产环境中，可能存在多个Envoy实例，你需要确保它们都是可访问的。在`envoy/service.yaml`中，我们有以下内容：
- en: '[PRE98]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Then we need to create a `Deployment`. This time, as we are in a development
    setting, we will deploy only one Pod for Envoy. All the rest of the configuration
    is similar to what we did with our gRPC server. In `envoy/deployment.yaml`, we
    have the following:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个`Deployment`。这次，因为我们处于开发环境，我们将只为Envoy部署一个Pod。其余的配置与我们在gRPC服务器上所做的类似。在`envoy/deployment.yaml`中，我们有以下内容：
- en: '[PRE99]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'We can now run all of this. I am assuming that you did not tear down the previous
    step that we did for deploying microservices. Right now, you should have the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行所有这些。我假设你没有拆掉我们之前为部署微服务所做的步骤。现在，你应该有以下内容：
- en: '[PRE100]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'So, now we can first add the service and then the deployment for Envoy:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以首先添加服务，然后添加Envoy的部署：
- en: '[PRE101]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Finally, before running the client, we can use the `port-forward` command to
    forward Envoy’s port `50051` to `localhost:50051`:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在运行客户端之前，我们可以使用`port-forward`命令将Envoy的端口`50051`转发到`localhost:50051`：
- en: '[PRE102]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We can then run the client and we should be able to get some results:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行客户端，并且应该能够得到一些结果：
- en: '[PRE103]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Notice that because of the load balancing and the fact that we do not use a
    real database, the Pods are not able to find tasks that are stored in other Pods’
    memory. This is normal in our case, but in production, you would be relying on
    a shared database and these problems would not arise.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于负载均衡以及我们没有使用真实数据库，Pod无法找到存储在其他Pod内存中的任务。在我们的情况下这是正常的，但在生产环境中，你会依赖于共享数据库，这些问题就不会出现。
- en: To conclude, we saw that we can instantiate Envoy in front of our services to
    redirect requests with a certain load-balancing policy. This time, contrary to
    the load balancing we saw in `chapter7`, the client does not actually know any
    addresses for the servers. It connects to Envoy and Envoy is redirecting requests
    and responses. We obviously did not cover all the possible configurations for
    Envoy and I would recommend that you check out other features, such as rate limiting
    and authentication.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们看到了我们可以在服务前实例化Envoy，以使用特定的负载均衡策略重定向请求。这次，与我们在`第7章`中看到的负载均衡不同，客户端实际上并不知道任何服务器地址。它连接到Envoy，Envoy正在重定向请求和响应。显然，我们没有涵盖Envoy的所有可能配置，我建议你查看其他功能，如速率限制和身份验证。
- en: Summary
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered unit and load testing. We saw that we can find bugs
    and performance issues by extensively testing different parts of our system. Then,
    we saw how to debug our application when we found a bug. We used server reflection
    and grpcurl to interact with our API from the terminal. Finally, we saw how we
    can containerize our services and deploy them on Kubernetes. We saw that we can
    create headless services to expose our microservices with a DNS A record per gRPC
    server, and we saw that we can put Envoy in front of them to do load balancing,
    rate limiting, authentication, and so on.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了单元测试和负载测试。我们看到了通过广泛测试我们系统的不同部分，我们可以找到错误和性能问题。然后，我们看到了当我们找到错误时如何调试我们的应用程序。我们使用了服务器反射和grpcurl从终端与我们的API交互。最后，我们看到了我们如何容器化我们的服务并在Kubernetes上部署它们。我们看到了我们可以创建无头服务，通过每个gRPC服务器的DNS
    A记录来公开我们的微服务，并且我们看到了我们可以在它们前面放置Envoy来进行负载均衡、速率限制、身份验证等。
- en: Quiz
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答
- en: What tool is useful for load testing?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个工具对负载测试有用？
- en: Wireshark
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wireshark
- en: grpcurl
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: grpcurl
- en: ghz
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ghz
- en: In Wireshark, what information can you look at?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Wireshark中，你可以查看哪些信息？
- en: gRPC HTTP/2 frames
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: gRPC HTTP/2帧
- en: Protobuf messages
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Protobuf消息
- en: All of them
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些
- en: What is Envoy used for?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Envoy用于什么？
- en: Redirecting requests and responses
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重定向请求和响应
- en: Logging
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录日志
- en: Exposing metrics
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暴露指标
- en: Load balancing
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: A and D
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: A和D
- en: B and C
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: B和C
- en: Answers
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: C
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: E
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E
- en: Challenges
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: Add support for a real database. You should be able to do so by implementing
    the `db` interface and creating an instance of your struct in the registered server
    instance.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加对真实数据库的支持。你应该可以通过实现`db`接口并在注册的服务器实例中创建你的结构体实例来实现这一点。
- en: Expose the Prometheus metrics in your Kubernetes cluster. You can take a look
    at `prometheus-operator` ([https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的 Kubernetes 集群中公开 Prometheus 指标。你可以查看 `prometheus-operator` ([https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator))。
- en: Epilogue
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尾声
- en: As we reach the end of this book on building gRPC microservices in Golang, I
    hope you have found gRPC Go interesting and useful, and that you are willing to
    try it on your next project. This book is the book I wish I had when I started
    learning this awesome technology and I hope this helped you in any way.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们来到这本关于在 Golang 中构建 gRPC 微服务的书籍的结尾，我希望你发现 gRPC Go 既有趣又有用，并且你愿意在下一个项目中尝试它。这本书是我开始学习这项令人惊叹的技术时希望拥有的书籍，并且希望它在任何方面都对你有所帮助。
- en: Throughout this book, we both explored some elements of theory and some practical
    implementations of gRPC services. From learning the networking concepts to the
    pure implementation and tool that we can use, passing by learning useful considerations
    when designing an API, you learned the most important skills that you will need
    for your career as backend engineer.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们共同探讨了 gRPC 服务的一些理论元素和一些实际实现。从学习网络概念到纯实现和我们可以使用的工具，再到学习在设计 API 时有用的考虑因素，你学习了作为后端工程师职业生涯中最重要的技能。
- en: To conclude this book, I would like to invite you to stay up to date with all
    the topics related to gRPC and Protobuf. You can do that by following GitHub Topics,
    read some blog post or simply getting involved in some open source project. This
    is a fascinating area of backend engineering that needs more attention, more help
    by building tools, and more people to form communities all around the world.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束这本书，我想邀请你关注所有与 gRPC 和 Protobuf 相关的主题。你可以通过关注 GitHub Topics、阅读一些博客文章或者简单地参与一些开源项目来实现这一点。这是一个需要更多关注、更多通过构建工具提供帮助以及更多人在全球范围内建立社区的后端工程领域。
- en: Thank you for accompanying me on this journey to make production-grade gRPC
    APIs. I wish you the very best in your future endeavors. May you create innovative
    and effective APIs.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你陪伴我走过这段制作生产级 gRPC API 的旅程。我祝愿你在未来的努力中一切顺利。愿你能创造出创新和有效的 API。
- en: Happy engineering!
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你工程愉快！
