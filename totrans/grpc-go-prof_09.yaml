- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Production-Grade APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we’ve focused on the features provided by gRPC and those added
    by community projects. That was an important topic but it wasn’t the whole story.
    We now need to think about how to test, debug, and deploy our gRPC server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to see how to unit and load test our services.
    Then, we are going to see how we can manually interact with our API to debug it.
    Finally, we are going to see how we can containerize and deploy our services.
    This chapter is divided into the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging using server reflection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying gRPC services on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter in the folder called chapter5 in the
    companion repo for this book at [https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9](https://github.com/PacktPublishing/gRPC-Go-for-Professionals/tree/main/chapter9).
    In this chapter, I will be using three main tools: `ghz`, `grpcurl`, and Wireshark.
    You should already have Wireshark installed from [*Chapter 1*](B19664_01.xhtml#_idTextAnchor014),
    but if this is not the case, you can find it at [https://www.wireshark.org/](https://www.wireshark.org/).
    ghz is a tool that will let us load test our API. You can get it by visiting [https://ghz.sh/](https://ghz.sh/).
    Finally, we will use grpcurl to interact with our API from the terminal. You should
    be able to get it from [https://github.com/fullstorydev/grpcurl](https://github.com/fullstorydev/grpcurl).'
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing production-grade APIs begins with writing comprehensive tests to
    ensure that the business requirements are met while also verifying the API’s consistency
    and performance. The first part is mostly handled in unit and integration tests
    and the second part with load testing.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this section, we are going to focus on unit testing the
    server. We are going to do one test per API type to understand how you can introduce
    more in the future. In the second part, we are going to introduce ghz, which is
    a tool for load testing gRPC APIs. We are going to introduce the different options
    that the tool has and how to load test an API with credentials, an auth token
    as a header, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, we are going to focus on unit testing the server. Before beginning,
    it is important to know that the tests presented here are not all the possible
    tests that we could do. To keep this book readable, I will be presenting how to
    write unit tests for each API type, and you can find an example of other tests
    in the `server/impl_test.go` file.
  prefs: []
  type: TYPE_NORMAL
- en: Before writing any tests, we need to do some setup. We are going to write some
    boilerplate for the different tests to share the same server and connection. This
    is mostly to avoid creating new servers and connections each time we are running
    a test. However, note that these are non-hermetic tests. This means that an unexpected
    state could be shared across multiple tests and make the tests flaky. We are going
    to introduce ways to deal with this and make sure we clear the states.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we can do is create a fake database. This is like what
    we did with `inMemoryDb`, and in fact, `FakeDb` is a wrapper around `inMemoryDb`,
    but we are also going to test problems due to connectivity with the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we are going to use the same pattern as `grpc.ServerOption`. `grpc.ServerOption`
    is a function applying a value to a private struct. An example of this is `grpc.Creds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It returns a function that, once called, will set the value of `c` to the `creds`
    property in `serverOptions`. Note that `serverOptions` is different from `ServerOption`.
    This is a private struct.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to create a function that tells us whether the database is available
    or not. Later, we are going to enable the option to return an error if it is not.
    In `test_options.go`, we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I’ll leave it up to you to check the rest of the content of `test_options.go`.
    The functions and structs there simply create some utilities and variables in
    order to be able to write the `IsAvailable` function and get default values for
    `isAvailable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create `FakeDb`. As mentioned, this is a wrapper around `inMemoryDb`,
    and it has some options. In `fake_db.go`, we can have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a `FakeDb` in multiple ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also override the `inMemoryDb` functions so that our `FakeDb` implements
    the `db` interface and so that we can instantiate a server with this database.
    Each function of `FakeDb` follows the same pattern. We check whether the database
    is available or not; if it is not, we return an error, and if it is, we return
    the result of `inMemoryDb`. An example of this is `addTask` (in `fake_db.go`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have that, we can move one step closer to writing an actual unit
    test. We now need to create a server. However, we do not want this server to actually
    use ports on our computer. Using an actual port could make our tests flaky because
    if the port is already in use, the test would directly return an error saying
    that it could not create the instance of the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve that, gRPC has a package called `bufconn` (`grpc/test/bufconn`). It
    lets us create a buffered connection and thus does not need to use ports. `bufconn.Listen`
    will create a listener and we will be able to use this listener to server requests.
    In `server_test.go`, we will share the listener and database as global variables.
    This will let us dispose of the listener after all tests and add/clear tasks in
    the database from within a test. On top of that, we will create a function that
    returns a `net.Conn` connection so that we can use it within the test to create
    a client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to notice is that we are using the Go `init()` function to do
    this setup before the tests are started. Then, notice that we create an instance
    of our server and register the implementation of our `TodoService`. Finally, the
    server is serving in a goroutine. So, we need to make sure that the goroutine
    is canceled.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are almost done with the boilerplate. We need to create a client that uses
    the `bufDialer` function to connect to the server through the buffered connection.
    In `impl_test.go`, we are going to create a function that returns `TodoServiceClient`
    and `grpc.ClientConn`. The first is obviously to call our endpoints but the second
    one is for us to close the client connection at the end of each test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: One important thing to understand here is that we are not testing the whole
    server that we wrote in `main.go`. We are simply testing our endpoints implementation.
    This is why we can connect to the server with insecure credentials. The interceptors,
    encryption, and so on should be tested in integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can create a small utility function that checks that an error is
    a grpc error and that it has an expected message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to write some unit tests. We are going to create a function
    that will run all the unit tests and dispose of the listener when all the subtests
    are finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now able to populate the `TestRunAll` function with subtests, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now write the `testAddTaskEmptyDescription` function, which checks that
    we get an error when we send a request with an empty description. We will create
    a new instance of a client, create an empty request, send it to `AddTask`, and
    finally, check that our error has an unknown code (returned by `protoc-gen-validate`)
    and that the message is `invalid AddTaskRequest.Description: value length must
    be at least 1 runes` (also from `protoc-gen-validate`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then add it to our `TestRunAll` function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this test, we can run the following command in the root folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, before moving on to looking at how to test streams, let us see how we
    can test with an unavailable database. This is almost the same as what we did
    in `testAddTaskEmptyDescription`, but we are going to override the database. Finally,
    we are going to check that we get an internal error and reset the database (to
    clear the options):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can see that it is easy to test a database failure. That is all for unary
    RPC. I will let you add `testAddTaskUnavailableDb` to `TestRunAll` and look at
    the other tests for `AddTasks` in `impl_test.go`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now going to test `ListTasks`. We will add some tasks to our fake database,
    call `ListTasks`, make sure that there is no error, and check that `ListTasks`
    iterated through all the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There is nothing new in terms of calling the API. We already know all of this
    from when we wrote the client. However, the main difference here, for this test,
    is we do not look at the values; we simply assert the time we looped. Of course,
    you could create more sophisticated tests out of this, but I wanted to show you
    a simple test on a server streaming API so that you can build upon it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us test the client streaming API endpoint. As we are working with
    the `UpdateTasks` endpoint, we will need to set data in our database. After that,
    we will basically create an array of `UpdateTasksRequest` in order to change all
    the items in the database, send the requests, and check that all the updates ran
    without error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to the previous test. We used a counter to check that all updates
    were “applied.” In an integration test, you would have to check that the value
    actually changed in the database; however, because we are in unit tests and we
    have an in-memory database, checking the actual values would not mean much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will test the bidirectional streaming API. This is a little bit
    more complex in the testing context, but we are going to tackle the problem step
    by step. Previously, in the client, when an error happened in a goroutine, we
    simply ran `log.Fatalf` to exit. However, here, because we want to keep track
    of errors and we cannot call `t.Fatalf` in a different goroutine from the one
    of the tests, we are going to use a channel of `struct` called `countAndError`.
    As its name suggests, this is a structure containing a counter and an optional
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is useful because now, we will be able to wait for the goroutine to finish
    and get a result of the channel. First, let us create the function that sends
    all the requests. This function is called `sendRequestsOverStream` and it will
    be called in a separate goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If an error occurs, we will close the waiting channel with an error set in the
    `countAndError` structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can create the function that reads responses. This function is called
    `readResponsesOverStream` and will also be called in a separate goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This time, if everything goes well, the channel will get a `countAndError` with
    a count set. This count is the same as what we did in previous tests. It checks
    the number of responses that were collected without error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have these two functions, we are ready to write the actual test
    for our bidirectional streaming API. This is similar to what we did for `ListTasks`
    and `UpdateTasks`; however, this time, we launch two goroutines, wait for the
    result, and check that we have no error and a count equal to the number of requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have finally finished testing all the different types of gRPC
    APIs. Once again, there are more tests that can be done, and other examples are
    available in `impl_test.go`. I strongly encourage you to take a look there so
    you can get more ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding all these tests to `TestRunAll`, you should be able to run them
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want a more detailed output of what test ran, you can add the `–v` option.
    This will return something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Bazel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to run tests with Bazel, you can run Gazelle to generate the `//``server:server_test`
    target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You will then have the target available in `server/BUILD.bazel`, and you should
    be able to run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to get a more verbose output for your tests, you can use the `–test_arg`
    option and set it to `-test.v`. It will return something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we saw how to test unary, server streaming, client streaming, and
    bidirectional streaming APIs. We saw that we do not need to use a port on the
    machine running the test when using `bufconn`. This makes our tests less reliant
    on the environment it runs on. Finally, we also saw that we can use fakes in order
    to test our system dependencies. This is a bit out of the scope of this book,
    but it was important to me to show you that you can write normal tests even if
    you are using gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important step when testing your services is to make sure that they
    are efficient and can handle a specific load. For this, we use load-testing tools
    that will concurrently send requests to our service. ghz is a tool that does just
    that. In this section, we are going to see how to use the tool and some options
    that we need to set in order to test our API.
  prefs: []
  type: TYPE_NORMAL
- en: 'ghz is a tool that is highly configurable. Run the following command to see
    and understand the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, we are not going to use all these options but we will examine the
    most common ones and the ones that we need to use in some specific cases. Let
    us start by trying to make a simple call.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In order to run the following load test, you will need to deactivate the rate-limiting
    middleware in the `server/main.go` file. You can do so by commenting `ratelimit.UnaryServerInterceptor`
    and `ratelimit.StreamServerInterceptor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first run our server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The first four options that we are going to talk about are the most common
    ones. We need to be able to name the service and method that we want to call (`--call`),
    indicate in which proto file the service is defined (`--proto`) and where to find
    the imports (`--import_paths`), and finally, specify the data to be sent as a
    request. In our case, a basic command, run from the `chapter9` folder, will look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you try to run this command, you will end up having an error message
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can surely guess from the message, this is because we set up our server
    to only accept secure connections. To solve this problem, we will use the `--cacert`
    option, which lets us specify a path to where the CA certificate is. If you remember,
    this is exactly what we did in the code for our client. `ghz` also needs that
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this command, you will get the same error as previously. This is
    because a certificate has a domain name associated with it. This means that only
    requests from a certain domain name will be accepted. However, because we are
    working from localhost, this simply does not meet that requirement and fails.
    To solve that, we are going to use the `--cname` option to override the domain
    name from which we are sending to comply with the certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used `check.test.example.com` because the generated certificate that
    we downloaded from [https://github.com/grpc/grpc-go/tree/master/examples/data/x509](https://github.com/grpc/grpc-go/tree/master/examples/data/x509)
    was generated with the DNS name `*.test.example.com` (see `openssl.cnf`). Also,
    note that this `--cacert` and `--cname` are only useful for self-signed certificates.
    In general, except for specific cases, these certificates are used for testing
    and non-production environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you run the previous command, you should get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'That should ring a bell. This is the error we are sending in our auth interceptor
    when a client does not provide `auth_token` metadata. In order to send that metadata,
    we are going to use the `--metadata` option, which takes a JSON string for keys
    and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After running with all these options, we should be able to run our first load
    test (the results might be different for you):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot to say and to look at in this summary. However, let us focus
    on some interesting points. The first one is the number of requests made. We can
    see that we made 200 of them in this test. This is the default number of requests.
    We can change that by using the `--total` option and setting another number (e.g.,
    500).
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the response time histogram, we can see that 111 out of 200 requests
    were executed in ~2.29 ms. Another interesting thing to see here is that we have
    some commands (50) running in more than 13 ms. If we were in production, we might
    want to dig deeper into this in order to find the cause of these “high” execution
    times. This depends a lot on the use case and requirements. In our case, this
    is almost certainly due to the inefficient “database” that we use, or more precisely,
    the `append` that we repeatedly call in `inMemoryDb.addTask`.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we have the distribution of our execution time. We can see that
    75% of our requests execute in under 2.39 ms. In fact, this is a similar information
    as presented previously. If we take the number of requests under 3.504 ms, add
    them up, and calculate the percentage, we get (1 + 111 + 38) * 100 / 200 = 75%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have the status code distribution. In our case, all 200 requests succeeded.
    However, in a production scenario, you might have something that looks more like
    this (from the ghz documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, one last thing that we cannot see here (because we do not have any)
    is the error distribution. This is the distribution of the error messages. Once
    again, in production, you might have something like the following (from the ghz
    documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There is obviously a lot more that we could do with this tool. As mentioned,
    it is highly configurable, and it is even possible to link the results in Grafana
    (`https://ghz.sh/docs/extras`) for visualization. However, this is out of the
    scope of this book. I will leave it up to you to try the different options and
    call ghz on our other API endpoints to see how they perform.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, we saw how can load test our service with ghz. We only saw how
    to use it for our unary API, but it is also useful for testing all the other streaming
    APIs. After executing the ghz command, we saw that we can get information about
    latency, error codes, error message distribution, and the fastest and slowest
    running times. All of this is useful, but it is important to understand that it
    can be even more powerful when linked with visualization tools such as Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how well we unit test our services, we are humans and humans make
    mistakes. At some point, we are going to need to debug a service. In this section,
    we are going to see how to approach debugging. We are first going to enable server
    reflection, which will let us call our service simply from the command line. After
    that, we will use Wireshark to inspect data on the wire. Finally, because the
    error might not always come directly from our code, we will see how we can take
    a look at the gRPC logs.
  prefs: []
  type: TYPE_NORMAL
- en: Server reflection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Server reflection is an interesting feature when it comes to exposing the API
    to external clients. This is because it lets the server describe itself. In other
    words, the server knows all the services registered and the message definition.
    If a client asks for more information, the server, through reflection, can list
    all the services, messages, and so on. With that, the client does not even need
    to have a copy of the proto file. Now, this is not only useful for exposing the
    API to external clients. It is also useful for manual testing/debugging. It lets
    developers/testers focus only on debugging the API and not getting the whole environment
    to work (copy the proto files, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling server reflection is an easy thing in gRPC Go. We only need two lines
    of code: an `import` statement and a call to the `reflection.Register` function
    to register the reflection service on our server. It looks like this (`server/main.go`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: However, even though this is enough to expose information, we will need to get
    a client that contacts the server and understand the information it is getting.
    There are multiple such tools out there. [The most popular one is `grpcurl` (`https`](https://github.com/fullstorydev/grpcurl)`://github.com/fullstorydev/grpcurl`).
    If you are familiar with cURL, this is basically a similar tool, but one that
    understands the gRPC protocol. Even though we are going to use this tool to explore
    server reflection, know that it can also make other normal requests. If you are
    interested in such a tool, the repository’s README is full of examples of how
    to use it for other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try to create a simple command with `grpcurl` first. We are going to
    use options that are similar to the ones we used in ghz. We are going to use the
    CA certificate and override the domain name with `–cacert` and `-authority`. Then,
    we are going to add an `auth_token` header for reflection with `-reflect-header`,
    and finally, we will use the list verb in order to list the services present on
    the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run this command, we should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have both our `TodoService` and the `ServerReflection` service
    that we registered earlier. With that, we can describe a service to get all the
    RPC endpoints it contains. We do that with the `describe` verb followed by the
    service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this command will show the definition of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also take a look at the message content by replacing the service’s name
    after `describe` with the name message. An example for `AddTaskRequest` is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as we are talking about debugging in this section, we want to be able
    to call these RPC endpoints and test them with different data. This is easy because
    we do not even need to have the proto file with us. The server reflection will
    help `grpcurl` figure everything out for us. Let us call the `AddTask` endpoint
    with an invalid request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we use other options here. We use the `-d` option to set the data
    that we want to send as `AddTaskRequest`. We use the `–use-reflection` option
    so that `grpcurl` can verify that the data is valid (we are going to see that
    soon) and the `–rpc-header` on top of `–reflect-header` because `–reflect-header`
    only sends the header to the `ServerReflection` service, and we also need to send
    the header to `TodoService`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the previous command returns the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as mentioned, grpcurl does not let us execute commands without adding
    guardrails. The use of reflection is useful here because it does not let us send
    data that cannot be deserialized in the request message. An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Finally, as we also have a non-unary RPC endpoint that we would want to test,
    we can use an interactive terminal. This will let us send and receive multiple
    messages. To do that, we will set the data to `@` and end the command with `<<EOF`,
    where `EOF` stands for end of file (you can use any suffix really). This will
    let us type data interactively, and when we are finished, we write `EOF` to let
    grpcurl know.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start by adding two new tasks to our server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use `ListTasks` to show the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Can you spot any bugs here? If not, do not worry; we are going to come back
    to it shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to call our client streaming API (`UpdateTasks`), we can use the following
    command on Linux/Mac (press *Enter* after the last `EOF`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Windows (PowerShell) users should use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: After that, another call to `ListTasks` should show the data with the new descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, while executing these functions, you might have noticed a bug. If you did
    not, there is nothing to worry about; we are going to solve the problem together.
    The problem is that we can send an empty `DueDate`, which then gets transformed
    to the `0` value in Unix Time (`1970-01-01`). This bug comes from the fact that
    `protoc-gen-validate` checks `DueDate` only if it is set.
  prefs: []
  type: TYPE_NORMAL
- en: To solve that, we can add another validation rule for `due_date` to our `todo.proto`
    file. This rule is `required`. It will make it impossible for the field to not
    be set. You might argue that a task does not need a due date, but we could make
    a different endpoint for `Adding Notes` and say that `Tasks` should have a due
    date, but not `Note`.
  prefs: []
  type: TYPE_NORMAL
- en: '`due_date` will now be defined as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can rerun the generation of the `validate` plugin (in `chapter9`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we shut our server down and relaunch it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If we rerun one of the previous `AddTask` commands, it should fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We solved a bug! How great is that?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to now send a request with a `due_data` value you will have to
    specify a date in `RFC3339` format as a string. An example using a `due_date`
    value of 500 years from the day of writing this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Bazel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to run the server with Bazel, you will have to update the dependencies.
    You can run Gazelle to update `//server:server`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will be able to run the server normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we saw that we can turn server reflection on in order to get information
    and interact with a server for debugging purposes. We saw that we can list services
    and describe both services and messages. We also saw that we can call unary RPC
    endpoints, and finally, we saw that we can also call streaming APIs with interactive
    terminals.
  prefs: []
  type: TYPE_NORMAL
- en: Using Wireshark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we need to be able to inspect the data that is going through the
    wire. This lets us get a sense of how heavy the payloads are, know if we execute
    too many requests, and so on. In this section, we are going to see how we can
    use Wireshark to analyze payloads and requests.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we need in order to get access to readable information
    is to disable the encryption via TLS that we enabled in `Chapter 7`. Note that
    this should be fine because we are in development mode, but you will need to make
    sure that encryption is on when you push back to production.
  prefs: []
  type: TYPE_NORMAL
- en: To disable the encryption, we are going to create a switch variable. With this
    variable, the TLS will be disabled by setting the `ENABLE_TLS` environment variable
    to `false`. Obviously, as we want to make TLS the default, we are going to check
    whether the environment variable value is different from `false`, so that if there
    is a typo in the value or the value is not set, TLS will be enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `server/main.go`, we can have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to do something similar on the client side (`client/main.go`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we can now enable/disable TLS easily. To run the server on Linux
    or Mac and without TLS, we can now run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows (PowerShell), we can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for the client, we can run the following (Linux/Mac):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows (PowerShell), we can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to start inspecting the data sent over the wire. In Wireshark,
    we will first check the network interface on which we want to intercept the payloads:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 9.1 – \uFEFFSelecting the network interface](img/B19664_09_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Selecting the network interface
  prefs: []
  type: TYPE_NORMAL
- en: 'The loopback interface is the one that we are working on: localhost. By double-clicking
    on it, we will enter the recording interface. But before doing that, we want to
    tell Wireshark where to find our proto files. Without them, it will show you only
    the field tags and the value. It would be better if we could have the field names
    too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we will go to `chapter9/proto` folder and the folder needed to
    access the Well-Known Types. The last path depends on how you installed `protoc`.
    Here are the most common paths:'
  prefs: []
  type: TYPE_NORMAL
- en: If installed through GitHub releases and you moved the `include` folder to `/usr/local`,
    then the second path is `/usr/local/include`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If installed through `brew`, you should be able to get the path on which protobuf
    is installed with the `brew --prefix protobuf` command. This will give you a path;
    simply append `/include` to the path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If installed through Chocolatey, you should run the `choco list --local-only
    --exact protoc --trace` command. This will list a path finishing with `.files`.
    Open the path in a tool such as Notepad, find a path containing `include/google/protobuf`,
    and select it up until the `include` folder – for example, `C:\ProgramData\chocolatey\lib\protoc\tools\include`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 9.2 – \uFEFFAdding path to proto files](img/B19664_09_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Adding path to proto files
  prefs: []
  type: TYPE_NORMAL
- en: Once this is done, we can go back to our loopback interface and double-click
    on it. We should now have the following recording interface.
  prefs: []
  type: TYPE_NORMAL
- en: We will then enter a filter to only show the requests on port `50051` and only
    the requests related to gRPC and Protobuf. Make sure you click on the arrow just
    next to the filter area; otherwise, you will get all the requests made on the
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Entering a filter](img/B19664_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Entering a filter
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we can go ahead and run the server and the client. Once the client
    is done executing, you will have some logs appearing in Wireshark. This should
    look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Logs appearing in Wireshark](img/B19664_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Logs appearing in Wireshark
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now understand what was sent over the network. If we are looking at
    the payloads, will should be looking at the `DATA (GRPC) (PROTOBUF)` frames. An
    example is the `DATA` frame for `AddTask`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if we are looking at gRPC-related recordings, we can take a look at
    the `HEADERS` and `DATA (GRPC)` frame. These can tell you when half-closes and
    trailers are sent and their size. An example of a half-close for `ListTasks` is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'An example trailer for `DeleteTasks` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: For the sake of keeping this book readable, we will have to end this section
    here. However, there is a lot more to look at and discover. We saw that we can
    use Wireshark to intercept messages over the wire. We made a switch variable to
    be able to disable TLS temporarily to not read encrypted data. We loaded protobuf
    messages into Wireshark to let it know how to deserialize messages. Finally, we
    saw that we can look at messages, as well as lower-level parts of the HTTP2 protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Turning gRPC logs on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, if you are ready to go to an even lower level than Wireshark to debug
    gRPC applications, gRPC provides two important environment variables to get logs
    from the framework.
  prefs: []
  type: TYPE_NORMAL
- en: The first environment variable is `GRPC_GO_LOG_SEVERITY_LEVEL`. It will give
    you the logs written by gRPC depending on certain severity levels (`debug`, `info`,
    or `error`). To enable this, you can simply execute your binary or Go command
    with `GRPC_GO_LOG_SEVERITY_LEVEL` set in front of it. We did something similar
    with our custom `ENABLE_TLS` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of running `GRPC_GO_LOG_SEVERITY_LEVEL` set with `info` while spinning
    up the server and closing it is as follows (for Linux/Mac):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows (PowerShell), we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'On top of the severity level, you can also set the verbosity of those logs
    with `GRPC_GO_LOG_VERBOSITY_LEVEL`, which takes a number between 2 and 99, where
    the bigger the number, the more verbose it will be. This will not be in a short-term
    runtime like the one we have right now. This will be more useful on long-term
    runs, which we normally have for servers. To enable it, we add `GRPC_GO_LOG_VERBOSITY_LEVEL`
    just after our `GRPC_GO_LOG_SEVERITY_LEVEL`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I know that I said there are two important environment variables but
    there is another that deserves a mention. This one is important if you are planning
    to parse logs. You can set the formatter for the logs. As of right now, we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'But we can set the formatter to JSON in order to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: You will now be able to deserialize the JSON and implement all the kinds of
    tools you need to monitor and report errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, in this brief section, we saw that we can get information for
    code that we do not write: the gRPC framework. I am aware that the examples presented
    in this section are superficial, but generally, these flags are set when something
    goes really wrong or if you are involved in the development of gRPC Go itself.
    I still think it is important to know about their existence and I encourage you
    to try getting more interesting messages out of it.'
  prefs: []
  type: TYPE_NORMAL
- en: There are as many ways to debug as there are requirements and settings. As such,
    we cannot cover everything here, but at least you have the basic skills and tools
    to get started with hacking. In this section, we saw that we can enable server
    reflection to get information from the server and interact with it with grpcurl.
    We also saw that we can intercept messages with Wireshark to get a sense of the
    requests made and their size. Finally, we saw that we can turn on a certain flag
    to get logs from gRPC. Before going on to the next section, I wanted to mention
    that there is another tool that you might find useful and that we did not cover
    here. This tool is called Channelz ([https://grpc.io/blog/a-short-introduction-to-channelz/](https://grpc.io/blog/a-short-introduction-to-channelz/)).
    Its purpose is to debug networking issues. You might want to take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another crucial step of production-grade APIs is deploying the services online.
    In this section, we will see how we can create a Docker image for gRPC Go, deploy
    it to Kubernetes, and finally deploy the Envoy proxy to let clients make requests
    from outside the cluster to a server inside it.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This first step in deploying is often containerizing your application with Docker.
    If we did not do so, we would have to deal with errors depending on the server
    architecture, tools not being available on it, and so on. By containerizing our
    application, we can build our image once and run it everywhere where Docker is
    available.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to focus on containerizing our server. This makes much more sense
    than working on the client because we will later deploy our gRPC server as microservices
    in Kubernetes and we will make the client, which is outside, make requests to
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we can think about is what all the steps needed to build
    our application are. We ran it quite a few times, but we need to remember all
    the tools that we set up in the first place. This includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: protoc to compile our proto files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proto Go, gRPC, and the `validate` plugin to generate Go code out of proto files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obviously, Golang
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us start with getting protoc. For that, we are going to create a first
    stage based on Alpine, which will use `wget` to get the protoc ZIP file and unzip
    it inside `/usr/local`. If you are impatient, you can find the whole Dockerfile
    in `server/Dockerfile`, but we are going to explain it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: There are quite a few things happening here. First notice that we are using
    the Docker BuildKit engine. This lets us use defined variables such as `BUILDPLATFORM`,
    `TARGETOS`, and `TARGETARCH`. We do that because even though we are containerizing
    our application to avoid dealing with architecture, running a container with the
    same architecture as the host (virtualization) is much more efficient than emulation.
    Furthermore, as you can see, we need to specify the architecture and OS in the
    URL to download protoc.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we define some variables that are important for building up the download
    URL. We set the version of protoc (here, 23.0). Then, we set the architecture
    we want to work on. This is based on the result of `uname –m`, which gives information
    about the machine. Notice that we use a little trick to replace `aarch64` with
    `aarch_64`. This is because if you take a look at the releases on the Protobuf
    repository (`https://github.com/protocolbuffers/protobuf/releases`), they use
    `aarch_64` in their ZIP filenames.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we use the `TARGETOS` variable to define which OS we want to deal
    with. Notice, once again, the similar trick to replace `darwin` with `linux`.
    This is simply because protoc does not have a binary specific to macOS. You can
    simply use a Linux one.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we do the actual downloading of the file by concatenating all the variables
    that we defined previously, and we unzip the file into `/usr/local`. Notice that
    we are extracting both the protoc binary (`/bin/protoc`) and the `/include` folder
    because the first one is the compiler that we are going to use and the second
    one is all the files needed to include Well-Known Types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this is done, we can create another stage for building the application
    with Go. Here, we are going to copy protoc from the previous stage, download the
    protoc plugins, compile the proto files, and compile the Go project. We are going
    to use an Alpine-based image for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: At this point, none of this should be confusing. This is exactly what we have
    done earlier in the book. However, I want to mention some non-trivial things that
    are happening here. We are once again taking BuildKit-defined parameters. This
    lets us use the `GOOS` and `GOARCH` environment variables to build a Go binary
    for this specific setting.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that we are copying both protoc and the `include` folder. As mentioned,
    the second one is the directory containing Well-Known Types and we use some of
    them in our proto files, so this is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I am using two linker flags. The `-s` flag is here to disable the generation
    of the Go symbol table. While I will not dive into what this means, this is sometimes
    used when creating smaller binaries to remove some information that should not
    impact the runtime capabilities. `-w` removes debug information. As these are
    not needed for production, we can just get rid of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will build our last stage, which will be based on a scratch image.
    This is an image that does not have any OS and that we use for hosting binaries
    and making our images really small. In there, we will copy our certificates into
    a `certs` directory, copy the binary we created with `go build`, and launch the
    application with the parameters that we usually set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we are ready to build our first image of the server. The first thing
    that we can create is a Docker Builder. As described in the Docker documentation:
    “Builder instances are isolated environments where builds can be invoked.” This
    is basically an environment that we need to launch the build of our images. To
    create that, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You need to make sure that Docker is running. This is as simple as making sure
    that Docker Desktop is running. Finally, you might need to prepend all the following
    Docker commands with `sudo` if you are on Linux/Mac and you did not create a Docker
    group and add your user to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we give this build environment the name `mybuild` and that we use
    the `docker-container` driver. This driver will let us generate multi-platform
    images. We are going to see that later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have executed the command, we will be able to use this Builder in another
    Docker command: `docker buildx build`. With this command, we are going to generate
    the image. We will give it a tag (a name), specify where to find the Dockerfile,
    specify the architecture we want to build on, and load the image into Docker.
    To build an image for `arm64` (you can try `amd64`), we run the following (from
    `chapter9`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'After everything is built, we should be able to see the image by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let us try to run the server image and make requests to it. We are
    going to run the image we just created and expose the ports we used for the server
    (`50051` and `50052`) to the same ports on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we run our client normally, we should be able to get all the logs we
    had previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we saw that we can create slim images around our gRPC applications.
    We used a multi-stage Dockerfile in which we first downloaded protoc and the Protobuf
    Well-Known Types. We then downloaded all the Golang dependencies and built a binary,
    and finally, we copied the binary into a scratch image to create a thin wrapper
    around it.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our server image, we can deploy multiple instances of our service
    and we will have created our to-do microservice. In this section, we are going
    to focus mostly on how to deploy our gRPC service. This means that we are going
    to write a Kubernetes configuration. If you are not familiar with Kubernetes,
    there is nothing to be afraid of. Our configuration is simple and I will explain
    all the blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to think about is how our service will be accessed.
    We have two major ways of exposing our services: making them accessible only from
    inside the cluster or accessible from outside the cluster. In most cases, we do
    not want our services to be accessed directly. We want to go through a proxy that
    will redirect and load balance the requests to multiple instances of our service.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, we will create a Kubernetes Service, which will itself assign a DNS
    A record for all the instances of our service. This basically means that each
    of our services will have its own internal address in the cluster. This will let
    our proxy resolve all the addresses and load balance across all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a service is called a headless service. In Kubernetes, this is a service
    with the `clusterIp` property set to `None`. Here is the service definition (`k8s/server.yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Note that we create a port called `grpc` with the value `50051`. This is because
    we want to be able to access all the services on port `50051`. Then, notice that
    we are creating a selector to specify which app this service will handle. In our
    case, we call it `todo-server` and this will be the name for our deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can think about creating instances of our service. We are going to
    do that with a Kubernetes Deployment. This will let us specify how many instances
    we want, which image to use, and which container port to use. This looks like
    the following (`k8s/server.yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Here, we specify that the name of the Pods will match `todo-server`. This makes
    them be handled by the service. We then specify that we want to use the image
    that we created earlier. However, notice here that we are setting `imagePullPolicy`
    to `Always`. This means that each time we create the Pods, they will pull a new
    image from the image registry. This makes sure that we always get the newest image
    on the registry. However, note that this might be inefficient if the images do
    not change often and if you have local copies of the images that are not outdated.
    I would recommend you check, depending on your Kubernetes environment, what to
    use as a value of `imagePullPolicy`. Finally, we use port `50051`. There is nothing
    more to it than specifying on which port our service is exposing the API.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this chapter, I expect that you already have a Kubernetes
    cluster. If you have one in the cloud, this is perfect and you can continue. If
    you do not have one, you can refer to Kind ([https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/)),
    and once installed, you can create a simple cluster with the configuration provided
    in `k8s/kind.yaml`. Simply run `kind create cluster --``config k8s/kind.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we can now deploy our three services. We will run the following
    command from the `chapter9` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to execute the following command to look at the Pod being created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, since we do not have a proxy, we will simply use the `port-forward` command
    from Kubernetes to access one server and see whether it works. This is purely
    for testing purposes, and we are going to see later how to hide the services behind
    a proxy. So, we run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we should be able to use our client normally on `localhost:50051`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, we saw that we can use a headless service to create a DNS A record
    for each of the Pods in the Deployment. We then deployed three Pods and saw that
    we can test whether they are working or not by using the `port-forward` command
    in `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: Envoy proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our microservices created, we need to add a proxy that will
    balance the load between all of them. This proxy is Envoy. This is one of the
    few proxies that can interact with gRPC services. We are going to see how to set
    up Envoy to redirect traffic to our services, load balance with the round robin
    algorithm, and enable TLS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us first focus on writing a listener. This is an entity that specifies
    the address and port on which to listen and defines some filters. These filters,
    at least in our case, will let us route the requests for `todo.v2.TodoService`
    to an Envoy cluster. A cluster is the entity that will let us define the actual
    endpoints and shows us how to load balance. We can first write our listener (`envoy/envoy.yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The most important things to note are that we defined a route matching all the
    gRPC requests from any domain names and matching the `/todo.v2.TodoService` prefix.
    Then, all these requests will be redirected to `grpc_cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, let us define our cluster. We are going to use the `STRICT_DNS`
    resolution to detect all the gRPC services by DNS A record. Then, we will specify
    that we are only accepting HTTP/2 requests. This is because, as you know, gRPC
    is based on HTTP/2\. After that, we will set the load balancing policy to use
    round robin. Finally, we will specify the address and port of the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we use the address generated by Kubernetes. This is of the form
    `$SERVICE_NAME-$NAMESPACE-svc-cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test our configuration, we can run everything locally first. We
    will temporarily make the `listener_0` port equal to `50050` so that it does not
    conflict with our server port:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also have to set the endpoint address to localhost to access the server
    running locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will run our server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run our envoy instance with `func-e`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run our client on port `50050`, not `50051`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: As you can guess, this is because Envoy is somehow breaking the TLS connection
    between the server and the client. To solve that, we are going to specify that
    the upstream of our cluster uses TLS and that the downstream of our listener also
    uses TLS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the filters, we will tell Envoy where to find our self-signed certificates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is probably not what you would do in production. You would use
    a tool such as Let’s Encrypt to automatically generate your certificates and link
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will tell the cluster that the upstream is also using TLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, this is not going to work directly. On our local computer, we do
    not have the `/etc/envoy/certs/server_cert.pem` and `/etc/envoy/certs/server_key.pem`
    files. But we have them in the `chapter9` `certs` folder. We will replace them
    temporarily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now kill the previous instance of Envoy and rerun it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we should be able to run our client and receive responses from our
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We are now certain that our requests go through Envoy and are redirected to
    our gRPC server. The next step will be reverting all the temporary changes that
    we made for testing (listener port to `50051`, endpoint address to `todo-server.default.svc.cluster.local`,
    and `certs` path to `/etc/envoy`) and creating a Docker image that we will use
    for deploying Envoy in our Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build such an image, we will copy the certificates to `/etc/envoy/certs`
    (once again, this is not recommended in production) and the configuration (`envoy.yaml`)
    to `/etc/envoy`. Finally, this image will run the `envoy` command with the `--config-path`
    flag, which will point to the `/etc/envoy/envoy.yaml` path. In `envoy/Dockerfile`,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now build the image for `arm64` (you can use `amd64`) like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'That is it! We are ready to deploy Envoy in front of our TODO microservices.
    We need a headless service for Envoy. This is for the same reasons that we had
    when creating a headless service for our microservices. In production, there will
    potentially be more than one instance of Envoy and you need to make sure they
    are all addressable. In `envoy/service.yaml`, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to create a `Deployment`. This time, as we are in a development
    setting, we will deploy only one Pod for Envoy. All the rest of the configuration
    is similar to what we did with our gRPC server. In `envoy/deployment.yaml`, we
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run all of this. I am assuming that you did not tear down the previous
    step that we did for deploying microservices. Right now, you should have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we can first add the service and then the deployment for Envoy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, before running the client, we can use the `port-forward` command to
    forward Envoy’s port `50051` to `localhost:50051`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run the client and we should be able to get some results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Notice that because of the load balancing and the fact that we do not use a
    real database, the Pods are not able to find tasks that are stored in other Pods’
    memory. This is normal in our case, but in production, you would be relying on
    a shared database and these problems would not arise.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, we saw that we can instantiate Envoy in front of our services to
    redirect requests with a certain load-balancing policy. This time, contrary to
    the load balancing we saw in `chapter7`, the client does not actually know any
    addresses for the servers. It connects to Envoy and Envoy is redirecting requests
    and responses. We obviously did not cover all the possible configurations for
    Envoy and I would recommend that you check out other features, such as rate limiting
    and authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered unit and load testing. We saw that we can find bugs
    and performance issues by extensively testing different parts of our system. Then,
    we saw how to debug our application when we found a bug. We used server reflection
    and grpcurl to interact with our API from the terminal. Finally, we saw how we
    can containerize our services and deploy them on Kubernetes. We saw that we can
    create headless services to expose our microservices with a DNS A record per gRPC
    server, and we saw that we can put Envoy in front of them to do load balancing,
    rate limiting, authentication, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What tool is useful for load testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wireshark
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: grpcurl
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ghz
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In Wireshark, what information can you look at?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: gRPC HTTP/2 frames
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Protobuf messages
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: All of them
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Envoy used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Redirecting requests and responses
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Exposing metrics
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A and D
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: B and C
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: E
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Add support for a real database. You should be able to do so by implementing
    the `db` interface and creating an instance of your struct in the registered server
    instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expose the Prometheus metrics in your Kubernetes cluster. You can take a look
    at `prometheus-operator` ([https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epilogue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we reach the end of this book on building gRPC microservices in Golang, I
    hope you have found gRPC Go interesting and useful, and that you are willing to
    try it on your next project. This book is the book I wish I had when I started
    learning this awesome technology and I hope this helped you in any way.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we both explored some elements of theory and some practical
    implementations of gRPC services. From learning the networking concepts to the
    pure implementation and tool that we can use, passing by learning useful considerations
    when designing an API, you learned the most important skills that you will need
    for your career as backend engineer.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this book, I would like to invite you to stay up to date with all
    the topics related to gRPC and Protobuf. You can do that by following GitHub Topics,
    read some blog post or simply getting involved in some open source project. This
    is a fascinating area of backend engineering that needs more attention, more help
    by building tools, and more people to form communities all around the world.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for accompanying me on this journey to make production-grade gRPC
    APIs. I wish you the very best in your future endeavors. May you create innovative
    and effective APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Happy engineering!
  prefs: []
  type: TYPE_NORMAL
