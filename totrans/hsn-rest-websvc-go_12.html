<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Containerizing REST Services for Deployment</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will explore how to containerize our Go applications using a few tools such as Docker, Docker Compose, Nginx, and Supervisord. Containerization is required to avoid platform dependency during deployment of an application. To deploy an application properly, we must prepare an ecosystem. That ecosystem consists of a web server, an application server, and a process monitor. This chapter deals with how to take our API server from a standalone application to a production-grade service.</p>
<p class="mce-root">In recent times, most cloud providers tend to host web applications. Some big players such as AWS, Azure, Google Cloud Platform, along with start-ups such as DigitalOcean and Heroku are a few such examples. In the upcoming sections, we will focus on making a platform ready for deploying REST services. In the next chapter, we will look at how to deploy this ecosystem on a famous cloud provider, AWS.</p>
<p>Nginx is a web server that can be a reverse proxy for a web application. It can also act as a load balancer when multiple instances of the server are running. Supervisord makes sure that an application server is up and running in the event of a crash or a system restart. An application server/REST service are both the same, so please consider them in equal context throughout this chapter.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Installing the Nginx server</li>
<li>What is a reverse proxy server?</li>
<li>Deploying a Go service using Nginx</li>
<li>Monitoring our Go API server with Supervisord</li>
<li><kbd>Makefile</kbd> and Docker Compose-based deployment</li>
</ul>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements </h1>
                </header>
            
            <article>
                
<p>The following is the software that should be preinstalled for running the code samples:</p>
<ul>
<li>OS: Linux (Ubuntu 18.04)/Windows 10/<span>Mac OS X</span> &gt;=10.13</li>
<li>Go stable version compiler &gt;= 1.13.5</li>
<li>Dep: A dependency management tool for Go &gt;= 0.5.3</li>
<li>Docker version &gt;= 18.09.2</li>
<li>Docker Compose &gt;= 1.23.2</li>
</ul>
<p><span>You can download the code for this chapter from </span><a href="https://github.com/PacktPublishing/Hands-On-Restful-Web-services-with-Go/tree/master/chapter12" target="_blank">https://github.com/PacktPublishing/Hands-On-Restful-Web-services-with-Go/tree/master/chapter12</a><span>. Clone the code and use the code samples in the</span><span> </span><kbd>chapter12</kbd> <span>directory.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the Nginx server</h1>
                </header>
            
            <article>
                
<p class="graf graf--p">Nginx is a high-performing web server and load balancer. It is well suited for deploying high-traffic websites and API servers. Even though this decision is opinionated, it is a community-driven, industry-strong web server. It is similar to the Apache2 web server.</p>
<p>Nginx can also act as a reverse proxy server that allows us to redirect our HTTP requests to multiple application servers running on the same network. The main contender of Nginx is Apache's <kbd>httpd</kbd>.<strong> </strong>Nginx is an excellent static file server that can be used by web clients. Since we are dealing with APIs, we will take a look at how to deal with HTTP requests.</p>
<p>We can access Nginx in two ways:</p>
<ul>
<li>Installation on a bare machine</li>
<li>Using a preinstalled Docker container</li>
</ul>
<p>Let's understand both in more detail. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation on a bare machine</h1>
                </header>
            
            <article>
                
<p class="graf graf--p">On Ubuntu 18.04, use these commands to install Nginx:</p>
<pre>&gt;<strong> sudo apt-get update</strong><br/>&gt;<strong> sudo apt-get install nginx</strong></pre>
<p>On <span>Mac OS X </span>, you can install it with <kbd>brew</kbd>:</p>
<pre>&gt;<strong> brew install nginx</strong></pre>
<p class="graf graf--p"><strong>brew </strong><a href="https://brew.sh/" target="_blank">(https://brew.sh/)</a> is a very useful software packaging system for <span>Mac OS X </span>users. My recommendation is that you use it for installing software. Once it is successfully installed, you can check it by opening the machine IP in the browser. Open <kbd>http://localhost/</kbd> on your web browser. You should see this:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a3bab13d-ea36-4be2-ac43-056cd178786a.png" style="width:28.67em;height:11.50em;"/></div>
<p>If you see the preceding message, that means Nginx has been successfully installed. It serves on port <kbd>80</kbd> and serves the default page. On <span>Mac OS X</span>, the default Nginx listening port will be <kbd>80</kbd>:</p>
<pre>&gt;<strong> sudo vi /usr/local/etc/nginx/nginx.conf</strong></pre>
<p>On Ubuntu (Linux), the file will be on this path:</p>
<pre>&gt;<strong> sudo vi /etc/nginx/nginx.conf</strong></pre>
<p>Open the file and search for a server block. If it is listening on port <kbd>80</kbd>, everything is fine. However, if it is on some other port, for example <kbd>8080</kbd>, then change it to <kbd>80</kbd>:</p>
<pre>server {<br/>        listen 80; # Nginx listen port<br/>        server_name localhost;<br/>        #charset koi8-r;<br/>        #access_log logs/host.access.log main;<br/>        location / {<br/>            root html;<br/>            index index.html index.htm;<br/>        }<br/>        <br/>        ... <br/>}</pre>
<p class="graf graf--p">Now, everything is ready. The server runs on the <kbd>80</kbd> HTTP port, which means a client can access it using a URL (<kbd>http://localhost/</kbd>). This basic server serves static files from a directory called <kbd>html</kbd>. The <kbd>root</kbd> parameter can be modified to any directory where we place our web assets. You can check the status of Nginx with the following command:</p>
<pre>&gt;<strong> service nginx status</strong></pre>
<div class="packt_infobox">Nginx for the Windows operating system is quite basic and is not really intended for production-grade deployments. Open source developers usually prefer Debian or Ubuntu servers for deploying the API servers with Nginx.</div>
<p>We can also get a Docker image that has Nginx installed already. In the next section, we will demonstrate how to install it as a Docker container.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation via a Docker container</h1>
                </header>
            
            <article>
                
<p>Getting a container that has preinstalled Nginx has two benefits:</p>
<ul>
<li>It is easy to ship containers.</li>
<li>We can destroy and recreate the containers any number of times.</li>
</ul>
<p>To get the latest Nginx image and start a container, run the following command:</p>
<pre>&gt;<strong> docker run --name nginxServer -d -p 80:80 nginx</strong></pre>
<p>This pulls the <kbd>nginx</kbd> image from Docker Hub (make sure you are connected to the internet). If the image is already pulled, it reuses that. Then, it starts a container with the name of <kbd>nginxServer</kbd> and serves it on port <kbd>80</kbd>. Now, visit <kbd>http://localhost</kbd> from your browser and you will see the Nginx home page.</p>
<p>However, the preceding command is not useful for configuring Nginx after starting the container. We have to mount a directory from localhost to the container or copy files to the container to make changes to the Nginx configuration file. Let's modify the command:</p>
<pre>&gt;<strong> docker run --name nginxServer -d -p 80:80 --mount source=/host/path/nginx.conf,destination=/etc/nginx/nginx.conf:readonly nginx</strong></pre>
<p>The extra command is <kbd>--mount</kbd>, which mounts a file/directory from the source (<em>host</em>) to the destination (<em>container</em>). If you modify a file on the host system in that directory, then it also reflects on the container. The <kbd>readonly</kbd> option stops users/system processes modifying the Nginx configuration inside the container.</p>
<p>In the preceding <span>command, we are mounting the Nginx configuration file,</span> <kbd>nginx.conf</kbd><span>. We use the Docker container-based deployment in the latter part of this chapter, where we use <kbd>docker-compose</kbd> to deploy our application.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is a reverse proxy server?</h1>
                </header>
            
            <article>
                
<p>A <strong>reverse proxy server</strong> is a server that holds the information regarding the original servers in it. It acts as the front-facing entity for the client request. Whenever a client makes an HTTP request, it can directly go to the application server. However, if the application server is written in a programming language, then you need a translator that can turn the application response into a  response understandable by the clients. <strong>Common Gateway Interface</strong> (<strong>CGI</strong>) does the same thing.</p>
<p>We can run a simple Go HTTP server and it can serve incoming requests (no CGI is required). We should also protect our application server from <strong>Denial of Service</strong> (<strong>DoS</strong>) attacks. So, why are we using another server called Nginx? Well, because it brings a lot of things into the picture.</p>
<p>The benefits of having a reverse proxy server (Nginx) are as follows:</p>
<ul>
<li>It can act as a load balancer.</li>
<li>It can provide access control and rate limiting.</li>
<li>It can sit in front of a cluster of applications and redirect HTTP requests.</li>
<li>It can serve a filesystem with a good performance. </li>
<li>It streams media very well.</li>
</ul>
<p>If the same machine is running on multiple applications, then we can bring all of those  <span>applications </span>under one umbrella. Nginx can also act as the API gateway that can be the starting point for multiple API endpoints. We will explore a dedicated API gateway in the next chapter, but it is good to know that Nginx can also work as one.</p>
<p>Nginx works as a traffic router for incoming requests. It is a protective shield for our application servers.</p>
<p class="mce-root"/>
<p>Take a look at the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/543de7d3-c339-49bc-b713-d5b5f293983b.png" style="width:50.17em;height:34.58em;"/></div>
<p><span>It has three apps running in different programming languages and <strong>Client</strong> only knows a single API endpoint. Let's say that all of the apps run on different ports.</span></p>
<p>As you can see, the diagram <strong>Client</strong> is talking directly to <strong>Nginx</strong> instead of the ports where other applications are running. In the diagram, Go is running on port <kbd>8000</kbd> and other applications are running on different ports. This means that the different servers are providing different API endpoints.</p>
<p>Without Nginx, if the client wishes to call an API, it needs to access three different endpoints (ports). Instead, if we have Nginx, it can act as a reverse proxy server for all three and simplifies the client request-response cycle.</p>
<p>Nginx is also an upstream server. An upstream server serves the requests from one server to the other. From the diagram, you can see that a Python app can request an API endpoint from a Go app and Nginx will take care of routing them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Important Nginx paths</h1>
                </header>
            
            <article>
                
<p>There are a few important Nginx paths that we need to know about in order to work with the proxy server. In Nginx, we can host multiple sites (<kbd>www.example1.com</kbd>, <kbd>www.example2.com</kbd>, and so on) at the same time. This means that many API servers can be run under one Nginx instance.</p>
<p>You should be aware of the following paths in the table to configure Nginx properly. An advanced deployment may require bypassing authentication (for example, the Health check API), rate limiting, and a backup of the logs.</p>
<p>Take a look at the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Type</strong></td>
<td><strong>Path</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>Configuration</kbd></td>
<td><span><kbd>/etc/nginx/nginx.conf</kbd></span></td>
<td>This is the base Nginx configuration file. It can be used as the default file.</td>
</tr>
<tr>
<td><kbd>Configuration</kbd></td>
<td><kbd><span>/etc/nginx/sites-available/</span></kbd></td>
<td>If we have multiple sites running within Nginx, we can have a configuration file for each site.</td>
</tr>
<tr>
<td><kbd>Configuration</kbd></td>
<td><kbd><span>/etc/nginx/sites-enabled/</span></kbd></td>
<td>These are the sites currently activated on Nginx.</td>
</tr>
<tr>
<td><kbd>Log</kbd></td>
<td><kbd><span>/var/log/nginx/access.log</span></kbd></td>
<td>This log file records the server activity, such as timestamps and API endpoints. </td>
</tr>
<tr>
<td><kbd>Log</kbd></td>
<td><kbd><span>/var/log/nginx/error.log</span></kbd></td>
<td>This log file logs all proxy server-related errors, such as disk space, filesystem permissions, and more.</td>
</tr>
</tbody>
</table>
<div class="packt_tip">These paths are in the Linux operating system. For <span>Mac OS X</span>, use <kbd>/usr/local/nginx</kbd> as the base path.</div>
<p>In the next section, we will explore server blocks that are mainly used for configuring applications with Nginx.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using server blocks</h1>
                </header>
            
            <article>
                
<p class="graf graf--p">Server blocks are the actual configuration pieces that tell the server what to serve and on which port to listen. We can define multiple server blocks in the <kbd>sites-available</kbd> folder. On Ubuntu, the location will be as follows:</p>
<pre><strong>/etc/nginx/sites-available</strong></pre>
<p>On <span>Mac OS X</span>, the location will be as follows:</p>
<pre><strong>/usr/local/etc/nginx/sites-available</strong></pre>
<p class="graf graf--p">Until we create a symlink from the <kbd>sites-available</kbd> to the <kbd>sites-enabled</kbd> directory, the configuration has no effect. So, always create a symlink for <kbd>sites-available</kbd> to <kbd>sites-enabled</kbd> for every new configuration you create.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying a Go service using Nginx</h1>
                </header>
            
            <article>
                
<p>As we have already discussed, Nginx can be a reverse proxy for a Go application. Let's say that we have a server that provides a REST API to access book data. A client can send a request and get it back in JSON. The server also stores all the logs in an external file. Let's take a look at the steps to create this application:</p>
<ol>
<li class="graf graf--p">Let's name our project <kbd>bookServer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;<strong> mkdir -p $GOPATH/src/github.com/git-user/chapter12/bookServer<br/>touch </strong><strong>$GOPATH/src/github.com/git-user/chapter12/bookServer/main.go</strong></pre>
<p style="padding-left: 60px">This file is a basic Go server to illustrate the functioning of a reverse proxy server. We first run our program on port <kbd>8000</kbd>. Then, we add a configuration that maps <kbd>8000</kbd> (Go's running port) to <kbd>80</kbd> (the Nginx HTTP port).</p>
<ol start="2">
<li>Now, let's write the code. We will use a few packages for our server. We can use Go's built-in <kbd>net/http</kbd> package for server implementation:</li>
</ol>
<div>
<pre style="padding-left: 60px">package main<br/><br/>import (<br/>    "encoding/json"<br/>    "fmt"<br/>    "log"<br/>    "net/http"<br/>    "os"<br/>    "time"<br/>)</pre></div>
<ol start="3">
<li>Now our server needs a struct to hold the book information. Let's create a struct with fields such as <kbd>ID</kbd>, <kbd>ISBN</kbd>, <kbd>Author</kbd>, and <kbd>PublishedYear</kbd>:</li>
</ol>
<pre style="padding-left: 60px">// Book holds data of a book<br/>type Book struct {<br/>    ID            int<br/>    ISBN          string<br/>    Author        string<br/>    PublishedYear string<br/>}</pre>
<ol start="4">
<li>Now goes our <kbd>main</kbd> function. It should open a file for writing logs. We can do that using the <kbd>os.Openfile</kbd> function. This takes the file and mode as arguments. Let's name the <span>file </span><kbd>app.log</kbd>:</li>
</ol>
<pre style="padding-left: 60px">func main() {<br/>    // File open for reading, writing and appending<br/>    f, err := os.OpenFile("app.log",<br/>     os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666)<br/>    if err != nil {<br/>        fmt.Printf("error opening file: %v", err)<br/>    }<br/>    defer f.Close()<br/>    // This attaches program logs to file<br/>    log.SetOutput(f)<br/>    // further code goes here...<br/>}</pre>
<p style="padding-left: 60px">The file permission, <kbd>os.O_RDWR|os.O_CREATE|os.O_APPEND</kbd>, allows the Go program to create, write, and append to the file. <kbd>log.SetOutput(f)</kbd> redirects app logs to the file.</p>
<ol start="5">
<li>Now, create a function handler and attach it to a route using the <kbd>net/http</kbd> function. The handler converts a struct into JSON and returns it as an HTTP response. Also, attach that handler to a route called <kbd>/api/books</kbd>:</li>
</ol>
<pre style="padding-left: 30px">    // Function handler for handling requests<br/>    http.HandleFunc("/api/books", func(w http.ResponseWriter,<br/>    r *http.Request) {<br/>        log.Printf("%q", r.UserAgent())<br/>        // Fill the book details<br/>        book := Book{<br/>            ID:            123,<br/>            ISBN:          "0-201-03801-3",<br/>            Author:        "Donald Knuth",<br/>            PublishedYear: "1968",<br/>        }<br/>        // Convert struct to JSON using Marshal<br/>        jsonData, _ := json.Marshal(book)<br/>        w.Header().Set("Content-Type", "application/json")<br/>        w.Write(jsonData)<br/>    })</pre>
<p style="padding-left: 90px">The previous code block essentially returns a book whenever a client requests <kbd>/api/books</kbd>.</p>
<ol start="6">
<li>Now, start an HTTP server that serves the whole application on port <kbd>8000</kbd>:</li>
</ol>
<pre style="padding-left: 30px">    s := &amp;http.Server{<br/>        Addr:           ":8000",<br/>        ReadTimeout:    10 * time.Second,<br/>        WriteTimeout:   10 * time.Second,<br/>        MaxHeaderBytes: 1 &lt;&lt; 20,<br/>    }<br/><br/>    log.Fatal(s.ListenAndServe())</pre>
<p style="padding-left: 60px">This finishes the main program.</p>
<ol start="7">
<li>We can run our application and see whether it is running correctly:</li>
</ol>
<pre style="padding-left: 60px">&gt;<strong> go run </strong><strong>$GOPATH/src/github.com/git-user/chapter12/bookServer/<br/>main.go</strong></pre>
<ol start="8">
<li>Now, open a shell and make a <kbd>curl</kbd> command:</li>
</ol>
<pre style="padding-left: 60px">&gt;<strong> curl -X GET "http://localhost:8000/api/books"</strong></pre>
<p style="padding-left: 60px">It returns the data:</p>
<pre style="padding-left: 60px">{<br/>  "ID":123,<br/>  "ISBN":"0-201-03801-3",<br/>  "Author":"Donald Knuth",<br/>  "PublishedYear":"1968"<br/>}</pre>
<ol start="9">
<li>However, the client needs to request to port <kbd>8000</kbd>. So, how can we proxy this server using Nginx? As we previously discussed, we need to edit the default <kbd>sites-available</kbd> server block, called <kbd>default</kbd>:</li>
</ol>
<pre class="graf graf--pre" style="padding-left: 60px">&gt;<strong> vi /etc/nginx/sites-available/default</strong></pre>
<ol start="10">
<li>Edit the preceding file, find the server block, and add <kbd>proxy_pass</kbd> to it:</li>
</ol>
<pre style="padding-left: 60px">server {<br/>        listen 80 default_server;<br/>        listen [::]:80 default_server ipv6only=on;<br/><br/> location / {<br/>                proxy_pass http://127.0.0.1:8000;<br/>        }<br/>}</pre>
<p style="padding-left: 60px">This section of the <kbd>config</kbd> file is called a <kbd>server</kbd> block. This controls the setting up of the proxy server where <kbd>listen</kbd> says where <kbd>nginx</kbd> should listen. <kbd>root</kbd><strong> </strong>and <kbd>index</kbd> point to the static files if we need to serve any file. <kbd>server_name</kbd><strong> </strong>is the domain name of yours.</p>
<p style="padding-left: 60px">Since we don't have a domain name with us, it is just localhost. <kbd>location</kbd><strong> </strong>is the key section here. In <kbd>location</kbd>, we can define our <kbd>proxy_pass</kbd>, which can reverse proxy to a given <kbd>URL:PORT</kbd>. Since our Go application is running on port <kbd>8000</kbd>, we mentioned it there. Let's try running our app on a different domain, <kbd>example.com</kbd>:</p>
<pre style="padding-left: 60px">http://example.com:8000</pre>
<p style="padding-left: 60px">We can give the same name as a parameter to <kbd>proxy_pass</kbd>. In order to take this configuration into effect, we need to restart the Nginx server. You can do that using the following:</p>
<pre style="padding-left: 60px">&gt;<strong> service nginx restart</strong></pre>
<ol start="11">
<li>Now, make a <kbd>curl</kbd> request to <kbd>http://localhost</kbd> and you will see the Go application's output:</li>
</ol>
<pre style="padding-left: 60px">&gt;<strong> curl -X GET "http://localhost"</strong><br/>{<br/>  "ID":123,<br/>  "ISBN":"0-201-03801-3",<br/>  "Author":"Donald Knuth",<br/>  "PublishedYear":"1968"<br/>}</pre>
<ol start="12">
<li>The <kbd>location</kbd> is a directive that defines a <strong>Unified Resource Identifier</strong> (<strong>URI</strong>) that can proxy a given <kbd>server:port</kbd> combination. This means that, by defining various URI, we can proxy multiple applications running on the same server. It looks like this:</li>
</ol>
<pre style="padding-left: 60px">server {<br/>    listen ...;<br/>    ...<br/>    location / {<br/>        proxy_pass http://127.0.0.1:8000;<br/>    }<br/>    <br/>    location /api {<br/>        proxy_pass http://127.0.0.1:8001;<br/>    }<br/>    location /mail {<br/>        proxy_pass http://127.0.0.1:8002;<br/>    }<br/>    ...<br/>}</pre>
<p style="padding-left: 60px">Here, three applications are running on different ports. These, after being added to our configuration file, can be accessed by the client as follows:</p>
<pre style="padding-left: 60px">http://localhost/<br/>http://localhost/api/<br/>http://localhost/mail/</pre>
<p>In the next section, we explore how to load balance API requests to multiple instances of applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load balancing with Nginx</h1>
                </header>
            
            <article>
                
<p>In practical cases, multiple servers are deployed instead of one for handling huge sets of incoming requests for APIs. But, who should forward an incoming client request to a server instance? A load balancer does that job. Load balancing is a process where the central server distributes the load to various servers based on certain criteria. Refer to the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/929be2ee-6e11-4130-9016-93ce0a4e8b74.jpeg" style="width:31.33em;height:22.33em;"/></div>
<p><span>A load balancer employs few strategies such as <kbd>Round Robin</kbd> or <kbd>Least Connection</kbd> for routing requests to instances. Let's take a look at what each does in a simple table:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Load-balancing method</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>Round Robin</kbd></td>
<td>The incoming requests are uniformly distributed across servers based on the criteria of server weights.</td>
</tr>
<tr>
<td><kbd>Least Connection</kbd></td>
<td>Requests are sent to the server that is currently serving the least number of clients.</td>
</tr>
<tr>
<td><kbd>IP Hash</kbd></td>
<td>This is used to send the requests from a given client's IP to the given server. Only when that server is not available is it given to another server. </td>
</tr>
<tr>
<td><kbd>Least Time</kbd></td>
<td>A request from the client is sent to the machine with the lowest average latency (<span>the time-to-serve client</span>) and the least number of active connections.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can set which strategy to apply for load balancing <span>in the Nginx configuration</span>.</p>
<p>Let's explore how load balancing is practically achieved in Nginx for our Go API servers. The first step in this process is to create an <kbd>upstream cluster</kbd> in the <kbd>http</kbd> section of the Nginx configuration file:</p>
<pre>http {<br/>    upstream cluster {<br/>        server site1.mysite.com weight=5;<br/>        server site2.mysite.com weight=2;<br/>        server backup.mysite.com backup;<br/>    }<br/>}</pre>
<p>Here, servers are the IP addresses or domain names of the servers running the same code. We are defining an upstream called cluster here. It is a server group that we can refer to in our <kbd>location</kbd> directive. Weights should be given in proportion to the resources available. In the preceding code, <kbd>site1</kbd> is given a higher weight because it may be a bigger instance (memory and CPU). Now, in the <kbd>location</kbd> directive, we can specify the server group with the <kbd>proxy_pass</kbd> command:</p>
<pre>server {<br/>    location / {<br/>        proxy_pass http://cluster;<br/>    }<br/>}</pre>
<p>Now, the proxy server that is running will pass requests to the machines in the cluster for all API endpoints hitting the <kbd>/</kbd> endpoint. The default request routing algorithm will be <kbd>Round Robin</kbd>, which means that all of the server's turns will be repeated one after the other. If we need to change it, we can mention that in the upstream definition. Take a look at the following code snippet:</p>
<pre>http {<br/>    upstream cluster {<br/>        least_conn;<br/>        server site1.mysite.com weight=5;<br/>        server site2.mysite.com;<br/>        server backup.mysite.com backup;<br/>    }<br/>}<br/><br/>server {<br/>    location / {<br/>        proxy_pass http://cluster;<br/>    }<br/>}</pre>
<p>The preceding configuration says to <em>create a cluster of three machines and add load balancing method as least connections</em>. <kbd>least_conn</kbd> is the string we used to mention the load balancing method. The other values could be <kbd>ip_hash</kbd> or <kbd>least_time</kbd>. You can try this by having a set of machines in the <strong>Local Area Network</strong> (<strong>LAN</strong>). Otherwise, we can have Docker installed with multiple virtual containers as different machines to test out load balancing.</p>
<div class="packt_infobox"><span>We need to add that <kbd>http</kbd> block in the </span><kbd>/etc/nginx/nginx.conf</kbd><span> file, whereas the server block is in </span><kbd>/etc/nginx/sites-enabled/default</kbd>.<strong> </strong><span>It is better to separate these two settings.</span></div>
<p>Here's a small exercise: try to run three <kbd>bookServer</kbd> instances on different ports and enable load balancing on Nginx. In the next section, we'll examine how to rate limit an API in Nginx for certain clients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rate limiting our REST API</h1>
                </header>
            
            <article>
                
<p>We can also limit the rate of access to our Nginx proxy server by rate limiting. This provides a directive called <kbd>limit_conn_zone</kbd> (<a href="http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone">http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone</a>). The format of it is this:</p>
<pre>limit_conn_zone client_type zone=zone_type:size;</pre>
<p><kbd>client_type</kbd> can be one of two types:</p>
<ul>
<li>An IP address (limit requests from a given IP address)</li>
<li>A server name (limit requests from a server)</li>
</ul>
<p><kbd>zone_type</kbd><strong> </strong>also changes in correspondence to <kbd>client_type</kbd>. It takes values as per the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 48.1589%"><strong>Client type</strong></td>
<td style="width: 51.1628%"><strong>Zone type</strong></td>
</tr>
<tr>
<td style="width: 48.1589%"><kbd>$binary_remote_address</kbd></td>
<td style="width: 51.1628%"><kbd>addr</kbd></td>
</tr>
<tr>
<td style="width: 48.1589%"><kbd>$server_name</kbd></td>
<td style="width: 51.1628%"><kbd>servers</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Nginx has to save a few things in memory to remember the IP addresses and servers for rate limiting. The <kbd>size</kbd> parameter is the storage that we allocate for Nginx to perform its memory operations. It takes values such as 8 m (8 MB) or 16 m (16 MB). Now, let's take a look at where to add these settings. The preceding one should be added as a global setting to the <kbd>http</kbd> directive in the <kbd>nginx.conf</kbd> file:</p>
<pre>http {<br/>    limit_conn_zone $server_name zone=servers:10m;<br/>}</pre>
<p>This allocates the shared memory for Nginx to use. Now, in the server directive of <kbd>sites-available/default</kbd>, add the following:</p>
<pre>server {<br/>   limit_conn servers 1000;<br/>}</pre>
<p>The total number of connections for the given server will not exceed <kbd>1000</kbd> in the preceding configuration using <kbd>limit_conn</kbd>. If we try to put the rate limit from a given IP address to the client, then use this:</p>
<pre>server {<br/>  location /api {<br/>      limit_conn addr 1;<br/>  }<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This setting stops a client (that is, IP address) from opening more than one connection to the server (for example, in an online railway booking session, a user can only use one session per IP address to book tickets). If we have a file that the client downloads and need to set a bandwidth constraint, use <kbd>limit_rate</kbd>: </p>
<pre>server {<br/>  location /download {<br/>      limit_conn addr 10;<br/>      limit_rate 50k;<br/>  }<br/>}</pre>
<p>In this way, we can control the client's interaction with our services that are proxied under Nginx.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Securing our Nginx proxy server</h1>
                </header>
            
            <article>
                
<p>This is the most important piece in the Nginx setup. In this section, we will look at how to restrict access to our server using basic authentication. This is very important for our REST API servers because, suppose we have servers <em>X</em>, <em>Y</em>, and <em>Z</em> that can talk to each other. <em>X</em> can serve clients directly, but <em>X</em> consults <em>Y</em> and <em>Z</em> for some information by calling an internal API. We should prevent clients from accessing <em>Y</em> and <em>Z</em>. We can allow or deny IP addresses using the <kbd>nginx</kbd> access module. It looks like this:</p>
<pre>location /api {<br/>    ...<br/>    deny 192.168.1.2;<br/>    allow 192.168.1.1/24;<br/>    allow 127.0.0.1;<br/>    deny all;<br/>}</pre>
<p>This configuration tells Nginx to allow requests from clients ranging <kbd>192.168.1.1/24</kbd>, excluding <kbd>192.168.1.2</kbd>. The next line tells us to allow requests from the same host and block all other requests from any other client. The complete server block looks like this:</p>
<pre>server {<br/>    listen 80 default_server;<br/>    root /usr/share/nginx/html;<br/><br/>    location /api {<br/><br/>        deny 192.168.1.2;<br/>        allow 192.168.1.1/24;<br/>        allow 127.0.0.1;<br/>        deny all;<br/>    }<br/>}</pre>
<p>For more information regarding this, you can refer to the documentation at <a href="http://nginx.org/en/docs/http/ngx_http_access_module.html?_ga=2.117850185.1364707364.1504109372-1654310658.1503918562" target="_blank">http://nginx.org/en/docs/http/ngx_http_access_module.html?_ga=2.117850185.1364707364.1504109372-1654310658.1503918562</a>. We can also add password-secured access to our <span>Nginx serving static files</span>. It is mostly not applicable to the API because there, the application takes care of authenticating the user. The whole idea is to only allow the IP that is approved by us and deny all other requests.</p>
<p>Nginx can only serve requests when the application server's health is good. If the application crashes, we have to restart it manually. A crash can occur from a system shutdown, a problem in the network storage, or various other external factors. In the next section, we will discuss a process monitoring tool called <kbd>supervisord</kbd> that can automatically restart a crashed application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring our Go API server with Supervisord</h1>
                </header>
            
            <article>
                
<p class="graf graf--p">Sometimes, a web application server may stop due to an operating system restarting or crashing. Whenever a web server is killed, it is someone's job to bring it back to life. It is wonderful if that is automated. Supervisord is a tool that comes to the rescue. To make our API server run all of the time, we need to monitor it and recover it quickly. Supervisord is a generic tool that can monitor running processes (systems) and can restart them when they are terminated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Supervisord</h1>
                </header>
            
            <article>
                
<p class="graf graf--p">We can easily install Supervisord using Python's <kbd>pip</kbd> command.</p>
<pre>&gt;<strong> sudo pip install supervisor</strong></pre>
<p class="graf graf--p">On Ubuntu 18.04, you can also use the <kbd>apt-get</kbd> command:</p>
<pre>&gt;<strong> sudo apt-get install -y supervisor</strong></pre>
<p class="graf graf--p">This installs two tools, <kbd>supervisor</kbd> and <kbd>supervisorctl</kbd>. <kbd>Supervisorctl</kbd> is intended to control the supervisor to add tasks, restart tasks, and more.</p>
<p class="graf graf--p">Let's use the <kbd>bookServer.go</kbd> program we created for illustrating process monitoring. Install the binary to the <kbd>$GOPATH/bin</kbd> directory using this command:</p>
<pre>&gt;<strong> go install $GOPATH/src/github.com/git-user/chapter12/bookServer/main.go</strong></pre>
<div class="packt_tip">Always add <kbd><span>$GOPATH/bin</span></kbd> to the system path. Whenever you install the project binary, it is available as a normal executable from the overall system environment. You can add following line to the <kbd>~/.profile</kbd> or <kbd>~/.bashrc</kbd> file:<br/>
<kbd>export PATH=$PATH:$GOPATH/bin</kbd></div>
<p class="graf graf--p">Now, create a new configuration file for <kbd>supervisor</kbd>:</p>
<pre class="mce-root"><strong>/etc/supervisor/conf.d/supervisord.conf</strong></pre>
<p>The supervisor reads this file and looks for processes to monitor and rules to apply when they are started/stopped.</p>
<p class="graf graf--p">You can add any number of configuration files and <kbd>supervisord</kbd> treats them as separate processes to run.</p>
<p class="graf graf--p">By default, we have a file called <kbd>supervisord.conf</kbd> in <kbd>/etc/supervisor/</kbd>. Look at it for further reference:</p>
<ul class="postList">
<li class="graf graf--li">The <kbd>[supervisord]</kbd> section gives the location of the log file for <kbd>supervisord</kbd>.</li>
<li class="graf graf--li"><kbd>[program:myserver]</kbd> is the task block that defines a command.</li>
</ul>
<p class="graf graf--p">Modify the content of the <kbd>supervisord.conf</kbd><span> f</span><span>ile to the following:</span></p>
<pre><strong>[supervisord]</strong><br/><strong>logfile = /tmp/supervisord.log</strong><br/><strong>[program:myserver]</strong><br/><strong>command=/root/workspace/bin/bookServer</strong><br/><strong>autostart=true</strong><br/><strong>autorestart=true</strong><br/><strong>redirect_stderr=tru</strong></pre>
<p>The command in the file is the command to launch the application server. <kbd>/root/workspace</kbd> is <kbd>$GOPATH</kbd>.</p>
<div class="packt_infobox">Please use an absolute path while running a command in Supervisord. Relative paths will not work by default.</div>
<p class="graf graf--p">Now, we can ask our <kbd>supervisorctl reread</kbd> to reread the configuration and start the task (process). For that, just say the following:</p>
<pre>&gt;<strong> supervisorctl reread</strong><br/>&gt;<strong> supervisorctl update</strong></pre>
<p class="graf graf--p">Then, launch the controller tool, <kbd>supervisorctl</kbd>:</p>
<pre class="graf graf--pre graf--empty">&gt;<strong> supervisorctl</strong></pre>
<p>You should see something like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e175901e-6972-410f-9f84-fd8e0be4bdec.png" style="width:40.50em;height:13.00em;"/></div>
<p>So, here, our book service is getting monitored by <kbd>Supervisor</kbd>. Let's try to kill the process manually and see what <kbd>Supervisor</kbd> does:</p>
<pre>&gt;<strong> kill 6886</strong></pre>
<p>Now, as soon as possible, <kbd>Supervisor</kbd> starts a new process (using a different pid) by running the binary:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb4790ea-1ec2-43c3-b4f9-42f5b6092c50.png" style="width:31.25em;height:6.25em;"/></div>
<p>This is very useful in a production where a service requires the least downtime. So, how do we start/stop an application service manually? well, you can use the <kbd>start</kbd> and <kbd>stop</kbd> commands from <kbd>supervisorctl</kbd> for those operations:</p>
<pre>&gt;<strong> supervisorctl&gt; stop myserver</strong><br/>&gt;<strong> supervisorctl&gt; start myserver</strong></pre>
<p class="mce-root">For more <span><span>information</span></span> about the supervisor, visit <a href="http://supervisord.org/">http://supervisord.org/</a>.</p>
<p class="mce-root">In the next section, we will try to simplify our deployment using containers. We will launch the application and Nginx as separate containers and establish a communication channel between them with the help of <kbd>docker-compose</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Makefile and Docker Compose-based deployment</h1>
                </header>
            
            <article>
                
<p>Until now, we have seen the manual deployment of a reverse proxy server (Nginx). Let's automate that by gluing things together. We are going to use a few tools, as follows:</p>
<ul>
<li><kbd>Make</kbd></li>
<li><kbd>docker-compose</kbd></li>
</ul>
<p>On Linux-based machines (Ubuntu and <span>Mac OS X</span>), <kbd>Make</kbd> is available as part of GCC (the C language toolchain). You can install <kbd>docker-compose</kbd> using the Python <kbd>pip</kbd> tool:</p>
<pre>&gt;<strong> sudo pip install docker-compose</strong></pre>
<p>On Windows OS, <kbd>docker-compose</kbd> is already available as part of Docker Desktop. Our goal is to bundle all deployable entities with one single <kbd>Make</kbd> command. <kbd>Makefile</kbd> is used to write control commands for the application. You should define a rule and the <kbd>Make</kbd> tool will execute it (<a href="https://www.gnu.org/software/make/manual/make.html#Rule-Example" target="_blank">https://www.gnu.org/software/make/manual/make.html#Rule-Example</a>).</p>
<p>Let's create a directory called <kbd>deploySetup</kbd>. It holds the whole code we are going to show. It has two directories – one for the app and another for Nginx:</p>
<pre>&gt;<strong> mkdir -p $GOPATH/src/github.com/git-user/chapter12/deploySetup<br/></strong><strong>mkdir $GOPATH/src/github.com/git-user/chapter12/deploySetup/nginx-conf</strong></pre>
<p>Now, let's copy our <kbd>bookServer</kbd> project into <kbd>deploySetup</kbd> like this:</p>
<pre>&gt;<strong> cp -r $GOPATH/src/github.com/git-user/chapter12/bookServer </strong><strong>$GOPATH/src/github.com/git-user/chapter12/deploySetup</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We need this to build an executable and copy it to the container. We should containerize both the Go application and Nginx in order to use them together. So, this is the plan for creating such a workflow:</p>
<ol>
<li>Create a <kbd>Dockerfile</kbd> for copying Go build into the container.</li>
<li>Create an Nginx configuration file called <kbd>nginx.conf</kbd> to copy into Nginx container.</li>
<li>Write a <kbd>Makefile</kbd> to build binary as well as deploy the containers.</li>
</ol>
<p>So, first, we should build and run the docker containers for application and Nginx. For that, we can use <kbd>docker-compose</kbd>. The <kbd>docker-compose</kbd> tool is very handy for managing multiple containers. It also builds and runs the containers on the fly.</p>
<p>In the <kbd>bookServer</kbd> directory, we need a Dockerfile that stores a project build binary. Let's say we build our project in <kbd>app</kbd>. We use Alpine Linux (lightweight) as the base Docker image, so we should target our build to that Linux platform. We should copy the binary on the Docker container and execute it. Let's say we chose the app path as <kbd>/go/bin/app</kbd>. Create a <kbd>Dockerfile</kbd> at this location:</p>
<pre>&gt;<strong> touch $GOPATH/src/github.com/git-user/chapter12/deploySetup/<br/>bookServer/Dockerfile</strong></pre>
<p>The <kbd>Dockerfile</kbd> looks like this:</p>
<pre>FROM alpine<br/>WORKDIR /go/bin/<br/>COPY app .<br/>CMD ["./app"]</pre>
<p>The Dockerfile is basically pulling the Alpine Linux image. It creates and sets the working directory for the application binary. Then, it copies the application binary to the given path, <kbd>/go/bin</kbd>. After copying, it runs the binary.</p>
<p>Before copying the application binary, someone has to build it. Let's write a <kbd>Make</kbd> command for building <kbd>bookServer</kbd> in this <kbd>Makefile</kbd> here:</p>
<pre>&gt;<strong> touch $GOPATH/src/github.com/git-user/chapter12/deploySetup/Makefile</strong></pre>
<p>It consists of commands and their respective executions. First, let's add a <kbd>build</kbd> command:</p>
<pre><strong>PROJECT_NAME=bookServer</strong><br/><strong>BINARY_NAME=app</strong><br/><strong>GOCMD=go</strong><br/><strong>GOBUILD=$(GOCMD) build</strong><br/><br/><br/><strong>build:</strong><br/>   <strong> $(info Building the book server binary...)</strong><br/><strong>    cd ${PROJECT_NAME} &amp;&amp; GOOS=linux GOARCH=arm ${GOBUILD} <br/>     -o "$(BINARY_NAME)" -v</strong></pre>
<p>The top-level variables in the <kbd>Makefile</kbd> declare the project root and build (binary) name. It also composes the build commands. The interesting command is <kbd>build</kbd>, which simply calls the Go build tool with a few <kbd>GOOS</kbd> and <kbd>GOARCH</kbd> <span>flags. </span>Those <kbd>build</kbd> flags are required to target a <kbd>binary</kbd> for Alpine Linux. Now from the <kbd>deploySetup</kbd> directory, run this command:</p>
<pre>&gt;<strong> make build</strong><br/><strong>Building the book server binary...</strong><br/><strong>cd bookServer &amp;&amp; GOOS=linux GOARCH=arm go build -o "app" -v</strong></pre>
<p>If you look in the <kbd>bookServer</kbd> directory, there is an <kbd>app</kbd> binary newly created. That is our application server. We are launching this binary directly in the container.</p>
<p>Now, let's create a <kbd>docker-compose</kbd> file that defines two services:</p>
<ul>
<li><kbd>App Service</kbd></li>
<li><kbd>Nginx Service</kbd></li>
</ul>
<p>Each of these services has instructions for where to build the image, which ports to be opened, which network bridge to be used, and more. For more information about <kbd>docker-compose</kbd>, please refer to (<kbd>https://docs.docker.com/compose/</kbd>). Let's create a <kbd>docker-compose.yml</kbd> file in the <kbd>deploySetup</kbd> directory:</p>
<pre><strong># Docker Compose file Reference (https://docs.docker.com/compose/compose-file/)</strong><br/><strong>version: '3'</strong><br/><br/><strong>services:</strong><br/><strong> # App Service</strong><br/><strong> app:</strong><br/><strong> build:</strong><br/><strong> context: ./bookServer</strong><br/><strong> dockerfile: Dockerfile</strong><br/><strong> expose:</strong><br/><strong> - 8000</strong><br/><strong> restart: unless-stopped</strong><br/><strong> networks:</strong><br/><strong> - app-network</strong><br/><br/><strong> # Nginx Service </strong><br/><strong> nginx:</strong><br/><strong> image: nginx:alpine</strong><br/><strong> restart: unless-stopped</strong><br/><strong> ports:</strong><br/><strong> - "80:80"</strong><br/><strong> - "443:443"</strong><br/><strong> volumes:</strong><br/><strong> - ./nginx-conf:/etc/nginx/conf.d</strong><br/><strong> depends_on: </strong><br/><strong> - app</strong><br/><strong> networks:</strong><br/><strong> - app-network</strong><br/><br/><strong>networks:</strong><br/><strong> app-network:</strong><br/><strong> driver: bridge</strong></pre>
<p>In this file, we are defining a network called <kbd>app-network</kbd> and two services, namely <kbd>app</kbd> and <kbd>nginx</kbd>. For the <kbd>app</kbd> service, we are pointing to <kbd>bookServer</kbd> to pick <kbd>Dockerfile</kbd> to build an image.</p>
<div class="packt_tip" style="padding-left: 60px">We don't need the <kbd>supervisord</kbd> tool in the Docker deployment because <kbd>docker-compose</kbd> takes care of restarting crashed containers. It takes a decision from the <kbd>restart: unless-stopped</kbd> option in the <kbd>docker-compose</kbd> file.</div>
<p>For the <kbd>nginx</kbd> service in the Compose file, it pulls a default <kbd>nginx:alpine</kbd> image from Docker Hub. However, as we have to copy our own configuration file to the Nginx server, we should create a file in the <kbd>nginx-conf</kbd> directory:</p>
<pre>&gt;<strong> touch $GOPATH/src/github.com/git-user/chapter12/deploySetup/nginx-conf/nginx.conf</strong></pre>
<p>We can mount our configuration file in the <kbd>nginx-conf</kbd> directory to <kbd>/etc/nginx/conf.d</kbd> in the container using the <kbd>volumes</kbd> option. Both services use the same network so that they can discover each other. The Nginx service exposes port 80 to host, but <kbd>app</kbd> only opens up its port internally on <kbd>8000</kbd>.</p>
<p>Our <kbd>nginx.conf</kbd> file should have the proxy information like this:</p>
<pre>upstream service {<br/>  server app:8000;<br/>}<br/><br/>server {<br/>        listen 80 default_server;<br/>        listen [::]:80 default_server;<br/><br/>        location / {<br/>                proxy_pass http://service;<br/>        }<br/>}</pre>
<p>The <kbd>nginx.conf</kbd> file defines an upstream service. It connects to app service in <kbd>docker-compose.yml</kbd>. This is possible because of the bridging of the network. <kbd>docker-compose</kbd> takes care of assigning a hostname for the application container. In the last block, we are defining a location that reverses proxy requests to the <kbd>upstream service</kbd>.</p>
<p>Now, everything is ready. The <kbd>docker-compose.yml</kbd> file, Supervisord configuration, and Nginx configuration are in place. <kbd>docker-compose</kbd> has an option to start Docker containers by building images as specified in the <kbd>compose</kbd> file. We can bring containers up using this command:</p>
<pre>&gt;<strong> docker-compose up --build</strong></pre>
<p>Let's update the <kbd>Makefile</kbd> to add two new commands—one to <kbd>deploy</kbd>, and another one to <kbd>build</kbd> and <kbd>deploy</kbd> containers:</p>
<pre><strong>PROJECT_NAME=bookServer</strong><br/><strong>BINARY_NAME=app</strong><br/><strong>GOCMD=go</strong><br/><strong>GOBUILD=$(GOCMD) build</strong><br/><br/><strong>all:</strong><br/><strong>    make build &amp;&amp; make deploy</strong><br/><br/><strong>build:</strong><br/><strong>    $(info Building the book server binary...)</strong><br/><strong>    cd ${PROJECT_NAME} &amp;&amp; GOOS=linux GOARCH=arm ${GOBUILD}<br/>     -o "$(BINARY_NAME)" -v</strong><br/><br/><strong>deploy:</strong><br/><strong>    docker-compose rm -f</strong><br/><strong>    docker-compose up --build</strong></pre>
<p>With the <kbd>deploy</kbd> command, we are cleaning up the containers first and then launching new ones. We added one more command called <kbd>all</kbd>.</p>
<p>The <kbd>make all</kbd> command is a universal command that executes when no command is passed. For example, consider the following:</p>
<pre>&gt;<strong> make all</strong></pre>
<p>This executes <kbd>make all</kbd>. Our plan is to build the binary, and start the Docker containers using <kbd>docker-compose</kbd>.</p>
<p>Now, we have everything we need. From Terminal, run <kbd>make</kbd> to see the servers up and running:</p>
<pre>&gt;<strong> make</strong><br/><br/><strong>make build &amp;&amp; make deploy</strong><br/><strong>Building the book server binary...</strong><br/><strong>cd bookServer &amp;&amp; CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o "app" -v</strong><br/><strong>docker-compose rm -f</strong><br/><strong>No stopped containers</strong><br/><strong>docker-compose up --build</strong><br/><strong>Building app</strong><br/><strong>Step 1/4 : FROM alpine</strong><br/><strong> ---&gt; c85b8f829d1f</strong><br/><strong>Step 2/4 : WORKDIR /go/bin/</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; cc95562482f0</strong><br/><strong>Step 3/4 : COPY app .</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; 865952cdc77a</strong><br/><strong>Step 4/4 : CMD ["./app"]</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; 18d0f4ec074f</strong><br/><strong>Successfully built 18d0f4ec074f</strong><br/><strong>Successfully tagged deploysetup_app:latest</strong><br/><strong>Creating deploysetup_app_1 ... done</strong><br/><strong>Creating deploysetup_nginx_1 ... done</strong><br/><strong>Attaching to deploysetup_app_1, deploysetup_nginx_1</strong></pre>
<p>You can also confirm that the containers are up and running with the <kbd>docker ps</kbd> command:</p>
<pre>&gt;<strong> docker ps</strong><br/><strong>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong><br/><strong>5f78ea862376 nginx:alpine "nginx -g 'daemon of…" About a minute ago Up About a minute 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp deploysetup_nginx_1</strong><br/><strong>44973a15783a deploysetup_app "/usr/bin/supervisor…" About a minute ago Up About a minute 8000/tcp deploysetup_app_1</strong></pre>
<p>Now, make a <kbd>curl</kbd> request to see the server output:</p>
<pre>&gt;<strong> curl -X GET "http://localhost/api/books"<br/><br/>{"ID":123,"ISBN":"0-201-03801-3","Author":"Donald Knuth","PublishedYear":"1968"}</strong></pre>
<p>Instead of calling the API with the port, now the client is accessing the REST API via Nginx. Nginx routes the request to the application server that is started in the container. With this deployment setup, we can make our code changes and just run the <kbd>make</kbd> <span>command </span>to update the application service.</p>
<p>This is how Go applications can be containerized using <kbd>Makefile</kbd> and <kbd>docker-compose</kbd>. Servers stop gracefully when you hit <em>Ctrl </em>+ <em>C</em>. If you want them to run in the background, just add a <kbd>-d</kbd> flag to the <kbd>Makefile</kbd> <kbd>deploy</kbd> command:</p>
<div>
<pre>&gt;<strong><span> docker-compose up --build -d</span></strong></pre></div>
<p><kbd>-d</kbd> stands for run containers as daemons. Now, containers silently run in the background, and logs for the <kbd>nginx</kbd> and <kbd>app</kbd> containers can be seen with the <kbd>docker inspect CONTAINER_ID</kbd> command.</p>
<div class="packt_tip packt_infobox">Things may not work properly if the base image (Alpine Linux, in our case) of the container is changed. Always consider the image-specific default configuration path for Nginx (<kbd><span>/etc/nginx/conf.d</span></kbd>) to copy the custom configuration.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter demonstrated how to prepare API services for deployment in production. We need a web proxy server, application server, and a process monitor for deployment.</p>
<p>Nginx is a web proxy server that can pass requests to multiple servers running on the same host or on a different host.</p>
<p>We learned how to install Nginx and start configuring it. Nginx provides features such as load balancing and rate limiting, which are very important features for APIs to have. Load balancing is the process of distributing loads among similar servers. We explored all the available types of loading mechanisms: Round Robin, IP Hash, Least Connection, and more. Then, we looked at how to add access control to our servers by allowing and denying a few sets of IP addresses. We have to add rules in the Nginx server blocks to achieve that.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, we saw a process monitor named <kbd>Supervisord</kbd> that brings a crashed application back to life. We saw how to install Supervisord and also launch <kbd>supervisorctl</kbd>, a command-line application to control running servers. We then tried to automate the deployment process by creating a <kbd>Makefile</kbd> and <kbd>docker-compose</kbd> file. We also explored how to containerize a Go application along with Nginx using Docker and Docker Compose. In the real world, containers are the preferable way to deploy software.</p>
<p>In the next chapter, we are going to demonstrate how to make our REST services publicly visible with the help of AWS EC2 and Amazon API Gateway.</p>
<div class="content-3WfBL_0" style="background-color: #ffffff">
<div class="outputBox-qe9A4_0">
<div class="outputBox-3oESn_0"><span class="outputBox-13Ovx_0"> </span></div>
<div class="outputBox-17RAm_0"/>
</div>
</div>


            </article>

            
        </section>
    </body></html>