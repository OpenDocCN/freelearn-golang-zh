- en: Containerizing REST Services for Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to containerize our Go applications using
    a few tools such as Docker, Docker Compose, Nginx, and Supervisord. Containerization
    is required to avoid platform dependency during deployment of an application.
    To deploy an application properly, we must prepare an ecosystem. That ecosystem
    consists of a web server, an application server, and a process monitor. This chapter
    deals with how to take our API server from a standalone application to a production-grade
    service.
  prefs: []
  type: TYPE_NORMAL
- en: In recent times, most cloud providers tend to host web applications. Some big
    players such as AWS, Azure, Google Cloud Platform, along with start-ups such as
    DigitalOcean and Heroku are a few such examples. In the upcoming sections, we
    will focus on making a platform ready for deploying REST services. In the next
    chapter, we will look at how to deploy this ecosystem on a famous cloud provider,
    AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx is a web server that can be a reverse proxy for a web application. It
    can also act as a load balancer when multiple instances of the server are running.
    Supervisord makes sure that an application server is up and running in the event
    of a crash or a system restart. An application server/REST service are both the
    same, so please consider them in equal context throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Nginx server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a reverse proxy server?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a Go service using Nginx
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring our Go API server with Supervisord
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Makefile` and Docker Compose-based deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the software that should be preinstalled for running the code
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OS: Linux (Ubuntu 18.04)/Windows 10/Mac OS X >=10.13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go stable version compiler >= 1.13.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dep: A dependency management tool for Go >= 0.5.3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker version >= 18.09.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose >= 1.23.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can download the code for this chapter from [https://github.com/PacktPublishing/Hands-On-Restful-Web-services-with-Go/tree/master/chapter12](https://github.com/PacktPublishing/Hands-On-Restful-Web-services-with-Go/tree/master/chapter12).
    Clone the code and use the code samples in the `chapter12` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Nginx server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nginx is a high-performing web server and load balancer. It is well suited for
    deploying high-traffic websites and API servers. Even though this decision is
    opinionated, it is a community-driven, industry-strong web server. It is similar
    to the Apache2 web server.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx can also act as a reverse proxy server that allows us to redirect our
    HTTP requests to multiple application servers running on the same network. The
    main contender of Nginx is Apache's `httpd`.Nginx is an excellent static file
    server that can be used by web clients. Since we are dealing with APIs, we will
    take a look at how to deal with HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can access Nginx in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Installation on a bare machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a preinstalled Docker container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand both in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Installation on a bare machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Ubuntu 18.04, use these commands to install Nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'On Mac OS X , you can install it with `brew`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**brew **[(https://brew.sh/)](https://brew.sh/) is a very useful software packaging
    system for Mac OS X users. My recommendation is that you use it for installing
    software. Once it is successfully installed, you can check it by opening the machine
    IP in the browser. Open `http://localhost/` on your web browser. You should see
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3bab13d-ea36-4be2-ac43-056cd178786a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you see the preceding message, that means Nginx has been successfully installed.
    It serves on port `80` and serves the default page. On Mac OS X, the default Nginx
    listening port will be `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu (Linux), the file will be on this path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the file and search for a server block. If it is listening on port `80`,
    everything is fine. However, if it is on some other port, for example `8080`,
    then change it to `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, everything is ready. The server runs on the `80` HTTP port, which means
    a client can access it using a URL (`http://localhost/`). This basic server serves
    static files from a directory called `html`. The `root` parameter can be modified
    to any directory where we place our web assets. You can check the status of Nginx
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Nginx for the Windows operating system is quite basic and is not really intended
    for production-grade deployments. Open source developers usually prefer Debian
    or Ubuntu servers for deploying the API servers with Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: We can also get a Docker image that has Nginx installed already. In the next
    section, we will demonstrate how to install it as a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Installation via a Docker container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting a container that has preinstalled Nginx has two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to ship containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can destroy and recreate the containers any number of times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get the latest Nginx image and start a container, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This pulls the `nginx` image from Docker Hub (make sure you are connected to
    the internet). If the image is already pulled, it reuses that. Then, it starts
    a container with the name of `nginxServer` and serves it on port `80`. Now, visit
    `http://localhost` from your browser and you will see the Nginx home page.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the preceding command is not useful for configuring Nginx after starting
    the container. We have to mount a directory from localhost to the container or
    copy files to the container to make changes to the Nginx configuration file. Let''s
    modify the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The extra command is `--mount`, which mounts a file/directory from the source
    (*host*) to the destination (*container*). If you modify a file on the host system
    in that directory, then it also reflects on the container. The `readonly` option
    stops users/system processes modifying the Nginx configuration inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding command, we are mounting the Nginx configuration file, `nginx.conf`.
    We use the Docker container-based deployment in the latter part of this chapter,
    where we use `docker-compose` to deploy our application.
  prefs: []
  type: TYPE_NORMAL
- en: What is a reverse proxy server?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **reverse proxy server** is a server that holds the information regarding
    the original servers in it. It acts as the front-facing entity for the client
    request. Whenever a client makes an HTTP request, it can directly go to the application
    server. However, if the application server is written in a programming language,
    then you need a translator that can turn the application response into a  response
    understandable by the clients. **Common Gateway Interface** (**CGI**) does the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: We can run a simple Go HTTP server and it can serve incoming requests (no CGI
    is required). We should also protect our application server from **Denial of Service**
    (**DoS**) attacks. So, why are we using another server called Nginx? Well, because
    it brings a lot of things into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of having a reverse proxy server (Nginx) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It can act as a load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can provide access control and rate limiting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can sit in front of a cluster of applications and redirect HTTP requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can serve a filesystem with a good performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It streams media very well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the same machine is running on multiple applications, then we can bring all
    of those  applications under one umbrella. Nginx can also act as the API gateway
    that can be the starting point for multiple API endpoints. We will explore a dedicated
    API gateway in the next chapter, but it is good to know that Nginx can also work
    as one.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx works as a traffic router for incoming requests. It is a protective shield
    for our application servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/543de7d3-c339-49bc-b713-d5b5f293983b.png)'
  prefs: []
  type: TYPE_IMG
- en: It has three apps running in different programming languages and **Client**
    only knows a single API endpoint. Let's say that all of the apps run on different
    ports.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the diagram **Client** is talking directly to **Nginx** instead
    of the ports where other applications are running. In the diagram, Go is running
    on port `8000` and other applications are running on different ports. This means
    that the different servers are providing different API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Without Nginx, if the client wishes to call an API, it needs to access three
    different endpoints (ports). Instead, if we have Nginx, it can act as a reverse
    proxy server for all three and simplifies the client request-response cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx is also an upstream server. An upstream server serves the requests from
    one server to the other. From the diagram, you can see that a Python app can request
    an API endpoint from a Go app and Nginx will take care of routing them.
  prefs: []
  type: TYPE_NORMAL
- en: Important Nginx paths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few important Nginx paths that we need to know about in order to
    work with the proxy server. In Nginx, we can host multiple sites (`www.example1.com`,
    `www.example2.com`, and so on) at the same time. This means that many API servers
    can be run under one Nginx instance.
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware of the following paths in the table to configure Nginx properly.
    An advanced deployment may require bypassing authentication (for example, the
    Health check API), rate limiting, and a backup of the logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Path** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `Configuration` | `/etc/nginx/nginx.conf` | This is the base Nginx configuration
    file. It can be used as the default file. |'
  prefs: []
  type: TYPE_TB
- en: '| `Configuration` | `/etc/nginx/sites-available/` | If we have multiple sites
    running within Nginx, we can have a configuration file for each site. |'
  prefs: []
  type: TYPE_TB
- en: '| `Configuration` | `/etc/nginx/sites-enabled/` | These are the sites currently
    activated on Nginx. |'
  prefs: []
  type: TYPE_TB
- en: '| `Log` | `/var/log/nginx/access.log` | This log file records the server activity,
    such as timestamps and API endpoints.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Log` | `/var/log/nginx/error.log` | This log file logs all proxy server-related
    errors, such as disk space, filesystem permissions, and more. |'
  prefs: []
  type: TYPE_TB
- en: These paths are in the Linux operating system. For Mac OS X, use `/usr/local/nginx`
    as the base path.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore server blocks that are mainly used for
    configuring applications with Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: Using server blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Server blocks are the actual configuration pieces that tell the server what
    to serve and on which port to listen. We can define multiple server blocks in
    the `sites-available` folder. On Ubuntu, the location will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'On Mac OS X, the location will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Until we create a symlink from the `sites-available` to the `sites-enabled`
    directory, the configuration has no effect. So, always create a symlink for `sites-available`
    to `sites-enabled` for every new configuration you create.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Go service using Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have already discussed, Nginx can be a reverse proxy for a Go application.
    Let''s say that we have a server that provides a REST API to access book data.
    A client can send a request and get it back in JSON. The server also stores all
    the logs in an external file. Let''s take a look at the steps to create this application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s name our project `bookServer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This file is a basic Go server to illustrate the functioning of a reverse proxy
    server. We first run our program on port `8000`. Then, we add a configuration
    that maps `8000` (Go's running port) to `80` (the Nginx HTTP port).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s write the code. We will use a few packages for our server. We can
    use Go''s built-in `net/http` package for server implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our server needs a struct to hold the book information. Let''s create a
    struct with fields such as `ID`, `ISBN`, `Author`, and `PublishedYear`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now goes our `main` function. It should open a file for writing logs. We can
    do that using the `os.Openfile` function. This takes the file and mode as arguments.
    Let''s name the file `app.log`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The file permission, `os.O_RDWR|os.O_CREATE|os.O_APPEND`, allows the Go program
    to create, write, and append to the file. `log.SetOutput(f)` redirects app logs
    to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a function handler and attach it to a route using the `net/http`
    function. The handler converts a struct into JSON and returns it as an HTTP response.
    Also, attach that handler to a route called `/api/books`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The previous code block essentially returns a book whenever a client requests `/api/books`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, start an HTTP server that serves the whole application on port `8000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This finishes the main program.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run our application and see whether it is running correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open a shell and make a `curl` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the client needs to request to port `8000`. So, how can we proxy this
    server using Nginx? As we previously discussed, we need to edit the default `sites-available`
    server block, called `default`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the preceding file, find the server block, and add `proxy_pass` to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This section of the `config` file is called a `server` block. This controls
    the setting up of the proxy server where `listen` says where `nginx` should listen.
    `root`and `index` point to the static files if we need to serve any file. `server_name`is
    the domain name of yours.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we don''t have a domain name with us, it is just localhost. `location`is
    the key section here. In `location`, we can define our `proxy_pass`, which can
    reverse proxy to a given `URL:PORT`. Since our Go application is running on port `8000`,
    we mentioned it there. Let''s try running our app on a different domain, `example.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can give the same name as a parameter to `proxy_pass`. In order to take
    this configuration into effect, we need to restart the Nginx server. You can do
    that using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, make a `curl` request to `http://localhost` and you will see the Go application''s
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `location` is a directive that defines a **Unified Resource Identifier**
    (**URI**) that can proxy a given `server:port` combination. This means that, by
    defining various URI, we can proxy multiple applications running on the same server.
    It looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, three applications are running on different ports. These, after being
    added to our configuration file, can be accessed by the client as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we explore how to load balance API requests to multiple
    instances of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing with Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practical cases, multiple servers are deployed instead of one for handling
    huge sets of incoming requests for APIs. But, who should forward an incoming client request
    to a server instance? A load balancer does that job. Load balancing is a process
    where the central server distributes the load to various servers based on certain
    criteria. Refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/929be2ee-6e11-4130-9016-93ce0a4e8b74.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A load balancer employs few strategies such as `Round Robin` or `Least Connection`
    for routing requests to instances. Let''s take a look at what each does in a simple
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Load-balancing method** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `Round Robin` | The incoming requests are uniformly distributed across servers
    based on the criteria of server weights. |'
  prefs: []
  type: TYPE_TB
- en: '| `Least Connection` | Requests are sent to the server that is currently serving
    the least number of clients. |'
  prefs: []
  type: TYPE_TB
- en: '| `IP Hash` | This is used to send the requests from a given client''s IP to
    the given server. Only when that server is not available is it given to another
    server.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Least Time` | A request from the client is sent to the machine with the
    lowest average latency (the time-to-serve client) and the least number of active
    connections. |'
  prefs: []
  type: TYPE_TB
- en: We can set which strategy to apply for load balancing in the Nginx configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore how load balancing is practically achieved in Nginx for our
    Go API servers. The first step in this process is to create an `upstream cluster`
    in the `http` section of the Nginx configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, servers are the IP addresses or domain names of the servers running the
    same code. We are defining an upstream called cluster here. It is a server group
    that we can refer to in our `location` directive. Weights should be given in proportion
    to the resources available. In the preceding code, `site1` is given a higher weight
    because it may be a bigger instance (memory and CPU). Now, in the `location` directive,
    we can specify the server group with the `proxy_pass` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the proxy server that is running will pass requests to the machines in
    the cluster for all API endpoints hitting the `/` endpoint. The default request
    routing algorithm will be `Round Robin`, which means that all of the server''s
    turns will be repeated one after the other. If we need to change it, we can mention
    that in the upstream definition. Take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration says to *create a cluster of three machines and
    add load balancing method as least connections*. `least_conn` is the string we
    used to mention the load balancing method. The other values could be `ip_hash`
    or `least_time`. You can try this by having a set of machines in the **Local Area
    Network** (**LAN**). Otherwise, we can have Docker installed with multiple virtual
    containers as different machines to test out load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: We need to add that `http` block in the `/etc/nginx/nginx.conf` file, whereas
    the server block is in `/etc/nginx/sites-enabled/default`.It is better to separate
    these two settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a small exercise: try to run three `bookServer` instances on different
    ports and enable load balancing on Nginx. In the next section, we''ll examine
    how to rate limit an API in Nginx for certain clients.'
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting our REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also limit the rate of access to our Nginx proxy server by rate limiting.
    This provides a directive called `limit_conn_zone` ([http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone](http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone)).
    The format of it is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`client_type` can be one of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: An IP address (limit requests from a given IP address)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A server name (limit requests from a server)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zone_type`also changes in correspondence to `client_type`. It takes values
    as per the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client type** | **Zone type** |'
  prefs: []
  type: TYPE_TB
- en: '| `$binary_remote_address` | `addr` |'
  prefs: []
  type: TYPE_TB
- en: '| `$server_name` | `servers` |'
  prefs: []
  type: TYPE_TB
- en: 'Nginx has to save a few things in memory to remember the IP addresses and servers
    for rate limiting. The `size` parameter is the storage that we allocate for Nginx
    to perform its memory operations. It takes values such as 8 m (8 MB) or 16 m (16
    MB). Now, let''s take a look at where to add these settings. The preceding one
    should be added as a global setting to the `http` directive in the `nginx.conf`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This allocates the shared memory for Nginx to use. Now, in the server directive
    of `sites-available/default`, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of connections for the given server will not exceed `1000`
    in the preceding configuration using `limit_conn`. If we try to put the rate limit
    from a given IP address to the client, then use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This setting stops a client (that is, IP address) from opening more than one
    connection to the server (for example, in an online railway booking session, a
    user can only use one session per IP address to book tickets). If we have a file
    that the client downloads and need to set a bandwidth constraint, use `limit_rate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we can control the client's interaction with our services that
    are proxied under Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: Securing our Nginx proxy server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the most important piece in the Nginx setup. In this section, we will
    look at how to restrict access to our server using basic authentication. This
    is very important for our REST API servers because, suppose we have servers *X*, *Y*,
    and *Z* that can talk to each other. *X* can serve clients directly, but *X* consults
    *Y* and *Z* for some information by calling an internal API. We should prevent
    clients from accessing *Y* and *Z*. We can allow or deny IP addresses using the `nginx`
    access module. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration tells Nginx to allow requests from clients ranging `192.168.1.1/24`,
    excluding `192.168.1.2`. The next line tells us to allow requests from the same
    host and block all other requests from any other client. The complete server block
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For more information regarding this, you can refer to the documentation at [http://nginx.org/en/docs/http/ngx_http_access_module.html?_ga=2.117850185.1364707364.1504109372-1654310658.1503918562](http://nginx.org/en/docs/http/ngx_http_access_module.html?_ga=2.117850185.1364707364.1504109372-1654310658.1503918562).
    We can also add password-secured access to our Nginx serving static files. It
    is mostly not applicable to the API because there, the application takes care
    of authenticating the user. The whole idea is to only allow the IP that is approved
    by us and deny all other requests.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx can only serve requests when the application server's health is good.
    If the application crashes, we have to restart it manually. A crash can occur
    from a system shutdown, a problem in the network storage, or various other external
    factors. In the next section, we will discuss a process monitoring tool called
    `supervisord` that can automatically restart a crashed application.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring our Go API server with Supervisord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, a web application server may stop due to an operating system restarting
    or crashing. Whenever a web server is killed, it is someone's job to bring it
    back to life. It is wonderful if that is automated. Supervisord is a tool that
    comes to the rescue. To make our API server run all of the time, we need to monitor
    it and recover it quickly. Supervisord is a generic tool that can monitor running
    processes (systems) and can restart them when they are terminated.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Supervisord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can easily install Supervisord using Python's `pip` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu 18.04, you can also use the `apt-get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This installs two tools, `supervisor` and `supervisorctl`. `Supervisorctl` is
    intended to control the supervisor to add tasks, restart tasks, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `bookServer.go` program we created for illustrating process
    monitoring. Install the binary to the `$GOPATH/bin` directory using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Always add `$GOPATH/bin` to the system path. Whenever you install the project
    binary, it is available as a normal executable from the overall system environment.
    You can add following line to the `~/.profile` or `~/.bashrc` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`export PATH=$PATH:$GOPATH/bin`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a new configuration file for `supervisor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The supervisor reads this file and looks for processes to monitor and rules
    to apply when they are started/stopped.
  prefs: []
  type: TYPE_NORMAL
- en: You can add any number of configuration files and `supervisord` treats them
    as separate processes to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, we have a file called `supervisord.conf` in `/etc/supervisor/`.
    Look at it for further reference:'
  prefs: []
  type: TYPE_NORMAL
- en: The `[supervisord]` section gives the location of the log file for `supervisord`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[program:myserver]` is the task block that defines a command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modify the content of the `supervisord.conf` file to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The command in the file is the command to launch the application server. `/root/workspace`
    is `$GOPATH`.
  prefs: []
  type: TYPE_NORMAL
- en: Please use an absolute path while running a command in Supervisord. Relative
    paths will not work by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can ask our `supervisorctl reread` to reread the configuration and
    start the task (process). For that, just say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, launch the controller tool, `supervisorctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e175901e-6972-410f-9f84-fd8e0be4bdec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, here, our book service is getting monitored by `Supervisor`. Let''s try
    to kill the process manually and see what `Supervisor` does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as soon as possible, `Supervisor` starts a new process (using a different
    pid) by running the binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb4790ea-1ec2-43c3-b4f9-42f5b6092c50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is very useful in a production where a service requires the least downtime.
    So, how do we start/stop an application service manually? well, you can use the `start`
    and `stop` commands from `supervisorctl` for those operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: For more information about the supervisor, visit [http://supervisord.org/](http://supervisord.org/).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will try to simplify our deployment using containers.
    We will launch the application and Nginx as separate containers and establish
    a communication channel between them with the help of `docker-compose`.
  prefs: []
  type: TYPE_NORMAL
- en: Makefile and Docker Compose-based deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we have seen the manual deployment of a reverse proxy server (Nginx).
    Let''s automate that by gluing things together. We are going to use a few tools,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Make`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-compose`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On Linux-based machines (Ubuntu and Mac OS X), `Make` is available as part
    of GCC (the C language toolchain). You can install `docker-compose` using the
    Python `pip` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: On Windows OS, `docker-compose` is already available as part of Docker Desktop.
    Our goal is to bundle all deployable entities with one single `Make` command.
    `Makefile` is used to write control commands for the application. You should define
    a rule and the `Make` tool will execute it ([https://www.gnu.org/software/make/manual/make.html#Rule-Example](https://www.gnu.org/software/make/manual/make.html#Rule-Example)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a directory called `deploySetup`. It holds the whole code we
    are going to show. It has two directories – one for the app and another for Nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s copy our `bookServer` project into `deploySetup` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We need this to build an executable and copy it to the container. We should
    containerize both the Go application and Nginx in order to use them together.
    So, this is the plan for creating such a workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `Dockerfile` for copying Go build into the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an Nginx configuration file called `nginx.conf` to copy into Nginx container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a `Makefile` to build binary as well as deploy the containers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, first, we should build and run the docker containers for application and
    Nginx. For that, we can use `docker-compose`. The `docker-compose` tool is very
    handy for managing multiple containers. It also builds and runs the containers
    on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `bookServer` directory, we need a Dockerfile that stores a project build
    binary. Let''s say we build our project in `app`. We use Alpine Linux (lightweight)
    as the base Docker image, so we should target our build to that Linux platform.
    We should copy the binary on the Docker container and execute it. Let''s say we
    chose the app path as `/go/bin/app`. Create a `Dockerfile` at this location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dockerfile` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The Dockerfile is basically pulling the Alpine Linux image. It creates and sets
    the working directory for the application binary. Then, it copies the application
    binary to the given path, `/go/bin`. After copying, it runs the binary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before copying the application binary, someone has to build it. Let''s write
    a `Make` command for building `bookServer` in this `Makefile` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'It consists of commands and their respective executions. First, let''s add
    a `build` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The top-level variables in the `Makefile` declare the project root and build
    (binary) name. It also composes the build commands. The interesting command is
    `build`, which simply calls the Go build tool with a few `GOOS` and `GOARCH` flags. Those
    `build` flags are required to target a `binary` for Alpine Linux. Now from the
    `deploySetup` directory, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: If you look in the `bookServer` directory, there is an `app` binary newly created.
    That is our application server. We are launching this binary directly in the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a `docker-compose` file that defines two services:'
  prefs: []
  type: TYPE_NORMAL
- en: '`App Service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Nginx Service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these services has instructions for where to build the image, which
    ports to be opened, which network bridge to be used, and more. For more information
    about `docker-compose`, please refer to (`https://docs.docker.com/compose/`).
    Let''s create a `docker-compose.yml` file in the `deploySetup` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In this file, we are defining a network called `app-network` and two services,
    namely `app` and `nginx`. For the `app` service, we are pointing to `bookServer`
    to pick `Dockerfile` to build an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t need the `supervisord` tool in the Docker deployment because `docker-compose`
    takes care of restarting crashed containers. It takes a decision from the `restart:
    unless-stopped` option in the `docker-compose` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `nginx` service in the Compose file, it pulls a default `nginx:alpine`
    image from Docker Hub. However, as we have to copy our own configuration file
    to the Nginx server, we should create a file in the `nginx-conf` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We can mount our configuration file in the `nginx-conf` directory to `/etc/nginx/conf.d`
    in the container using the `volumes` option. Both services use the same network
    so that they can discover each other. The Nginx service exposes port 80 to host,
    but `app` only opens up its port internally on `8000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `nginx.conf` file should have the proxy information like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `nginx.conf` file defines an upstream service. It connects to app service
    in `docker-compose.yml`. This is possible because of the bridging of the network.
    `docker-compose` takes care of assigning a hostname for the application container.
    In the last block, we are defining a location that reverses proxy requests to
    the `upstream service`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, everything is ready. The `docker-compose.yml` file, Supervisord configuration,
    and Nginx configuration are in place. `docker-compose` has an option to start
    Docker containers by building images as specified in the `compose` file. We can
    bring containers up using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s update the `Makefile` to add two new commands—one to `deploy`, and another
    one to `build` and `deploy` containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: With the `deploy` command, we are cleaning up the containers first and then
    launching new ones. We added one more command called `all`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `make all` command is a universal command that executes when no command
    is passed. For example, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This executes `make all`. Our plan is to build the binary, and start the Docker
    containers using `docker-compose`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have everything we need. From Terminal, run `make` to see the servers
    up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also confirm that the containers are up and running with the `docker
    ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, make a `curl` request to see the server output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Instead of calling the API with the port, now the client is accessing the REST
    API via Nginx. Nginx routes the request to the application server that is started
    in the container. With this deployment setup, we can make our code changes and
    just run the `make` command to update the application service.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how Go applications can be containerized using `Makefile` and `docker-compose`.
    Servers stop gracefully when you hit *Ctrl *+ *C*. If you want them to run in
    the background, just add a `-d` flag to the `Makefile` `deploy` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`-d` stands for run containers as daemons. Now, containers silently run in
    the background, and logs for the `nginx` and `app` containers can be seen with
    the `docker inspect CONTAINER_ID` command.'
  prefs: []
  type: TYPE_NORMAL
- en: Things may not work properly if the base image (Alpine Linux, in our case) of
    the container is changed. Always consider the image-specific default configuration
    path for Nginx (`/etc/nginx/conf.d`) to copy the custom configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated how to prepare API services for deployment in production.
    We need a web proxy server, application server, and a process monitor for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx is a web proxy server that can pass requests to multiple servers running
    on the same host or on a different host.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned how to install Nginx and start configuring it. Nginx provides features
    such as load balancing and rate limiting, which are very important features for
    APIs to have. Load balancing is the process of distributing loads among similar
    servers. We explored all the available types of loading mechanisms: Round Robin,
    IP Hash, Least Connection, and more. Then, we looked at how to add access control
    to our servers by allowing and denying a few sets of IP addresses. We have to
    add rules in the Nginx server blocks to achieve that.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw a process monitor named `Supervisord` that brings a crashed
    application back to life. We saw how to install Supervisord and also launch `supervisorctl`,
    a command-line application to control running servers. We then tried to automate
    the deployment process by creating a `Makefile` and `docker-compose` file. We
    also explored how to containerize a Go application along with Nginx using Docker
    and Docker Compose. In the real world, containers are the preferable way to deploy
    software.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to demonstrate how to make our REST services
    publicly visible with the help of AWS EC2 and Amazon API Gateway.
  prefs: []
  type: TYPE_NORMAL
