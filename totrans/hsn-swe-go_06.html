<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Art of Testing</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"Program testing can be used to show the presence of bugs, but never to show their absence!"</div>
<div class="packt_quote CDPAlignRight CDPAlign"><span>- Edsger Dijkstra</span></div>
<p>Software systems are destined to grow and evolve over time. Open or closed source software projects have one thing in common: their<span> </span><em>complexity</em><span> </span>seems to follow an upward curve as the number of engineers working on the code base increases. To this end, having a comprehensive set of tests for the code base is of paramount importance. This chapter performs a deep dive into the different types of testing that can be applied to Go projects.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Identifying the differences between high-level primitives such as stubs, mocks, spies, and fake objects that you can use while writing unit tests as substitutes for objects that are used inside the code under test</li>
<li>Comparing black-box and white-box testing: what's the difference and why both are needed for writing comprehensive test suites</li>
<li>Differences between integration and functional (end-to-end) testing</li>
<li>Advanced test concepts: smoke tests, and one of my personal favorites <span>– </span>chaos tests!</li>
<li>Tips and tricks for writing clean tests in Go and pitfalls that you need to avoid</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The full code for the topics discussed within this chapter have been published to this book's GitHub repository under the<span> </span><kbd>Chapter04</kbd> folder.</p>
<div class="packt_infobox">You can access this book's GitHub repository by going to <a href="https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang">https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang</a>.</div>
<p>To get you up and running as quickly as possible, each example project includes a<span> makefile</span><span> </span><span>that defines the following set of targets:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Makefile target</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td><kbd>deps</kbd></td>
<td>Install any required dependencies</td>
</tr>
<tr class="even">
<td><kbd>test</kbd></td>
<td>Run all the tests and report coverage</td>
</tr>
<tr class="odd">
<td><kbd>lint</kbd></td>
<td>Check for lint errors</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As with all the chapters in this book, you will need a fairly recent version of Go, which you can download at<span> <a href="https://golang.org/dl/">https://golang.org/dl/</a></span><em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unit testing</h1>
                </header>
            
            <article>
                
<p>By definition, a unit is the smallest possible bit of code that we can test. In the context of Go programming, this would typically be a<span> </span><em>single function</em>. However, according to the SOLID design principles that we explored in the previous chapters, each<span> </span><em>Go package</em><span> </span>could also be construed as an independent unit and tested as such.</p>
<div class="packt_infobox">Th<span>e term</span><span> </span><em>unit testing</em><span> </span><span>refers to the process of testing each </span><em>unit</em><span> of an application in </span><em>isolation</em><span> </span><span>to verify that its behavior conforms to a particular set of specifications.</span></div>
<p>In this section, we will dive into the different methodologies of unit testing at our disposal (black- versus white-box testing). We will also examine strategies for making our code easier to unit test and cover the built-in Go testing packages, as well as third-party packages, that are designed to make writing tests more streamlined.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mocks, stubs, fakes, and spies – commonalities and differences</h1>
                </header>
            
            <article>
                
<p>Before digging deeper into the concepts behind unit testing, we need to discuss and disambiguate some of the terms that we will be using in the upcoming sections. While these terms have been out there for years, software engineers tend to occasionally conflate them when writing tests. A great example of such confusion becomes evident when you hear engineers use the terms<span> </span><em>mock</em><span> </span>and<span> </span><em>stub </em>interchangeably.</p>
<p>To establish some common ground for a fruitful discussion and to clear any confusion around this terminology, let's examine the definition of each term, as outlined by Gerard Meszaros<span> </span><span class="citation"><sup>[5]</sup> </span>in his <em>XUnit Test Patterns: Refactoring Test Code</em> book on test patterns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stubs and spies!</h1>
                </header>
            
            <article>
                
<p>A<span> </span><strong>stub</strong><span> </span>is the simplest test pattern that we can use in our tests. Stubs typically implement a particular interface and don't contain any real logic; they just provide fixed answers to calls that are performed through the course of a test.</p>
<p>Let's dissect a short code example that illustrates how we can effectively use the concept of stubs for our tests. The<span> </span><kbd>Chapter04/captcha</kbd><span> </span>package implements the verification logic behind a CAPTCHA test.</p>
<div class="packt_infobox">CAPTCHA is a fairly straightforward way to determine whether a system is interacting with a human user or another program. This is achieved by displaying a random, often noisy, image containing a distorted sequence of letters and numbers and then prompting the user to type the text content of the image.</div>
<p class="mce-root"/>
<p>As a big fan of the SOLID principles, I opted to define two interfaces, <kbd>Challenger</kbd><span> </span>and<span> </span><kbd>Prompter</kbd>, to abstract the CAPTCHA image generation and the user-prompting implementation. After all, there is a plethora of different approaches out there for generating CAPTCHA images: we could pick a random image from a fixed set of images, generate them using a neural network, or perhaps even call out to a third-party image generation service. The same could be said about the way we actually prompt our users for an answer. This is how the two interfaces are defined:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Challenger is implemented by objects that can generate CAPTCHA image <br/>// challenges.</span></a>
<a><span class="kw">type</span> Challenger <span class="kw">interface</span> {</a>
<a>    Challenge() (img image.Image, imgText <span class="dt">string</span>)</a>
<a>}</a>

<a><span class="co">// Prompter is implemented by objects that display a CAPTCHA image to the </span></a>
<a><span class="co">// user, ask them to type their contents and return back their response.</span></a>
<a><span class="kw">type</span> Prompter <span class="kw">interface</span> {</a>
<a>    Prompt(img image.Image) <span class="dt">string</span></a>
<a>}</a></pre></div>
<p>At the end of the day, the actual business logic doesn't really care how the CAPTCHA images or the users' answers were obtained. All we need to do is fetch a challenge, prompt the user, and then perform a simple string comparison operation, as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> ChallengeUser(c Challenger, p Prompter) <span class="dt">bool</span> {</a>
<a>    img, expAnswer := c.Challenge()</a>
<a>    userAnswer := p.Prompt(img)</a>

<a>    <span class="kw">if</span> subtle.ConstantTimeEq(<span class="dt">int32</span>(<span class="bu">len</span>(expAnswer)), <span class="dt">int32</span>(<span class="bu">len</span>(userAnswer))) <br/>     == <span class="dv">0</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>

<a>    <span class="kw">return</span> subtle.ConstantTimeCompare([]<span class="dt">byte</span>(userAnswer), []<span class="dt">byte</span>(expAnswer)) == <span class="dv">1</span></a>
<a>}</a></pre></div>
<p>One interesting, at least in my opinion, aspect of the preceding code is that it uses constant-time string comparisons instead of using the built-in equality operator for comparing the expected answer and the user's response.</p>
<div class="packt_infobox">Constant-time comparison checks are a common pattern in security-related code as it prevents information leaks, which can be exploited by adversaries to perform a timing side-channel attack. When executing a timing attack, the attacker provides variable-length inputs to a system and then employs statistical analysis to collect additional information about the system's implementation based on the time it takes to execute a particular action.<br/>
<br/>
Imagine if, in the preceding CAPTCHA scenario we had used a simple string comparison that essentially compares each character and returns false on the<span> </span><em>first mismatch</em>. Here's how an attacker could slowly brute-force the answer via a timing attack:
<ul>
<li>Start by providing answers following the <kbd>$a</kbd><span> pattern </span>and measuring the time it takes to get a response. The<span> </span><kbd>$</kbd><span> </span>symbol is a placeholder for all possible alphanumeric characters. In essence, we try combinations such as <kbd>aa</kbd>,<span> </span><kbd>ba</kbd>, and so on.</li>
<li>Once we have identified an operation that takes<span> </span><em>longer than the rest</em>, we can assume that that particular value of<span> </span><kbd>$</kbd><span> </span>(say,<span> </span><kbd>4</kbd>) is the expected first character of the CAPTCHA answer! The reason this takes longer is that the string comparison code matched the first character and then tried matching the next character instead of immediately returning it, like it would if there was a mismatch.</li>
<li>Continue the same process of providing answers but this time using the <kbd>4$a</kbd> pattern and keep extending the pattern until the expected CAPTCHA answer can be recovered.</li>
</ul>
</div>
<p>In order to test the<span> </span><kbd>ChallengeUser</kbd><span> </span>function, we need to create a stub for each of its arguments. This would provide us with complete control over the inputs to the comparison business logic. Here's what the stubs might look like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> stubChallenger <span class="dt">string</span></a>

<a><span class="kw">func</span> (c stubChallenger) Challenge() (image.Image, <span class="dt">string</span>) {</a>
<a>    <span class="kw">return</span> image.NewRGBA(image.Rect(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">100</span>)), <span class="dt">string</span>(c)</a>
<a>}</a>
<a><span class="kw">type</span> stubPrompter <span class="dt">string</span></a>

<a><span class="kw">func</span> (p stubPrompter) Prompt(_ image.Image) <span class="dt">string</span> {</a>
<a>    <span class="kw">return</span> <span class="dt">string</span>(p)</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Pretty simple, right? As you can see, the stubs are devoid of any logic; they just return a canned answer. With the two stubs in place, we can write two test functions that exercise the match/non-match code paths:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestChallengeUserSuccess(t *testing.T) {</a>
<a>    got := captcha.ChallengeUser(stubChallenger(<span class="st">"42"</span>), stubPrompter(<span class="st">"42"</span>))</a>
<a>    <span class="kw">if</span> got != <span class="ot">true</span> {</a>
<a>        t.Fatal(<span class="st">"expected ChallengeUser to return true"</span>)</a>
<a>    }</a>
<a>}</a>

<a><span class="kw">func</span> TestChallengeUserFail(t *testing.T) {</a>
<a>    got := captcha.ChallengeUser(stubChallenger(<span class="st">"lorem ipsum"</span>), stubPrompter(<span class="st">"42"</span>))</a>
<a>    <span class="kw">if</span> got != <span class="ot">false</span> {</a>
<a>        t.Fatal(<span class="st">"expected ChallengeUser to return false"</span>)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Now that we have a general understanding of how stubs work, let's look at another useful test pattern: spies! A<span> </span><strong>spy</strong><span> </span>is nothing more than a stub that keeps a detailed log of all the methods that are invoked on it. For each method invocation, the spy records the arguments that were provided by the caller and makes them available for inspection by the test code.</p>
<p>Surely, when it comes to Go, the most popular spy implementation is the venerable<span> </span><kbd>ResponseRecorder</kbd><span> </span>type, which is provided by the<span> </span><kbd>net/http/httptest</kbd><span> </span>package.<span> </span><kbd>ResponseRecorder</kbd><span> </span>implements the<span> </span><kbd>http.ResponseWriter</kbd><span> </span>interface and can be used for testing HTTP request handling code without the need to spin up an actual HTTP server. However, HTTP server testing is not that interesting; let's take a look at a slightly more engaging example. The<span> </span><kbd>Chapter04/chat</kbd><span> </span>package contains a simple chatroom implementation that is perfect for applying the spy test pattern. The following is the definition of the<span> </span><kbd>Room</kbd><span> </span>type and its constructor:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Publisher is implemented by objects that can send a message to a user.</span></a>
<a><span class="kw">type</span> Publisher <span class="kw">interface</span> {</a>
<a>    Publish(userID, message <span class="dt">string</span>) <span class="dt">error</span></a>
<a>}</a>

<a><span class="kw">type</span> Room <span class="kw">struct</span> {</a>
<a>    pub Publisher</a>
<a>    mu    sync.RWMutex</a>
<a>    users []<span class="dt">string</span></a>
<a>}</a>

<a><span class="co">// NewRoom creates a new chat root instance that used pub to broadcast <br/>// messages.</span></a>
<a><span class="kw">func</span> NewRoom(pub Publisher) *Room {</a>
<a>    <span class="kw">return</span> &amp;Room{pub: pub}</a>
<a>}</a></pre></div>
<p>As you can see, <kbd>Room</kbd><span> </span>contains a<span> </span><kbd>Publisher</kbd><span> </span>instance that gets initialized by the value that's passed to the<span> </span><kbd>NewRoom</kbd><span> </span>constructor. The other interesting public methods that are exposed by the<span> </span><kbd>Room</kbd><span> </span>type (not shown here but available in this book's GitHub repo) are<span> </span><kbd>AddUser</kbd><span> </span>and<span> </span><kbd>Broadcast</kbd>. The first method adds new users to the room, while the latter can be used to broadcast a particular message to all the users currently in the room.</p>
<p>Before we write our actual testing code, let's create a spy instance that implements the<span> </span><kbd>Publisher</kbd><span> </span>interface and records any published messages:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> entry <span class="kw">struct</span> {</a>
<a>    user    <span class="dt">string</span></a>
<a>    message <span class="dt">string</span></a>
<a>}</a>

<a><span class="kw">type</span> spyPublisher <span class="kw">struct</span> {</a>
<a>    published []entry</a>
<a>}</a>

<a><span class="kw">func</span> (p *spyPublisher) Publish(user, message <span class="dt">string</span>) <span class="dt">error</span> {</a>
<a>    p.published = <span class="bu">append</span>(p.published, entry{user: user, message: message})</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>In the preceding spy implementation, each time the<span> </span><kbd>Publish</kbd><span> </span>method is invoked, the stub will append a<span> </span><kbd>{user, message}</kbd><span> </span>tuple to the <kbd>published</kbd> slice. With our spy ready to be used, writing the actual test is a piece of cake:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestChatRoomBroadcast(t *testing.T) {</a>
<a>    pub := <span class="bu">new</span>(spyPublisher)</a>
<a>    room := chat.NewRoom(pub)</a>
<a>    room.AddUser(<span class="st">"bob"</span>)</a>
<a>    room.AddUser(<span class="st">"alice"</span>)</a>
<a>    _ = room.Broadcast(<span class="st">"hi"</span>)</a>
<a>    exp := []entry{</a>
<a>        {user: <span class="st">"bob"</span>, message: <span class="st">"hi"</span>},</a>
<a>        {user: <span class="st">"alice"</span>, message: <span class="st">"hi"</span>},</a>
<a>    }</a>
<a>    <span class="kw">if</span> got := pub.published; !reflect.DeepEqual(got, exp) {</a>
<a>        t.Fatalf(<span class="st">"expected the following messages:</span><span class="ch">\n</span><span class="st">%#+v</span><span class="ch">\n</span><span class="st">got:</span><span class="ch">\n</span><span class="st">%#+v"</span>, exp, got)</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>This test scenario involves creating a new room, adding some users to it, and broadcasting a message to everyone who has joined the room. The test runner's task is to verify that the call to<span> </span><kbd>Broadcast</kbd><span> </span>did in fact broadcast the message to all the users. We can achieve this by examining the list of messages that have been recorded by our injected spy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mocks</h1>
                </header>
            
            <article>
                
<p>You can think of<span> </span><strong>mocks</strong><span> </span>as stubs on steroids! Contrary to the fixed behavior exhibited by stubs, mocks allow us to specify, in a<span> </span><em>declarative</em><strong> </strong>way, not only the list of calls that the mock is expected to receive but also their order and expected argument values. In addition, mocks allow us to specify different return values for each method invocation, depending on the argument tuple provided by the method caller.</p>
<p>All things considered, mocks are a very powerful primitive at our disposal for writing advanced tests. However, building mocks from scratch for every single object we want to substitute as part of our tests is quite a tedious task. This is why it's often better to use an external tool and code generation to automate the creation of the mocks that are needed for our tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing gomock</h1>
                </header>
            
            <article>
                
<p><span><span>In this section, we will be introducing</span></span> <kbd>gomock</kbd> <sup><span class="citation">[4]</span></sup>, a very popular mocking framework for Go that leverages reflection and code generation to automatically create mocks based on Go interface definitions.</p>
<p>The framework and its supporting tools can be installed by running the following commands:</p>
<pre class="console">$ go get github.com/golang/mock/gomock
$ go install github.com/golang/mock/mockgen</pre>
<p>The<span> </span><kbd>mockgen</kbd><span> </span>tool is responsible for analyzing either individual Go files or entire packages and generating mocks for all (or specific) interfaces that are defined within them. It supports two modes of operation:</p>
<ul>
<li><strong>Source code scanning</strong>: We pass a Gi file to <kbd>mockgen</kbd>, which is then parsed in order to detect interface definitions.</li>
<li><strong>Reflection-assisted mode</strong>: We pass a package and a list of interfaces to <kbd>mockgen</kbd>. The tool uses the Go reflection package to analyze the structure of each interface.</li>
</ul>
<p class="mce-root"/>
<p><kbd>gomock</kbd> provides a simple and concise API for specifying the expected behavior of mock instances that are created via the <kbd>mockgen</kbd> tool. To access this API, you need to create a new instance of the mock and invoke its oddly-cased<span> </span><kbd>EXPECT</kbd><span> </span>method.<span> </span><kbd>EXPECT</kbd><span> </span>returns a special object (a <em>recorder</em>, in <kbd>gomock</kbd> terminology) that provides the means for us to declare the behavior of the method calls that are performed against the mock. </p>
<p>To register a new expectation, we need to do the following:</p>
<ol>
<li>Declare the name of the method that we expect to be called, along with its arguments.</li>
<li>Specify the return value (or values) that the mock should return to the caller when it invokes the method with the specified set of arguments.</li>
<li>Optionally, we need to specify the number of times that the caller is expected to invoke the method.</li>
</ol>
<p>To further streamline the creation of tests,<span> </span><kbd>mockgen</kbd><span> populates the returned recorder instances with </span><span>methods whose names match the i</span><span>nterfaces that we are trying to mock. All we need to do is invoke those methods on the recorder object and specify the arguments that the mock expects to receive from the caller as a variadic list of</span><span> </span><kbd>interface{}</kbd><span> </span><span>values. When defining the expected set of arguments, you basically have two options:</span></p>
<ul>
<li>Specify a value whose <em>type</em> matches the one from the method signature (for example, <kbd>foo</kbd> if the argument is of the <kbd>string</kbd> type). <kbd>gomock</kbd> will only match a call to an expectation if the input argument, <em>value</em>, is<span> </span><em>strictly equal</em><span> </span>to the value that's specified as part of the expectation.</li>
<li><span>Provide a value that implements the</span><span> </span><kbd>gomock.Matcher</kbd><span> </span><span>interface. In this case, <kbd>gomock</kbd> will delegate the comparison to the matcher itself. This powerful feature gives us the flexibility to model any custom test predicate that we can think of. <kbd>gomock</kbd> already defines a few handy built-in matchers that we can use in our tests:</span><span> </span><kbd>Any</kbd><span>,</span><span> </span><kbd>AssignableToTypeOf</kbd><span>,</span><span> </span><kbd>Nil</kbd><span>, </span><span>and</span><span> </span><kbd>Not</kbd><span>.</span></li>
</ul>
<p>After specifying the expected method call and its arguments, <kbd>gomock</kbd> will return an expectation object that provides auxiliary methods so that we can configure the expected behavior further. For instance, we can use the expectation object's<span> </span><kbd>Return</kbd><span> </span>method to define the set of values to be returned to the caller once the expectation is matched. It is also important to note that unless we<span> </span><em>explicitly</em><span> </span>specify the expected number of calls to the mocked method, <span><kbd>gomock</kbd> will assume that the method can only be invoked <em>o</em></span><span><em>nce</em> </span><span>and will trigger a test failure if the method is not invoked at all or is invoked multiple times. If you require more fine-grained control over the number of expected invocations, the returned expectation object provides the following set of helper methods:</span><span> </span><kbd>Times</kbd><span>,</span><span> </span><kbd>MinTimes</kbd><span>, </span><span>and</span><span> </span><kbd>MaxTimes</kbd><span>.</span></p>
<p class="mce-root"/>
<p>In the next two sections, we will analyze an example project and go through all the individual steps for writing a complete, mock-based unit test for it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the details of the project we want to write tests for</h1>
                </header>
            
            <article>
                
<p>For the purpose of demonstrating the creation and use of mocks in our code, we will be working with the example code from the<span> <kbd>Chapter04/dependency</kbd></span><span> </span>package. This package defines a <kbd>Collector</kbd> type whose purpose is to assemble a set of direct and indirect (transitive) dependencies for a given project ID. To make things a bit more interesting, let's assume that each dependency can belong to one of the following two categories:</p>
<ul>
<li>A resource that we need to include (for example, an image file) or reserve (for example, a block of memory or an amount of disk space)</li>
<li>Another project with its <em>own set of dependencies</em></li>
</ul>
<p><span>To obtain the list of <em>direct</em> dependencies and their respective types, the</span> <kbd>Collector</kbd><span> dependency will be performing a series of calls to an external service. To ensure that the implementation lends itself to easier testing, we will not be working with a concrete client instance for the external service. Instead, we will define an interface with the set of required methods for accessing the service and have our test code inject a mock that satisfies that interface. Consider the following definition for th</span><span>e </span><kbd>API</kbd><span> </span><span>interface:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> API <span class="kw">interface</span> {</a>
<a>    <span class="co">// ListDependencies returns the list of direct dependency IDs for a</span></a>
<a>    <span class="co">// particular project ID or an error if a non-project ID argument is</span></a>
<a>    <span class="co">// provided.</span></a>
<a>    ListDependencies(projectID <span class="dt">string</span>) ([]<span class="dt">string</span>, <span class="dt">error</span>)</a>

<a>    <span class="co">// DependencyType returns the type of a particular dependency.</span></a>
<a>    DependencyType(dependencyID <span class="dt">string</span>) (DepType, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>To create a new<span> </span><kbd>Collector</kbd><span> </span>instance, we need to invoke the <kbd>NewCollector</kbd> constructor (not shown) and provide an API instance as an argument. Then, the <em>unique</em> set of dependencies for a particular project ID can be obtained via a call to the<span> </span><kbd>AllDependencies</kbd><span> method. It's a pretty short method whose full implementation is as follows</span>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *Collector) AllDependencies(projectID <span class="dt">string</span>) ([]<span class="dt">string</span>, <span class="dt">error</span>) {</a>
<a>    ctx := newDepContext(projectID)</a>
<a>    <span class="kw">for</span> ctx.HasUncheckedDeps() {</a>
<a>        projectID = ctx.NextUncheckedDep()</a>
<a>        projectDeps, err := c.api.ListDependencies(projectID)</a>
<a>        <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>            <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"unable to list dependencies for project %q: %w"</span>, projectID, err)</a>
<a>        }</a>
<a>        <span class="kw">if</span> err = c.scanProjectDependencies(ctx, projectDeps); err != <span class="ot">nil</span> {</a>
<a>            <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>        }</a>
<a>    }</a>
<a>    <span class="kw">return</span> ctx.depList, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The preceding block of code is nothing more than a <strong>breadth-first search</strong> (<strong>BFS</strong>) algorithm in disguise! The <kbd>ctx</kbd> variable stores an auxiliary structure that contains the following:</p>
<ul>
<li><em>A queue</em> whose entries correspond to the set of dependencies (resources or projects) that we haven't visited yet. <span>As we visit the nodes of the project dependency graph, any newly discovered dependencies will be appended to the tail of the queue so that they can be visited in a future search loop iteration.</span></li>
<li>The unique set of discovered dependency IDs that are returned to the caller once all the entries in the queue have been processed.</li>
</ul>
<p>To seed the search, initially, we populate the queue with the <kbd>projectID</kbd><span> </span>value that was passed in as an argument to the method. With each loop iteration, we dequeue an unchecked dependency ID and invoke the<span> </span><kbd>ListDependencies</kbd><span> </span>API call to get a list of all its direct dependencies. The obtained list of dependency IDs is then passed as input to the <kbd>scanProjectDependencies</kbd><span> </span>method, whose role is to examine the dependency list and update the contents of the <kbd>ctx</kbd> variable. The method's implementation is pretty straightforward:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *Collector) scanProjectDependencies(ctx *depCtx, depList []<span class="dt">string</span>) <span class="dt">error</span> {</a>
<a>    <span class="kw">for</span> _, depID := <span class="kw">range</span> depList {</a>
<a>        <span class="kw">if</span> ctx.AlreadyChecked(depID) {</a>
<a>            <span class="kw">continue</span></a>
<a>        }</a>
<a>        ctx.AddToDepList(depID)</a>
<a>        depType, err := c.api.DependencyType(depID)</a>
<a>        <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>            <span class="kw">return</span> xerrors.Errorf(<span class="st">"unable to get dependency type for id %q: %w"</span>, depID, err)</a>
<a>        }</a>
<a>        <span class="kw">if</span> depType == DepTypeProject {</a>
<a>            ctx.AddToUncheckedList(depID)</a>
<a>        }</a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>While iterating the dependency list, the implementation automatically skips any dependency that has already been visited. On the other hand, new dependency IDs are appended to the set of unique dependencies that have been tracked by the <kbd>ctx</kbd> variable via a call to the <kbd>AddToDepList</kbd> method.</p>
<p>As we mentioned previously, if the dependency corresponds to another project, we need to <em>recursively</em> visit its own dependencies and add them to our set as <em>transitive<strong> </strong></em>dependencies<em>.</em> The <kbd>DependencyType</kbd> method from the <kbd>API</kbd> interface provides us with the means for querying the type of a dependency by its ID. If the dependency does in fact point to a<span> </span><em>project</em>, we append it to the tail of the unvisited dependencies queue via a call to the <kbd>AddToUncheckedList</kbd> method. The last step guarantees that the dependency will eventually be processed by the search loop inside the <kbd>AllDependencies</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leveraging gomock to write a unit test for our application</h1>
                </header>
            
            <article>
                
<p>Now that we are aware of the implementation details of our example project, we can go ahead and write a simple, mock-based unit test for it. Before we begin, we need to create a mock for the <kbd>API</kbd> interface. This can be achieved by invoking<span> the </span><kbd>mockgen</kbd><span> tool </span>with the following options:</p>
<pre class="console">$ mockgen \
    -destination mock/dependency.go \
    github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang/<span>Chapter04</span>/dependency \
    API</pre>
<p>The preceding command does the following:</p>
<ul>
<li>Creates a <kbd>mock</kbd><span> </span><span>folder in the</span><span> </span><kbd>dependency</kbd><span> </span><span>package</span></li>
<li>Generates a file called<span> </span><kbd>dependency.go</kbd><em> </em>with the appropriate code for mocking the<span> </span><kbd>API</kbd><span> </span>interface and places it in the <kbd>mock</kbd><span> </span>folder</li>
</ul>
<div class="packt_tip">To save you the trouble of having to manually type in the preceding command, the <kbd>Makefile</kbd> in the <kbd><span>Chapter04</span>/dependency</kbd><span> </span>folder includes a predefined target for rebuilding the mocks that were used in this example. All you need to do is switch to the folder with the example code in it and run <kbd>make mocks</kbd>.</div>
<p class="mce-root"/>
<p>So far, so good. How can we use the mock in our tests though? The first thing we need to do is create a <kbd>gomock</kbd><span> </span><em>controller</em><span> </span>and associate it with the<span> </span><kbd>testing.T</kbd><span> </span>instance that gets passed to our test function by the Go standard library. The controller instance defines a<span> </span><kbd>Finish</kbd><span> </span>method that our code<span> </span><em>must always run before returning from the test</em><span> </span>(for example, via a<span> </span><em>defer</em><span> </span>statement). This method checks the expectations that were registered on each mock object and automatically fails the test if they were not met. Here's what the preamble of our test function would look like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Create a controller to manage all our mock objects and make sure</span></a>
<a><span class="co">// that all expectations were met before completing the test</span></a>
<a>ctrl := gomock.NewController(t)</a>
<a><span class="kw">defer</span> ctrl.Finish()</a>

<a><span class="co">// Obtain a mock instance that implements API and associate it with the controller.</span></a>
<a>api := mock_dependency.NewMockAPI(ctrl)</a></pre></div>
<p>The purpose of this particular unit test is to verify that a call to the <kbd>AllDependencies</kbd> method with a specific input yields an expected list of dependency IDs. As we saw in the previous section, the implementation of the <span><kbd>AllDependencies</kbd> method uses an externally-provided <kbd>API</kbd> instance to retrieve information about each dependency. Given that our test will inject a mocked API instance into the <kbd>Collector</kbd> dependency, our test code must</span> declare the expected set of calls to the mock. Consider the following block of code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>gomock.InOrder(</a>
<a>    api.EXPECT().</a>
<a>        ListDependencies(<span class="st">"proj0"</span>).Return([]<span class="dt">string</span>{<span class="st">"proj1"</span>, <span class="st">"res1"</span>}, <span class="ot">nil</span>),</a>
<a>    api.EXPECT().</a>
<a>        DependencyType(<span class="st">"proj1"</span>).Return(dependency.DepTypeProject, <span class="ot">nil</span>),</a>
<a>    api.EXPECT().</a>
<a>        DependencyType(<span class="st">"res1"</span>).Return(dependency.DepTypeResource, <span class="ot">nil</span>),</a>
<a>    api.EXPECT().</a>
<a>        ListDependencies(<span class="st">"proj1"</span>).Return([]<span class="dt">string</span>{<span class="st">"res1"</span>, <span class="st">"res2"</span>}, <span class="ot">nil</span>),</a>
<a>    api.EXPECT().</a>
<a>        DependencyType(<span class="st">"res2"</span>).Return(dependency.DepTypeResource, <span class="ot">nil</span>),</a>
<a>)</a></pre></div>
<p>Under normal circumstances, <kbd>gomock</kbd> would just check that the method call expectations are met, <em>regardless of the order that they were invoked in</em>. However, if a test relies on a sequence of method calls being performed in a particular order, it can specify this to <kbd>gomock</kbd> by invoking the <kbd>gomock.InOrder</kbd> helper function with an ordered list of expectations as arguments. This particular pattern can be seen in the preceding code snippet.</p>
<p class="mce-root"/>
<p>With the mock expectations in place, we can complete our unit by introducing the necessary logic to wire everything together, invoke the <kbd>AllDependencies</kbd> method with the input (<kbd>proj0</kbd>) that our mock expects, and validate that the returned output matches a predefined value (<kbd>"proj1", "res1", "res2"</kbd>):</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>collector := dependency.NewCollector(api)</a>
<a>depList, err := collector.AllDependencies(<span class="st">"proj0"</span>)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    t.Fatal(err)</a>
<a>}</a>

<a><span class="kw">if</span> exp := []<span class="dt">string</span>{<span class="st">"proj1"</span>, <span class="st">"res1"</span>, <span class="st">"res2"</span>}; !reflect.DeepEqual(depList, exp) {</a>
<a>    t.Fatalf(<span class="st">"expected dependency list to be:</span><span class="ch">\n</span><span class="st">%v</span><span class="ch">\n</span><span class="st">got:</span><span class="ch">\n</span><span class="st">%v"</span>, exp, depList)</a>
<a>}</a></pre></div>
<p>This concludes our short example about using <kbd>gomock</kbd> to accelerate the authoring of mock-based tests. As a fun learning activity, you can experiment with changing the expected output for the preceding test so that the test fails. Then, you can work backward and try to figure out how to tweak the mock expectations to make the test pass again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fake objects</h1>
                </header>
            
            <article>
                
<p>In a similar fashion to the other test patterns that we have discussed so far,<span> </span><strong>fake objects</strong><span> </span>also adhere to a specific interface, which allows us to inject them into the subject under test. The main difference is that fake objects do, in fact, contain a <em>fully working</em> implementation whose behavior matches the objects that they are meant to substitute.</p>
<div class="packt_infobox">So, what's the catch? Fake object implementations are typically optimized for running tests and, as such, they are not meant to be used in production. For example, we could provide an in-memory key-value store implementation for our tests, but our production deployments would require something with better availability guarantees.</div>
<p>To achieve a better understanding of how fake objects work, let's take a look at the contents of the <kbd><span>Chapter04</span>/compute</kbd><span> </span>package. This package exports a function called<span> </span><kbd>SumOfSquares</kbd>, which operates on a slice of 32-bit floating-point values. The function squares each element of the slice, adds the results together, and returns their sum. Note that we are using a single function purely for demonstration purposes; in a real-world scenario, we would compose this function with other similar functions to form a compute graph that our implementation would then proceed to evaluate.</p>
<p>To purposefully add a bit of extra complexity to this particular scenario, let's assume that the input slices that are passed to this function typically contain a<span> </span><em>very large number of values</em>. It is still possible, of course, to use the CPU to calculate the result. Unfortunately, the production service that depends on this functionality has a pretty strict time budget, so using the CPU is not an option. To this end, we have decided to implement a vectorized solution by offloading the work to a GPU.</p>
<p>The<span> </span><kbd>Device</kbd><span> </span>interface describes the set of operations that can be offloaded to the GPU:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Device <span class="kw">interface</span> {</a>
<a>    Square([]<span class="dt">float32</span>) []<span class="dt">float32</span></a>
<a>    Sum([]<span class="dt">float32</span>) <span class="dt">float32</span></a>
<a>}</a></pre></div>
<p>Given an object instance that implements<span> </span><kbd>Device</kbd>, we can define the<span> </span><kbd>SumOfSquares</kbd><span> </span>function as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> SumOfSquares(c Device, in []<span class="dt">float32</span>) <span class="dt">float32</span> {</a>
<a>    sq := c.Square(in)</a>
<a>    <span class="kw">return</span> c.Sum(sq)</a>
<a>}</a></pre></div>
<p>Nothing too complicated here... Alas, it wasn't until we started working on our tests that we realized that while the compute nodes where we normally run our production code do provide beefy GPUs, the same could not be said for<span> </span><em>each one</em><span> </span>of the machines that's used locally by our engineers or the CI environment that runs our tests each time we create a new pull request.</p>
<p>Obviously, even though our real workload deals with lengthy inputs, there is no strict requirement to do the same within our tests; as we will see in the following sections, this is a job for an end-to-end test. Therefore, we can fall back to a CPU implementation if a GPU is not available when our tests are running. This is an excellent example of where a fake object could help us out. So, let's start by defining a <kbd>Device</kbd><span> </span>implementation that uses the CPU for all its calculations:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> cpuComputeDevice <span class="kw">struct</span>{}</a>

<a><span class="kw">func</span> (d cpuComputeDevice) Square(in []<span class="dt">float32</span>) []<span class="dt">float32</span> {</a>
<a>    <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">len</span>(in); i++ {</a>
<a>        in[i] *= in[i]</a>
<a>    }</a>
<a>    <span class="kw">return</span> in</a>
<a>}</a>

<a><span class="kw">func</span> (d cpuComputeDevice) Sum(in []<span class="dt">float32</span>) (sum <span class="dt">float32</span>) {</a>
<a>    <span class="kw">for</span> _, v := <span class="kw">range</span> in {</a>
<a>        sum += v</a>
<a>    }</a>
<a>    <span class="kw">return</span> sum</a>
<a>}</a></pre></div>
<p>Our test code can then switch between the GPU- or the CPU-based implementation on the fly, perhaps by inspecting the value of an environment variable or some command-line flag that gets passed as an argument to the test:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestSumOfSquares(t *testing.T) {</a>
<a>    <span class="kw">var</span> dev compute.Device</a>
<a>    <span class="kw">if</span> os.Getenv(<span class="st">"USE_GPU"</span>) != <span class="st">""</span> {</a>
<a>        t.Log(<span class="st">"using GPU device"</span>)</a>
<a>        dev = gpu.NewDevice()</a>
<a>    } <span class="kw">else</span> {</a>
<a>        t.Log(<span class="st">"using CPU device"</span>)</a>
<a>        dev = cpuComputeDevice{}</a>
<a>    }</a>
<a>    <span class="co">// Generate deterministic sample data and return the expected sum</span></a>
<a>    in, expSum := genTestData(<span class="dv">1024</span>)</a>
<a>    <span class="kw">if</span> gotSum := compute.SumOfSquares(dev, in); gotSum != expSum {</a>
<a>        t.Fatalf(<span class="st">"expected SumOfSquares to return %f; got %f"</span>, expSum, gotSum)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>With the help of a fake object, we can always run our tests while still offering this ability to engineers who do have local access to GPUs to run the tests using the GPU-based implementation. Success!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Black-box versus white-box testing for Go packages – an example</h1>
                </header>
            
            <article>
                
<p>Black- and white-box testing are two different approaches to authoring unit tests. Each approach has its own set of merits and goals. Consequently, we shouldn't treat them as competing approaches but rather as one complementing the other. So, what is the major difference between these two types of tests?</p>
<p class="mce-root"/>
<div class="packt_infobox">Black-box testing works under the assumption that the underlying implementation details of the package that we test, also known as<span> the </span><strong>subject under test</strong><span> </span>(<strong>SUT</strong>), are totally opaque (hence the name black-box) to us, the tester. As a result, we can only test the<span> </span><strong>public interface</strong><span> </span>or behavior of a particular package and make sure it adheres to its advertised contract.<br/>
<br/>
On the other hand, white-box testing assumes that we have <em>prior</em><span> </span>knowledge of the implementation details of a particular package. This allows the tester to either craft each test so that it exercises a particular code path within the package or to directly test the package's internal implementation.</div>
<p>To understand the difference between these two approaches, let's take a look at a short example. The <kbd><span>Chapter04</span>/retail</kbd><span> </span>package implements a<span> </span><em>facade</em><span> </span>called<span> </span><kbd>PriceCalculator</kbd>.</p>
<div class="packt_infobox">A facade is a software design pattern that abstracts the complexity of one or more software components behind a simple interface.<br/>
In the context of microservice-based design, the facade pattern allows us to transparently compose or aggregate data across multiple, specialized microservices while providing a simple API for the facade clients to access it.</div>
<p>In this particular scenario, the facade receives a UUID representing an item and a date representing the period we are interested in as input. Then, it communicates with two backend microservices to retrieve information about the item's price and the VAT rate that was applied on that particular date. Finally, it returns the VAT-inclusive price for the item to the facade's client.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The services behind the facade</h1>
                </header>
            
            <article>
                
<p>Before we dive deeper into the inner workings of the price calculator, let's spend a bit of time examining how the two microservice dependencies work; after all, we will need this information to write our tests.</p>
<p>The<span> </span><kbd>price</kbd><span> </span>microservice provides a REST endpoint for retrieving an item's published price on a particular date. The service responds with a JSON payload that looks like this:</p>
<div class="sourceCode">
<pre class="sourceCode json"><a><span class="fu">{</span></a>
<a>    <span class="dt">"price"</span><span class="fu">:</span> <span class="fl">10.0</span><span class="fu">,</span></a>
<a>    <span class="dt">"currency"</span><span class="fu">:</span> <span class="st">"GBP"</span></a>
<a><span class="fu">}</span></a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The second microservice in this example is called<span> </span><kbd>vat</kbd><span> </span>and is also RESTful. It exposes an endpoint for retrieving the VAT rate that was applicable on a particular date. The service responds with a JSON payload as follows:</p>
<div class="sourceCode">
<pre class="sourceCode json"><a><span class="fu">{</span></a>
<a>    <span class="dt">"vat_rate"</span><span class="fu">:</span> <span class="fl">0.29</span></a>
<a><span class="fu">}</span></a></pre></div>
<p>As you can see, the returned JSON payload is quite simple and it would be trivial for our test code to mock it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing black-box tests</h1>
                </header>
            
            <article>
                
<p>For the purpose of writing our black-box tests, we will start by examining the<span> </span><em>public </em>interface of the<span> </span><kbd>retail</kbd><span> </span>package. A quick browse of the<span> </span><kbd>retail.go</kbd><span> </span>file reveals a<span> </span><kbd>NewPriceCalculator</kbd><span> </span>function that receives the URLs to the<span> </span><kbd>price</kbd><span> </span>and<span> </span><kbd>vat</kbd><span> </span>services as arguments and returns a<span> </span><kbd>PriceCalculator</kbd><span> </span>instance. The calculator instance can be used to obtain an item's VAT-inclusive price by invoking the<span> </span><kbd>PriceForItem</kbd> method and passing the item's UUID as an argument. On the other hand, if we are interested in obtaining a <span>VAT</span>-inclusive item price for a particular date in the past, we can invoke the<span> </span><kbd>PriceForItemAtDate</kbd><span> </span>method, which also accepts a time period argument.</p>
<p>The black-box tests will live <em>in a separate package</em> with the name<span> </span><kbd>retail_test</kbd>. The<span> </span><kbd>$PACKAGE_test</kbd><span> </span>naming convention is, more or less, the standard way for doing black-box testing as the name itself alludes to the package being tested while at the same time preventing our test code from accessing the internals of the package under test.</p>
<p>One caveat of black-box testing is that we need to mock/stub any external objects and/or services that the tested code depends on. In this particular case, we need to provide stubs for the<span> </span><kbd>price</kbd><span> </span>and<span> </span><kbd>vat</kbd><span> </span>services. Fortunately, the<span> </span><kbd>net/http/httptest</kbd><span> </span>package, which ships with the Go standard library provides a convenient helper for spinning up a local HTTPS server using random, unused ports. Since we need to spin up two servers for our tests, let's create a small helper function to do exactly that:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> spinUpTestServer(t *testing.T, res <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}) *httptest.Server {</a>
<a>    encResponse, err := json.Marshal(res)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        t.Fatal(err)</a>
<a>    }</a>

<a>    <span class="kw">return</span> httptest.NewServer(http.HandlerFunc(<span class="kw">func</span>(w http.ResponseWriter, req *http.Request) {</a>
<a>        w.Header().Set(<span class="st">"Content-Type"</span>, <span class="st">"application/json"</span>)</a>
<a>        <span class="kw">if</span> _, wErr := w.Write(encResponse); wErr != <span class="ot">nil</span> {</a>
<a>            t.Fatal(wErr)</a>
<a>        }</a>
<a>    }))</a>
<a>}</a></pre></div>
<p>Nothing too complicated here; the<span> </span><kbd>spinUpTestServer</kbd><span> </span>function receives a map with the expected response's content and returns a server (which our test code needs to explicitly close) that always responds with the response payload formatted in JSON. With this helper function in place, setting up the stubs for our services becomes really easy:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// t is a testing.T instance</span></a>
<a>priceSvc := spinUpTestServer(t, <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"price"</span>: <span class="dv">10</span><span class="fl">.0</span>,</a>
<a>})</a>
<a><span class="kw">defer</span> priceSvc.Close()</a>

<a>vatSvc := spinUpTestServer(t, <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"vat_rate"</span>: <span class="dv">0</span><span class="fl">.29</span>,</a>
<a>})</a>
<a><span class="kw">defer</span> vatSvc.Close()</a></pre></div>
<p>So, all we need to do now is call the<span> </span><kbd>NewPriceCalculator</kbd><span> </span>constructor and pass the addresses of the two fake servers. Hold on a minute! If those servers always listen on a random port, how do we know which addresses to pass to the constructor? One particularly convenient feature of the <kbd>Server</kbd> implementation that's provided by the<span> </span><kbd>httptest</kbd><span> </span>package is that it exposes the endpoint where the server is listening for incoming connections via a public attribute called<span> </span><kbd>URL</kbd>. Here's what the rest of our black-box test would look like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>pc := retail.NewPriceCalculator(priceSvc.URL, vatSvc.URL)</a>
<a>got, err := pc.PriceForItem(<span class="st">"1b6f8e0f-bbda-4f4e-ade5-aa1abcc99586"</span>)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    t.Fatal(err)</a>
<a>}</a>

<a><span class="kw">if</span> exp := <span class="dv">12</span><span class="fl">.9</span>; got != exp {</a>
<a>    t.Fatalf(<span class="st">"expected calculated retail price to be %f; got %f"</span>, exp, got)</a>
<a>}</a></pre></div>
<p>As we mentioned previously, the preceding code snippet lives in a different package, so our tests must import the package under test and access its public contents using the<span> </span><kbd>retail</kbd><span> </span>selector.</p>
<p>We could add a few more tests, for example, to validate the<span> </span><kbd>PriceForItem</kbd><span> </span>behavior when one or both of the services return an error, but that's as far as we can test using black-box testing alone! Let's run our test and see what sort of coverage we can get:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/3d8416ee-3964-412c-a36d-c15033cb4867.png" style="width:57.25em;height:8.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Running just the black-box tests</div>
<p>Not bad at all! However, if we need to boost our test coverage metrics further, we'll need to invest some time and come up with some white-box tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting code coverage via white-box tests</h1>
                </header>
            
            <article>
                
<p>One major difference compared to the tests we wrote in the previous section is that the new set of tests will live in the<span> </span><em>same</em><span> </span>package as the package we are testing. To differentiate from the black-box tests that we authored previously and hint to other engineers perusing the test code that these are internal tests, we will place the new tests in a file named<span> </span><kbd>retail_internal_test.go</kbd>.</p>
<p>Now, it's time to pull the curtain back and examine the implementation details of the<span> </span><kbd>retail</kbd><span> </span>package! The public API of the package is always a good place to begin our exploratory work. An effective strategy would be to identify each exported function and then (mentally) follow its call-graph to locate other candidate functions/methods that we can exercise via our white-box tests. In the unlikely case that the package does not export any functions, we can shift our attention to other exported symbols, such as structs or interfaces. For instance, here is the definition of the<span> </span><kbd>PriceCalculator</kbd><span> </span>struct from the <kbd>retail</kbd> package:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> PriceCalculator <span class="kw">struct</span> {</a>
<a>    priceSvc svcCaller</a>
<a>    vatSvc   svcCaller</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>As we can see, the struct contains two private fields of the <kbd>svcCaller</kbd><span> </span>type whose names clearly indicate they are somehow linked to the two services that the facade needs to call out to. If we keep browsing through the code, we will discover that<span> </span><kbd>svcCaller</kbd><span> </span>is actually an interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> svcCaller <span class="kw">interface</span> {</a>
<a>    Call(req <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}) (io.ReadCloser, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Call</kbd><span> </span>method receives a map of request parameters and returns a response stream as an<span> </span><kbd>io.ReadCloser</kbd>. From the perspective of a test writer, the use of such an abstraction should make us quite happy since it provides us with an easy avenue for mocking the actual calls to the two services!</p>
<p>As we saw in the previous section, the public API exposed by the<span> </span><kbd>PriceCalculator</kbd><span> </span>type is composed of two methods:</p>
<ul>
<li><kbd>PriceForItem</kbd><span>, which returns the price of an item at this point in time</span></li>
<li><kbd>PriceForItemAtDate</kbd><span>, </span>which returns the price of an item at a particular point intime</li>
</ul>
<p>Since the <span><kbd>PriceForItem</kbd> </span>method is a simple wrapper that calls<span> </span><kbd>PriceForItemAtDate</kbd><span> </span>with the current date/time as an argument, we will focus our analysis on the latter. The implementation of <span><kbd>PriceForItemAtDate</kbd> is presented as follows:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (pc *PriceCalculator) PriceForItemAtDate(itemUUID <span class="dt">string</span>, date time.Time) (<span class="dt">float64</span>, <span class="dt">error</span>) {</a>
<a>    priceRes := <span class="kw">struct</span> {</a>
<a>        Price <span class="dt">float64</span> <span class="st">`json:"price"`</span></a>
<a>    }{}</a>
<a>    vatRes := <span class="kw">struct</span> {</a>
<a>        Rate <span class="dt">float64</span> <span class="st">`json:"vat_rate"`</span></a>
<a>    }{}</a>
<a>    req := <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{<span class="st">"item"</span>: itemUUID, <span class="st">"period"</span>: date}</a>
<a>    <span class="kw">if</span> err := pc.callService(pc.priceSvc, req, &amp;priceRes); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="dv">0</span>, xerrors.Errorf(<span class="st">"unable to retrieve item price: %w"</span>, err)</a>
<a>    }</a>
<a>    req = <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{<span class="st">"period"</span>: date}</a>
<a>    <span class="kw">if</span> err := pc.callService(pc.vatSvc, req, &amp;vatRes); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="dv">0</span>, xerrors.Errorf(<span class="st">"unable to retrieve vat percent: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">return</span> vatInclusivePrice(priceRes.Price, vatRes.Rate), <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p><span>The preceding code block makes use of a helper called <kbd>callService</kbd> to send out a request to the </span><kbd>price</kbd><span> and </span><kbd>vat</kbd><span> services and unpack their responses into the <kbd>priceRes</kbd> and <kbd>vatRes</kbd> variables. To gain a clearer understanding of what happens under the hood, let's take a quick peek into the implementation of <kbd>callService</kbd>:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (pc *PriceCalculator) callService(svc svcCaller, req <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}, res <span class="kw">interface</span>{}) <span class="dt">error</span> {</a>
<a>    svcRes, err := svc.Call(req)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"call to remote service failed: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">defer</span> drainAndClose(svcRes)</a>

<a>    <span class="kw">if</span> err = json.NewDecoder(svcRes).Decode(res); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"unable to decode remote service response: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>callService</kbd><span> </span>method implementation is pretty straightforward. All it does is invoke the<span> </span><kbd>Call</kbd><span> method </span>on the provided<span> </span><kbd>svcCaller</kbd><span> </span>instance, treats the returned output as a JSON stream, and attempts to unmarshal it into the<span> </span><kbd>res</kbd><span> </span>argument that's provided by the caller.</p>
<p><span>Now, let's go back to the implementation of the <kbd>PriceForItemAtDate</kbd> method. Assuming that no error occurred while contacting the remote services, their individual </span><span>responses are passed as arguments to the</span> <kbd>vatInclusivePrice</kbd><span> helper function.</span></p>
<p><span>As you can probably tell by its name, it implements the business logic of applying VAT rates to prices. Keeping the business logic separate from the code that is responsible for talking to other services is not only a good indicator of a well-thought-out design but it also makes our test-writing job easier. Let's add a small table-driven test to validate the business logic:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestVatInclusivePrice(t *testing.T) {</a>
<a>    specs := []<span class="kw">struct</span> {</a>
<a>        price   <span class="dt">float64</span></a>
<a>        vatRate <span class="dt">float64</span></a>
<a>        exp     <span class="dt">float64</span></a>
<a>    }{</a>
<a>        {<span class="dv">42</span><span class="fl">.0</span>, <span class="dv">0</span><span class="fl">.1</span>, <span class="dv">46</span><span class="fl">.2</span>},</a>
<a>        {<span class="dv">10</span><span class="fl">.0</span>, <span class="dv">0</span>, <span class="dv">10</span><span class="fl">.0</span>},</a>
<a>    }</a>
<a>    <span class="kw">for</span> specIndex, spec := <span class="kw">range</span> specs {</a>
<a>        <span class="kw">if</span> got := vatInclusivePrice(spec.price, spec.vatRate); got != spec.exp {</a>
<a>            t.Errorf(<span class="st">"[spec %d] expected to get: %f; got: %f"</span>, specIndex, spec.exp, got)</a>
<a>        }</a>
<a>    }</a>
<a>}</a></pre></div>
<p>With that test in place, the next thing we want to test is<span> </span><kbd>PriceForItem</kbd>. To do that, we need to somehow control access to the external services. Although we will be using stubs for simplicity, we could also use any of the other test patterns that we discussed in the previous section. Here is a stub that implements the same approach as the test server from our black-box tests but without the need to actually spin up a server!</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> stubSvcCaller <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}</a>

<a><span class="kw">func</span> (c stubSvcCaller) Call(<span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}) (io.ReadCloser, <span class="dt">error</span>) {</a>
<a>    data, err := json.Marshal(c)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>

<a>    <span class="kw">return</span> ioutil.NopCloser(bytes.NewReader(data)), <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>Using the preceding stub definition, let's add a test for the<span> </span><kbd>PriceForItem</kbd><span> </span>method's happy path:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestPriceForItem(t *testing.T) {</a>
<a>    pc := &amp;PriceCalculator{</a>
<a>        priceSvc: stubSvcCaller{ <span class="st">"price"</span>: <span class="dv">42</span><span class="fl">.0</span>, },</a>
<a>        vatSvc: stubSvcCaller{ <span class="st">"vat_rate"</span>: <span class="dv">0</span><span class="fl">.10</span>, },</a>
<a>    }</a>

<a>    got, err := pc.PriceForItem(<span class="st">"foo"</span>)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        t.Fatal(err)</a>
<a>    }</a>

<a>    <span class="kw">if</span> exp := <span class="dv">46</span><span class="fl">.2</span>; got != exp {</a>
<a>        t.Fatalf(<span class="st">"expected calculated retail price to be %f; got %f"</span>, exp, got)</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>Of course, our tests wouldn't really be complete without explicitly testing what happens when a required dependency fails! For this, we need yet another stub, which always returns an error:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> stubErrCaller <span class="kw">struct</span> {</a>
<a>    err <span class="dt">error</span></a>
<a>}</a>

<a><span class="kw">func</span> (c stubErrCaller) Call(<span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}) (io.ReadCloser, <span class="dt">error</span>) { </a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, c.err </a>
<a>}</a></pre></div>
<p>With this stub implementation, we can test how the<span> </span><kbd>PriceCalculator</kbd><span> method </span>behaves when particular<span> </span><em>classes of errors</em><span> </span>occur. For example, here is a test that simulates a 404 response from the<span> </span><kbd>vat</kbd><span> </span>service to indicate to the caller that no VAT rate data is available for the specified time period:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestVatSvcErrorHandling(t *testing.T) {</a>
<a>    pc := &amp;PriceCalculator{</a>
<a>        priceSvc: stubSvcCaller{ <span class="st">"price"</span>: <span class="dv">42</span><span class="fl">.0</span>, },</a>
<a>        vatSvc: stubErrCaller{</a>
<a>            err: errors.New(<span class="st">"unexpected response status code: 404"</span>),</a>
<a>        },</a>
<a>    }</a>

<a>    expErr := <span class="st">"unable to retrieve vat percent: call to remote service failed: unexpected response status code: 404"</span></a>
<a>    _, err := pc.PriceForItem(<span class="st">"foo"</span>)</a>
<a>    <span class="kw">if</span> err == <span class="ot">nil</span> || err.Error() != expErr {</a>
<a>        t.Fatalf(<span class="st">"expected to get error:</span><span class="ch">\n</span><span class="st"> %s</span><span class="ch">\n</span><span class="st">got:</span><span class="ch">\n</span><span class="st"> %v"</span>, expErr, err)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Let's run the black- and white-box tests together to check how the total coverage has changed now that we've introduced the new tests:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5f2ba14b-73df-4fe4-b95f-f14bcdfcc3ba.png" style="width:59.17em;height:15.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 2: </span>Running both black- and white-box tests</div>
<p>While the ratio of white-box and black-box tests in the Go standard library's sources seems to strongly favor white-box testing, this should not be construed as a hint that you shouldn't be writing black-box tests! Black-box tests certainly have their place and are very useful when you're attempting to replicate the exact set of conditions and inputs that trigger the particular bug that you are trying to track down. What's more, as we will see in the upcoming sections, black-box tests can often serve as templates for constructing another class of tests, commonly referred to as<span> </span><em>integration tests</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Table-driven tests versus subtests</h1>
                </header>
            
            <article>
                
<p>In this section, we will be comparing two slightly different approaches when it comes to grouping and executing multiple test cases together. These two approaches, namely table-driven tests and subtests, can easily be implemented using the basic primitives provided by Go's built-in<span> </span><kbd>testing</kbd><span> </span>package. For each approach, we will discuss the pros and cons and eventually outline a strategy to fuse the two approaches together so that we can get the best of both worlds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Table-driven tests</h1>
                </header>
            
            <article>
                
<p>Table-driven tests are a quite compact and rather terse way to efficiently test the behavior of a particular piece of code in a host of different scenarios. The format of a typical table-driven test consists of two distinct parts: the test case definitions and the test-runner code.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To demonstrate this, let's examine a possible implementation of the infamous <kbd><span>FizzBuzz</span></kbd><span> </span>test: given a number, <em>N</em>, the <kbd>FizzBuzz</kbd> implementation is expected to return<span> </span><kbd>Fizz</kbd><span> </span>if the number is evenly divisible by 3,<span> </span><kbd>Buzz</kbd><span> </span>if the number is evenly divisible by 5,<span> </span><kbd>FizzBuzz</kbd><span> </span>if the number is evenly divisible by<span> </span><em>both</em><span> </span>3 and 5, or the number itself in all other cases. Here is a listing from the <kbd>Chapter04/table-driven/fizzbuzz.go</kbd><span> </span>file, which contains the implementation we will be working with:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> Evaluate(n <span class="dt">int</span>) <span class="dt">string</span> {</a>
<a>    <span class="kw">if</span> n != <span class="dv">0</span> {</a>
<a>        <span class="kw">switch</span> {</a>
<a>        <span class="kw">case</span> n%<span class="dv">3</span> == <span class="dv">0</span> &amp;&amp; n%<span class="dv">5</span> == <span class="dv">0</span>:</a>
<a>            <span class="kw">return</span> <span class="st">"FizzBuzz"</span></a>
<a>        <span class="kw">case</span> n%<span class="dv">3</span> == <span class="dv">0</span>:</a>
<a>            <span class="kw">return</span> <span class="st">"Fizz"</span></a>
<a>        <span class="kw">case</span> n%<span class="dv">5</span> == <span class="dv">0</span>:</a>
<a>            <span class="kw">return</span> <span class="st">"Buzz"</span></a>
<a>        }</a>
<a>    }</a>
<a>    <span class="kw">return</span> fmt.Sprint(n)</a>
<a>}</a></pre></div>
<p>In the majority of cases, test scenarios will only be accessed by a single test function. With that in mind, a good strategy would be to encapsulate the scenario list inside the test function with the help of a pretty nifty Go feature: anonymous structs. Here is how you would go about defining the struct that contains the scenarios and a scenario list using a single block of code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>specs := []<span class="kw">struct</span> {</a>
<a>    descr <span class="dt">string</span></a>
<a>    input <span class="dt">int</span></a>
<a>    exp   <span class="dt">string</span></a>
<a>}{</a>
<a>    {descr: <span class="st">"evenly divisible by 3"</span>, input: <span class="dv">9</span>, exp: <span class="st">"Fizz"</span>},</a>
<a>    {descr: <span class="st">"evenly divisible by 5"</span>, input: <span class="dv">25</span>, exp: <span class="st">"Buzz"</span>},</a>
<a>    {descr: <span class="st">"evenly divisible by 3 and 5"</span>, input: <span class="dv">15</span>, exp: <span class="st">"FizzBuzz"</span>},</a>
<a>    <span class="co">// The following case is intentionally wrong to trigger a test failure!</span></a>
<a>    {descr: <span class="st">"example of incorrect expectation"</span>, input: <span class="dv">0</span>, exp: <span class="st">"FizzBuzz"</span>},</a>
<a>    {descr: <span class="st">"edge case"</span>, input: <span class="dv">0</span>, exp: <span class="st">"0"</span>},</a>
<a>}</a></pre></div>
<p class="mce-root">In the preceding code snippet, you may have noticed that I included a description for each test case. This is more of a personal preference, but in my opinion, it makes the test code more pleasant to the eyes and, more importantly, helps us easily locate the specs for failing test cases as opposed to visually scanning the entire list looking for the N<sup><sub>th</sub></sup><span> </span>scenario that corresponds to a failed test. Granted, either approach would be efficient for the<span> </span><em>preceding</em><span> </span>example where every test case is neatly laid out in a single line, but think how much more difficult things would be if each spec block contained nested objects and thus each spec was defined using a variable number of lines.</p>
<p>Once we have written down our specs, making sure that we have also included any<span> </span><em>edge</em><span> </span>cases that we can think of, it is time to run the test. This is actually the easy part! All we need to do is iterate the list of specs, invoke the subject under test with the input(s) provided by each spec, and verify that the outputs conform to the expected values:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">for</span> specIndex, spec := <span class="kw">range</span> specs {</a>
<a>    <span class="kw">if</span> got := fizzbuzz.Evaluate(spec.input); got != spec.exp {</a>
<a>        t.Errorf(<span class="st">"[spec %d: %s] expected to get %q; got %q"</span>, specIndex, spec.descr, spec.exp, got)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>One important aspect of the preceding test-runner implementation is that even when a test case fails, we don't<span> </span><em>immediately</em><span> </span>abort the test by invoking any of the<span> </span><kbd>t.Fail/FailNow</kbd><span> </span>or<span> </span><kbd>t.Fatal/f</kbd><span> </span>helpers, but rather exhaust our list of test cases. This is intentional as it allows us to see an overview of all the failing cases in one go. If we were to run the preceding code, we would get the following output:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/75062476-12f6-4b38-93e9-280da38a2930.png" style="width:54.50em;height:12.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3: </span>Example of a failing case in a table-driven test</div>
<p class="mce-root"/>
<p>One unfortunate caveat of this approach is that we cannot request for the<span> </span><kbd>go test</kbd><span> </span>command to explicitly target a specific test case. We can always ask<span> </span><kbd>go test</kbd><span> </span>to only run a<span> </span><em>specific test function</em><span> </span>in isolation (for example, <kbd>go test -run TestFizzBuzzTableDriven</kbd>), but not to<span> </span><em>only</em><span> </span>run the failing test case number 3 within that test function; we need to sequentially test all the cases every single time! Being able to target specific test cases would be a time-saver if our test-runner code was complex and each test case took quite a bit of time to execute.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subtests</h1>
                </header>
            
            <article>
                
<p>With the release of Go 1.7, the built-in<span> </span><em>testing</em><span> </span>package gained support for running subtests. Subtests are nothing more than a hierarchy of test functions that are executed sequentially. This hierarchical structuring of the test code is akin to the notion of a test suite that you may have been exposed to in other programming languages.</p>
<p>So, how does it work? The<span> </span><kbd>testing.T</kbd><span> </span>type has been augmented with a new method called<span> </span><kbd>Run</kbd><span> </span>that has the following signature:</p>
<pre>Run(description string, func(t *testing.T)) </pre>
<p>This new method provides a new mechanism for spawning subtests that will run in isolation while still retaining the ability to use the parent test function to perform any required setup and teardown steps.</p>
<p>As you might expect, since each subtest function receives its own<span> </span><kbd>testing.T</kbd><span> </span>instance argument, it can, in turn, spawn additional subtests that are nested underneath it. Here's what a typical test would look like when following this approach:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestXYZ(t *testing.T){</a>
<a>    <span class="co">// Run suite setup code...</span></a>

<a>    t.Run(<span class="st">"test1"</span>, <span class="kw">func</span>(t *testing.T){</a>
<a>        <span class="co">// test1 code</span></a>
<a>    })</a>
<a>    </a>
<a>    t.Run(<span class="st">"test2"</span>, <span class="kw">func</span>(t *testing.T){</a>
<a>        <span class="co">// test2 code</span></a>
<a>    })</a>

<a>    <span class="co">// Run suite tear-down code...</span></a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>What's more, each subtest gets its own unique name, which is generated by concatenating the names of all its ancestor test functions and the description string that gets passed to the invocation of<span> </span><kbd>Run</kbd>. This makes it easy to target any subtest in a particular hierarchy tree by specifying its name to the<span> </span><kbd>-run</kbd><span> </span>argument when invoking<span> </span><kbd>go test</kbd>. For example, in the preceding code snippet, we can target<span> </span><kbd>test2</kbd><span> </span>by running<span> </span><kbd>go test -run TestXYZ/test2</kbd>.</p>
<p>One disadvantage of subtests compared to their test-driven brethren is that they are defined in a much more verbose way. This could prove to be a bit of a challenge if we need to define a large number of test scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The best of both worlds</h1>
                </header>
            
            <article>
                
<p>At the end of the day, nothing precludes us from combining these two approaches into a hybrid approach that gives us the best of both worlds: the terseness of table-driven tests and the selective targeting of subtests.</p>
<p>To achieve this, we need to define our table-driven specs, just like we did before. Following that, we iterate the spec list and spawn a subtest for each test case. Here's how we could adapt our <kbd>FizzBuzz</kbd> tests so that they follow this pattern:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestFizzBuzzTableDrivenSubtests(t *testing.T) {</a>
<a>    specs := []<span class="kw">struct</span> {</a>
<a>        descr, exp <span class="dt">string</span></a>
<a>        input      <span class="dt">int</span></a>
<a>    }{</a>
<a>        {descr: <span class="st">"evenly divisible by 3"</span>, input: <span class="dv">9</span>, exp: <span class="st">"Fizz"</span>},</a>
<a>        {descr: <span class="st">"evenly divisible by 3 and 5"</span>, input: <span class="dv">15</span>, exp: <span class="st">"FizzBuzz"</span>},</a>
<a>        {descr: <span class="st">"edge case"</span>, input: <span class="dv">0</span>, exp: <span class="st">"0"</span>},</a>
<a>    }</a>
<a>    <span class="kw">for</span> specIndex, spec := <span class="kw">range</span> specs {</a>
<a>        t.Run(spec.descr, <span class="kw">func</span>(t *testing.T) {</a>
<a>            <span class="kw">if</span> got := fizzbuzz.Evaluate(spec.input); got != spec.exp {</a>
<a>                t.Errorf(<span class="st">"[spec %d: %s] expected to get %q; got %q"</span>, specIndex, spec.descr, spec.exp, got)</a>
<a>            }</a>
<a>        })</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Let's say we wanted to only run the second test case. We can easily achieve this by passing its fully qualified name as the value of the<span> </span><kbd>-run</kbd><span> </span>flag when running <kbd>go test</kbd>:</p>
<pre>go test -run TestFizzBuzzTableDrivenSubtests/evenly_divisible_by_3_and_5</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using third-party testing frameworks</h1>
                </header>
            
            <article>
                
<p>One great thing about testing Go code is that the language itself comes with batteries included: it ships with a built-in, albeit minimalistic, framework for authoring and running tests.</p>
<p>From a purist's perspective, that's all that you need to be up and running! The built-in<span> </span><kbd>testing</kbd><span> </span>package provides all the required mechanisms for running, skipping, or failing tests. All the software engineer needs to do is set up the required test dependencies and write the appropriate predicates for each test. One caveat of using the<span> </span><kbd>testing</kbd><span> </span>package is that it does not provide any of the more sophisticated test primitives, such as assertions or mocks, that you may be used to if you've come from a Java, Ruby, or Python background. Of course, nothing prevents you from implementing these yourself!</p>
<p>Alternatively, if importing additional test dependencies is something you don't object to, you can make use of one of the several readily available third-party packages that provide all these missing features. Since a full, detailed listing of all third-party test packages is outside of the scope of this book, we will focus our attention on one of the most popular test framework packages out there:<span> </span><kbd>gocheck</kbd>.</p>
<p>The<span> </span><kbd>gocheck</kbd><span> </span>package<span> </span><sup><span class="citation">[3]</span></sup><span> </span>can be installed by running<span> </span><kbd>go get gopkg.in/check.v1</kbd>. It builds on top of the standard Go<span> </span><kbd>testing</kbd> package and provides support for organizing tests into test suites. Each suite is defined using a regular Go struct that you can also exploit so that it stores any additional bits of information that might be needed by your tests.</p>
<p>In order to run each test suite as part of your tests, you need to register it with <kbd>gocheck</kbd> and hook <kbd>gocheck</kbd> to the Go testing package. The following is a short example of how to do that:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">import</span> (</a>
<a>    <span class="st">"testing"</span></a>

<a>    <span class="st">"gopkg.in/check.v1"</span></a>
<a>)</a>

<a><span class="kw">type</span> MySuite <span class="kw">struct</span>{}</a>

<a><span class="co">// Register suite with go check</span></a>
<a><span class="kw">var</span> _ = check.Suite(<span class="bu">new</span>(MySuite))</a>

<a><span class="co">// Hook up gocheck into the "go test" runner.</span></a>
<a><span class="kw">func</span> Test(t *testing.T) { check.TestingT(t) }</a></pre></div>
<p>As you would expect of any framework that supports test suites, <kbd>gocheck</kbd> allows you to optionally specify setup and teardown methods for both the suite and each test by defining any of the following methods on the suite type:</p>
<ul>
<li><kbd>SetUpSuite(c *check.C)</kbd></li>
<li><kbd>SetUpTest(c *check.C)</kbd></li>
<li><kbd>TearDownTest(c *check.C)</kbd></li>
<li><kbd>TearDownSuite(c *check.C)</kbd></li>
</ul>
<p>Likewise, any suite method matching the <kbd>TestXYZ(c *check.C)</kbd><span> pattern w</span>ill be treated as a test and executed when the suit runs. The<span> </span><kbd>check.C</kbd><span> </span>type gives you access to some useful methods, such as the following:</p>
<ul>
<li><kbd>Log/Logf</kbd><span>: P</span>rints a message to the test log</li>
<li><kbd>MkDir</kbd><span>: C</span>reates a temporary folder that is automatically removed after the<span> </span><em>suite</em><span> </span>finishes running</li>
<li><kbd>Succeed/SucceedNow/Fail/FailNow/Fatal/Fatalf</kbd><span>: C</span>ontrols the outcome of a running test</li>
<li><kbd>Assert</kbd><span>: F</span>ails the test if the specified predicate condition isn't met</li>
</ul>
<p>By<span> </span><em>default</em>, <kbd>gocheck</kbd> buffers all its output and only emits it when a test fails. While this helps cut down the noise and speeds up the execution of chatty tests, you might prefer to see all the output. Fortunately, <kbd>gocheck</kbd> supports two levels of verbosity that can be controlled via command-line flags that are passed to the<span> </span><kbd>go test</kbd><span> </span>invocation.</p>
<p>To force <kbd>gocheck</kbd> to output its buffered debug log for all tests, regardless of their pass/fail status, you can run <kbd>go test</kbd> with the<span> </span><kbd>-check.v</kbd><span> </span>argument. The fact that <kbd>gocheck</kbd> prefers to buffer all the logging output is less than ideal when you're trying to figure out why one of your tests hangs. For such situations, you can dial up the verbosity and disable buffering by running <kbd>gocheck</kbd> with the<span> </span><kbd>-check.vv</kbd><span> </span>argument. Finally, if you wish to run a particular test from a test suite (akin to<span> </span><kbd>go test -run XYZ</kbd>), you can run <kbd>gocheck</kbd> with <kbd>-check.f XYZ</kbd><span>, </span>where<span> </span><kbd>XYZ</kbd><span> </span>is a regular expression matching the names of the test(s) you wish to run.</p>
<p>While we mentioned that the<span> </span><kbd>check.C</kbd><span> </span>object provides an<span> </span><kbd>Assert</kbd><span> </span>method, we haven't really gone into any detail on how it works or how the assertion predicates are defined. The signature of<span> </span><kbd>Assert</kbd><span> </span>is as follows:<span> </span></p>
<pre>Assert(obtained interface{}, checker Checker, args ...interface{}) </pre>
<p>The following table<span> </span>contains a list of useful<span> </span><kbd>Checker</kbd><span> </span>implementations provided by <kbd>gocheck</kbd> that you can use to write your test assertions.</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Checker</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="odd">
<td><kbd>Equals</kbd></td>
<td>Check for equality</td>
<td><kbd>c.Assert(res, check.Equals, 42)</kbd></td>
</tr>
<tr class="even">
<td><kbd>DeepEquals</kbd></td>
<td>Check interfaces, slices, and others for equality</td>
<td><kbd>c.Assert(res, check.DeepEquals, []string{"hello", "world"})</kbd></td>
</tr>
<tr class="odd">
<td><kbd>IsNil</kbd></td>
<td>Check if the value is nil</td>
<td><kbd>c.Assert(err, check.IsNil)</kbd></td>
</tr>
<tr class="even">
<td><kbd>HasLen</kbd></td>
<td>Check the length of the slice/map/channel/strings</td>
<td><kbd>c.Assert(list, check.HasLen, 2)</kbd></td>
</tr>
<tr class="odd">
<td><kbd>Matches</kbd></td>
<td>Check that the string matches the regex</td>
<td><kbd>c.Assert(val, check.Matches, ".*hi.*")</kbd></td>
</tr>
<tr class="even">
<td><kbd>ErrorMatches</kbd></td>
<td>Check that the error message matches the regex</td>
<td><kbd>c.Assert(err, check.Matches, ".*not found")</kbd></td>
</tr>
<tr class="odd">
<td><kbd>FitsTypeOf</kbd></td>
<td>Check that the argument is assigned to a variable with the given type</td>
<td><kbd>c.Assert(impl, check.FitsTypeOf, os.Error(nil)</kbd></td>
</tr>
<tr class="even">
<td><kbd>Not</kbd></td>
<td>Invert the check result</td>
<td><kbd>c.Assert(val, check.Not(check.Equals)), 42)</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Of course, if your tests require more sophisticated predicates than the ones built into <kbd>gocheck</kbd>, you can always roll your own by implementing the<span> </span><kbd>Checker</kbd><span> </span>interface.</p>
<p>This concludes our tour of <kbd>gocheck</kbd>. If you are interested in using it in your projects, I would definitely recommend visiting the package home<span> </span><sup><span class="citation">[3]</span></sup><span> </span>and reading its excellent documentation. If you already use <kbd>gocheck</kbd> but want to explore other popular testing frameworks for Go, I would suggest taking a look at the<span> </span><kbd>stretchr/testify</kbd><span> </span>package<span> </span><sup><span class="citation">[7]</span></sup><span>, </span>which offers similar functionality (test suites, assertions, and so on) to <kbd>gocheck</kbd> but also includes support for more advanced test primitives such as mocks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration versus functional testing</h1>
                </header>
            
            <article>
                
<p>In this section, we will attempt to dispel any confusion between the definitions of two very important and useful types of testing:<span> </span><strong>integration</strong><span> </span>tests and<span> </span><strong>functional</strong><span> </span>tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration tests</h1>
                </header>
            
            <article>
                
<p>Integration tests pick up from where unit testing left off. Whereas unit testing ensures that each individual unit of a system works correctly in isolation, integration testing ensures that different units (or services, in a microservice architecture) interoperate correctly.</p>
<p>Let's consider a hypothetical scenario where we are building an e-shop application. Following the SOLID design principles, we have split our backend implementation into a bunch of microservices. Each microservice comes with its own set of unit tests and, by design, exposes an API that adheres to a contract agreed on by<span> </span><em>all</em><span> </span>engineering teams. For the purpose of this demonstration, and to keep things simple, we want to focus our efforts on authoring an integration test for the following two microservices:</p>
<ul>
<li>The<span> </span><strong>product</strong><span> </span>microservice performs the following functions:
<ul>
<li>It exposes a mechanism for manipulating and querying product metadata; for example, to add or remove products, return information about item prices, descriptions, and so on</li>
<li>It provides a notification mechanism for metadata changes that other services can subscribe to</li>
</ul>
</li>
<li>The<span> </span><strong>basket</strong><span> </span>microservice stores the list of items that have been selected by the customer. When a new item is inserted into a customer's basket, the basket service queries the product service for the item metadata and updates the price summary for the basket. At the same time, it subscribes to the product service change stream and updates the basket's contents if the product metadata is updated.</li>
</ul>
<p>One important implementation aspect to be aware of is that each microservice uses its own dedicated data store. Keep in mind though that this approach does not necessarily mean that the data stores are physically separated. Perhaps we are using a single database server and each microservice gets its own database on that server.</p>
<p class="mce-root"/>
<p>The integration test for these two services would live in a separate Go test file, perhaps with an <kbd>_integration_test.go</kbd><span> </span>suffix so that we can immediately tell its purpose just by looking at the filename. The setup phase of the tests expects that the DB instance(s) that are required by the services have already been externally prepared. As we will see later in this chapter, a simple way to provide DB connection settings to our tests is via the use of environment variables. The tests would proceed to spin up the services that we want to test and then run the following integration scenarios:</p>
<ul>
<li>Invoke the product service API to insert a new product into the catalog. Then, it would use the basket service API to add the product to a customer basket and verify that the DB that's used by the basket service contains an entry with the correct product metadata.</li>
<li>Add a product to a customer basket. Then, it would use the product service API to mutate the item description and verify that the relevant basket DB entry is updated correctly.</li>
</ul>
<p>One caveat of integration tests is that we need to maintain strict isolation between individual tests. Consequently, before running each test scenario, we must ensure that the internal state of each service is reset properly. Typically, this means that we need to flush the database that's used by each service and perhaps also restart the services in case they also maintain any additional in-memory state.</p>
<p>Evidently, the effort that's required to set up, wire together, and prime the various components that are needed for each integration test makes writing such tests quite a tedious process. Not to diminish the significance of integration testing, it is my belief that engineers can make better use of their time by writing a large number of unit tests and just a handful of integration tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Functional tests</h1>
                </header>
            
            <article>
                
<p>Functional or end-to-end tests take system testing to a whole new level. The primary purpose of functional testing is to ensure that the<span> </span><em>complete </em>system is working as expected. To this end, functional tests are designed to model complex interaction scenarios that involve multiple system components. A very common use case for functional tests is to verify end-to-end correctness by simulating a user's journey through the system.</p>
<p>For instance, a functional test for an online music streaming service would act as a new user who would subscribe to the service, search for a particular song, add it to their playlist, and perhaps submit a rating for the song once it's done playing.</p>
<p>It is important to clarify that all the preceding interactions are meant to occur via the web browser. This is a clear-cut case where we need to resort to a<span> </span><em>scriptable</em><span> </span>browser automation framework such as Selenium<span> </span><sup><span class="citation">[6]</span></sup><span> </span>in order to accurately model all the required button clicks that we expect a real user to perform while using the system.</p>
<p>While you could probably find a package that provides Go bindings for Selenium, the truth of the matter is that Go is not the best tool for writing functional tests. Contrary to unit and integration tests, which live within Go files, functional tests are normally written in languages such as Python, JavaScript, or Ruby. Another important distinction is that, due to their increased complexity, functional tests take<span> a </span><em>significantly</em><span> </span>longer time to run.</p>
<p>While it's not uncommon for software engineers working on a particular feature to also provide functional test suites, in the majority of cases, the task of authoring functional tests is one of the primary responsibilities of the <strong>quality assurance</strong> (<strong>QA</strong>) team. As a matter of fact, functional tests are the front and center part of the pre-release workflow that's followed by QA engineers before they can give the green light for a new release.</p>
<p>Functional tests don't usually target production systems; you wouldn't want to fill up your production DB with dummy user accounts, right? Instead, functional tests target<span> </span><strong>staging environments</strong>, which are isolated and often downsized sandboxes that mirror the setup of the actual production environment. This includes all the services and resources (databases, message queues, and so on ) that are needed for the system to operate. One exception is that access to external third-party services such as payment gateways or email providers is typically mocked unless a particular functional test requests otherwise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Functional tests part deux – testing in production!</h1>
                </header>
            
            <article>
                
<p>That's not to say that you cannot actually run your functional tests in a live production environment! Surely whether that's a good or bad idea is a debatable point, but if you do decide to go down that route, there are a few patterns that you can apply to achieve this in a<span> </span><em>safe</em><span> </span>and<span> </span><em>controlled</em><span> </span>way.</p>
<p>To get the ball rolling, you can begin by revising your DB schemas so that they include a field that indicates whether each row contains real data or is part of a test run. Each service could then silently ignore any test records when it handles live traffic.</p>
<p>If you are working with a microservice architecture, you can engineer your services so that they do not talk to other services directly but rather to do so via a local proxy that is deployed in tandem with each service as a<span> </span><em>sidecar</em><span> </span>process. This pattern is known as the<span> </span><em>ambassador</em><span> </span>pattern and opens up the possibility of implementing a wide range of really cool tricks, as we will see later in this chapter.</p>
<p>Since all the proxies are initially configured to talk to the already deployed services, nothing prevents us from deploying a newer version of a particular service and have it run side-by-side with the existing version. Since no traffic can reach the newly deployed service, it is common to use the term<span> </span><strong>dark launch</strong><span> </span>to refer to this kind of deployment.</p>
<p>Once the new versions of the services that we need to test against have been successfully deployed, each functional test can reconfigure the local proxies to divert<span> </span><em>test</em><span> </span>traffic (identified perhaps by an HTTP header or an other type of tag) to the newly deployed services. This can be seen in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/bcc8ab64-b290-42f0-899b-63dfc6b4a975.png" style="width:54.67em;height:6.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 4:</span><span> </span>Using the ambassador pattern to test in production</div>
<p>This neat trick allows us to run our tests in production without interfering with live traffic. As you can tell, live testing requires substantially more preparation effort compared to testing in a sandbox. This is probably one of the reasons why QA teams seem to prefer using staging environments instead.</p>
<p>In my view, if your system is built in such a way that you can easily introduce one of these patterns to facilitate live testing, you should definitely go for it. After all, there is only so much data that you can collect when running in an isolated environment whose load and traffic profiles don't really align with the ones of your production systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Smoke tests</h1>
                </header>
            
            <article>
                
<p>Smoke tests or build acceptance tests constitute a special family of tests that are traditionally used as early sanity checks by QA teams.</p>
<p>The use of the word<span> </span><em>smoke</em><span> </span>alludes to the old adage that<span> </span><em>wherever there is smoke, there is also fire</em>. These checks are explicitly designed to identify early warning signals that something is wrong. It goes without saying that any issue uncovered by a smoke test is treated by the QA team as a show-stopper; if smoke tests fail, no further testing is performed. The QA team reports its findings to the development team and waits for a revised release candidate to be submitted for testing.</p>
<p>Once the smoke tests successfully pass, the QA team proceeds to run their suite of functional tests before giving the green light for release. The following diagram summarizes the process of running smoke tests for QA purposes:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/18852742-a560-4427-8b00-fed2b5b8b59d.png" style="width:56.42em;height:4.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 5:</span><span> </span>Running smoke tests as part of the QA process</div>
<p>When it comes to execution, smoke tests are the exact antithesis of functional tests. While functional tests are allowed to execute for long periods of time, smoke tests must execute as quickly as possible. As a result, smoke tests are crafted so as to exercise specific, albeit limited, flows in the user-facing parts of a system that are deemed critical for the system's operation. For example, smoke tests for a social network application would verify the following:</p>
<ul>
<li>A user can login with a valid username and password</li>
<li>Clicking the <span class="packt_screen">like</span> button on a post increases the like counter for that post</li>
<li>Deleting a contact removes them from the user's friends list</li>
<li>Clicking the <span class="packt_screen">logout</span> button signs the user out of the service</li>
</ul>
<p>The responsibility for authoring, evolving, and maintaining smoke tests usually falls on the shoulders of the QA team. Consequently, it makes sense for the QA team to maintain smoke tests in a separate, dedicated repository that they own and control. An interesting question here is whether the QA team will opt to execute the smoke tests manually or invest the time and effort that's required to automate the process. The logical, albeit slightly cliché, answer is: it depends...</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At the end of the day, the decision boils down to the size of the QA team, the individual preferences of the team's members, and the test infrastructure that's available and is at the team's disposal. Needless to say, automated smoke tests are, hands down, the recommended option since the QA team can efficiently verify a plethora of scenarios in a small amount of time. On the other hand, if the build release frequency is low, you could argue that doing manual smoke tests has a smaller cost and makes better use of the QA team's time and resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chaos testing – breaking your systems in fun and interesting ways!</h1>
                </header>
            
            <article>
                
<p>Let me begin this section with a question! How confident are you about the quality of your current software stack? If your answer happens to be something along the lines of,<span> </span><em>I don't really know until I make it fail</em>, then we are in total agreement! If not, let me introduce you to the concept of<span> </span><strong>chaos testing</strong>.</p>
<p>Chaos testing is a term that was initially coined by the engineering team at Netflix. The key point behind chaos testing is to evaluate your system's behavior when various components exhibit different types of failure. So, what kinds of failure are we talking about here? Here are a few interesting examples, ordered by their relative severity (low to high):</p>
<ul>
<li>A service fails to reach another service it depends on</li>
<li>Calls between services exhibit high latency/jitter</li>
<li>Network links experience packet loss</li>
<li>A database node fails</li>
<li>We lose a critical piece of storage</li>
<li>Our cloud provider suffers an outage in an entire availability zone</li>
</ul>
<p>Netflix engineers point out that we shouldn't be afraid of failure but rather embrace it and learn as much as we can about it. All these learnings can be applied to fine-tune the design of our systems so that they become incrementally more and more robust and resilient against failure.</p>
<p>Some of these types of failure have a low likelihood of occurring. Nevertheless, it's better if we are prepared to mitigate them when they actually<span> </span><em>do</em><span> </span>occur. After all, from a system stability perspective, it's always preferred to operate in a preventive fashion rather than trying to react (often under lots of pressure) when an outage occurs.</p>
<p class="mce-root"/>
<p>You might be wondering: <em>but, if some failures are statistically unlikely to occur, how can we trigger them in the first place?</em> The only way to do this is to engineer our systems in such a way that failure can be injected on demand. In the <em>Functional tests part deux – testing in production!</em> section, we talked about the ambassador pattern, which can help us achieve exactly that.</p>
<div class="packt_infobox">The ambassador pattern decouples service discovery and communication from the actual service implementation. This is achieved with the help of a sidecar process that gets deployed with each service and acts as a proxy.<br/>
<br/>
The sidecar proxy service can be used for other purposes, such as conditionally routing traffic based on tags or headers, acting as a circuit breaker, bifurcating traffic to perform A/B testing, logging requests, enforcing security rules, or to<span> </span><em>inject artificial failures into the system</em>.</div>
<p>From a chaos engineering perspective, the sidecar proxy is an easy avenue for introducing failures. Let's look at some examples of how we can exploit the proxy to inject failure into the system:</p>
<ul>
<li>Instruct the proxy to delay outgoing requests or wait before returning upstream responses to the service that initiated the request. This is an effective way to model latency. If we opt not to use fixed intervals but to randomize them, we can inject jitter into intra-service communication.</li>
<li>Configure the proxy to drop outgoing requests with probability <em>P</em>. This emulates a degraded network connection.</li>
<li>Configure the proxy for a single service to drop all outgoing traffic to another service. <span>At the same time, all the other service proxies are set up to forward traffic as usual. This emulates a network partition</span>.</li>
</ul>
<p>That's not all. We can take chaos testing even further if we are running our systems on a cloud provider that provides us with an API that we can use to break even more things! For instance, we could use such an API to randomly start killing nodes or to take down one or all of our load balancers and check whether our system can automatically recover by itself. With chaos testing, the only limit is your own imagination!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tips and tricks for writing tests</h1>
                </header>
            
            <article>
                
<p>In this section, I will be going through some interesting ideas that can help super-charge your daily test workflow. What's more, we will also be exploring some neat tricks that you can use to isolate tests, mock calls to system binaries, and control time within your tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using environment variables to set up or skip tests</h1>
                </header>
            
            <article>
                
<p>In a project of any size, you are eventually bound to come across a series of tests that depend on external resources that are created or configured in an ad hoc fashion.</p>
<p>A typical example of such a use case would be a test suite that talks to a database. As the engineers working locally on the code base, we would probably spin up a local database instance with a more or less predictable endpoint and use that for testing. However, when running under CI, we might be required to use an already provisioned database instance on some cloud provider or, more often than not, the CI setup phase may need to start a database in a Docker container, a process that would yield a non-predictable endpoint to be connected to.</p>
<p>To support scenarios such as these, we must avoid hardcoding the location of resource endpoints to our tests and<span> </span><em>defer</em><span> </span>their discovery and configuration until the time when the test runs. To this end, one solution would be to use a set of environment variables to supply this information to our tests. Here is a simple test example from the <kbd><span>Chapter04</span>/db</kbd> package that illustrates how this can be achieved:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestDBConnection(t *testing.T) {</a>
<a>    host, port, dbName, user, pass := os.Getenv(<span class="st">"DB_HOST"</span>), os.Getenv(<span class="st">"DB_PORT"</span>),</a>
<a>        os.Getenv(<span class="st">"DB_NAME"</span>), os.Getenv(<span class="st">"DB_USER"</span>), os.Getenv(<span class="st">"DB_PASS"</span>)</a>

<a>    db, err := sql.Open(<span class="st">"postgres"</span>, makeDSN(user, pass, dbName, host, port))</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        t.Fatal(err)</a>
<a>    }</a>
<a>    _ = db.Close()</a>
<a>    t.Log(<span class="st">"Connection to DB succeeded"</span>)</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>The preceding example makes testing a breeze, regardless of whether we run the tests locally or in a CI environment. But what if our tests require a specialized DB that is not that easy to spin up locally? Maybe we need a DB that operates in a clustered configuration or one whose memory requirements exceed the memory that's available on our development machine. Wouldn't it be great if we could just<span> </span><em>skip</em><span> </span>that test when running locally?</p>
<p>It turns out that this is also quite easy to achieve with exactly the same mechanism that we used for configuring our DB endpoint. To be more precise, the<span> </span><em>absence</em><span> </span>of the required configuration settings could serve as a hint to the test that it needs to be skipped. In the preceding example, we can achieve this by adding a simple <kbd>if</kbd> block after fetching the environment values for the DB configuration:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">if</span> host == <span class="st">""</span> {</a>
<a>    t.Skip(<span class="st">"Skipping test as DB connection info is not present"</span>)</a>
<a>}</a></pre></div>
<p>Excellent! Now, if we don't export the<span> </span><kbd>DB_HOST</kbd><span> environment variable </span>before running our tests, this particular test will be skipped.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speeding up testing for local development</h1>
                </header>
            
            <article>
                
<p>In this section, we will be covering a couple of approaches to accelerating testing when working locally. Just to clarify, I am assuming that you already have a proper CI infrastructure in place; no matter what shortcuts we will be taking here, the CI will always run all the tests.</p>
<p>The first item on our agenda is slow versus fast tests. For the sake of argument, say that we find ourselves in a situation where we are writing a fully-fledged, pure CPU ray tracer implementation in Go. To ensure correctness and avoid regressions while we are tweaking our implementation, we have introduced a test suite that renders a sequence of example scenes and compares the ray tracer output to a series of prerendered reference images.</p>
<p>Since this is a pure CPU implementation and our tests render at full-HD resolution, running each test would take, as you can imagine, quite a bit of time. This is not an issue when running on the CI but can definitely be an impediment when working locally.</p>
<p>To make matters worse,<span> </span><kbd>go test</kbd> will try to run all the tests, even if one of them fails. Additionally, it will automatically fail tests that take a long time (over 10 minutes) to run. Fortunately, the<span> </span><kbd>go test</kbd><span> </span>command supports some really useful flags that we can use to rectify these issues.</p>
<p class="mce-root"/>
<p>To begin with, we can notify long-running tests that they should try to shorten their runtime by passing the<span> </span><kbd>-short</kbd><span> </span>flag to the<span> </span><kbd>go test</kbd><span> </span>invocation. This flag gets exposed by the<span> </span><kbd>testing</kbd><span> </span>package via the<span> </span><kbd>Short</kbd><span> </span>helper function, which returns<span> </span><kbd>true</kbd><span> </span>when the<span> </span><kbd>-short</kbd><span> </span>flag is defined. So, how can we use this flag to make our ray tracer tests run faster?</p>
<p>One approach would be to simply skip tests that are known to take a really long time to run. A much better alternative would be to detect the presence of the<span> </span><kbd>-short</kbd><span> </span>flag and<span> </span><em>dial down</em><span> </span>the output resolution of the ray tracer, say, to something such as a quarter of the original resolution. This change would still allow us to verify the rendering output when testing locally while at the same time would constrain the total runtime of our tests to an acceptable level.</p>
<p>Coming back to the issue of<span> </span><kbd>go test</kbd><span> </span>running all the tests, even if one of them fails, we can actually instruct<span> </span><kbd>go test</kbd><span> </span>to immediately abort if it detects a failing test by passing the<span> </span><kbd>-failfast</kbd><span> </span>command-line flag. Moreover, we can tune the maximum, per-test execution time with the help of the<span> </span><kbd>-timeout</kbd><span> </span>flag. It accepts any string that can be parsed by the<span> </span><kbd>time.Duration</kbd><span> </span>type (for example,<span> </span><em>1h</em>), but if your tests take an unpredictable amount of time to run, you could also pass a timeout value of<span> </span><em>0</em><span> </span>to disable timeouts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Excluding classes of tests via build flags</h1>
                </header>
            
            <article>
                
<p>So far, we have discussed white- and black-box tests, integration, and end-to-end tests. By including tests from all these categories in our projects, we can rest assured that the code base will behave as expected in a multitude of different scenarios.</p>
<p>Now, imagine we are working on a particular feature and we<span> </span><em>only</em><span> </span>want to run the unit tests. Alternatively, we may <em>only</em><span> </span>need to run the integration tests to ensure that our changes do not introduce regression to other packages. How can we do that?</p>
<p>The rather simplistic approach would be to maintain separate folders for each test category, but that would veer away from what is considered to be idiomatic Go. Another alternative would be to add the category name as a prefix or suffix to our tests and run<span> </span><kbd>go test</kbd> with the<span> </span><kbd>-run</kbd><span> </span>flag (or with the<span> </span><kbd>-check.f</kbd><span> </span>flag if we are using a third-party package such as <kbd>gocheck</kbd><span> </span><sup><span class="citation">[3]</span></sup>) to only run the tests whose names match a particular regular expression. It stands to reason that while this approach will work, it's quite error-prone; for larger code bases, we would need to compose elaborate regular expressions that might not match all the tests that we need to run.</p>
<p>A smarter solution would be to take advantage of Go's support for conditional compilation and repurpose it to serve our needs. This is a great time to explain what conditional compilation is all about and, most importantly, how it works under the hood.</p>
<p>When a package is being built, the<span> </span><kbd>go build</kbd><span> </span>command scans the comments inside each Go file, looking for special keywords that can be interpreted as compiler directives.<span> </span><strong>Build tags</strong><span> </span>are one example of such an annotation. They are used by<span> </span><kbd>go build</kbd><span> </span>to decide whether a particular Go file in a package should be passed to the Go compiler. The general syntax for a build tag is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// +build tag1 ... tagN</span></a>

<a><span class="kw">package</span> some_package</a></pre></div>
<p>To be correctly recognized by<span> </span><kbd>go build</kbd>, all the build tags must appear as a comment at the<span> </span><em>top</em><span> </span>of a Go file. While you are allowed to define multiple build tags, it is very important that the<span> </span><em>last</em><span> </span>build tag is separated with a blank (non-comment) line from the package name declaration. Otherwise,<span> </span><kbd>go build</kbd><span> </span>will just assume that the build tag is part of a package-level comment and simply ignore it. Software engineers that are new to the concept of Go build tags occasionally fall into this trap, so if you find yourself scratching your head, wondering why build tags are not being picked up, the lack of a blank line after the build tag is the most likely suspect.</p>
<p>Let's take a closer look at the intricacies of the tag syntax and elaborate on the rules that are applied by<span> </span><kbd>go build</kbd><span> </span>to interpret the list of tags following the<span> </span><kbd>+build</kbd><span> </span>keyword:</p>
<ul>
<li>Tags separated by<span> </span><em>whitespace</em><span> </span>are evaluated as a list of OR conditions.</li>
<li>Tags separated by a<span> </span><em>comma</em><span> </span>are evaluated as a list of AND conditions.</li>
<li>Tags beginning with<span> </span><kbd>!</kbd><span> </span>are treated as NOT conditions.</li>
<li>If multiple<span> </span><kbd>+build</kbd><span> </span>lines are defined, they are joined together as an AND condition.</li>
</ul>
<p>The<span> </span><kbd>go build</kbd><span> </span>command recognizes several predefined tags for the target operating system (for example, <kbd>linux, windows, darwin</kbd>), CPU architecture (for example,<span> </span><kbd>amd64, 386, arm64</kbd>), and even the version of the Go compiler (for example, <kbd>go1.10</kbd><span> </span>to specify Go 1.10 onward). The following table<span> </span>shows a few examples that use tags to model complex build constraints.</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Build Target Scenario</strong></td>
<td><strong>Build tag</strong></td>
</tr>
<tr class="odd">
<td>Only when the target is Linux</td>
<td>linux</td>
</tr>
<tr class="even">
<td>Linux or macOS</td>
<td>linux darwin</td>
</tr>
<tr class="odd">
<td>x64 targets but only with Go compiler &gt;= 1.10</td>
<td>amd64,go1.10</td>
</tr>
<tr class="even">
<td>32-bit Linux OR 64-bit all platforms <em>except</em> OS X</td>
<td>linux,386 amd64,!darwin</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>By now, you should have a better understanding of how build tags work. But how does all this information apply to our particular use case? First of all, let me highlight the fact that test files are also regular Go files and, as such, they are also scanned for the presence of build tags! Secondly, we are not limited to the built-in tags <span>– we</span> can also define our own custom tags and pass them to<span> </span><kbd>go build</kbd> <em>or</em><span> </span><kbd>go test</kbd><span> </span>via the<span> </span><kbd>-tags</kbd><span> </span>command-line flag.</p>
<p>You can probably see where I am going with this… We can start by defining a build tag for each family of tests, for example, <kbd>integration_tests</kbd>,<span> </span><kbd>unit_tests</kbd><span>, </span>and<span> </span><kbd>e2e_tests</kbd>. Additionally, we will define an<span> </span><kbd>all_tests</kbd><span> </span>tag since we need to retain the capability to run all the tests together. Finally, we will edit our test files and add the following build tag annotations:</p>
<ul>
<li><kbd>+build unit_tests all_tests</kbd><span> </span>to the files containing the unit tests</li>
<li><kbd>+build integration_tests all_tests</kbd><span> </span>to the files containing the integration tests</li>
<li><kbd>+build e2e_tests all_tests</kbd><span> </span>to the files containing the end-to-end tests</li>
</ul>
<p>If you wish to experiment with the preceding example, you can check out the contents of the <kbd><span>Chapter04</span>/buildtags</kbd><span> </span>package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">This is not the output you are looking for – mocking calls to external binaries</h1>
                </header>
            
            <article>
                
<p>Have you ever struggled when trying to test code that calls out to an external process and then uses the output as part of the implemented business logic? In some cases, it might be possible to use some of the tricks we have discussed so far to decorate our code with hooks that tests can use to mock the executed command's output. Unfortunately, sometimes this will not be possible. For instance, the code under test could import a third-party package that is actually the one that's responsible for executing some external command.</p>
<p>The <kbd><span>Chapter04</span>/pinger</kbd><span> </span>package exports a function called<span> </span><kbd>RoundtripTime</kbd>. Its job is to calculate the round-trip time for reaching a remote host. Under the hood, it calls out to the<span> </span><kbd>ping</kbd><span> </span>command and parses its output. This is how it is implemented:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> RoundtripTime(host <span class="dt">string</span>) (time.Duration, <span class="dt">error</span>) {</a>
<a>    <span class="kw">var</span> argList = []<span class="dt">string</span>{host}</a>
<a>    <span class="kw">if</span> runtime.GOOS == <span class="st">"windows"</span> {</a>
<a>        argList = <span class="bu">append</span>(argList, <span class="st">"-n"</span>, <span class="st">"1"</span>, <span class="st">"-l"</span>, <span class="st">"32"</span>)</a>
<a>    } <span class="kw">else</span> {</a>
<a>        argList = <span class="bu">append</span>(argList, <span class="st">"-c"</span>, <span class="st">"1"</span>, <span class="st">"-s"</span>, <span class="st">"32"</span>)</a>
<a>    }</a>

<a>    out, err := exec.Command(<span class="st">"ping"</span>, argList...).Output()</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="dv">0</span>, xerrors.Errorf(<span class="st">"command execution failed: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">return</span> extractRTT(<span class="dt">string</span>(out))</a>
<a>}</a></pre></div>
<p>Since the<span> </span><kbd>ping</kbd><span> </span>command flag names are slightly different between Unix-like systems and Windows, the code relies on OS sniffing to select the appropriate set of flags so that <kbd>ping</kbd> will send out a single request with a 32-byte payload. The<span> </span><kbd>extractRTT</kbd><span> </span>helper function just applies a regular expression to extract the timing information and convert it into a<span> </span><kbd>time.Duration</kbd><span> </span>value.</p>
<p>For the purpose of this demonstration, let's assume that we are operating a video streaming service and our business logic (which lives in another Go package) uses the<span> </span><kbd>RoundtripTime</kbd><span> </span>results to redirect our customers to the edge server that is closest to them. We have been tasked with writing an <em>end-to-end</em> test for the service so, unfortunately, we are not allowed to mock any of the<span> </span>calls<span> </span>to the<span> </span><kbd>RoundtripTime</kbd> function; our test actually needs to invoke the <kbd>ping</kbd> command!</p>
<p>If you ever find yourself in a similar situation, let me suggest a nice trick that you can use to mock calls to external processes. I came across the concept that I am about to describe when I first joined Canonical to work on the<span> </span>juju<span> </span>code base. In hindsight, the idea is pretty straightforward. The implementation, however, is not something immediately obvious and requires some platform-specific tweaks, so kudos to the engineers that came up with it.</p>
<p>This approach exploits the fact that when you try to execute a binary (for example, using the<span> </span><kbd>Command</kbd><span> </span>function from the<span> </span><kbd>os/exec</kbd><span> </span>package), the operating system will look for the binary in the current working directory and if that fails, it will sequentially scan each entry in the system's<span> </span><kbd>PATH</kbd><span> </span>environment variable, trying to locate it. To our advantage, both Unix-like systems and Windows follow the same logic. Another interesting observation is that when you ask Windows to execute a command named<span> </span><kbd>foo</kbd>, it will search for an executable called<span> </span><kbd>foo.exe</kbd><span> </span><em>or</em><span> </span>a batch file called<span> </span><kbd>foo.bat</kbd>.</p>
<p>To mock an external process, we need to provide two pieces of information: the expected process output and an appropriate status code; an exit status code of<span> </span><em>zero</em><span> </span>would indicate that the process completed successfully. Therefore, if we could somehow create an<span> </span><em>executable</em><span> </span>shell script that prints out the expected output before exiting with a particular status code and prepend its path to the<span> </span><em>front</em><span> </span>of the system's<span> </span><kbd>PATH</kbd><span> </span>variable, we could trick the operating system into executing our script instead of the real binary!</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At this point, we are entering the realm of OS-specific code. This practice will probably be frowned upon by some engineers, with the argument that Go programs are<span> </span><em>usually</em><span> </span>supposed to be portable across operating systems and CPU architectures. In this case, however, we just need to deal with two operating system families so we can probably get away with it. Let's take a look at the templates for the Unix and Windows shell scripts that our test code will be injecting. Here is the one for Unix:</p>
<div class="sourceCode">
<pre class="sourceCode bash"><a><span class="co">#!/bin/bash </span></a>
<a><span class="fu">cat</span> <span class="op">&lt;&lt;!!!EOF!!!</span> <span class="kw">|</span> <span class="fu">perl</span> -pe <span class="st">'chomp if eof'</span></a>
<a>%s</a>
<a>!!!EOF!!!</a>
<a>exit %d</a></pre></div>
<p>The script uses the here document syntax<span> </span><sup><span class="citation">[1]</span></sup><span> </span>to output the text between the two<span> </span><kbd>!!!EOF!!!</kbd><span> </span>labels in verbatim. Since here documents include an extra, trailing line-feed character, we pipe the output to a Perl one-liner to strip it off. The<span> </span><kbd>%s</kbd><span> </span>placeholder will be replaced with the text (which can span several lines) that we want our command to output. Finally, the<span> </span><kbd>%d</kbd><span> </span>placeholder will be replaced with the exit code that the command will return.</p>
<p>The Windows version is much simpler since here documents are not supported by the built-in shell interpreter (<kbd>cmd.exe</kbd>). Due to this, I have opted to write the output to a file and just have the shell script print it to the standard output. Here's what this looks like:</p>
<div class="sourceCode">
<pre class="sourceCode bash"><a><span class="ex">@echo</span> off</a>
<a><span class="bu">type</span> %s</a>
<a><span class="bu">exit</span> /B %d</a></pre></div>
<p>In this case, the<span> </span><kbd>%s</kbd><span> </span>placeholder will be replaced with the path to the external file containing the output for the mocked command and, as before, the<span> </span><kbd>%d</kbd><span> </span>placeholder will be replaced with the exit code for the command.</p>
<p>In our test file, we will define a helper function called<span> </span><kbd>mockCmdOutput</kbd>. Due to space constraints, I will not be including the full listing of the function here but rather a short synopsis of how it works (for the full implementation, you can check out the <kbd><span>Chapter04</span>/pinger</kbd><span> </span>sources). In a nutshell,<span> </span><kbd>mockCmdOutput</kbd> does the following:</p>
<ul>
<li>Creates a temporary folder that will be automatically removed after the test completes</li>
<li>Selects the appropriate shell script template, depending on the operating system</li>
<li>Writes the shell script to the temporary folder and changes its permissions so that it becomes executable (important for Unix-like systems)</li>
<li>Prepends the temporary folder to the beginning of the<span> </span><kbd>PATH</kbd><span> </span>environment variable for the currently running process (<kbd>go test</kbd>)</li>
</ul>
<p>Since <kbd>mockCmdOutput</kbd><span> </span>modifies the system path, we<span> </span><em>must</em><span> </span>ensure that it gets reset to its original value<span> </span><em>before</em><span> </span>each of our tests runs. We can easily achieve this by grouping our tests into a<span> </span><kbd>gocheck</kbd><span> </span>test suite and providing a test setup function to save the original<span> </span><kbd>PATH</kbd><span> </span>value and a test teardown function to restore it from the saved value. With all the plumbing in place, here is how we can write a test function that mocks the output of<span> </span><kbd>ping</kbd>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *PingerSuite) TestFakePing(c *check.C) {</a>
<a>    mock := <span class="st">"32 bytes from 127.0.0.1: icmp_seq=0 ttl=32 time=42000 ms"</span></a>
<a>    mockCmdOutput(c, <span class="st">"ping"</span>, mock, <span class="dv">0</span>)</a>

<a>    got, err := pinger.RoundtripTime(<span class="st">"127.0.0.1"</span>)</a>
<a>    c.Assert(err, check.IsNil)</a>
<a>    c.Assert(got, check.Equals, <span class="dv">42</span>*time.Second)</a>
<a>}</a></pre></div>
<p>To make sure that the command was mocked correctly, we set up our test to do a round-trip measurement to localhost (typically taking 1 ms or less) and mock the <kbd>ping</kbd> command to return a ridiculously high number (42 seconds). Try running the test on OS X, Linux, or Windows; you will always get consistent results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing timeouts is easy when you have all the time in the world!</h1>
                </header>
            
            <article>
                
<p>I am pretty sure that, at some point, you have written some code that relies on the time-keeping functions provided by the standard library's<span> </span><kbd>time</kbd><span> </span>package. Perhaps it's some code that periodically polls a remote endpoint <span>– </span>a great case for using<span> </span><kbd>time.NewTicker</kbd> <span>– or maybe </span>you are using<span> </span><kbd>time.After</kbd><span> </span>to implement a timeout mechanism inside a go-routine that waits for an event to occur. In a slightly different scenario, using<span> </span><kbd>time.NewTimer</kbd><span> </span>to provide your server code with ample time to drain all its connections before shutting down would also be a stellar idea.</p>
<p class="mce-root"/>
<p><span>However, testing code that uses any of these patterns is not a trivial thing. For example, let's say that you are trying to test a piece of code that blocks until an event is received or a specific amount of time elapses without receiving an event. In the latter case, it would return some sort of timeout error to the caller. To verify that the timeout logic works as expected and to avoid locking up the test runner if the blocking code never returns, the typical approach would be to spin up a go-routine that runs the blocking code and then signals (for example, over a channel) when the expected error is returned. The test function that starts the go-routine would then use a </span><kbd>select</kbd><span> block to wait for either a success signal from the go-routine or for a fixed amount of time to elapse, after which it would automatically fail the test.</span></p>
<p><span>If we were to apply this approach, how long should such a test wait for before giving up? If the max wait time for the blocking piece of code is known in advance (for example, defined as a </span><em>constant</em><span>), then things are relatively easy; our test needs to wait for at least that amount of time, </span><em>plus</em><span> some extra time to account for speed discrepancies when running tests in different environments (for example, locally versus on the CI). Failure to account for these discrepancies can lead to flaky tests – tests that </span><em>randomly</em><span> fail, making your CI system vehemently complain.</span></p>
<p>Things are much easier if the timeout is configurable or at least specified as a<span> </span><em>global variable</em><span> </span>that our tests can patch while they are executing. What if, however, the test time is specified as a constant, but its value is in the order of a couple of seconds. Clearly, having several tests that run for that amount of time literally doing<span> </span><em>nothing but waiting</em><span> </span>is counter-productive.</p>
<p>Similarly, in some cases, timeouts might be calculated via some formula that includes a random component. That would make the timeout much harder to predict in a deterministic way without resorting to hacks such as setting the random number generator's seed to a specific value. Of course, in this scenario, our tests would just break if another engineer even slightly tweaked the formula that's used to calculate the timeouts.</p>
<p>The <kbd><span>Chapter04</span>/dialer</kbd><span> </span>package is an interesting case for further examination as it exhibits both issues that I've described here: long wait times that are calculated via a formula! This package provides a dialing wrapper that overlays an exponential backoff retry mechanism on top of a network dialing function (for example, <kbd>net.Dial</kbd>).</p>
<p>To create a new retrying dialer, we need to call the<span> </span><kbd>NewRetryingDialer</kbd><span> </span>constructor:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> NewRetryingDialer(ctx context.Context, dialFunc DialFunc, maxAttempts <span class="dt">int</span>) *RetryingDialer {</a>
<a>    <span class="kw">if</span> maxAttempts &gt; <span class="dv">31</span> {</a>
<a>        <span class="bu">panic</span>(<span class="st">"maxAttempts cannot exceed 31"</span>)</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;RetryingDialer{</a>
<a>        ctx:         ctx,</a>
<a>        dialFunc:    dialFunc,</a>
<a>        maxAttempts: maxAttempts,</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The caller provides a<span> </span><kbd>context.Context</kbd><span> </span>instance, which can be used to abort pending dial attempts if, for instance, the application receives a signal to shut down. Now, let's move on to the meat of the dialer implementation <span>–</span> the<span> </span><kbd>Dial</kbd><span> </span>call:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (d *RetryingDialer) Dial(network, address <span class="dt">string</span>) (conn net.Conn, err <span class="dt">error</span>) {</a>
<a>    <span class="kw">for</span> attempt := <span class="dv">1</span>; attempt &lt;= d.maxAttempts; attempt++ {</a>
<a>        <span class="kw">if</span> conn, err = d.dialFunc(network, address); err == <span class="ot">nil</span> {</a>
<a>            <span class="kw">return</span> conn, <span class="ot">nil</span></a>
<a>        }</a>

<a>        log.Printf(<span class="st">"dial %q: attempt %d failed; retrying after %s"</span>, address, attempt, expBackoff(attempt))</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> &lt;-time.After(expBackoff(attempt)): <span class="co">// Try again</span></a>
<a>        <span class="kw">case</span> &lt;-d.ctx.Done():</a>
<a>            <span class="kw">return</span> <span class="ot">nil</span>, d.ctx.Err()</a>
<a>        }</a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, ErrMaxRetriesExceeded</a>
<a>}</a></pre></div>
<p>This is a pretty straightforward implementation: each time a dial attempt fails, we invoke the<span> </span><kbd>expBackoff</kbd><span> </span>helper to calculate the wait time for the next attempt. Then, we block until the wait time elapses or the context gets cancelled. Finally, if we happen to exceed the maximum configured number of retry attempts, the code will automatically bail out and return an error to the caller. How about writing a short test to verify that the preceding code handles timeouts as expected? This is what it would look like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> TestRetryingDialerWithRealClock(t *testing.T) {</a>
<a>    log.SetFlags(<span class="dv">0</span>)</a>

<a>    <span class="co">// Dial a random local port that nothing is listening on.</span></a>
<a>    d := dialer.NewRetryingDialer(context.Background(), net.Dial, <span class="dv">20</span>)</a>
<a>    _, err := d.Dial(<span class="st">"tcp"</span>, <span class="st">"127.0.0.1:65000"</span>)</a>
<a>    <span class="kw">if</span> err !=  {</a>
<a>        t.Fatal(err)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Running the preceding test yields the following output:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a8d7fa75-add5-4684-aae9-153d9ffdff2b.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: Testing the retrying dialer with a real clock</div>
<p>Success! The test passed. But hold on a minute; look at the test's runtime!<span> </span><em>9 seconds</em>!!! Surely we can do better than this. Wouldn't it be great if we could somehow mock time in Go as we do when writing tests for other programming languages? It turns out that it is indeed possible with the help of packages such as <kbd>jonboulle/clockwork</kbd><span> </span><sup><span class="citation">[2]</span></sup><span> </span>and<span> </span><kbd>juju/clock</kbd><span> </span><sup><span class="citation">[8]</span></sup>. We will be using the latter package for our testing purposes as it also supports mock timers.</p>
<p>The<span> </span><kbd>juju/clock</kbd><span> </span>package exposes a<span> </span><kbd>Clock</kbd><span> </span>interface whose method signatures match the functions that are exported by the built-in<span> </span><kbd>time</kbd><span> </span>package. What's more, it provides a real clock implementation (<kbd>juju.WallClock</kbd>) that we should be injecting into production code, as well as a fake clock implementation that we can manipulate within our tests.</p>
<p>If we can inject a<span> </span><kbd>clock.Clock</kbd><span> </span>instance into the<span> </span><kbd>RetryingDialer</kbd><span> </span>struct, we can use it as a replacement for the<span> </span><kbd>time.After</kbd><span> </span>call in the retry code. That's easy: just modify the dialer constructor argument list so that it includes a clock instance.</p>
<p>Now, let's create a copy of the previous test but this time inject a fake clock into the dialer. To control the time, we will spin up a go-routine to keep advancing the clock by a fixed amount of time until the test completes. For brevity, the following listing only includes the code for controlling the clock; other than that, the rest of the test's setup and its expectations are exactly the same as before:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>doneCh := <span class="bu">make</span>(<span class="kw">chan</span> <span class="kw">struct</span>{})</a>
<a><span class="kw">defer</span> <span class="bu">close</span>(doneCh)</a>
<a>clk := testclock.NewClock(time.Now())</a>
<a><span class="kw">go</span> <span class="kw">func</span>() {</a>
<a>    <span class="kw">for</span> {</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> &lt;-doneCh: <span class="co">// test completed; exit go-routine</span></a>
<a>            <span class="kw">return</span></a>
<a>        <span class="kw">default</span>:</a>
<a>            clk.Advance(<span class="dv">1</span> * time.Minute)</a>
<a>        }</a>
<a>    }</a>
<a>}()</a></pre></div>
<p>As expected, our new test also passes successfully. However, compared to the previous test run, the new test ran in a fraction of the time <span>–</span> just<span> </span><span class="packt_screen">0.010s</span>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/ce947238-e5ef-42b5-906b-4a67e19babaf.png" style="width:54.67em;height:20.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: Testing the retrying dialer with a fake clock</div>
<p>Personally speaking, fake clocks are one of my favorite test primitives. If you are not using fake clocks in your tests, I would strongly recommend that you at least experiment with them. I am sure that you will also reach the conclusion that fake clocks are a great tool for writing well-behaved tests for any piece of code that deals with some aspect of time. Moreover, increasing the stability of your test suites is a fair trade-off for the small bit of refactoring that's required to introduce clocks into your existing code base.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>As the old proverb goes: you cannot build a house without good foundations. The same principle also applies to software engineering. Having a solid test infrastructure in place goes a long way to allowing engineers to work on new features while being confident that their changes will not break the existing code.</p>
<p>Through the course of this chapter, we performed a deep dive into the different types of testing that you need to be aware of when working on medium- to large-scale systems. To begin with, we discussed the concept of unit testing, the essential <em>must-have</em> type of test for all projects, regardless of size, whose primary role is to ensure that individual units of code work as expected in isolation. Then, we tackled more complex patterns, such as integration and functional testing, which verify that units and, by extension, the complete system work harmoniously together. The last part of this chapter was dedicated to exploring advanced test concepts such as smoke tests and chaos testing and concluded with a list of practical tips and tricks for writing tests in a more efficient manner.</p>
<p>Now, it's time for you to put on your software engineering hat and put all of the knowledge you have acquired so far to good use. To this end, over the course of the following chapters, we will be speccing out and building, from scratch, a complete end-to-end system using Go. This system will serve as a sandbox for the practical exploration of each of the concepts we will introduce throughout the rest of this book.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>What is the difference between a stub and a mock?</li>
<li>Explain how fake objects work and describe an example scenario where you would opt to use a fake object instead of a mock.</li>
<li>What are the main components of a table-driven test?</li>
<li>What is the difference between a unit test and an integration test?</li>
<li>What is the difference between an integration test and a functional test?</li>
<li>Describe the<span> </span><em>ambassador</em><span> </span>pattern and how it can be exploited to safely run tests in production.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Bash manual</strong>: here documents: <a href="https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Here-Documents">https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Here-Documents</a>.</li>
<li><kbd>clockwork</kbd>: A fake clock for <kbd>golang</kbd>: <a href="https://github.com/jonboulle/clockwork">https://github.com/jonboulle/clockwork</a>.</li>
<li><kbd>gocheck</kbd>: Rich testing for the Go language: <a href="http://labix.org/gocheck">http://labix.org/gocheck</a>.</li>
<li><kbd>gomock</kbd>: A mocking framework for the Go programming language: <a href="https://github.com/golang/mock">https://github.com/golang/mock</a>.</li>
<li><span class="smallcaps">Meszaros, Gerard</span>:<span> </span><em>XUnit Test Patterns: Refactoring Test Code</em>. Upper Saddle River, NJ, USA : Prentice Hall PTR, 2006 <span>–</span> ISBN 0131495054 (<a href="https://www.worldcat.org/title/xunit-test-patterns-refactoring-test-code/oclc/935197390">https://www.worldcat.org/title/xunit-test-patterns-refactoring-test-code/oclc/935197390</a>).</li>
<li><strong>Selenium</strong>: Browser automation: <a href="https://www.seleniumhq.org">https://www.seleniumhq.org</a>.</li>
<li><kbd>testify</kbd>: A toolkit with common assertions and mocks that plays nicely with the standard library: <a href="https://github.com/stretchr/testify">https://github.com/stretchr/testify</a>.</li>
<li><kbd>juju/clock</kbd>: Clock definition and a testing clock: <a href="https://github.com/juju/clock">https://github.com/juju/clock</a>.</li>
</ol>


            </article>

            
        </section>
    </body></html>