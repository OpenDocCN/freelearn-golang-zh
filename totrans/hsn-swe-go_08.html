<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Links 'R'; Us Project</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"The hardest part of the software task is arriving at a complete and consistent specification, and much of the essence of building a program is in fact the debugging of the specification."</div>
<div class="packt_quote CDPAlignRight CDPAlign">- Frederick P. Brooks <sup>[3]</sup></div>
<p>In this chapter, we will be discussing Links 'R' Us, a Go project that we will be building from scratch throughout the remaining chapters in this book. This project has been specifically designed to combine everything you have learned so far with some of the more technical topics that we will be touching on in the following chapters: databases, pipelines, graph processing, gRPC, instrumentation, and monitoring.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>A brief overview of the system that we will be building and its primary function</li>
<li>Selecting an appropriate SDLC model for the project</li>
<li>Functional and non-functional requirements analysis</li>
<li>Component-based modeling of the Links 'R' Us service</li>
<li>Choosing an appropriate architecture (monolith versus microservices) for the project</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System overview – what are we going to be building?</h1>
                </header>
            
            <article>
                
<p>Throughout the next chapters, we will be assembling, piece by piece, our very own<span> </span><em>search-engine</em>. As with all projects, we need to come up with a cool-sounding name for it. Let me introduce you to<span> </span><em>Links 'R' Us</em>!</p>
<p>So, what are the core functionalities of the Links 'R' Us project? The primary, and kind of obvious, functionality is being able to search for content. However, before we can make our search engine available to the public, we first need to seed it with content. To this end, we need to provide the means for users to submit URLs to our search engine. The search engine would then crawl those links, index their content, and add any newly encountered links to its database for further crawling.</p>
<p>Is this all we need for launching Links 'R' Us? The short answer is no! While user searches would return results containing the keywords from the users' search queries, we would lack the capability to<span> </span><em>order</em><span> </span>them in a meaningful way, especially if the results range in the thousands.</p>
<p>Consequently, we need to introduce some sort of a link or content quality metric to our system and order the returned results by it. Instead of re-inventing the wheel, we will be stepping on the shoulders of search-engine <em>giants</em> (that would be Google) and implementing a battle-tested algorithm called<span> </span><kbd>PageRank</kbd>.</p>
<div class="packt_infobox">The <kbd>PageRank</kbd> algorithm was introduced by a nowadays very popular and heavily cited paper titled <em>The PageRank Citation Ranking: Bringing Order to the Web</em>. The original paper was authored back in 1998 by Larry Page, Sergey Brin, Rajeev Motwani, and Terry Winograd<span> </span><sup><span class="citation">[9]</span></sup><span> </span>and, over the years, has served as the basis for the search-engine implementation at Google.<br/>
<br/>
Given a graph containing links between web-pages, the <kbd>PageRank</kbd> algorithm assigns an importance score to each link in the graph taking into account the number of links that lead to it and their relative importance scores.<br/>
<br/>
While <kbd>PageRank</kbd> was initially introduced as a tool for organizing web content, its generalized form applies to any type of link graph. For the last few years, there has been on-going research into applying <kbd>PageRank</kbd> ideas in a multitude of fields ranging from biochemistry<span> </span><sup><span class="citation">[5]</span></sup><span> </span>to traffic optimization<span> </span><sup><span class="citation">[10]</span></sup>.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will be exploring the <kbd>PageRank</kbd> algorithm in more detail in <a href="c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml">Chapter 8</a>, <em>Graph-Based Data Processing</em>, and <a href="67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml">Chapter 12</a>, <em>Building Distributed Graph-Processing Systems</em>, as part of a larger discussion centered around the various approaches we can employ to facilitate processing of large graphs on a single node or across a cluster of nodes (out-of-core graph processing).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting an SDLC model for our project</h1>
                </header>
            
            <article>
                
<p>Before delving into the details of the Links 'R' Us project, we need to consider the SDLC models we discussed in <a href="5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml">Chapter 1</a>, <em>A Bird's-Eye View of Software Engineering</em>, and select one that makes more sense for this type of project. The choice of a suitable model is of paramount importance: it will serve as our guide for capturing the requirements for the project, defining the components and the interface contracts between them, and appropriately dividing the work to be done in logical chunks that can be built and tested independently of each other.</p>
<p>In this section, we will outline the main reasoning behind the selection of an Agile framework for our project and elaborate on a set of interesting approaches for speeding up our development velocity using a technique known as <em>elephant carpaccio</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating faster using an Agile framework</h1>
                </header>
            
            <article>
                
<p>To begin with, for all intents and purposes, Links 'R' Us is a typical example of a green-field type of project. Since there are no pressing deadlines for delivering the project, we should definitely take our time to explore the pros and cons of any alternative technologies at our disposal for implementing the various components of the system.</p>
<p>For instance, when it comes to indexing and searching the documents that our system will be crawling, there are several competing products/services that we need to evaluate before deciding on which one to use. Furthermore, if we decide to containerize our project using a tool such as Docker, there are several orchestration frameworks (for example, Kubernetes <sup>[6]</sup>, Apache Mesos<span> </span><sup><span class="citation">[2]</span></sup>,<span> </span>or Docker Swarm<span> </span><sup><span class="citation">[11]</span></sup>) available for deploying our services to our staging and production environments.</p>
<p>As far as the software development pace is concerned, we are going to be<span> </span><em>gradually</em><span> </span>and<span> </span><em>incrementally</em><span> </span>building the various components of Links 'R' Us for the next few chapters. Given that we are working on what is essentially a user-facing product, it is imperative to work in small iterations so that we can get the prototype versions out to user focus groups as early as possible. This will enable us to collect valuable feedback that will aid us in fine-tuning and polishing our product as development goes on.</p>
<p class="mce-root"/>
<p>For all of the preceding reasons, I think it would be prudent to adopt an Agile approach to developing Links 'R' Us. My personal preference would be to use Scrum. As we don't really have an actual development team to back the project's development, concepts such as stand-ups, planning, and retrospective sessions do not apply to our particular case. Instead, we need to compromise and adopt some of the ideas behind Scrum in our own Agile workflow.</p>
<p>To this end, in the requirements analysis section, we will focus on creating user stories. Once that process is complete, we will use those stories as input to infer the set of high-level components that we need to build, as well as the ways they are expected to interact with each other. Finally, when the time comes to implement each user story, we will assume the role of the<span> </span><em>product owner</em><span> </span>and break each story down into a set of cards which we will then arrange in a Kanban board.</p>
<p>But before we start working on user stories, I would like to introduce a quite useful and helpful technique that can help you to iterate even faster with your own projects:<span> </span><em>elephant carpaccio</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elephant carpaccio – how to iterate even faster!</h1>
                </header>
            
            <article>
                
<p>This peculiarly-named technique owes its existence to an exercise invented by Dr. Alistair Cockburn. The purpose of this exercise is to help people (engineers and non-engineers alike) to practice and learn how they can split complex story cards (the elephant) into very<span> </span><em>thin vertical slices</em><span> </span>that teams can oftentimes tackle in parallel.</p>
<p>It may strike you as odd but the slice size that I have found most helpful in projects that I have been involved with in the past is nothing more than a<span> </span><em>single day's worth of work</em>. The rationale of the one-day split is to ship (behind a feature flag) small parts of the total work every single day, an approach that is congruent with the <em>ship fast</em> motto advocated by Agile development.</p>
<p>Suffice it to say, splitting cards into one-day slices is certainly not a trivial task. It does take a bit of practice and patience to condition your brain so it switches its focus from long-running tasks to breaking down and optimizing workloads for much shorter periods of time. On the flip side, this approach allows engineering teams to identify and resolve potential blockers as early as possible; it goes without saying that we would obviously prefer to detect blockers near the beginning of the sprint rather than the middle, or, even worse, close to the end of the sprint cycle!</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Another advantage of this technique, at least from the perspective of Go engineers, is that it makes us think more carefully about the best way to organize our code base to ensure that, by the end of each day, we always have a piece of software that can be cleanly compiled and deployed. This constraint forces us into developing the good habit of thinking about code in terms of interfaces as per the tenets of the SOLID design principles we explored in <a href="96fb70cb-8134-4156-bd3e-48ca53224683.xhtml"/><a href="96fb70cb-8134-4156-bd3e-48ca53224683.xhtml">Chapter 2</a>, <em>Best Practices for Writing Clean and Maintainable Go Code</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Requirements analysis</h1>
                </header>
            
            <article>
                
<p>To perform a detailed requirements analysis for the Links 'R' Us project, we need to essentially come up with answers for two key questions:<span> </span><em>what</em><span> </span>do we need to build and<span> </span><em>how well</em><span> </span>would our proposed design fare against a set of goals?</p>
<p>To answer the<span> </span><em>what</em><span> </span>question, we need to list all of the core functionalities that our system is expected to implement as well as describe how the various actors will interact with it. This forms the <strong>Functional Requirements</strong> (<strong>FRs</strong>) for our analysis.</p>
<p>To answer the latter question, we have to state the <strong>Non-Functional Requirements</strong> (<strong>NFRs</strong>) for our solution. Typically, the list of non-functional requirements includes items such as <strong>Service-Level Objectives</strong> (<strong>SLOs</strong>) and capacity and scalability requirements, as well as security-related considerations for our project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Functional requirements</h1>
                </header>
            
            <article>
                
<p>As we have already decided on utilizing an Agile model for implementing our project, the next logical step for defining our functional list of requirements is to establish<span> </span><em>user stories</em>.</p>
<div class="packt_infobox">The concept of user stories pertains to the need of expressing software requirements from the perspective of an actor that interacts with the system. In many types of projects, actors are typically considered to be the end users of the system. However, in the general case,<span> </span><em>other systems</em><span> </span>(for example, a backend service) may also assume the role of an actor.</div>
<p>Each user story begins with a<span> </span><em>succinct</em><span> </span>requirement specification. It is important to note that the specification itself must<span> </span><em>always</em><span> </span>be expressed from the viewpoint of the actor that will be impacted by it. Furthermore, when creating user stories, we should always strive to capture the<span> </span><em>business value</em>, also referred to as the<span> </span><em>true reason</em>, behind each requirement. What's more, one of the core values of Agile development is the so-called<span> </span><em>definition of done</em>. When authoring stories, we need to include a list of<span> </span><em>acceptance criteria</em><span> </span>that will be used as a verification tool to ensure that each story goal has been successfully met.</p>
<p class="mce-root"/>
<div class="packt_tip">For defining the functional requirements for Links 'R' Us, we will be utilizing the following, rather standardized, Agile template:<br/>
<br/>
As an <kbd>[actor]</kbd>,<br/>
I need to be able to <kbd>[short requirement]</kbd>,<br/>
so as to <kbd>[reason/business value]</kbd>.<br/>
<br/>
The acceptance criteria for this user story are as follows:<br/>
<kbd>[list of criteria]</kbd></div>
<p>One final thing that I would like to point out is that, while each story will record a<span> </span><em>need</em><span> </span>for a particular feature, all of them will be completely devoid of any sort of implementation detail. This is quite intentional, and congruent with the recommended practices when working with any Agile framework. As we discussed in <a href="5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml">Chapter 1</a>, <em>A Bird's-Eye View of Software Engineering</em>, our goal is to defer any technical implementation decisions up to the last possible moment. If we were to decide up-front about how we are going to implement each user story, we would be placing unnecessary constraints on our development process, hence limiting our flexibility and the amount of work we can achieve given a particular time budget.</p>
<p>Let's now apply the preceding template to capture the set of functional requirements for the Links 'R' Us project as a list of user stories that will be individually tackled throughout the following chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User story – link submission</h1>
                </header>
            
            <article>
                
<p>As an<span> </span><kbd>end user</kbd>,<br/>
I need to be able to<span> </span><kbd>submit new links to Links 'R' Us</kbd>,<br/>
so as to<span> </span><kbd>update the link graph and make their contents searchable</kbd>.</p>
<p>The acceptance criteria for this user story are as follows:</p>
<ul>
<li>A frontend or API endpoint is provided for facilitating the link submission journey for the end users.</li>
<li>Submitted links have the following criteria:
<ul>
<li>Must be added to the graph</li>
<li>Must be crawled by the system and added to their index</li>
</ul>
</li>
<li>Already submitted links should be accepted by the backend but not inserted twice to the graph.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User story – search</h1>
                </header>
            
            <article>
                
<p>As an<span> </span><kbd>end user</kbd>,<br/>
I need to be able to<span> </span><kbd>submit full-text search queries</kbd>,<br/>
so as to<span> </span><kbd>to retrieve a list of relevant matching results from the content indexed by Links 'R' Us</kbd>.</p>
<p>The acceptance criteria for this user story are as follows:</p>
<ul>
<li>A frontend or API endpoint is provided for the users to submit a full-text query.</li>
<li>If the query matches multiple items, they are returned as a list that the end user can paginate through.</li>
<li>Each entry in the result list must contain the following items: title or link description, the link to the content, and a timestamp indicating when the link was last crawled.<span> </span><em>If feasible</em>, the link may also contain a relevance score expressed as a percentage.</li>
<li>When the query does not match any item, an appropriate response should be returned to the end user.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User story – crawl link graph</h1>
                </header>
            
            <article>
                
<p>As the<span> </span><kbd>crawler backend system</kbd>,<br/>
I need to be able to<span> </span><kbd>obtain a list of sanitized links from the link graph</kbd>,<br/>
so as to<span> </span><kbd>fetch and index their contents while at the same time expanding the link graph with newly discovered links</kbd>.</p>
<p>The acceptance criteria for this user story are as follows:</p>
<ul>
<li>The crawler can query the link graph and receive a list of stale links that need to be crawled.</li>
<li>Links received by the crawler are retrieved from the remote hosts unless the remote server provides an<span> </span><kbd>ETag</kbd><span> </span>or<span> </span><kbd>Last Modified</kbd><span> </span>header that the crawler has already seen before.</li>
<li>Retrieved content is scanned for links and the link graph gets updated.</li>
<li>Retrieved content is indexed and added to the search corpus.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User story – calculate PageRank scores</h1>
                </header>
            
            <article>
                
<p>As the<span> </span><kbd>PageRank calculator backend system</kbd>,<br/>
I need to be able to<span> </span><kbd>access the link graph</kbd>,<br/>
so as to<span> </span><kbd>calculate and persist the PageRank score for each link</kbd>.</p>
<p>The acceptance criteria for this user story are as follows:</p>
<ul>
<li>The PageRank calculator can obtain an immutable snapshot of the entire link graph.</li>
<li>A PageRank score is assigned to every link in the graph.</li>
<li>The search corpus entries are annotated with the updated PageRank scores.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User story – monitor Links 'R' Us health</h1>
                </header>
            
            <article>
                
<p>As a<span> </span><kbd>member of the Links 'R' Us Site Reliability Engineering (SRE) team</kbd>,<br/>
I need to be able to<span> </span><kbd>monitor the health of all Links 'R' Us services</kbd>,<br/>
so as to<span> </span><kbd>detect and address issues that cause degraded service performance</kbd>.</p>
<p>The acceptance criteria for this user story are as follows:</p>
<ul>
<li>All Links 'R' Us services should periodically submit health- and performance-related metrics to a centralized metrics collection system.</li>
<li>A monitoring dashboard is created for each service.</li>
<li>A high-level monitoring dashboard tracks the overall system health.</li>
<li>Metric-based alerts are defined and linked to a paging service. Each alert comes with its own<span> </span><em>playbook</em><span> </span>with a set of steps that need to be performed by a member of the SRE team that is on-call.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Non-functional requirements</h1>
                </header>
            
            <article>
                
<p>In this section, we will go through a list of non-functional requirements for the Links 'R' Us project. Please keep in mind that this list is not exhaustive. Since this is not a real-world project, I opted to describe only a small subset of the possible non-functional requirements that make sense from the viewpoint of the components that we will be building in the following chapters.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service-level objectives</h1>
                </header>
            
            <article>
                
<p>From a <strong>Site Reliability Engineering</strong> (<strong>SRE</strong>) perspective, we need to come up with a list of SLOs that will be used as a gauge for the Links 'R' Us project's performance. Ideally, we should be defining individual SLOs for each one of our services. That may not be immediately possible at the design stage but, at the very least, we need to come up with a realistic SLO for the user-facing components of our system.</p>
<p>SLOs consist of three parts: a description of the thing that we are measuring, the expected service level expressed as a percentage, and the period where the measurement takes place. The following table<span> </span>lists some initial and fairly standard SLOs for Links 'R' Us:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Metric</strong></td>
<td><strong>Expectation</strong></td>
<td><strong>Measurement Period</strong></td>
<td><strong>Notes</strong></td>
</tr>
<tr class="odd">
<td>Links 'R' Us availability</td>
<td>99% uptime</td>
<td>Yearly</td>
<td>Tolerates up to 3d 15h 39m of downtime per year</td>
</tr>
<tr class="even">
<td>Index service availability</td>
<td>99.9% uptime</td>
<td>Yearly</td>
<td>Tolerates up to 8h 45m of downtime per year</td>
</tr>
<tr class="odd">
<td>PageRank calculator service availability</td>
<td>70% uptime</td>
<td>Yearly</td>
<td>Not a user-facing component of our system; the service can endure longer periods of downtime</td>
</tr>
<tr class="even">
<td>Search response time</td>
<td>30% of requests answered in 0.5s</td>
<td>Monthly</td>
<td/>
</tr>
<tr class="odd">
<td>Search response time</td>
<td>70% of requests answered in 1.2s</td>
<td>Monthly</td>
<td/>
</tr>
<tr class="even">
<td>Search response time</td>
<td>99% of requests answered in 2.0s</td>
<td>Monthly</td>
<td/>
</tr>
<tr class="odd">
<td>CPU utilization for the PageRank calculator service</td>
<td>90%</td>
<td>Weekly</td>
<td>We shouldn't be paying for idle computing nodes</td>
</tr>
<tr class="even">
<td>SRE team incident response time</td>
<td>90% of tickets resolved within 8h</td>
<td>Monthly</td>
<td/>
</tr>
</tbody>
</table>
<p> </p>
<p>Keep in mind that, at this stage, we don't really have any prior data available; we are more or less using <em>guesstimates</em> for the service levels that we are targeting. We will be revisiting and updating the SLOs to better reflect reality once the complete system gets deployed to production and our SRE team acquires a better understanding of the system's idiosyncrasies.</p>
<p>As a quick heads-up, <a href="56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml">Chapter 13</a>, <em>Metrics Collection and Visualization</em>, of this book exclusively focuses on the SRE aspects involved in operating production services. In that chapter, we will be elaborating on popular tools that we can use to capture, visualize, and alert on our service-level-related metrics.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Security considerations</h1>
                </header>
            
            <article>
                
<p>As we all know, when it comes to online services,<span> </span><em>security</em><span> </span>is characterized, above all, as a factor that can make or break a particular product. To this end, we need to discuss some potential security issues that may arise when building a project such as Links 'R' Us and devise strategies for dealing with them.</p>
<p>Our analysis operates under the premise that<span> </span><em>you should never trust the client</em>, in this case, the user interacting with Links 'R' Us. Should our project become successful, it will inadvertently attract the attention of malicious actors that will, at some point, try to locate and exploit security holes in our system.</p>
<p>One of the use cases that I presented earlier involves the user submitting a URL that the service will eventually crawl and add to its search index. You may be wondering what could possibly go wrong with a service that just crawls user-submitted URLs? Here are a few interesting examples.</p>
<p>Most cloud providers run an internal metadata service that each computing node can query to obtain information about itself. This service is typically accessed via a<span> </span><em>link-local</em><span> </span>address such as<span> </span><kbd>169.254.169.254</kbd><span> </span>and nodes can perform simple HTTP GET requests to retrieve the information they are interested in.</p>
<div class="packt_infobox">Link-local addresses are a special block of addresses reserved by the <strong>Internet Engineering Task Force</strong> (<strong>IETF</strong>). The range of IPv4 addresses within that block is described in CIDR notation as<span> </span><kbd>169.254.0.0/16</kbd><span> </span>(65,536 unique addresses). Similarly, the following address block has been reserved for use with IPv6:<span> </span><kbd>fe80::/10</kbd>.<br/>
<br/>
These addresses are special in that they are only valid within a particular network segment and they are not route-able beyond that; that is, routers will refuse to forward them to other networks. Link-local addresses are therefore safe to use internally.</div>
<p>Let's assume that we have deployed the Links 'R' Us project to Amazon EC2. The documentation page<span> </span><sup><span class="citation">[1]</span></sup><span> </span>for the EC2 metadata service references quite a few link-local endpoints that a malicious adversary could use. Here are two of the more interesting ones from an attacker's perspective:</p>
<ul>
<li><kbd>http://169.254.169.254/latest/meta-data/iam/info</kbd><span> </span>returns information about the roles associated with the compute node that the call originates from.</li>
<li><kbd>http://169.254.169.254/latest/meta-data/iam/security-credentials/&lt;role-name&gt;</kbd><span> </span>returns a set of temporary security credentials associated with a particular role.</li>
</ul>
<p>In a potential attack scenario, the malicious user submits the first URL to the crawler. If the crawler simply fetches the link and adds the response to the search index, the attacker can perform a targeted search and obtain the<span> </span><em>name</em><span> </span>of a security role associated with the nodes where the crawling service is deployed. Using that information, the attacker would then submit the second URL to the crawler, hoping to get lucky and retrieve a list of valid credentials by waiting once more for the link to be indexed and then performing a second targeted search query. This way, the adversary could gain unauthorized access to another service that the project is using <em>internally</em> (for example, a storage service such as S3).</p>
<p>In case you are wondering, both Google Cloud and Microsoft Azure mitigate this information leak loophole by requiring a special HTTP header to be present when a compute node contacts their metadata services. However, this doesn't mean that we shouldn't be excluding<span> </span><em>other</em><span> </span>IP ranges from our crawl operations. For starters, we should always exclude <em>private network</em> addresses. After all, some of the services that we might opt to use (Elasticsearch comes to mind) could expose potentially unauthenticated RESTful APIs that can be reached by the compute nodes running the crawler code. Evidently, we don't want information from our backend services to appear in our search index! The following table<span> </span>lists some of the special IPv4 ranges that we should definitely avoid crawling:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>IP block (CIDR notation)</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td>10.0.0.0/8</td>
<td class="CDPAlignLeft CDPAlign">Private network</td>
</tr>
<tr class="even">
<td>172.16.0.0/12</td>
<td>Private network</td>
</tr>
<tr class="odd">
<td>192.168.0.0/16</td>
<td>Private network</td>
</tr>
<tr class="even">
<td>169.254.0.0/16</td>
<td>Link-local addresses</td>
</tr>
<tr class="odd">
<td>127.0.0.1</td>
<td>Loop-back IP address</td>
</tr>
<tr class="even">
<td>0.0.0.0/8</td>
<td>All IP addresses on the local machine</td>
</tr>
<tr class="odd">
<td>255.255.255.255/32</td>
<td>The broadcast address for the current network</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>The list from the preceding table is not complete. In fact, you would need to exclude a few more IP blocks such as the ones reserved for carrier-grade traffic, multicast, and test networks. What's more, we should also exclude the equivalent IPv6 ranges if our cloud provider's network stack supports IPv6. If you are interested in learning more about this topic, you can find a comprehensive IPv4 black-list at the GitHub repository for the MASSCAN project<span> </span><sup><span class="citation">[8]</span></sup>.</p>
<div class="packt_tip">One final thing that you may or may not be aware of is that many URL crawling libraries support schemes other than<span> </span><kbd>http/https</kbd>. One example of those schemes is<span> </span><kbd>file</kbd>,<span> </span>which, unless disabled, might allow an attacker to trick the crawler into reading and indexing the contents of a local file (for example, <kbd>/etc/passwd</kbd>) from the node the crawler is executing on.<br/>
<br/>
If you haven't used the file protocol scheme before, try typing the following address into your favorite web-browser:<span> </span><kbd>file:///</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Being good netizens</h1>
                </header>
            
            <article>
                
<p>While our end-goal is to be able to crawl and index the entire internet, the truth of the matter is that the links that we are retrieving and indexing point to content that belongs to someone else. It can so happen that those third parties object to us indexing<span> </span><em>some</em><span> </span>or<span> </span><em>all</em><span> </span>links to the domains under their control.</p>
<p>Fortunately, there is a standardized way for web-masters to notify crawlers not only about which links they can crawl and which we are not allowed to but also to dictate an acceptable crawl speed to <span>not</span><span> </span><span>incur a high load on the remote host. This is all achieved by authoring a</span><span> </span><kbd>robots.txt</kbd><span> </span><span>file and placing it at the root of each domain. The file contains a set of directives like the following:</span></p>
<ul>
<li><strong>User-Agent</strong>: The name of the crawler (user agent string) which the following instructions apply to</li>
<li><strong>Disallow</strong>:<strong> </strong>A regular expression that excludes any matching URL from being crawled</li>
<li><strong>Crawl-Delay</strong>:<strong> </strong>The number of seconds for the crawler to wait before crawling subsequent links from this domain</li>
<li><strong>Sitemap</strong>: A link to an XML file which defines all links within a domain and provides metadata such as a<span> </span><em>last-update</em><span> </span>timestamp that crawlers can use to optimize their link access patterns</li>
</ul>
<p>To be good netizens, we need to ensure that our crawler implementation respects the contents of any <kbd>robots.txt</kbd> file that it encounters. Last but not least, our parser should be able to properly handle the various status codes returned by remote hosts and dial down its crawl speed if it detects an issue with the remote host or the remote host decides to throttle us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">System component modeling</h1>
                </header>
            
            <article>
                
<p>As the first step in mapping the project's architecture, we will begin by creating a UML component diagram. The main goal here is to identify and describe the structural connections between the various<span> </span><em>components</em><span> </span>that comprise our system.</p>
<div class="packt_infobox">A component is defined as an encapsulated standalone unit that constitutes an integral part of a system or a sub-system. Components communicate with each other by exposing and consuming one or several interfaces.<br/>
<br/>
One key point of component-based design is that components should always be considered as abstract, logical entities that expose a particular behavior. This design approach is closely aligned with the SOLID principles and offers us the flexibility to freely change or even swap component implementations at any point throughout the project's development.</div>
<p class="mce-root"/>
<p><span>The following diagram </span>breaks down the Links 'R' Us project into high-level components and visually illustrates the interfaces exposed and consumed by each one of them:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/eaaec353-d95b-4ae6-b4c2-2ae596ef9871.png" style="width:49.42em;height:26.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 1:</span><span> </span>The UML component diagram for the Links 'R' Us project</div>
<p>In case you are not familiar with the symbols used by this type of diagram, here is a quick explanation of what each symbol represents:</p>
<ul>
<li>Boxes with two port-like symbols on the side represent components.</li>
<li>Components can also be nested within other components. In that case, each sub-component is encapsulated within a box that represents its parent component. In the preceding diagram,<span> </span><strong>Link Filter</strong><span> </span>is a sub-component of<span> </span><strong>Crawler</strong>.</li>
<li>A full circle represents an<span> </span><em>interface</em><span> </span><strong>implemented</strong><span> </span>by a particular component. For instance,<span> </span><strong>Search</strong><span> </span>is one of the interfaces implemented by the<span> </span><strong>Content Indexer</strong><span> </span>component.</li>
<li>A half-circle indicates that a component<span> </span><em>requires</em><span> </span>a particular interface. For example, the<span> </span><strong>Link Extractor</strong><span> </span>component requires the<span> </span><strong>Insert Link</strong><span> </span>interface implemented by the<span> </span><strong>Link Graph</strong><span> </span>component.</li>
</ul>
<p>Now that we have mapped out a high-level view of the system components required for constructing our project, we need to spend some time and examine each one in a bit more detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The crawler</h1>
                </header>
            
            <article>
                
<p>The crawler component is effectively the heart of the search engine. It operates on a set of links that are either seeded into the system or discovered while crawling a previous set of links. As you can see in the preceding component model diagram, the crawler itself is in fact a package that encapsulates several other sub-components that operate in a pipeline-like kind of configuration. Let's examine the role of each one of those sub-components in a bit more detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The link filter</h1>
                </header>
            
            <article>
                
<p>A naive crawler implementation would attempt to retrieve any links that are provided as input to it. But as we all know, the web is home to all sorts of content ranging from text or HTML documents to images, music, videos, and a wide variety of other types of binary data (for example, archives, ISOs, executables, and so on).</p>
<p>You would probably agree that attempting to download items that cannot be processed by the search engine would not only be a waste of resources but it would also incur additional running costs to the operator of the Links 'R' Us service: us! Consequently, excluding such content from the crawler would be a beneficial cost reduction strategy.</p>
<p>This is where the<span> </span><em>link filter</em><span> </span>component comes into play. Before we try to fetch a remote link, the link filter will first attempt to identify the content at the other side and drop any links that do not seem to point to content that we can process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The link fetcher</h1>
                </header>
            
            <article>
                
<p>All links that survive the link filter are consumed by the<span> </span><em>link fetcher</em><span> </span>component. As its name implies, this component is responsible for establishing an HTTP connection to each link target and retrieving any content returned by the server at the other end.</p>
<p>The fetcher meticulously processes the HTTP status code and any HTTP headers returned by remote servers. If the returned status code indicates that the content has been moved to a different location (that is, 301 or 302), the fetcher will automatically follow redirects until it reaches the content's final destination. It stands to reason that we would not want our fetcher to get stuck in an infinite redirect loop trying to crawl an incorrectly configured (or malicious) remote host. To this end, the crawler will need to maintain a redirect hop counter and abort the crawl attempt when it exceeds a particular value.</p>
<p class="mce-root"/>
<p>Another important HTTP header that the fetcher pays close attention to is the<span> </span><kbd>Content-Type</kbd><span> </span>header. This header is populated by the remote server and identifies the type (also known as MIME type) of data returned by the server. If the remote server replies with an unsupported content type header (for example, indicating an image or a JavaScript file), the fetcher should automatically drop the link and prevent it from reaching the next stages of the crawl pipeline, the<span> </span><strong>content extractor</strong><span> </span>and the<span> </span><strong>content indexer</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The content extractor</h1>
                </header>
            
            <article>
                
<p>The<span> </span><strong>content extractor</strong><span> </span>attempts to identify and extract all text from a document downloaded from a remote server. For instance, if the link is pointed to a plaintext document, then the extractor would emit the document content as is. On the other hand, if the link pointed to an HTML document, the extractor would strip off any HTML elements and emit the text-only portion of the document.</p>
<p>The emitted content is sent off to the<span> </span><strong>content indexer</strong><span> </span>component so it can be tokenized and update the Links 'R' Us full-text search index.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The link extractor</h1>
                </header>
            
            <article>
                
<p>The last crawler component that we will be examining is the<span> </span><strong>link extractor</strong>. It scans retrieved HTML documents and attempts to identify and extract all links present inside.</p>
<p>Link extraction is unfortunately not a trivial task. While it's true that the majority of links can be extracted via a bunch of regular expressions, there are a few edge-cases that require additional logic from our end, as in the following examples:</p>
<ul>
<li>Relative links need to be converted into absolute links.</li>
<li>If the document<span> </span><kbd>&lt;head&gt;</kbd><span> </span>section includes the<span> </span><kbd>&lt;base href="xxx"&gt;</kbd><span> </span>tag, we need to parse it and use its content to rewrite relative links.</li>
<li>We might encounter links that<span> </span><em>do not</em><span> </span>specify a protocol. These special links begin with<span> </span><kbd>//</kbd><span> </span>and are commonly used when referencing content from a CDN or in HTTPS pages that include static resources from non-HTTPS sources (for example, images in a cart checkout page). When a web-browser encounters such links, it will automatically use the protocol from the current URL to fetch those links.</li>
</ul>
<p>The link extractor will transmit all newly discovered links to the<span> </span><em>link graph</em><span> </span>component so that existing graph connections can be updated and new ones created.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The content indexer</h1>
                </header>
            
            <article>
                
<p>The<span> </span><strong>content indexer</strong><span> </span>is yet another very important component for the Links 'R' Us project. This component performs two distinct functions.</p>
<p>To begin with, the component maintains a full-text index for all documents retrieved by the crawler. Any new or updated document that is emitted by the<span> </span><strong>content extractor</strong><span> </span>component is propagated to the<span> </span><strong>content indexer</strong><span> </span>so that the index can be updated.</p>
<p>It stands to reason that having an index with no means of searching greatly diminishes its usefulness. To this end, the content indexer exposes mechanisms that allow other components to perform full-text searches against the index and to order the results according to retrieval date and/or PageRank score.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The link provider</h1>
                </header>
            
            <article>
                
<p>The<strong><span> </span>link provider</strong><span> </span>component periodically scrubs the link graph and collects a list of candidate links for a new crawl pass. Candidate links include the following:</p>
<ul>
<li>Recently discovered links that haven't been crawled yet</li>
<li>Links for which recent crawl attempts failed (for example, the crawler received a 404/NOT-FOUND response from the remote server)</li>
<li>Links that the crawler successfully processed in the past but need to be re-visited in case the content they point to has changed</li>
</ul>
<p>Given that the WWW is comprised of a mind-boggling number of pages (approximately 6.16 billion as of January 2020), it makes sense to assume that, as our discovered graph of links grows over time, we will eventually reach a point where the set of links we need to crawl will exceed the available memory capacity of our compute nodes! This is why the link provider component employs a <em>streaming</em> approach: while the link scrubbing process is executing, any selected link candidate will be immediately passed along to the<span> </span><em>crawler</em><span> </span>component for further processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The link graph</h1>
                </header>
            
            <article>
                
<p>The<span> </span><strong>link graph</strong><span> </span>is responsible for keeping track not only of all links that the crawler has discovered so far but also of how they are connected. It exposes interfaces for other components to add or remove links from the graph and, of course, query the graph.</p>
<p class="mce-root"/>
<p>Several other system components depend on the interfaces exposed by the link graph component:</p>
<ul>
<li>The<span> </span><em>link provider</em><span> </span>queries the link graph to decide which links should be crawled next.</li>
<li>The<span> </span><em>link extractor</em><span> </span>sub-component of the crawler adds newly discovered links to the graph.</li>
<li>The<span> </span><em>PageRank calculator</em><span> </span>components require access to the entire graph's connectivity information so that it can calculate the PageRank score of each link.</li>
</ul>
<p>Note that I am not talking about a<span> </span><em>single</em><span> </span>interface but I am using the plural form:<span> </span><em>interfaces</em>. This is deliberate as the link graph component is a prime candidate for implementing the <strong>Command Query Responsibility Segregation</strong> (<strong>CQRS</strong>) pattern.</p>
<div class="packt_infobox">The CQRS pattern belongs to the family of architectural patterns. The key idea behind CQRS is to separate the write and read models exposed by a particular component so they can be optimized in isolation.<span> </span><strong>Commands</strong><span> </span>refer to operations that mutate the state of the model, whereas<span> </span><em>queries</em><span> </span>retrieve and return the current model state.<br/>
<br/>
This separation allows us to execute different business logic paths for reads and writes, and, in effect, enables us to implement complex access patterns. For example, writes could be a synchronous process whereas reads might be asynchronous and provide a limited view over the data.<br/>
<br/>
As another example, the component could utilize separate data stores for writes and reads. Writes would eventually trickle into the read store but perhaps the read store data could also be augmented with external data obtained from other downstream components.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The PageRank calculator</h1>
                </header>
            
            <article>
                
<p>The<span> </span><strong>PageRank</strong> calculator implements an asynchronous, periodic process for re-evaluating the PageRank scores for each link in the Links 'R' Us graph.</p>
<p class="mce-root"/>
<p>Before starting a new calculation pass, the PageRank component will first use the interfaces exposed by the link graph component to obtain a snapshot of the current state of the graph. This includes both the graph vertices (links destinations) and the edges (links) connecting them.</p>
<p>Once the PageRank values for each link have been calculated, the PageRank component will contact the text indexer component and annotate each indexed document with its updated PageRank score. This is an asynchronous process and does not otherwise interfere with any searches performed by the Links 'R' Us users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The metrics store</h1>
                </header>
            
            <article>
                
<p>Given that the Links 'R' Us project consists of multiple components, it would make sense for us to deploy monitoring infrastructure so that we can keep track of the health of each component. This way, we can identify components that exhibit elevated error rates or experience high load and need to be scaled up.</p>
<p>This is the primary role of the<span> </span><strong>metrics store</strong><span> </span>component. As you can see in the component diagram, all components in our design transmit metrics to the metrics collector and therefore depend on it. Of course, this is not a<span> </span><em>hard</em><span> </span>dependency: our system design should assume that the metrics collector could go offline at any given moment and make sure that none of the other components are affected should this occur in production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The frontend</h1>
                </header>
            
            <article>
                
<p>The purpose of the<span> </span><strong>frontend</strong><span> </span>component is to render a simple, static HTML-based user interface that will facilitate the users' interaction with the project. More specifically, the design of the frontend component will enable users to perform the following set of functions:</p>
<ul>
<li>Directly submit new URLs for indexing.</li>
<li>Type a keyword or phrase-based search query.</li>
<li>Paginate the search results for a particular query.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It is important to note that, in our current design, the frontend component serves as the entry-point for making our project accessible by the outside world! Given that <em>none</em> of the other project components can be directly accessed by the end users, we could argue that the frontend also doubles as an <em>API gateway</em>, where each incoming API request is mapped to <em>one or more</em> calls to the internal system components. Besides the obvious security benefits of isolating our internal components from the rest of the world, the API gateway pattern provides the following set of additional benefits:</p>
<ul>
<li>If some of the internal calls need to be asynchronous, the gateway can execute them in parallel and wait for them to complete before aggregating their responses and returning them to the user in a synchronous manner.</li>
<li>It enables us to decouple the way that our internal components communicate with each other from the mechanism that the outside world uses to interface with our system. This means that we can expose a RESTful API to the outside world while still retaining the flexibility to select the most suitable transport for each internal component (for example, REST, gRPC, or perhaps a message queue).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monolith or microservices? The ultimate question</h1>
                </header>
            
            <article>
                
<p>Before commencing development of the Links 'R' Us service, we need to decide whether our system components will be developed as parts of a big, monolithic service or whether we will just bite the bullet and implement a service-oriented architecture right from the start.</p>
<p>While the concept of using microservices does indeed seem enticing from the outside, it comes with a lot of operational overhead. Besides the mental effort required for building and wiring all components together, we would additionally need to worry about questions like the following:</p>
<ul>
<li>How does each service get deployed? Are we doing rolling deployments? What about dark or test releases? How easy is it to roll back to a previous deployment when something goes wrong?</li>
<li>Are we going to use a container orchestration layer such as Kubernetes<span> </span><sup><span class="citation">[6]</span></sup>? How does traffic get routed between services? Do we need to use a service mesh such as Istio<span> </span><sup><span class="citation">[4]</span></sup><span> </span>or Linkerd<span> </span><sup><span class="citation">[7]</span></sup>?</li>
<li>How can we monitor the health of our services? Furthermore, how can we collect the logs from all our services?</li>
<li>How are we going to handle service downtime? Do we need to implement circuit-breakers to prevent a problematic service from breaking upstream services that depend on it?</li>
</ul>
<p>Sure, we are all aware of the shortcomings of monolithic designs but, on the other hand, we don't have any available data to justify the extra cost of splitting components into microservices from the start of the project.</p>
<p>Weighing the pros and cons of each approach, it looks like the best course of action is to follow a hybrid approach! We will initially develop our components using a monolithic design. However, and this is the twist, each component will define an interface that other components will use to communicate with it.</p>
<p>To connect components without introducing any coupling between their concrete implementations, we will be making use of the<span> </span><em>proxy</em><span> </span>design pattern. Initially, we will be providing dummy proxy implementations that facilitate inter-component communication within the<span> </span><em>same process</em>. This is, of course, functionally equivalent to directly wiring components together as we would normally do in a monolithic design.</p>
<p>As our system grows and evolves, we will eventually reach a point where we need to extract one or more components into standalone services. Using the preceding pattern, all we need to do is update our proxies to use the appropriate transport (for example, REST, gRPC, and message queues) for connecting components together without having to modify any of the existing component implementations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This concludes the presentation of the Links 'R' Us project. I hope that, by this point, you have acquired a general understanding of what we are going to be building over the next few chapters. <span>If you find yourself wondering about the technical implementation details associated with some of the project components, that's perfectly normal. The main purpose of this chapter was to introduce a high-level overview of the project. We will analyze the construction of each one of these components in <em>extensive</em> detail in the pages that follow!</span></p>
<p class="mce-root"/>
<p>To make the concepts and code for the following chapters easier to follow, we will be splitting each chapter into two core parts:</p>
<ul>
<li>In the first half of each chapter, we will be performing a deep dive into a particular technical topic, for example, a survey of popular types of databases (relational, NoSQL, and so on), how you can create pipelines in Go, how you can run graph operations at scale, what gRPC is and how you can use it, and so on.</li>
<li>In the second half of the chapter, we will be taking the concepts from the first half and applying them toward building one or more components of the Links 'R' Us project.</li>
</ul>
<p>In the next chapter, we will focus our attention on building one of the key components for the Links 'R' Us project: a fully-functioning data persistence layer for storing the links discovered by the crawler and indexing the contents of each web page retrieved by the crawler.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>What is the difference between a functional and a non-functional requirement?</li>
<li>Describe the main components of a user story.</li>
<li>What things could possibly go wrong in the Links 'R' Us scenario if we blindly crawl any link that a user submits to the system?</li>
<li>Name the key components of an SLO.</li>
<li>What is the purpose of a UML component diagram?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li>Amazon Elastic Compute Cloud: <em>Instance Metadata and User Data</em>: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></li>
<li>Apache Mesos: Program against your data center like it's a single pool of resources:<span> </span><a href="https://mesos.apache.org">https://mesos.apache.org</a></li>
<li><span class="smallcaps">Brooks, Frederick P.,<span> </span><span>Jr.</span></span>:<span> </span><em>The Mythical Man-Month (Anniversary Ed.)</em>. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc., 1995 — <a href="https://www.worldcat.org/title/mythical-man-month/oclc/961280727">https://www.worldcat.org/title/mythical-man-month/oclc/961280727</a></li>
<li>Istio: <em>Connect, secure, control, and observe services</em>:<span> </span><a href="https://istio.io">https://istio.io</a></li>
<li><span class="smallcaps">Ivn, Gbor and </span><span class="smallcaps">Grolmusz, Vince</span>: <em>When the Web Meets the Cell: Using Personalized PageRank for Analyzing Protein Interaction Networks</em>.</li>
</ol>
<ol start="6">
<li>Kubernetes: <em>Production-Grade Container Orchestration</em>: <a href="https://kubernetes.io">https://kubernetes.io</a></li>
<li>Linkerd: <em>Ultralight service mesh for Kubernetes and beyond</em>:<span> </span><a href="https://linkerd.io">https://linkerd.io</a></li>
<li>MASSCAN: Mass IP port scanner; reserved IP exclusion list:<span> </span><a href="https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf">https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf</a></li>
<li><span class="smallcaps">Page, L.</span>;<span> </span><span class="smallcaps">Brin, S.</span>;<span> </span><span class="smallcaps">Motwani, R.</span>; and<span> </span><span class="smallcaps">Winograd, T.</span>: <em>The PageRank Citation Ranking: Bringing Order to the Web</em>. In:<span> </span>Proceedings of the 7th International World Wide Web Conference. Brisbane, Australia, 1998, S. 161–172</li>
<li><span class="smallcaps">Pop, Florin</span><span> </span>;<span> </span><span class="smallcaps">Dobre, Ciprian</span>: <em>An Efficient PageRank Approach for Urban Traffic Optimization</em>.</li>
<li>Swarm: <em>a Docker-native clustering system</em>:<span> </span><a href="https://github.com/docker/swarm">https://github.com/docker/swarm</a></li>
</ol>


            </article>

            
        </section>
    </body></html>