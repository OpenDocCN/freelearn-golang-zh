- en: The Links 'R'; Us Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Links 'R' Us 项目
- en: '"The hardest part of the software task is arriving at a complete and consistent
    specification, and much of the essence of building a program is in fact the debugging
    of the specification."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “软件任务中最困难的部分是达到完整和一致的需求规格，而构建程序的本质实际上是对规格的调试。”
- en: '- Frederick P. Brooks ^([3])'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 弗雷德里克·P·布鲁克斯 ^([3])'
- en: 'In this chapter, we will be discussing Links ''R'' Us, a Go project that we
    will be building from scratch throughout the remaining chapters in this book.
    This project has been specifically designed to combine everything you have learned
    so far with some of the more technical topics that we will be touching on in the
    following chapters: databases, pipelines, graph processing, gRPC, instrumentation,
    and monitoring.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论 Links 'R' Us，这是一个我们将从本书剩余章节开始从头构建的 Go 项目。这个项目被特别设计用来结合您迄今为止所学的一切，以及我们在以下章节中将要涉及的一些更技术性的主题：数据库、管道、图处理、gRPC、仪表化和监控。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: A brief overview of the system that we will be building and its primary function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将要构建的系统及其主要功能的简要概述
- en: Selecting an appropriate SDLC model for the project
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为项目选择合适的软件开发生命周期（SDLC）模型
- en: Functional and non-functional requirements analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能性和非功能性需求分析
- en: Component-based modeling of the Links 'R' Us service
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Links 'R' Us 服务的基于组件的建模
- en: Choosing an appropriate architecture (monolith versus microservices) for the
    project
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为项目选择合适的架构（单体与微服务）
- en: System overview – what are we going to be building?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统概述 – 我们将要构建什么？
- en: Throughout the next chapters, we will be assembling, piece by piece, our very
    own *search-engine*. As with all projects, we need to come up with a cool-sounding
    name for it. Let me introduce you to *Links 'R' Us*!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐步组装我们的**搜索引擎**。与所有项目一样，我们需要为它想出一个听起来很酷的名字。让我向您介绍**Links 'R' Us**！
- en: So, what are the core functionalities of the Links 'R' Us project? The primary,
    and kind of obvious, functionality is being able to search for content. However,
    before we can make our search engine available to the public, we first need to
    seed it with content. To this end, we need to provide the means for users to submit
    URLs to our search engine. The search engine would then crawl those links, index
    their content, and add any newly encountered links to its database for further
    crawling.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Links 'R' Us 项目的核心功能是什么？主要且相当明显的一个功能是能够搜索内容。然而，在我们能够将我们的搜索引擎向公众开放之前，我们首先需要用内容来填充它。为此，我们需要为用户提供提交
    URL 到我们的搜索引擎的手段。搜索引擎随后会爬取这些链接，索引其内容，并将任何新遇到的链接添加到其数据库中以便进一步爬取。
- en: Is this all we need for launching Links 'R' Us? The short answer is no! While
    user searches would return results containing the keywords from the users' search
    queries, we would lack the capability to *order* them in a meaningful way, especially
    if the results range in the thousands.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为 Links 'R' Us 的启动做所有这些吗？简短的答案是：不！虽然用户搜索会返回包含用户搜索查询关键词的结果，但我们缺乏以有意义的方式对这些结果进行排序的能力，尤其是如果结果数量达到数千个。
- en: Consequently, we need to introduce some sort of a link or content quality metric
    to our system and order the returned results by it. Instead of re-inventing the
    wheel, we will be stepping on the shoulders of search-engine *giants* (that would
    be Google) and implementing a battle-tested algorithm called `PageRank`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要在我们的系统中引入某种链接或内容质量指标，并按此对返回的结果进行排序。为了避免重蹈覆辙，我们将站在搜索引擎**巨头**（即谷歌）的肩膀上，并实现一个经过实战考验的算法，称为
    `PageRank`。
- en: 'The `PageRank` algorithm was introduced by a nowadays very popular and heavily
    cited paper titled *The PageRank Citation Ranking: Bringing Order to the Web*.
    The original paper was authored back in 1998 by Larry Page, Sergey Brin, Rajeev
    Motwani, and Terry Winograd ^([9]) and, over the years, has served as the basis
    for the search-engine implementation at Google.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`PageRank` 算法是由一篇如今非常流行且被广泛引用的论文《The PageRank Citation Ranking: Bringing Order
    to the Web》提出的。这篇原始论文于1998年由拉里·佩奇、谢尔盖·布林、拉杰夫·莫特瓦尼和特里·温格拉德撰写，并在多年以来一直作为谷歌搜索引擎实现的基础。'
- en: Given a graph containing links between web-pages, the `PageRank` algorithm assigns
    an importance score to each link in the graph taking into account the number of
    links that lead to it and their relative importance scores.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含网页之间链接的图，`PageRank` 算法会根据指向它的链接数量及其相对重要性分数，为图中的每个链接分配一个重要性分数。
- en: While `PageRank` was initially introduced as a tool for organizing web content,
    its generalized form applies to any type of link graph. For the last few years,
    there has been on-going research into applying `PageRank` ideas in a multitude
    of fields ranging from biochemistry ^([5]) to traffic optimization ^([10]).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`PageRank`最初被引入作为一种组织网络内容工具，但其通用形式适用于任何类型的链接图。在过去的几年里，人们一直在研究将`PageRank`理念应用于众多领域，从生物化学^([5])到交通优化^([10])。
- en: We will be exploring the `PageRank` algorithm in more detail in [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    *Graph-Based Data Processing*, and [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph-Processing Systems*, as part of a larger discussion
    centered around the various approaches we can employ to facilitate processing
    of large graphs on a single node or across a cluster of nodes (out-of-core graph
    processing).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8章[基于图的数据处理](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml)和第12章[构建分布式图处理系统](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml)中更详细地探讨`PageRank`算法，作为围绕我们可以采用的多种方法以促进单个节点或节点集群（离核图处理）上大型图处理的大讨论的一部分。
- en: Selecting an SDLC model for our project
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为我们的项目选择SDLC模型
- en: 'Before delving into the details of the Links ''R'' Us project, we need to consider
    the SDLC models we discussed in [Chapter 1](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml), *A
    Bird''s-Eye View of Software Engineering*, and select one that makes more sense
    for this type of project. The choice of a suitable model is of paramount importance:
    it will serve as our guide for capturing the requirements for the project, defining
    the components and the interface contracts between them, and appropriately dividing
    the work to be done in logical chunks that can be built and tested independently
    of each other.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨“链接之用”项目细节之前，我们需要考虑我们在第1章[软件工程的鸟瞰](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml)中讨论的SDLC模型，并选择一个更适合此类项目的模型。选择合适的模型至关重要：它将作为我们捕捉项目需求、定义组件及其接口合同、以及合理划分独立构建和测试的工作块（逻辑块）的指南。
- en: In this section, we will outline the main reasoning behind the selection of
    an Agile framework for our project and elaborate on a set of interesting approaches
    for speeding up our development velocity using a technique known as *elephant
    carpaccio*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述选择敏捷框架作为我们项目的主要理由，并详细阐述一套使用称为*大象生鱼片*的技术来加快我们开发速度的有趣方法。
- en: Iterating faster using an Agile framework
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用敏捷框架加速迭代
- en: To begin with, for all intents and purposes, Links 'R' Us is a typical example
    of a green-field type of project. Since there are no pressing deadlines for delivering
    the project, we should definitely take our time to explore the pros and cons of
    any alternative technologies at our disposal for implementing the various components
    of the system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从所有目的来看，Links 'R' Us是一个典型的绿色领域项目类型。由于没有紧迫的项目交付截止日期，我们绝对应该花时间探索我们可用于实现系统各个组件的任何替代技术的优缺点。
- en: For instance, when it comes to indexing and searching the documents that our
    system will be crawling, there are several competing products/services that we
    need to evaluate before deciding on which one to use. Furthermore, if we decide
    to containerize our project using a tool such as Docker, there are several orchestration
    frameworks (for example, Kubernetes ^([6]), Apache Mesos ^([2]), or Docker Swarm ^([11]))
    available for deploying our services to our staging and production environments.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当涉及到索引和搜索我们系统将要抓取的文档时，在决定使用哪一个之前，我们需要评估几个相互竞争的产品/服务。此外，如果我们决定使用Docker等工具来容器化我们的项目，那么有几个编排框架（例如，Kubernetes^([6])、Apache
    Mesos^([2])或Docker Swarm^([11]）可用于将我们的服务部署到我们的预发布和生产环境中。
- en: As far as the software development pace is concerned, we are going to be *gradually* and *incrementally* building
    the various components of Links 'R' Us for the next few chapters. Given that we
    are working on what is essentially a user-facing product, it is imperative to
    work in small iterations so that we can get the prototype versions out to user
    focus groups as early as possible. This will enable us to collect valuable feedback
    that will aid us in fine-tuning and polishing our product as development goes
    on.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就软件开发的速度而言，在接下来的几章中，我们将逐步和分阶段地构建“链接之用”的各个组件。鉴于我们正在开发的是一个面向用户的产品，必须以小迭代的方式进行工作，以便我们尽可能早地将原型版本提供给用户焦点小组。这将使我们能够收集宝贵的反馈，有助于我们在开发过程中对产品进行微调和抛光。
- en: For all of the preceding reasons, I think it would be prudent to adopt an Agile
    approach to developing Links 'R' Us. My personal preference would be to use Scrum.
    As we don't really have an actual development team to back the project's development,
    concepts such as stand-ups, planning, and retrospective sessions do not apply
    to our particular case. Instead, we need to compromise and adopt some of the ideas
    behind Scrum in our own Agile workflow.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有上述原因，我认为采用敏捷方法来开发“链接之用”是明智的。我个人的偏好是使用Scrum。由于我们实际上没有支持项目开发的真实开发团队，站立会议、计划会议和回顾会议等概念不适用于我们的具体情况。相反，我们需要妥协，并在我们自己的敏捷工作流程中采用Scrum背后的某些想法。
- en: To this end, in the requirements analysis section, we will focus on creating
    user stories. Once that process is complete, we will use those stories as input
    to infer the set of high-level components that we need to build, as well as the
    ways they are expected to interact with each other. Finally, when the time comes
    to implement each user story, we will assume the role of the *product owner* and
    break each story down into a set of cards which we will then arrange in a Kanban
    board.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，在需求分析部分，我们将专注于创建用户故事。一旦这个过程完成，我们将使用这些故事作为输入，推断出我们需要构建的一组高级组件，以及它们之间预期的交互方式。最后，当实施每个用户故事的时候，我们将扮演“产品负责人”的角色，将每个故事分解成一系列卡片，然后我们将这些卡片安排在看板板上。
- en: But before we start working on user stories, I would like to introduce a quite
    useful and helpful technique that can help you to iterate even faster with your
    own projects: *elephant carpaccio*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们开始处理用户故事之前，我想介绍一个非常有用且有帮助的技术，可以帮助你更快地迭代自己的项目：“大象生鱼片”。
- en: Elephant carpaccio – how to iterate even faster!
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “大象生鱼片”——如何更快地迭代！
- en: This peculiarly-named technique owes its existence to an exercise invented by
    Dr. Alistair Cockburn. The purpose of this exercise is to help people (engineers
    and non-engineers alike) to practice and learn how they can split complex story
    cards (the elephant) into very *thin vertical slices* that teams can oftentimes
    tackle in parallel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名字独特的技巧的存在归功于阿利斯泰尔·科克本博士发明的一项练习。这个练习的目的是帮助人们（无论是工程师还是非工程师）练习和学习如何将复杂的用户故事卡片（即“大象”）拆分成非常薄的垂直切片，这样团队就可以经常并行处理。
- en: It may strike you as odd but the slice size that I have found most helpful in
    projects that I have been involved with in the past is nothing more than a *single
    day's worth of work*. The rationale of the one-day split is to ship (behind a
    feature flag) small parts of the total work every single day, an approach that
    is congruent with the *ship fast* motto advocated by Agile development.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人觉得奇怪，但我发现我在过去参与的项目中最有帮助的切片大小仅仅是“一天的工作量”。一天分割的合理性是每天（在功能标志后面）交付总工作量的小部分，这种方法与敏捷开发倡导的“快速发布”座右铭是一致的。
- en: Suffice it to say, splitting cards into one-day slices is certainly not a trivial
    task. It does take a bit of practice and patience to condition your brain so it
    switches its focus from long-running tasks to breaking down and optimizing workloads
    for much shorter periods of time. On the flip side, this approach allows engineering
    teams to identify and resolve potential blockers as early as possible; it goes
    without saying that we would obviously prefer to detect blockers near the beginning
    of the sprint rather than the middle, or, even worse, close to the end of the
    sprint cycle!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 说到将卡片拆分为一天的工作量，这当然不是一件简单的事情。这需要一点练习和耐心，以使你的大脑从长期任务转移到分解和优化更短时间的工作负载。另一方面，这种方法允许工程团队尽早识别和解决潜在的阻碍因素；不言而喻，我们当然更希望在大冲刺的早期而不是中期，甚至更糟糕的是，在大冲刺周期的接近结束时发现阻碍因素！
- en: Another advantage of this technique, at least from the perspective of Go engineers,
    is that it makes us think more carefully about the best way to organize our code
    base to ensure that, by the end of each day, we always have a piece of software
    that can be cleanly compiled and deployed. This constraint forces us into developing
    the good habit of thinking about code in terms of interfaces as per the tenets
    of the SOLID design principles we explored in [Chapter 2](96fb70cb-8134-4156-bd3e-48ca53224683.xhtml),
    *Best Practices for Writing Clean and Maintainable Go Code*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的另一个优点，至少从Go工程师的角度来看，是它让我们更加仔细地思考如何组织我们的代码库，以确保在每天结束时，我们总能有一块可以干净编译和部署的软件。这种约束迫使我们养成按照我们在[第2章](96fb70cb-8134-4156-bd3e-48ca53224683.xhtml)中探讨的SOLID设计原则来思考代码的习惯，即*最佳实践编写清晰且可维护的Go代码*。
- en: Requirements analysis
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求分析
- en: To perform a detailed requirements analysis for the Links 'R' Us project, we
    need to essentially come up with answers for two key questions: *what* do we need
    to build and *how well* would our proposed design fare against a set of goals?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要对“Links 'R' Us”项目进行详细的需求分析，我们基本上需要回答两个关键问题：我们需要构建什么，以及我们的设计建议如何与一系列目标相匹配？
- en: To answer the *what* question, we need to list all of the core functionalities
    that our system is expected to implement as well as describe how the various actors
    will interact with it. This forms the **Functional Requirements** (**FRs**) for
    our analysis.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答*什么*的问题，我们需要列出我们系统预期实现的所有核心功能，以及描述各种参与者如何与之交互。这形成了我们分析中的**功能需求**（**FRs**）。
- en: To answer the latter question, we have to state the **Non-Functional Requirements**
    (**NFRs**) for our solution. Typically, the list of non-functional requirements
    includes items such as **Service-Level Objectives** (**SLOs**) and capacity and
    scalability requirements, as well as security-related considerations for our project.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答后面的问题，我们必须声明我们解决方案的**非功能需求**（**NFRs**）。通常，非功能需求列表包括诸如**服务级别目标**（**SLOs**）和容量和可扩展性要求，以及与我们项目相关的安全考虑因素。
- en: Functional requirements
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功能需求
- en: As we have already decided on utilizing an Agile model for implementing our
    project, the next logical step for defining our functional list of requirements
    is to establish *user stories*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经决定利用敏捷模型来实施我们的项目，那么定义我们的功能需求列表的下一步逻辑步骤就是建立*用户故事*。
- en: The concept of user stories pertains to the need of expressing software requirements
    from the perspective of an actor that interacts with the system. In many types
    of projects, actors are typically considered to be the end users of the system.
    However, in the general case, *other systems* (for example, a backend service)
    may also assume the role of an actor.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用户故事的概念涉及到从与系统交互的参与者角度表达软件需求的需要。在许多类型的项目中，参与者通常被认为是系统的最终用户。然而，在一般情况下，*其他系统*（例如，后端服务）也可能扮演参与者的角色。
- en: Each user story begins with a *succinct* requirement specification. It is important
    to note that the specification itself must *always* be expressed from the viewpoint
    of the actor that will be impacted by it. Furthermore, when creating user stories,
    we should always strive to capture the *business value*, also referred to as the *true
    reason*, behind each requirement. What's more, one of the core values of Agile
    development is the so-called *definition of done*. When authoring stories, we
    need to include a list of *acceptance criteria* that will be used as a verification
    tool to ensure that each story goal has been successfully met.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户故事都以一个 *succinct* 的需求规范开始。重要的是要注意，规范本身必须 *始终* 从受其影响的行为者的视角来表述。此外，在创建用户故事时，我们应该始终努力捕捉每个需求的背后 *业务价值*，也称为 *真正的原因*。更重要的是，敏捷开发的核心理念之一是所谓的 *完成定义*。在编写故事时，我们需要包括一个 *验收标准* 列表，该列表将用作验证工具，以确保每个故事目标都已被成功实现。
- en: 'For defining the functional requirements for Links ''R'' Us, we will be utilizing
    the following, rather standardized, Agile template:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义Links 'R' Us的功能需求，我们将使用以下相对标准化的敏捷模板：
- en: As an `[actor]`,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `[行为者]`，
- en: I need to be able to `[short requirement]`,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `[简短需求]`，
- en: so as to `[reason/business value]`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了[原因/业务价值]。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此用户故事的验收标准如下：
- en: '`[list of criteria]`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[标准]列表'
- en: One final thing that I would like to point out is that, while each story will
    record a *need* for a particular feature, all of them will be completely devoid
    of any sort of implementation detail. This is quite intentional, and congruent
    with the recommended practices when working with any Agile framework. As we discussed
    in [Chapter 1](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml), *A Bird's-Eye View
    of Software Engineering*, our goal is to defer any technical implementation decisions
    up to the last possible moment. If we were to decide up-front about how we are
    going to implement each user story, we would be placing unnecessary constraints
    on our development process, hence limiting our flexibility and the amount of work
    we can achieve given a particular time budget.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想指出最后一件事，虽然每个故事都会记录特定功能的 *需求*，但它们都将完全缺乏任何形式的实现细节。这是故意的，并且与使用任何敏捷框架时的推荐实践一致。正如我们在[第1章](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml)中讨论的，*软件工程的鸟瞰图*，我们的目标是将任何技术实现决策推迟到最后时刻。如果我们一开始就决定如何实现每个用户故事，我们就会对我们的开发过程施加不必要的限制，从而限制我们的灵活性和在特定时间预算内可以完成的工作量。
- en: Let's now apply the preceding template to capture the set of functional requirements
    for the Links 'R' Us project as a list of user stories that will be individually
    tackled throughout the following chapters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用前面的模板来捕捉Links 'R' Us项目的功能需求集合，作为一个用户故事列表，这些故事将在接下来的章节中逐一解决。
- en: User story – link submission
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户故事 – 链接提交
- en: As an `end user`,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `最终用户`，
- en: I need to be able to `submit new links to Links 'R' Us`,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `提交新的链接到Links 'R' Us`，
- en: so as to `update the link graph and make their contents searchable`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新链接图并使其内容可搜索。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此用户故事的验收标准如下：
- en: A frontend or API endpoint is provided for facilitating the link submission
    journey for the end users.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供前端或API端点，以方便最终用户提交链接。
- en: 'Submitted links have the following criteria:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交的链接有以下标准：
- en: Must be added to the graph
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须添加到图中
- en: Must be crawled by the system and added to their index
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须被系统爬取并添加到其索引中
- en: Already submitted links should be accepted by the backend but not inserted twice
    to the graph.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已提交的链接应由后端接受，但不得两次插入到图中。
- en: User story – search
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户故事 – 搜索
- en: As an `end user`,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `最终用户`，
- en: I need to be able to `submit full-text search queries`,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `提交全文搜索查询`，
- en: so as to `to retrieve a list of relevant matching results from the content indexed
    by Links 'R' Us`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从Links 'R' Us索引的内容中检索相关匹配结果列表。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此用户故事的验收标准如下：
- en: A frontend or API endpoint is provided for the users to submit a full-text query.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为用户提供前端或API端点以提交全文查询。
- en: If the query matches multiple items, they are returned as a list that the end
    user can paginate through.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果查询匹配多个项目，它们将以列表形式返回，最终用户可以分页浏览。
- en: 'Each entry in the result list must contain the following items: title or link
    description, the link to the content, and a timestamp indicating when the link
    was last crawled. *If feasible*, the link may also contain a relevance score expressed
    as a percentage.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果列表中的每个条目都必须包含以下项目：标题或链接描述、指向内容的链接，以及表示链接上次爬取时间的戳。*如果可行*，链接还可以包含一个表示为百分比的关联度分数。
- en: When the query does not match any item, an appropriate response should be returned
    to the end user.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当查询不匹配任何项目时，应向最终用户返回适当的响应。
- en: User story – crawl link graph
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户故事 – 爬取链接图
- en: As the `crawler backend system`,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `爬虫后端系统`，
- en: I need to be able to `obtain a list of sanitized links from the link graph`,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `从链接图中获取一个经过清理的链接列表`，
- en: so as to `fetch and index their contents while at the same time expanding the
    link graph with newly discovered links`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以便 `在同时获取和索引其内容的同时，通过新发现的链接扩展链接图`。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用户故事的验收标准如下：
- en: The crawler can query the link graph and receive a list of stale links that
    need to be crawled.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫可以查询链接图并接收需要爬取的过时链接列表。
- en: Links received by the crawler are retrieved from the remote hosts unless the
    remote server provides an `ETag` or `Last Modified` header that the crawler has
    already seen before.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫接收到的链接是从远程主机检索的，除非远程服务器提供了爬虫之前已经看到的 `ETag` 或 `Last Modified` 头信息。
- en: Retrieved content is scanned for links and the link graph gets updated.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取的内容被扫描以查找链接，并且链接图得到更新。
- en: Retrieved content is indexed and added to the search corpus.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取的内容被索引并添加到搜索语料库中。
- en: User story – calculate PageRank scores
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户故事 – 计算PageRank分数
- en: As the `PageRank calculator backend system`,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `PageRank 计算器后端系统`，
- en: I need to be able to `access the link graph`,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `访问链接图`，
- en: so as to `calculate and persist the PageRank score for each link`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以便 `计算并持久化每个链接的 PageRank 分数`。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用户故事的验收标准如下：
- en: The PageRank calculator can obtain an immutable snapshot of the entire link
    graph.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PageRank 计算器可以获取整个链接图的不可变快照。
- en: A PageRank score is assigned to every link in the graph.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中的每个链接都被分配一个PageRank分数。
- en: The search corpus entries are annotated with the updated PageRank scores.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索语料库条目被标注为更新的PageRank分数。
- en: User story – monitor Links 'R' Us health
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户故事 – 监控 Links 'R' Us 健康状况
- en: As a `member of the Links 'R' Us Site Reliability Engineering (SRE) team`,
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `Links 'R' Us 网站可靠性工程 (SRE) 团队的成员`，
- en: I need to be able to `monitor the health of all Links 'R' Us services`,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要能够 `监控所有 Links 'R' Us 服务的健康状况`，
- en: so as to `detect and address issues that cause degraded service performance`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以便 `检测和解决导致服务性能下降的问题`。
- en: 'The acceptance criteria for this user story are as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用户故事的验收标准如下：
- en: All Links 'R' Us services should periodically submit health- and performance-related
    metrics to a centralized metrics collection system.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 Links 'R' Us 服务应定期向集中式指标收集系统提交健康和性能相关的指标。
- en: A monitoring dashboard is created for each service.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个服务创建一个监控仪表板。
- en: A high-level monitoring dashboard tracks the overall system health.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个高级监控仪表板跟踪整体系统健康状况。
- en: Metric-based alerts are defined and linked to a paging service. Each alert comes
    with its own *playbook* with a set of steps that need to be performed by a member
    of the SRE team that is on-call.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于指标的警报被定义并链接到分页服务。每个警报都附带一个自己的*剧本*，其中包含需要由值班SRE团队成员执行的一系列步骤。
- en: Non-functional requirements
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非功能性需求
- en: In this section, we will go through a list of non-functional requirements for
    the Links 'R' Us project. Please keep in mind that this list is not exhaustive.
    Since this is not a real-world project, I opted to describe only a small subset
    of the possible non-functional requirements that make sense from the viewpoint
    of the components that we will be building in the following chapters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 Links 'R' Us 项目的非功能性需求列表。请记住，这个列表并不详尽。由于这不是一个真实世界的项目，我选择只描述一小部分从我们将要在下一章中构建的组件的角度来看有意义的可能非功能性需求。
- en: Service-level objectives
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务级别目标
- en: From a **Site Reliability Engineering** (**SRE**) perspective, we need to come
    up with a list of SLOs that will be used as a gauge for the Links 'R' Us project's
    performance. Ideally, we should be defining individual SLOs for each one of our
    services. That may not be immediately possible at the design stage but, at the
    very least, we need to come up with a realistic SLO for the user-facing components
    of our system.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从 **站点可靠性工程** (**SRE**) 的角度来看，我们需要制定一个 SLO 列表，这些 SLO 将作为衡量 Links 'R' Us 项目性能的标尺。理想情况下，我们应该为我们的每个服务定义单独的
    SLO。在设计阶段这可能不是立即可行的，但至少我们需要为系统面向用户的部分制定一个现实的 SLO。
- en: 'SLOs consist of three parts: a description of the thing that we are measuring,
    the expected service level expressed as a percentage, and the period where the
    measurement takes place. The following table lists some initial and fairly standard
    SLOs for Links ''R'' Us:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SLO 由三个部分组成：我们正在测量的东西的描述、以百分比表示的预期服务级别，以及测量发生的周期。以下表格列出了 Links 'R' Us 的一些初始和相当标准的
    SLO：
- en: '| **Metric** | **Expectation** | **Measurement Period** | **Notes** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **预期** | **测量周期** | **备注** |'
- en: '| Links ''R'' Us availability | 99% uptime | Yearly | Tolerates up to 3d 15h
    39m of downtime per year |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Links ''R'' Us 可用性 | 99% 的正常运行时间 | 每年 | 每年可容忍 3 天 15 小时 39 分钟的停机时间 |'
- en: '| Index service availability | 99.9% uptime | Yearly | Tolerates up to 8h 45m
    of downtime per year |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 索引服务可用性 | 99.9% 的正常运行时间 | 每年 | 每年可容忍 8 小时 45 分钟的停机时间 |'
- en: '| PageRank calculator service availability | 70% uptime | Yearly | Not a user-facing
    component of our system; the service can endure longer periods of downtime |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| PageRank 计算器服务可用性 | 70% 的正常运行时间 | 每年 | 不是我们系统面向用户的部分；该服务可以承受更长时间的停机 |'
- en: '| Search response time | 30% of requests answered in 0.5s | Monthly |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 搜索响应时间 | 30% 的请求在 0.5 秒内得到响应 | 每月 |  |'
- en: '| Search response time | 70% of requests answered in 1.2s | Monthly |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 搜索响应时间 | 70% 的请求在 1.2 秒内得到响应 | 每月 |  |'
- en: '| Search response time | 99% of requests answered in 2.0s | Monthly |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 搜索响应时间 | 99% 的请求在 2.0 秒内得到响应 | 每月 |  |'
- en: '| CPU utilization for the PageRank calculator service | 90% | Weekly | We shouldn''t
    be paying for idle computing nodes |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PageRank 计算器服务的 CPU 利用率 | 90% | 每周 | 我们不应该为闲置的计算节点付费 |'
- en: '| SRE team incident response time | 90% of tickets resolved within 8h | Monthly
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SRE 团队事件响应时间 | 90% 的工单在 8 小时内得到解决 | 每月 |  |'
- en: Keep in mind that, at this stage, we don't really have any prior data available;
    we are more or less using *guesstimates* for the service levels that we are targeting.
    We will be revisiting and updating the SLOs to better reflect reality once the
    complete system gets deployed to production and our SRE team acquires a better
    understanding of the system's idiosyncrasies.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个阶段，我们实际上并没有任何先前数据可用；我们更多或更少地使用 *猜测* 来确定我们希望实现的服务级别。一旦完整系统部署到生产环境，并且我们的
    SRE 团队更好地理解了系统的独特之处，我们将重新审视并更新 SLO，以更好地反映现实情况。
- en: As a quick heads-up, [Chapter 13](56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml),
    *Metrics Collection and Visualization*, of this book exclusively focuses on the
    SRE aspects involved in operating production services. In that chapter, we will
    be elaborating on popular tools that we can use to capture, visualize, and alert
    on our service-level-related metrics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速提醒，本书的第 13 章 [第 13 章](56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml)，*指标收集与可视化*，专门关注在生产服务中涉及到的
    SRE 方面。在该章中，我们将详细阐述我们可以使用的流行工具，用于捕获、可视化和警报我们的服务级别相关指标。
- en: Security considerations
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全考虑
- en: As we all know, when it comes to online services, *security* is characterized,
    above all, as a factor that can make or break a particular product. To this end,
    we need to discuss some potential security issues that may arise when building
    a project such as Links 'R' Us and devise strategies for dealing with them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，当涉及到在线服务时，*安全* 是一个可以决定特定产品成败的因素。为此，我们需要讨论在构建像 Links 'R' Us 这样的项目时可能出现的潜在安全问题，并制定应对这些问题的策略。
- en: Our analysis operates under the premise that *you should never trust the client*,
    in this case, the user interacting with Links 'R' Us. Should our project become
    successful, it will inadvertently attract the attention of malicious actors that
    will, at some point, try to locate and exploit security holes in our system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析基于这样一个前提，*你永远不应该信任客户端*，在这种情况下，与 Links 'R' Us 互动的用户。如果我们的项目取得成功，它无意中会吸引恶意行为者的注意，他们会在某个时候试图定位并利用我们系统中的安全漏洞。
- en: One of the use cases that I presented earlier involves the user submitting a
    URL that the service will eventually crawl and add to its search index. You may
    be wondering what could possibly go wrong with a service that just crawls user-submitted
    URLs? Here are a few interesting examples.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提出的一个用例涉及用户提交一个服务最终会爬取并添加到其搜索索引中的URL。你可能想知道一个只爬取用户提交的URL的服务可能会出什么问题？以下是一些有趣的例子。
- en: Most cloud providers run an internal metadata service that each computing node
    can query to obtain information about itself. This service is typically accessed
    via a *link-local* address such as `169.254.169.254` and nodes can perform simple
    HTTP GET requests to retrieve the information they are interested in.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云服务提供商运行一个内部元数据服务，每个计算节点都可以查询以获取自身的信息。此服务通常通过一个*链路本地*地址（例如`169.254.169.254`）访问，节点可以通过简单的HTTP
    GET请求检索它们感兴趣的信息。
- en: Link-local addresses are a special block of addresses reserved by the **Internet
    Engineering Task Force** (**IETF**). The range of IPv4 addresses within that block
    is described in CIDR notation as `169.254.0.0/16` (65,536 unique addresses). Similarly,
    the following address block has been reserved for use with IPv6: `fe80::/10`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 链路本地地址是由**互联网工程任务组**（**IETF**）保留的特殊地址块。该块中IPv4地址的范围用CIDR表示法描述为`169.254.0.0/16`（65,536个唯一地址）。类似地，以下地址块已被保留用于IPv6：`fe80::/10`。
- en: These addresses are special in that they are only valid within a particular
    network segment and they are not route-able beyond that; that is, routers will
    refuse to forward them to other networks. Link-local addresses are therefore safe
    to use internally.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些地址是特殊的，因为它们只在特定的网络段内有效，并且不能超出该网络段进行路由；也就是说，路由器将拒绝将它们转发到其他网络。因此，链路本地地址在内部使用是安全的。
- en: 'Let''s assume that we have deployed the Links ''R'' Us project to Amazon EC2\.
    The documentation page ^([1]) for the EC2 metadata service references quite a
    few link-local endpoints that a malicious adversary could use. Here are two of
    the more interesting ones from an attacker''s perspective:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经将“链接‘R’我们”项目部署到Amazon EC2。EC2元数据服务的文档页面^([1])引用了许多恶意对手可能会使用的链路本地端点。以下是两个从攻击者角度来看更有趣的例子：
- en: '`http://169.254.169.254/latest/meta-data/iam/info` returns information about
    the roles associated with the compute node that the call originates from.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://169.254.169.254/latest/meta-data/iam/info`返回有关调用来源的计算节点关联的角色信息。'
- en: '`http://169.254.169.254/latest/meta-data/iam/security-credentials/<role-name>` returns
    a set of temporary security credentials associated with a particular role.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://169.254.169.254/latest/meta-data/iam/security-credentials/<role-name>`返回与特定角色关联的一组临时安全凭证。'
- en: In a potential attack scenario, the malicious user submits the first URL to
    the crawler. If the crawler simply fetches the link and adds the response to the
    search index, the attacker can perform a targeted search and obtain the *name* of
    a security role associated with the nodes where the crawling service is deployed.
    Using that information, the attacker would then submit the second URL to the crawler,
    hoping to get lucky and retrieve a list of valid credentials by waiting once more
    for the link to be indexed and then performing a second targeted search query.
    This way, the adversary could gain unauthorized access to another service that
    the project is using *internally* (for example, a storage service such as S3).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在的攻击场景中，恶意用户将第一个URL提交给爬虫。如果爬虫只是简单地抓取链接并将响应添加到搜索索引中，攻击者就可以执行有针对性的搜索并获取与部署爬虫服务的节点相关的安全角色的*名称*。利用这些信息，攻击者随后将第二个URL提交给爬虫，希望再次走运并等待链接被索引后，通过执行第二次有针对性的搜索查询来检索一组有效凭证。这样，对手就可以未经授权访问项目内部使用的另一个服务（例如，一个存储服务如S3）。
- en: 'In case you are wondering, both Google Cloud and Microsoft Azure mitigate this
    information leak loophole by requiring a special HTTP header to be present when
    a compute node contacts their metadata services. However, this doesn''t mean that
    we shouldn''t be excluding *other* IP ranges from our crawl operations. For starters,
    we should always exclude *private network* addresses. After all, some of the services
    that we might opt to use (Elasticsearch comes to mind) could expose potentially
    unauthenticated RESTful APIs that can be reached by the compute nodes running
    the crawler code. Evidently, we don''t want information from our backend services
    to appear in our search index! The following table lists some of the special IPv4
    ranges that we should definitely avoid crawling:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对此感到好奇，谷歌云和微软Azure通过要求计算节点在联系其元数据服务时必须存在一个特殊的HTTP头来缓解这种信息泄露漏洞。然而，这并不意味着我们不应该在我们的爬取操作中排除*其他*IP范围。首先，我们应该始终排除*私有网络*地址。毕竟，我们可能选择使用的一些服务（例如Elasticsearch）可能会暴露未经身份验证的RESTful
    API，这些API可以通过运行爬虫代码的计算节点访问。显然，我们不希望我们的后端服务信息出现在我们的搜索索引中！以下表格列出了我们应该绝对避免爬取的一些特殊IPv4地址范围：
- en: '| **IP block (CIDR notation)** | **Description** |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **IP块（CIDR表示法）** | **描述** |'
- en: '| 10.0.0.0/8 | Private network |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 10.0.0.0/8 | 私有网络 |'
- en: '| 172.16.0.0/12 | Private network |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 172.16.0.0/12 | 私有网络 |'
- en: '| 192.168.0.0/16 | Private network |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 192.168.0.0/16 | 私有网络 |'
- en: '| 169.254.0.0/16 | Link-local addresses |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 169.254.0.0/16 | 链路本地地址 |'
- en: '| 127.0.0.1 | Loop-back IP address |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 127.0.0.1 | 环回IP地址 |'
- en: '| 0.0.0.0/8 | All IP addresses on the local machine |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0.0.0.0/8 | 本地机器上的所有IP地址 |'
- en: '| 255.255.255.255/32 | The broadcast address for the current network |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 255.255.255.255/32 | 当前网络的广播地址 |'
- en: The list from the preceding table is not complete. In fact, you would need to
    exclude a few more IP blocks such as the ones reserved for carrier-grade traffic,
    multicast, and test networks. What's more, we should also exclude the equivalent
    IPv6 ranges if our cloud provider's network stack supports IPv6\. If you are interested
    in learning more about this topic, you can find a comprehensive IPv4 black-list
    at the GitHub repository for the MASSCAN project ^([8]).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表格中的列表并不完整。实际上，你可能需要排除一些额外的IP块，例如为运营商级流量、多播和测试网络预留的IP块。更重要的是，如果我们云提供商的网络堆栈支持IPv6，我们还应该排除等效的IPv6地址范围。如果你对这个主题感兴趣，你可以在MASSCAN项目的GitHub仓库中找到一个全面的IPv4黑名单^([8])。
- en: One final thing that you may or may not be aware of is that many URL crawling
    libraries support schemes other than `http/https`. One example of those schemes
    is `file`, which, unless disabled, might allow an attacker to trick the crawler
    into reading and indexing the contents of a local file (for example, `/etc/passwd`)
    from the node the crawler is executing on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能知道也可能不知道的是，许多URL爬取库支持除了`http/https`之外的其他方案。这些方案中的一个例子是`file`，除非禁用，否则可能会允许攻击者欺骗爬虫读取和索引爬虫执行节点上的本地文件内容（例如，`/etc/passwd`）。
- en: If you haven't used the file protocol scheme before, try typing the following
    address into your favorite web-browser: `file:///`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有使用过文件协议方案，请尝试在你的浏览器中输入以下地址：`file:///`。
- en: Being good netizens
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成为好网民
- en: While our end-goal is to be able to crawl and index the entire internet, the
    truth of the matter is that the links that we are retrieving and indexing point
    to content that belongs to someone else. It can so happen that those third parties
    object to us indexing *some* or *all* links to the domains under their control.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的最终目标是能够爬取和索引整个互联网，但事实是，我们检索和索引的链接指向的是属于他人的内容。可能发生的情况是，第三方反对我们索引其控制下的域的*一些*或*所有*链接。
- en: 'Fortunately, there is a standardized way for web-masters to notify crawlers
    not only about which links they can crawl and which we are not allowed to but
    also to dictate an acceptable crawl speed to not incur a high load on the remote
    host. This is all achieved by authoring a `robots.txt` file and placing it at
    the root of each domain. The file contains a set of directives like the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，网站管理员有一种标准化的方式来通知爬虫，不仅关于它们可以爬取哪些链接以及哪些链接不允许爬取，还可以指定一个可接受的爬取速度，以避免对远程主机造成高负载。这一切都是通过编写一个`robots.txt`文件并将其放置在每个域的根目录下实现的。该文件包含一系列如下所示的指令：
- en: '**User-Agent**: The name of the crawler (user agent string) which the following
    instructions apply to'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**User-Agent**：以下指令适用的爬虫（用户代理字符串）的名称'
- en: '**Disallow**:A regular expression that excludes any matching URL from being
    crawled'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Disallow**：一个正则表达式，用于排除任何匹配的URL被爬取'
- en: '**Crawl-Delay**:The number of seconds for the crawler to wait before crawling
    subsequent links from this domain'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Crawl-Delay**：爬虫在爬取该域名后续链接之前等待的秒数'
- en: '**Sitemap**: A link to an XML file which defines all links within a domain
    and provides metadata such as a *last-update* timestamp that crawlers can use
    to optimize their link access patterns'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sitemap**：一个指向XML文件的链接，该文件定义了域名内的所有链接，并提供爬虫可以用来优化其链接访问模式的元数据，如*最后更新*时间戳'
- en: To be good netizens, we need to ensure that our crawler implementation respects
    the contents of any `robots.txt` file that it encounters. Last but not least,
    our parser should be able to properly handle the various status codes returned
    by remote hosts and dial down its crawl speed if it detects an issue with the
    remote host or the remote host decides to throttle us.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为好网民，我们需要确保我们的爬虫实现尊重它遇到的任何`robots.txt`文件的内容。最后但同样重要的是，我们的解析器应该能够正确处理远程主机返回的各种状态码，并在检测到远程主机问题或远程主机决定限制我们时降低爬取速度。
- en: System component modeling
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统组件建模
- en: As the first step in mapping the project's architecture, we will begin by creating
    a UML component diagram. The main goal here is to identify and describe the structural
    connections between the various *components* that comprise our system.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射项目架构的第一步，我们将开始创建一个UML组件图。这里的主要目标是识别和描述构成我们系统的各种*组件*之间的结构连接。
- en: A component is defined as an encapsulated standalone unit that constitutes an
    integral part of a system or a sub-system. Components communicate with each other
    by exposing and consuming one or several interfaces.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 组件被定义为封装的独立单元，它是系统或子系统的组成部分。组件通过暴露和消费一个或多个接口相互通信。
- en: One key point of component-based design is that components should always be
    considered as abstract, logical entities that expose a particular behavior. This
    design approach is closely aligned with the SOLID principles and offers us the
    flexibility to freely change or even swap component implementations at any point
    throughout the project's development.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于组件设计的要点之一是，组件应始终被视为抽象的逻辑实体，它们暴露特定的行为。这种设计方法与SOLID原则紧密一致，并为我们提供了在项目开发过程中的任何时刻自由更改或甚至交换组件实现的灵活性。
- en: 'The following diagram breaks down the Links ''R'' Us project into high-level
    components and visually illustrates the interfaces exposed and consumed by each
    one of them:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表将“链接之用”项目分解为高级组件，并直观地展示了每个组件暴露和使用的接口：
- en: '![](img/eaaec353-d95b-4ae6-b4c2-2ae596ef9871.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eaaec353-d95b-4ae6-b4c2-2ae596ef9871.png)'
- en: Figure 1: The UML component diagram for the Links 'R' Us project
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：“链接之用”项目的UML组件图
- en: 'In case you are not familiar with the symbols used by this type of diagram,
    here is a quick explanation of what each symbol represents:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这种图表所使用的符号不熟悉，这里有一个快速说明，解释每个符号代表什么：
- en: Boxes with two port-like symbols on the side represent components.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两侧带有两个类似端口符号的框表示组件。
- en: Components can also be nested within other components. In that case, each sub-component
    is encapsulated within a box that represents its parent component. In the preceding
    diagram, **Link Filter** is a sub-component of **Crawler**.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件也可以嵌套在其他组件内部。在这种情况下，每个子组件都被封装在其父组件表示的框内。在前面的图表中，**链接过滤器**是**爬虫**的子组件。
- en: A full circle represents an *interface* **implemented** by a particular component.
    For instance, **Search** is one of the interfaces implemented by the **Content
    Indexer** component.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的圆圈表示特定组件*实现*的*接口*。例如，**搜索**是**内容索引器**组件实现的接口之一。
- en: A half-circle indicates that a component *requires* a particular interface.
    For example, the **Link Extractor** component requires the **Insert Link** interface
    implemented by the **Link Graph** component.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半圆表示一个组件*需要*特定的接口。例如，**链接提取器**组件需要由**链接图**组件实现的**插入链接**接口。
- en: Now that we have mapped out a high-level view of the system components required
    for constructing our project, we need to spend some time and examine each one
    in a bit more detail.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经绘制出了构建我们项目所需系统组件的高级视图，我们需要花些时间更详细地检查每一个。
- en: The crawler
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬虫
- en: The crawler component is effectively the heart of the search engine. It operates
    on a set of links that are either seeded into the system or discovered while crawling
    a previous set of links. As you can see in the preceding component model diagram,
    the crawler itself is in fact a package that encapsulates several other sub-components
    that operate in a pipeline-like kind of configuration. Let's examine the role
    of each one of those sub-components in a bit more detail.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '爬虫组件实际上是搜索引擎的核心。它在一个链接集上运行，这些链接要么被播种到系统中，要么在爬取先前链接集时发现。正如你在前面的组件模型图中看到的，爬虫本身实际上是一个封装了其他几个子组件的包，这些子组件以类似管道的配置运行。让我们更详细地检查每个子组件的作用。 '
- en: The link filter
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接过滤器
- en: A naive crawler implementation would attempt to retrieve any links that are
    provided as input to it. But as we all know, the web is home to all sorts of content
    ranging from text or HTML documents to images, music, videos, and a wide variety
    of other types of binary data (for example, archives, ISOs, executables, and so
    on).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的爬虫实现会尝试检索作为输入提供的任何链接。但正如我们所知，网络上有各种内容，从文本或HTML文档到图像、音乐、视频以及各种其他类型的二进制数据（例如，归档、ISO、可执行文件等）。
- en: 'You would probably agree that attempting to download items that cannot be processed
    by the search engine would not only be a waste of resources but it would also
    incur additional running costs to the operator of the Links ''R'' Us service:
    us! Consequently, excluding such content from the crawler would be a beneficial
    cost reduction strategy.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会同意，尝试下载搜索引擎无法处理的物品不仅会浪费资源，而且还会给“链接R我们”服务的运营商（也就是我们）带来额外的运行成本。因此，从爬虫中排除此类内容将是一种有益的成本降低策略。
- en: This is where the *link filter* component comes into play. Before we try to
    fetch a remote link, the link filter will first attempt to identify the content
    at the other side and drop any links that do not seem to point to content that
    we can process.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**链接过滤器**组件发挥作用的地方。在我们尝试获取远程链接之前，链接过滤器将首先尝试识别另一侧的内容，并丢弃任何看起来不指向我们可以处理的内容的链接。
- en: The link fetcher
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接获取器
- en: All links that survive the link filter are consumed by the *link fetcher* component.
    As its name implies, this component is responsible for establishing an HTTP connection
    to each link target and retrieving any content returned by the server at the other
    end.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通过链接过滤器筛选存活的链接都将被**链接获取器**组件消耗。正如其名称所暗示的，这个组件负责与每个链接目标建立HTTP连接，并检索服务器另一端返回的任何内容。
- en: The fetcher meticulously processes the HTTP status code and any HTTP headers
    returned by remote servers. If the returned status code indicates that the content
    has been moved to a different location (that is, 301 or 302), the fetcher will
    automatically follow redirects until it reaches the content's final destination.
    It stands to reason that we would not want our fetcher to get stuck in an infinite
    redirect loop trying to crawl an incorrectly configured (or malicious) remote
    host. To this end, the crawler will need to maintain a redirect hop counter and
    abort the crawl attempt when it exceeds a particular value.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 获取器会仔细处理远程服务器返回的HTTP状态码和任何HTTP头信息。如果返回的状态码表明内容已移动到不同的位置（即301或302），获取器将自动跟随重定向，直到到达内容的最终目的地。从逻辑上讲，我们不希望我们的获取器陷入无限重定向循环，试图爬取配置错误（或恶意）的远程主机。为此，爬虫需要维护一个重定向跳数计数器，并在超过特定值时终止爬取尝试。
- en: Another important HTTP header that the fetcher pays close attention to is the `Content-Type` header.
    This header is populated by the remote server and identifies the type (also known
    as MIME type) of data returned by the server. If the remote server replies with
    an unsupported content type header (for example, indicating an image or a JavaScript
    file), the fetcher should automatically drop the link and prevent it from reaching
    the next stages of the crawl pipeline, the **content extractor** and the **content
    indexer**.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 获取器还非常关注另一个重要的HTTP头信息，即`Content-Type`头信息。这个头信息由远程服务器填充，并标识服务器返回的数据类型（也称为MIME类型）。如果远程服务器回复一个不支持的MIME类型头信息（例如，指示图像或JavaScript文件），获取器应自动丢弃链接，并防止其进入爬取管道的下一阶段，即**内容提取器**和**内容索引器**。
- en: The content extractor
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容提取器
- en: The **content extractor** attempts to identify and extract all text from a document
    downloaded from a remote server. For instance, if the link is pointed to a plaintext
    document, then the extractor would emit the document content as is. On the other
    hand, if the link pointed to an HTML document, the extractor would strip off any
    HTML elements and emit the text-only portion of the document.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容提取器**试图从从远程服务器下载的文档中识别并提取所有文本。例如，如果链接指向一个纯文本文档，那么提取器会原样输出文档内容。另一方面，如果链接指向一个HTML文档，提取器会移除任何HTML元素，并输出文档的纯文本部分。'
- en: The emitted content is sent off to the **content indexer** component so it can
    be tokenized and update the Links 'R' Us full-text search index.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的内容被发送到**内容索引器**组件，以便对其进行分词并更新Links 'R' Us全文搜索索引。
- en: The link extractor
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接提取器
- en: The last crawler component that we will be examining is the **link extractor**.
    It scans retrieved HTML documents and attempts to identify and extract all links
    present inside.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要检查的最后一个爬虫组件是**链接提取器**。它扫描检索到的HTML文档，并尝试识别和提取其中所有的链接。
- en: 'Link extraction is unfortunately not a trivial task. While it''s true that
    the majority of links can be extracted via a bunch of regular expressions, there
    are a few edge-cases that require additional logic from our end, as in the following
    examples:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 链接提取是一个不幸的简单任务。虽然确实大多数链接可以通过一系列正则表达式提取，但还有一些边缘情况需要我们额外的逻辑处理，如下面的例子所示：
- en: Relative links need to be converted into absolute links.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对链接需要转换为绝对链接。
- en: If the document `<head>` section includes the `<base href="xxx">` tag, we need
    to parse it and use its content to rewrite relative links.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果文档的`<head>`部分包含`<base href="xxx">`标签，我们需要解析它并使用其内容来重写相对链接。
- en: We might encounter links that *do not* specify a protocol. These special links
    begin with `//` and are commonly used when referencing content from a CDN or in
    HTTPS pages that include static resources from non-HTTPS sources (for example,
    images in a cart checkout page). When a web-browser encounters such links, it
    will automatically use the protocol from the current URL to fetch those links.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会遇到没有指定协议的链接。这些特殊链接以`//`开头，通常用于引用CDN中的内容或在包含来自非HTTPS源静态资源的HTTPS页面中（例如，购物车结账页面中的图片）。当网络浏览器遇到这样的链接时，它会自动使用当前URL的协议来获取这些链接。
- en: The link extractor will transmit all newly discovered links to the *link graph* component
    so that existing graph connections can be updated and new ones created.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 链接提取器会将所有新发现的链接传输到**链接图**组件，以便更新现有图连接并创建新的连接。
- en: The content indexer
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容索引器
- en: The **content indexer** is yet another very important component for the Links
    'R' Us project. This component performs two distinct functions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容索引器**是Links ''R'' Us项目中的另一个非常重要的组件。该组件执行两个不同的功能。'
- en: To begin with, the component maintains a full-text index for all documents retrieved
    by the crawler. Any new or updated document that is emitted by the **content extractor** component
    is propagated to the **content indexer** so that the index can be updated.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，该组件维护一个全文索引，用于爬虫检索到的所有文档。任何由**内容提取器**组件输出的新或更新文档都会传播到**内容索引器**，以便更新索引。
- en: It stands to reason that having an index with no means of searching greatly
    diminishes its usefulness. To this end, the content indexer exposes mechanisms
    that allow other components to perform full-text searches against the index and
    to order the results according to retrieval date and/or PageRank score.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑上讲，如果一个索引没有搜索手段，那么它的实用性将大大降低。为此，内容索引器公开了允许其他组件对索引执行全文搜索并按检索日期和/或PageRank分数排序的机制。
- en: The link provider
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接提供者
- en: 'The** link provider** component periodically scrubs the link graph and collects
    a list of candidate links for a new crawl pass. Candidate links include the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**链接提供者**组件定期清理链接图，并收集新爬取遍历的候选链接列表。候选链接包括以下内容：'
- en: Recently discovered links that haven't been crawled yet
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近发现的尚未被爬取的链接
- en: Links for which recent crawl attempts failed (for example, the crawler received
    a 404/NOT-FOUND response from the remote server)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近爬取尝试失败的链接（例如，爬虫从远程服务器收到了404/NOT-FOUND响应）
- en: Links that the crawler successfully processed in the past but need to be re-visited
    in case the content they point to has changed
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫过去成功处理但需要重新访问的链接，以防指向的内容已更改
- en: 'Given that the WWW is comprised of a mind-boggling number of pages (approximately
    6.16 billion as of January 2020), it makes sense to assume that, as our discovered
    graph of links grows over time, we will eventually reach a point where the set
    of links we need to crawl will exceed the available memory capacity of our compute
    nodes! This is why the link provider component employs a *streaming* approach:
    while the link scrubbing process is executing, any selected link candidate will
    be immediately passed along to the *crawler* component for further processing.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于万维网由令人难以置信数量的页面组成（截至2020年1月，大约有61.6亿个页面），因此假设随着时间的推移，我们发现的链接图增长，我们最终会达到一个点，即我们需要抓取的链接集将超过我们计算节点的可用内存容量！这就是为什么链接提供者组件采用
    *流式* 方法：当链接清洗过程正在执行时，任何选定的链接候选者将立即传递给 *捕收器* 组件以进行进一步处理。
- en: The link graph
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接图
- en: The **link graph** is responsible for keeping track not only of all links that
    the crawler has discovered so far but also of how they are connected. It exposes
    interfaces for other components to add or remove links from the graph and, of
    course, query the graph.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**链接图** 负责跟踪不仅包括捕收器迄今为止发现的全部链接，还包括它们的连接方式。它为其他组件提供了添加或从图中删除链接的接口，当然，查询图。'
- en: 'Several other system components depend on the interfaces exposed by the link
    graph component:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 几个其他系统组件依赖于链接图组件公开的接口：
- en: The *link provider* queries the link graph to decide which links should be crawled
    next.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*链接提供者* 会查询链接图来决定哪些链接应该被接下来抓取。'
- en: The *link extractor* sub-component of the crawler adds newly discovered links
    to the graph.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕收器的 *链接提取器* 子组件会将新发现的链接添加到图中。
- en: The *PageRank calculator* components require access to the entire graph's connectivity
    information so that it can calculate the PageRank score of each link.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PageRank 计算器* 组件需要访问整个图的连接信息，以便计算每个链接的 PageRank 分数。'
- en: Note that I am not talking about a *single* interface but I am using the plural
    form: *interfaces*. This is deliberate as the link graph component is a prime
    candidate for implementing the **Command Query Responsibility Segregation** (**CQRS**)
    pattern.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我并不是在谈论一个 *单一* 接口，而是在使用复数形式：*接口*。这是故意的，因为链接图组件是实施 **命令查询责任分离**（**CQRS**）模式的理想候选者。
- en: The CQRS pattern belongs to the family of architectural patterns. The key idea
    behind CQRS is to separate the write and read models exposed by a particular component
    so they can be optimized in isolation. **Commands** refer to operations that mutate
    the state of the model, whereas *queries* retrieve and return the current model
    state.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: CQRS 模式属于架构模式家族。CQRS 背后的关键思想是将特定组件公开的写入和读取模型分离，以便它们可以独立优化。**命令** 指的是改变模型状态的操作，而
    *查询* 是检索和返回当前模型状态。
- en: This separation allows us to execute different business logic paths for reads
    and writes, and, in effect, enables us to implement complex access patterns. For
    example, writes could be a synchronous process whereas reads might be asynchronous
    and provide a limited view over the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分离使我们能够为读取和写入执行不同的业务逻辑路径，并且实际上使我们能够实现复杂的访问模式。例如，写入可以是同步过程，而读取可能是异步的，并且可以提供对数据的有限视图。
- en: As another example, the component could utilize separate data stores for writes
    and reads. Writes would eventually trickle into the read store but perhaps the
    read store data could also be augmented with external data obtained from other
    downstream components.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，该组件可以为写入和读取使用不同的数据存储。写入最终会缓慢地流入读取存储，但也许读取存储的数据也可以通过从其他下游组件获取的外部数据来增强。
- en: The PageRank calculator
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PageRank 计算器
- en: The **PageRank** calculator implements an asynchronous, periodic process for
    re-evaluating the PageRank scores for each link in the Links 'R' Us graph.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**PageRank** 计算器实现了异步、周期性的过程，用于重新评估 Links ''R'' Us 图中每个链接的 PageRank 分数。'
- en: Before starting a new calculation pass, the PageRank component will first use
    the interfaces exposed by the link graph component to obtain a snapshot of the
    current state of the graph. This includes both the graph vertices (links destinations)
    and the edges (links) connecting them.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始新的计算遍历之前，PageRank 组件将首先使用链接图组件公开的接口来获取当前图状态的快照。这包括图顶点（链接目的地）和连接它们的边（链接）。
- en: Once the PageRank values for each link have been calculated, the PageRank component
    will contact the text indexer component and annotate each indexed document with
    its updated PageRank score. This is an asynchronous process and does not otherwise
    interfere with any searches performed by the Links 'R' Us users.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个链接的PageRank值被计算出来，PageRank组件将联系文本索引器组件，并使用更新的PageRank分数标注每个索引文档。这是一个异步过程，并且不会干扰“链接之用”用户执行的任何搜索。
- en: The metrics store
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标存储
- en: Given that the Links 'R' Us project consists of multiple components, it would
    make sense for us to deploy monitoring infrastructure so that we can keep track
    of the health of each component. This way, we can identify components that exhibit
    elevated error rates or experience high load and need to be scaled up.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于“链接之用”项目由多个组件组成，因此对我们来说部署监控基础设施是有意义的，这样我们就可以跟踪每个组件的健康状况。这样，我们可以识别出那些表现出高错误率或负载过高的组件，并需要扩展。
- en: 'This is the primary role of the **metrics store** component. As you can see
    in the component diagram, all components in our design transmit metrics to the
    metrics collector and therefore depend on it. Of course, this is not a *hard* dependency:
    our system design should assume that the metrics collector could go offline at
    any given moment and make sure that none of the other components are affected
    should this occur in production.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**指标存储**组件的主要作用。正如您在组件图中可以看到的，我们设计中的所有组件都将指标传输到指标收集器，因此依赖于它。当然，这并不是一个**硬**依赖：我们的系统设计应该假设指标收集器在任何时候都可能离线，并确保在生产中发生这种情况时，其他组件不会受到影响。
- en: The frontend
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前端
- en: 'The purpose of the **frontend** component is to render a simple, static HTML-based
    user interface that will facilitate the users'' interaction with the project.
    More specifically, the design of the frontend component will enable users to perform
    the following set of functions:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 前端组件的目的是渲染一个简单、静态的基于HTML的用户界面，这将使用户能够方便地与项目交互。更具体地说，前端组件的设计将使用户能够执行以下一系列功能：
- en: Directly submit new URLs for indexing.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接提交新的URL进行索引。
- en: Type a keyword or phrase-based search query.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入一个关键词或基于短语搜索查询。
- en: Paginate the search results for a particular query.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分页特定查询的搜索结果。
- en: 'It is important to note that, in our current design, the frontend component
    serves as the entry-point for making our project accessible by the outside world! Given
    that *none* of the other project components can be directly accessed by the end
    users, we could argue that the frontend also doubles as an *API gateway*, where
    each incoming API request is mapped to *one or more* calls to the internal system
    components. Besides the obvious security benefits of isolating our internal components
    from the rest of the world, the API gateway pattern provides the following set
    of additional benefits:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在我们的当前设计中，前端组件作为我们项目对外部世界可访问的入口点！鉴于其他项目组件**不能**直接被最终用户访问，我们可以认为前端还充当了一个**API网关**，其中每个传入的API请求都映射到内部系统组件的**一个或多个**调用。除了将我们的内部组件与外界隔离的明显安全优势之外，API网关模式还提供了以下一系列额外的好处：
- en: If some of the internal calls need to be asynchronous, the gateway can execute
    them in parallel and wait for them to complete before aggregating their responses
    and returning them to the user in a synchronous manner.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些内部调用需要异步执行，网关可以并行执行它们，并在聚合它们的响应并同步返回给用户之前等待它们完成。
- en: It enables us to decouple the way that our internal components communicate with
    each other from the mechanism that the outside world uses to interface with our
    system. This means that we can expose a RESTful API to the outside world while
    still retaining the flexibility to select the most suitable transport for each
    internal component (for example, REST, gRPC, or perhaps a message queue).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使我们能够将内部组件之间通信的方式与外部世界用于与我们的系统接口的机制解耦。这意味着我们可以向外部世界公开RESTful API，同时仍然保留选择每个内部组件最合适的传输方式（例如，REST、gRPC或可能是一个消息队列）的灵活性。
- en: Monolith or microservices? The ultimate question
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成式或微服务？终极问题
- en: Before commencing development of the Links 'R' Us service, we need to decide
    whether our system components will be developed as parts of a big, monolithic
    service or whether we will just bite the bullet and implement a service-oriented
    architecture right from the start.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始开发“链接之用”服务之前，我们需要决定我们的系统组件是作为大型单体服务的一部分开发，还是我们将直接实施面向服务的架构。
- en: 'While the concept of using microservices does indeed seem enticing from the
    outside, it comes with a lot of operational overhead. Besides the mental effort
    required for building and wiring all components together, we would additionally
    need to worry about questions like the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从外面看，使用微服务的概念确实很有吸引力，但它也带来了很多运营开销。除了构建和连接所有组件所需的精神努力外，我们还需要担心以下问题：
- en: How does each service get deployed? Are we doing rolling deployments? What about
    dark or test releases? How easy is it to roll back to a previous deployment when
    something goes wrong?
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个服务是如何部署的？我们是进行滚动部署吗？那么，关于暗黑或测试版本呢？当出现问题需要回滚时，回滚到之前的部署有多容易？
- en: Are we going to use a container orchestration layer such as Kubernetes ^([6])?
    How does traffic get routed between services? Do we need to use a service mesh
    such as Istio ^([4]) or Linkerd ^([7])?
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否会使用像 Kubernetes 这样的容器编排层？服务之间的流量是如何路由的？我们需要使用像 Istio 或 Linkerd 这样的服务网格吗？
- en: How can we monitor the health of our services? Furthermore, how can we collect
    the logs from all our services?
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何监控我们服务的健康状态？此外，我们如何收集所有服务的日志？
- en: How are we going to handle service downtime? Do we need to implement circuit-breakers
    to prevent a problematic service from breaking upstream services that depend on
    it?
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何处理服务中断？我们需要实现断路器来防止有问题的服务破坏依赖于它的上游服务吗？
- en: Sure, we are all aware of the shortcomings of monolithic designs but, on the
    other hand, we don't have any available data to justify the extra cost of splitting
    components into microservices from the start of the project.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们所有人都知道单体设计的缺点，但另一方面，我们没有任何可用数据来证明从项目开始就拆分组件成微服务的额外成本是合理的。
- en: Weighing the pros and cons of each approach, it looks like the best course of
    action is to follow a hybrid approach! We will initially develop our components
    using a monolithic design. However, and this is the twist, each component will
    define an interface that other components will use to communicate with it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每种方法的优缺点，看起来最好的行动方案是采取混合方法！我们最初将使用单体设计来开发我们的组件。然而，这里有个转折，每个组件都将定义一个接口，其他组件将使用该接口与其通信。
- en: To connect components without introducing any coupling between their concrete
    implementations, we will be making use of the *proxy* design pattern. Initially,
    we will be providing dummy proxy implementations that facilitate inter-component
    communication within the *same process*. This is, of course, functionally equivalent
    to directly wiring components together as we would normally do in a monolithic
    design.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不引入它们具体实现之间的任何耦合的情况下连接组件，我们将使用*代理*设计模式。最初，我们将提供模拟代理实现，以促进*同一进程*内组件之间的通信。这当然在功能上等同于我们通常在单体设计中直接连接组件。
- en: As our system grows and evolves, we will eventually reach a point where we need
    to extract one or more components into standalone services. Using the preceding
    pattern, all we need to do is update our proxies to use the appropriate transport
    (for example, REST, gRPC, and message queues) for connecting components together
    without having to modify any of the existing component implementations.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的系统增长和演变，我们最终会达到一个需要将一个或多个组件提取为独立服务的地方。使用前面的模式，我们只需要更新我们的代理以使用适当的传输（例如，REST、gRPC
    和消息队列）来连接组件，而无需修改任何现有的组件实现。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: This concludes the presentation of the Links 'R' Us project. I hope that, by
    this point, you have acquired a general understanding of what we are going to
    be building over the next few chapters. If you find yourself wondering about the
    technical implementation details associated with some of the project components,
    that's perfectly normal. The main purpose of this chapter was to introduce a high-level
    overview of the project. We will analyze the construction of each one of these
    components in *extensive* detail in the pages that follow!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着“链接之用”项目的介绍结束。我希望到这一点，你已经对接下来几章将要构建的内容有了大致的了解。如果你对项目组件的技术实现细节感到好奇，那是非常正常的。本章的主要目的是介绍项目的高级概述。我们将在接下来的页面中详细分析每个组件的构建！
- en: 'To make the concepts and code for the following chapters easier to follow,
    we will be splitting each chapter into two core parts:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使以下章节的概念和代码更容易理解，我们将把每一章分成两个核心部分：
- en: In the first half of each chapter, we will be performing a deep dive into a
    particular technical topic, for example, a survey of popular types of databases
    (relational, NoSQL, and so on), how you can create pipelines in Go, how you can
    run graph operations at scale, what gRPC is and how you can use it, and so on.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一章的前半部分，我们将深入探讨一个特定的技术主题，例如，对流行数据库类型（关系型、NoSQL等）的调查，如何在Go中创建管道，如何大规模运行图操作，什么是gRPC以及如何使用它，等等。
- en: In the second half of the chapter, we will be taking the concepts from the first
    half and applying them toward building one or more components of the Links 'R'
    Us project.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们将把前半部分的概念应用到构建“链接之用”项目的一个或多个组件上。
- en: 'In the next chapter, we will focus our attention on building one of the key
    components for the Links ''R'' Us project: a fully-functioning data persistence
    layer for storing the links discovered by the crawler and indexing the contents
    of each web page retrieved by the crawler.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将集中精力构建“链接之用”项目的关键组件之一：一个完整功能的数据持久层，用于存储爬虫发现的链接和索引爬虫检索到的每个网页的内容。
- en: Questions
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between a functional and a non-functional requirement?
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 功能性需求与非功能性需求之间的区别是什么？
- en: Describe the main components of a user story.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述用户故事的主要组成部分。
- en: What things could possibly go wrong in the Links 'R' Us scenario if we blindly
    crawl any link that a user submits to the system?
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们盲目地爬取用户提交给系统的任何链接，在“链接之用”场景中可能会出现哪些问题？
- en: Name the key components of an SLO.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出SLO的关键组成部分。
- en: What is the purpose of a UML component diagram?
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UML组件图的目的是什么？
- en: Further reading
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Amazon Elastic Compute Cloud: *Instance Metadata and User Data*: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 亚马逊弹性计算云：*实例元数据和用户数据*：[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html)
- en: 'Apache Mesos: Program against your data center like it''s a single pool of
    resources: [https://mesos.apache.org](https://mesos.apache.org)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Mesos: 将数据中心当作单一资源池来编程：[https://mesos.apache.org](https://mesos.apache.org)'
- en: 'Brooks, Frederick P., Jr.: *The Mythical Man-Month (Anniversary Ed.)*. Boston,
    MA, USA: Addison-Wesley Longman Publishing Co., Inc., 1995 — [https://www.worldcat.org/title/mythical-man-month/oclc/961280727](https://www.worldcat.org/title/mythical-man-month/oclc/961280727)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 布鲁克斯，弗雷德里克·P.，小：*《人月神话（周年纪念版）》*。波士顿，马萨诸塞州，美国：Addison-Wesley Longman 出版公司，1995
    — [https://www.worldcat.org/title/mythical-man-month/oclc/961280727](https://www.worldcat.org/title/mythical-man-month/oclc/961280727)
- en: 'Istio: *Connect, secure, control, and observe services*: [https://istio.io](https://istio.io)'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Istio: *连接、安全、控制和观察服务*：[https://istio.io](https://istio.io)'
- en: 'Ivn, Gbor and Grolmusz, Vince: *When the Web Meets the Cell: Using Personalized
    PageRank for Analyzing Protein Interaction Networks*.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ivn，Gbor 和 Grolmusz，Vince：*当网络遇到细胞：使用个性化PageRank分析蛋白质相互作用网络*。
- en: 'Kubernetes: *Production-Grade Container Orchestration*: [https://kubernetes.io](https://kubernetes.io)'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kubernetes: *生产级容器编排*：[https://kubernetes.io](https://kubernetes.io)'
- en: 'Linkerd: *Ultralight service mesh for Kubernetes and beyond*: [https://linkerd.io](https://linkerd.io)'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Linkerd: *适用于Kubernetes及更广泛的超轻量级服务网格*：[https://linkerd.io](https://linkerd.io)'
- en: 'MASSCAN: Mass IP port scanner; reserved IP exclusion list: [https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf](https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'MASSCAN: 大规模IP端口扫描器；保留IP排除列表：[https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf](https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf)'
- en: 'Page, L.; Brin, S.; Motwani, R.; and Winograd, T.: *The PageRank Citation Ranking:
    Bringing Order to the Web*. In: Proceedings of the 7th International World Wide
    Web Conference. Brisbane, Australia, 1998, S. 161–172'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Page, L.; Brin, S.; Motwani, R.; and Winograd, T.: *PageRank引文排名：为网络带来秩序*.
    In: 第七届国际万维网会议论文集. 澳大利亚布里斯班，1998年，第161–172页'
- en: 'Pop, Florin ; Dobre, Ciprian: *An Efficient PageRank Approach for Urban Traffic
    Optimization*.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Pop, Florin ; Dobre, Ciprian: *一种高效的PageRank方法用于城市交通优化*.'
- en: 'Swarm: *a Docker-native clustering system*: [https://github.com/docker/swarm](https://github.com/docker/swarm)'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Swarm: *一个Docker原生集群系统*: [https://github.com/docker/swarm](https://github.com/docker/swarm)'
