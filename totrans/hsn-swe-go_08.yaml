- en: The Links 'R'; Us Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The hardest part of the software task is arriving at a complete and consistent
    specification, and much of the essence of building a program is in fact the debugging
    of the specification."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Frederick P. Brooks ^([3])'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be discussing Links ''R'' Us, a Go project that we
    will be building from scratch throughout the remaining chapters in this book.
    This project has been specifically designed to combine everything you have learned
    so far with some of the more technical topics that we will be touching on in the
    following chapters: databases, pipelines, graph processing, gRPC, instrumentation,
    and monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of the system that we will be building and its primary function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting an appropriate SDLC model for the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional and non-functional requirements analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Component-based modeling of the Links 'R' Us service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing an appropriate architecture (monolith versus microservices) for the
    project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System overview – what are we going to be building?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the next chapters, we will be assembling, piece by piece, our very
    own *search-engine*. As with all projects, we need to come up with a cool-sounding
    name for it. Let me introduce you to *Links 'R' Us*!
  prefs: []
  type: TYPE_NORMAL
- en: So, what are the core functionalities of the Links 'R' Us project? The primary,
    and kind of obvious, functionality is being able to search for content. However,
    before we can make our search engine available to the public, we first need to
    seed it with content. To this end, we need to provide the means for users to submit
    URLs to our search engine. The search engine would then crawl those links, index
    their content, and add any newly encountered links to its database for further
    crawling.
  prefs: []
  type: TYPE_NORMAL
- en: Is this all we need for launching Links 'R' Us? The short answer is no! While
    user searches would return results containing the keywords from the users' search
    queries, we would lack the capability to *order* them in a meaningful way, especially
    if the results range in the thousands.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, we need to introduce some sort of a link or content quality metric
    to our system and order the returned results by it. Instead of re-inventing the
    wheel, we will be stepping on the shoulders of search-engine *giants* (that would
    be Google) and implementing a battle-tested algorithm called `PageRank`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PageRank` algorithm was introduced by a nowadays very popular and heavily
    cited paper titled *The PageRank Citation Ranking: Bringing Order to the Web*.
    The original paper was authored back in 1998 by Larry Page, Sergey Brin, Rajeev
    Motwani, and Terry Winograd ^([9]) and, over the years, has served as the basis
    for the search-engine implementation at Google.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a graph containing links between web-pages, the `PageRank` algorithm assigns
    an importance score to each link in the graph taking into account the number of
    links that lead to it and their relative importance scores.
  prefs: []
  type: TYPE_NORMAL
- en: While `PageRank` was initially introduced as a tool for organizing web content,
    its generalized form applies to any type of link graph. For the last few years,
    there has been on-going research into applying `PageRank` ideas in a multitude
    of fields ranging from biochemistry ^([5]) to traffic optimization ^([10]).
  prefs: []
  type: TYPE_NORMAL
- en: We will be exploring the `PageRank` algorithm in more detail in [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    *Graph-Based Data Processing*, and [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph-Processing Systems*, as part of a larger discussion
    centered around the various approaches we can employ to facilitate processing
    of large graphs on a single node or across a cluster of nodes (out-of-core graph
    processing).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an SDLC model for our project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before delving into the details of the Links ''R'' Us project, we need to consider
    the SDLC models we discussed in [Chapter 1](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml), *A
    Bird''s-Eye View of Software Engineering*, and select one that makes more sense
    for this type of project. The choice of a suitable model is of paramount importance:
    it will serve as our guide for capturing the requirements for the project, defining
    the components and the interface contracts between them, and appropriately dividing
    the work to be done in logical chunks that can be built and tested independently
    of each other.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will outline the main reasoning behind the selection of
    an Agile framework for our project and elaborate on a set of interesting approaches
    for speeding up our development velocity using a technique known as *elephant
    carpaccio*.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating faster using an Agile framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin with, for all intents and purposes, Links 'R' Us is a typical example
    of a green-field type of project. Since there are no pressing deadlines for delivering
    the project, we should definitely take our time to explore the pros and cons of
    any alternative technologies at our disposal for implementing the various components
    of the system.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when it comes to indexing and searching the documents that our
    system will be crawling, there are several competing products/services that we
    need to evaluate before deciding on which one to use. Furthermore, if we decide
    to containerize our project using a tool such as Docker, there are several orchestration
    frameworks (for example, Kubernetes ^([6]), Apache Mesos ^([2]), or Docker Swarm ^([11]))
    available for deploying our services to our staging and production environments.
  prefs: []
  type: TYPE_NORMAL
- en: As far as the software development pace is concerned, we are going to be *gradually* and *incrementally* building
    the various components of Links 'R' Us for the next few chapters. Given that we
    are working on what is essentially a user-facing product, it is imperative to
    work in small iterations so that we can get the prototype versions out to user
    focus groups as early as possible. This will enable us to collect valuable feedback
    that will aid us in fine-tuning and polishing our product as development goes
    on.
  prefs: []
  type: TYPE_NORMAL
- en: For all of the preceding reasons, I think it would be prudent to adopt an Agile
    approach to developing Links 'R' Us. My personal preference would be to use Scrum.
    As we don't really have an actual development team to back the project's development,
    concepts such as stand-ups, planning, and retrospective sessions do not apply
    to our particular case. Instead, we need to compromise and adopt some of the ideas
    behind Scrum in our own Agile workflow.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, in the requirements analysis section, we will focus on creating
    user stories. Once that process is complete, we will use those stories as input
    to infer the set of high-level components that we need to build, as well as the
    ways they are expected to interact with each other. Finally, when the time comes
    to implement each user story, we will assume the role of the *product owner* and
    break each story down into a set of cards which we will then arrange in a Kanban
    board.
  prefs: []
  type: TYPE_NORMAL
- en: But before we start working on user stories, I would like to introduce a quite
    useful and helpful technique that can help you to iterate even faster with your
    own projects: *elephant carpaccio*.
  prefs: []
  type: TYPE_NORMAL
- en: Elephant carpaccio – how to iterate even faster!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This peculiarly-named technique owes its existence to an exercise invented by
    Dr. Alistair Cockburn. The purpose of this exercise is to help people (engineers
    and non-engineers alike) to practice and learn how they can split complex story
    cards (the elephant) into very *thin vertical slices* that teams can oftentimes
    tackle in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: It may strike you as odd but the slice size that I have found most helpful in
    projects that I have been involved with in the past is nothing more than a *single
    day's worth of work*. The rationale of the one-day split is to ship (behind a
    feature flag) small parts of the total work every single day, an approach that
    is congruent with the *ship fast* motto advocated by Agile development.
  prefs: []
  type: TYPE_NORMAL
- en: Suffice it to say, splitting cards into one-day slices is certainly not a trivial
    task. It does take a bit of practice and patience to condition your brain so it
    switches its focus from long-running tasks to breaking down and optimizing workloads
    for much shorter periods of time. On the flip side, this approach allows engineering
    teams to identify and resolve potential blockers as early as possible; it goes
    without saying that we would obviously prefer to detect blockers near the beginning
    of the sprint rather than the middle, or, even worse, close to the end of the
    sprint cycle!
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of this technique, at least from the perspective of Go engineers,
    is that it makes us think more carefully about the best way to organize our code
    base to ensure that, by the end of each day, we always have a piece of software
    that can be cleanly compiled and deployed. This constraint forces us into developing
    the good habit of thinking about code in terms of interfaces as per the tenets
    of the SOLID design principles we explored in [Chapter 2](96fb70cb-8134-4156-bd3e-48ca53224683.xhtml),
    *Best Practices for Writing Clean and Maintainable Go Code*.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform a detailed requirements analysis for the Links 'R' Us project, we
    need to essentially come up with answers for two key questions: *what* do we need
    to build and *how well* would our proposed design fare against a set of goals?
  prefs: []
  type: TYPE_NORMAL
- en: To answer the *what* question, we need to list all of the core functionalities
    that our system is expected to implement as well as describe how the various actors
    will interact with it. This forms the **Functional Requirements** (**FRs**) for
    our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: To answer the latter question, we have to state the **Non-Functional Requirements**
    (**NFRs**) for our solution. Typically, the list of non-functional requirements
    includes items such as **Service-Level Objectives** (**SLOs**) and capacity and
    scalability requirements, as well as security-related considerations for our project.
  prefs: []
  type: TYPE_NORMAL
- en: Functional requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already decided on utilizing an Agile model for implementing our
    project, the next logical step for defining our functional list of requirements
    is to establish *user stories*.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of user stories pertains to the need of expressing software requirements
    from the perspective of an actor that interacts with the system. In many types
    of projects, actors are typically considered to be the end users of the system.
    However, in the general case, *other systems* (for example, a backend service)
    may also assume the role of an actor.
  prefs: []
  type: TYPE_NORMAL
- en: Each user story begins with a *succinct* requirement specification. It is important
    to note that the specification itself must *always* be expressed from the viewpoint
    of the actor that will be impacted by it. Furthermore, when creating user stories,
    we should always strive to capture the *business value*, also referred to as the *true
    reason*, behind each requirement. What's more, one of the core values of Agile
    development is the so-called *definition of done*. When authoring stories, we
    need to include a list of *acceptance criteria* that will be used as a verification
    tool to ensure that each story goal has been successfully met.
  prefs: []
  type: TYPE_NORMAL
- en: 'For defining the functional requirements for Links ''R'' Us, we will be utilizing
    the following, rather standardized, Agile template:'
  prefs: []
  type: TYPE_NORMAL
- en: As an `[actor]`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `[short requirement]`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `[reason/business value]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[list of criteria]`'
  prefs: []
  type: TYPE_NORMAL
- en: One final thing that I would like to point out is that, while each story will
    record a *need* for a particular feature, all of them will be completely devoid
    of any sort of implementation detail. This is quite intentional, and congruent
    with the recommended practices when working with any Agile framework. As we discussed
    in [Chapter 1](5ef11f50-0620-4132-8bfe-c30d6c79f2f5.xhtml), *A Bird's-Eye View
    of Software Engineering*, our goal is to defer any technical implementation decisions
    up to the last possible moment. If we were to decide up-front about how we are
    going to implement each user story, we would be placing unnecessary constraints
    on our development process, hence limiting our flexibility and the amount of work
    we can achieve given a particular time budget.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now apply the preceding template to capture the set of functional requirements
    for the Links 'R' Us project as a list of user stories that will be individually
    tackled throughout the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: User story – link submission
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an `end user`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `submit new links to Links 'R' Us`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `update the link graph and make their contents searchable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A frontend or API endpoint is provided for facilitating the link submission
    journey for the end users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Submitted links have the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be added to the graph
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be crawled by the system and added to their index
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Already submitted links should be accepted by the backend but not inserted twice
    to the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User story – search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an `end user`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `submit full-text search queries`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `to retrieve a list of relevant matching results from the content indexed
    by Links 'R' Us`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A frontend or API endpoint is provided for the users to submit a full-text query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the query matches multiple items, they are returned as a list that the end
    user can paginate through.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each entry in the result list must contain the following items: title or link
    description, the link to the content, and a timestamp indicating when the link
    was last crawled. *If feasible*, the link may also contain a relevance score expressed
    as a percentage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the query does not match any item, an appropriate response should be returned
    to the end user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User story – crawl link graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the `crawler backend system`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `obtain a list of sanitized links from the link graph`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `fetch and index their contents while at the same time expanding the
    link graph with newly discovered links`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The crawler can query the link graph and receive a list of stale links that
    need to be crawled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Links received by the crawler are retrieved from the remote hosts unless the
    remote server provides an `ETag` or `Last Modified` header that the crawler has
    already seen before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieved content is scanned for links and the link graph gets updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieved content is indexed and added to the search corpus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User story – calculate PageRank scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the `PageRank calculator backend system`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `access the link graph`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `calculate and persist the PageRank score for each link`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank calculator can obtain an immutable snapshot of the entire link
    graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PageRank score is assigned to every link in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The search corpus entries are annotated with the updated PageRank scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User story – monitor Links 'R' Us health
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a `member of the Links 'R' Us Site Reliability Engineering (SRE) team`,
  prefs: []
  type: TYPE_NORMAL
- en: I need to be able to `monitor the health of all Links 'R' Us services`,
  prefs: []
  type: TYPE_NORMAL
- en: so as to `detect and address issues that cause degraded service performance`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The acceptance criteria for this user story are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: All Links 'R' Us services should periodically submit health- and performance-related
    metrics to a centralized metrics collection system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A monitoring dashboard is created for each service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level monitoring dashboard tracks the overall system health.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric-based alerts are defined and linked to a paging service. Each alert comes
    with its own *playbook* with a set of steps that need to be performed by a member
    of the SRE team that is on-call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-functional requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will go through a list of non-functional requirements for
    the Links 'R' Us project. Please keep in mind that this list is not exhaustive.
    Since this is not a real-world project, I opted to describe only a small subset
    of the possible non-functional requirements that make sense from the viewpoint
    of the components that we will be building in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Service-level objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a **Site Reliability Engineering** (**SRE**) perspective, we need to come
    up with a list of SLOs that will be used as a gauge for the Links 'R' Us project's
    performance. Ideally, we should be defining individual SLOs for each one of our
    services. That may not be immediately possible at the design stage but, at the
    very least, we need to come up with a realistic SLO for the user-facing components
    of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLOs consist of three parts: a description of the thing that we are measuring,
    the expected service level expressed as a percentage, and the period where the
    measurement takes place. The following table lists some initial and fairly standard
    SLOs for Links ''R'' Us:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Expectation** | **Measurement Period** | **Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| Links ''R'' Us availability | 99% uptime | Yearly | Tolerates up to 3d 15h
    39m of downtime per year |'
  prefs: []
  type: TYPE_TB
- en: '| Index service availability | 99.9% uptime | Yearly | Tolerates up to 8h 45m
    of downtime per year |'
  prefs: []
  type: TYPE_TB
- en: '| PageRank calculator service availability | 70% uptime | Yearly | Not a user-facing
    component of our system; the service can endure longer periods of downtime |'
  prefs: []
  type: TYPE_TB
- en: '| Search response time | 30% of requests answered in 0.5s | Monthly |  |'
  prefs: []
  type: TYPE_TB
- en: '| Search response time | 70% of requests answered in 1.2s | Monthly |  |'
  prefs: []
  type: TYPE_TB
- en: '| Search response time | 99% of requests answered in 2.0s | Monthly |  |'
  prefs: []
  type: TYPE_TB
- en: '| CPU utilization for the PageRank calculator service | 90% | Weekly | We shouldn''t
    be paying for idle computing nodes |'
  prefs: []
  type: TYPE_TB
- en: '| SRE team incident response time | 90% of tickets resolved within 8h | Monthly
    |  |'
  prefs: []
  type: TYPE_TB
- en: Keep in mind that, at this stage, we don't really have any prior data available;
    we are more or less using *guesstimates* for the service levels that we are targeting.
    We will be revisiting and updating the SLOs to better reflect reality once the
    complete system gets deployed to production and our SRE team acquires a better
    understanding of the system's idiosyncrasies.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick heads-up, [Chapter 13](56c5302a-2c1a-4937-bb65-1b280f27ebed.xhtml),
    *Metrics Collection and Visualization*, of this book exclusively focuses on the
    SRE aspects involved in operating production services. In that chapter, we will
    be elaborating on popular tools that we can use to capture, visualize, and alert
    on our service-level-related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Security considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we all know, when it comes to online services, *security* is characterized,
    above all, as a factor that can make or break a particular product. To this end,
    we need to discuss some potential security issues that may arise when building
    a project such as Links 'R' Us and devise strategies for dealing with them.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis operates under the premise that *you should never trust the client*,
    in this case, the user interacting with Links 'R' Us. Should our project become
    successful, it will inadvertently attract the attention of malicious actors that
    will, at some point, try to locate and exploit security holes in our system.
  prefs: []
  type: TYPE_NORMAL
- en: One of the use cases that I presented earlier involves the user submitting a
    URL that the service will eventually crawl and add to its search index. You may
    be wondering what could possibly go wrong with a service that just crawls user-submitted
    URLs? Here are a few interesting examples.
  prefs: []
  type: TYPE_NORMAL
- en: Most cloud providers run an internal metadata service that each computing node
    can query to obtain information about itself. This service is typically accessed
    via a *link-local* address such as `169.254.169.254` and nodes can perform simple
    HTTP GET requests to retrieve the information they are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Link-local addresses are a special block of addresses reserved by the **Internet
    Engineering Task Force** (**IETF**). The range of IPv4 addresses within that block
    is described in CIDR notation as `169.254.0.0/16` (65,536 unique addresses). Similarly,
    the following address block has been reserved for use with IPv6: `fe80::/10`.
  prefs: []
  type: TYPE_NORMAL
- en: These addresses are special in that they are only valid within a particular
    network segment and they are not route-able beyond that; that is, routers will
    refuse to forward them to other networks. Link-local addresses are therefore safe
    to use internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we have deployed the Links ''R'' Us project to Amazon EC2\.
    The documentation page ^([1]) for the EC2 metadata service references quite a
    few link-local endpoints that a malicious adversary could use. Here are two of
    the more interesting ones from an attacker''s perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://169.254.169.254/latest/meta-data/iam/info` returns information about
    the roles associated with the compute node that the call originates from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://169.254.169.254/latest/meta-data/iam/security-credentials/<role-name>` returns
    a set of temporary security credentials associated with a particular role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a potential attack scenario, the malicious user submits the first URL to
    the crawler. If the crawler simply fetches the link and adds the response to the
    search index, the attacker can perform a targeted search and obtain the *name* of
    a security role associated with the nodes where the crawling service is deployed.
    Using that information, the attacker would then submit the second URL to the crawler,
    hoping to get lucky and retrieve a list of valid credentials by waiting once more
    for the link to be indexed and then performing a second targeted search query.
    This way, the adversary could gain unauthorized access to another service that
    the project is using *internally* (for example, a storage service such as S3).
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you are wondering, both Google Cloud and Microsoft Azure mitigate this
    information leak loophole by requiring a special HTTP header to be present when
    a compute node contacts their metadata services. However, this doesn''t mean that
    we shouldn''t be excluding *other* IP ranges from our crawl operations. For starters,
    we should always exclude *private network* addresses. After all, some of the services
    that we might opt to use (Elasticsearch comes to mind) could expose potentially
    unauthenticated RESTful APIs that can be reached by the compute nodes running
    the crawler code. Evidently, we don''t want information from our backend services
    to appear in our search index! The following table lists some of the special IPv4
    ranges that we should definitely avoid crawling:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **IP block (CIDR notation)** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 10.0.0.0/8 | Private network |'
  prefs: []
  type: TYPE_TB
- en: '| 172.16.0.0/12 | Private network |'
  prefs: []
  type: TYPE_TB
- en: '| 192.168.0.0/16 | Private network |'
  prefs: []
  type: TYPE_TB
- en: '| 169.254.0.0/16 | Link-local addresses |'
  prefs: []
  type: TYPE_TB
- en: '| 127.0.0.1 | Loop-back IP address |'
  prefs: []
  type: TYPE_TB
- en: '| 0.0.0.0/8 | All IP addresses on the local machine |'
  prefs: []
  type: TYPE_TB
- en: '| 255.255.255.255/32 | The broadcast address for the current network |'
  prefs: []
  type: TYPE_TB
- en: The list from the preceding table is not complete. In fact, you would need to
    exclude a few more IP blocks such as the ones reserved for carrier-grade traffic,
    multicast, and test networks. What's more, we should also exclude the equivalent
    IPv6 ranges if our cloud provider's network stack supports IPv6\. If you are interested
    in learning more about this topic, you can find a comprehensive IPv4 black-list
    at the GitHub repository for the MASSCAN project ^([8]).
  prefs: []
  type: TYPE_NORMAL
- en: One final thing that you may or may not be aware of is that many URL crawling
    libraries support schemes other than `http/https`. One example of those schemes
    is `file`, which, unless disabled, might allow an attacker to trick the crawler
    into reading and indexing the contents of a local file (for example, `/etc/passwd`)
    from the node the crawler is executing on.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't used the file protocol scheme before, try typing the following
    address into your favorite web-browser: `file:///`.
  prefs: []
  type: TYPE_NORMAL
- en: Being good netizens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While our end-goal is to be able to crawl and index the entire internet, the
    truth of the matter is that the links that we are retrieving and indexing point
    to content that belongs to someone else. It can so happen that those third parties
    object to us indexing *some* or *all* links to the domains under their control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a standardized way for web-masters to notify crawlers
    not only about which links they can crawl and which we are not allowed to but
    also to dictate an acceptable crawl speed to not incur a high load on the remote
    host. This is all achieved by authoring a `robots.txt` file and placing it at
    the root of each domain. The file contains a set of directives like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-Agent**: The name of the crawler (user agent string) which the following
    instructions apply to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disallow**:A regular expression that excludes any matching URL from being
    crawled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crawl-Delay**:The number of seconds for the crawler to wait before crawling
    subsequent links from this domain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sitemap**: A link to an XML file which defines all links within a domain
    and provides metadata such as a *last-update* timestamp that crawlers can use
    to optimize their link access patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be good netizens, we need to ensure that our crawler implementation respects
    the contents of any `robots.txt` file that it encounters. Last but not least,
    our parser should be able to properly handle the various status codes returned
    by remote hosts and dial down its crawl speed if it detects an issue with the
    remote host or the remote host decides to throttle us.
  prefs: []
  type: TYPE_NORMAL
- en: System component modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the first step in mapping the project's architecture, we will begin by creating
    a UML component diagram. The main goal here is to identify and describe the structural
    connections between the various *components* that comprise our system.
  prefs: []
  type: TYPE_NORMAL
- en: A component is defined as an encapsulated standalone unit that constitutes an
    integral part of a system or a sub-system. Components communicate with each other
    by exposing and consuming one or several interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: One key point of component-based design is that components should always be
    considered as abstract, logical entities that expose a particular behavior. This
    design approach is closely aligned with the SOLID principles and offers us the
    flexibility to freely change or even swap component implementations at any point
    throughout the project's development.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram breaks down the Links ''R'' Us project into high-level
    components and visually illustrates the interfaces exposed and consumed by each
    one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eaaec353-d95b-4ae6-b4c2-2ae596ef9871.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1: The UML component diagram for the Links 'R' Us project
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you are not familiar with the symbols used by this type of diagram,
    here is a quick explanation of what each symbol represents:'
  prefs: []
  type: TYPE_NORMAL
- en: Boxes with two port-like symbols on the side represent components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components can also be nested within other components. In that case, each sub-component
    is encapsulated within a box that represents its parent component. In the preceding
    diagram, **Link Filter** is a sub-component of **Crawler**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A full circle represents an *interface* **implemented** by a particular component.
    For instance, **Search** is one of the interfaces implemented by the **Content
    Indexer** component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A half-circle indicates that a component *requires* a particular interface.
    For example, the **Link Extractor** component requires the **Insert Link** interface
    implemented by the **Link Graph** component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have mapped out a high-level view of the system components required
    for constructing our project, we need to spend some time and examine each one
    in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The crawler component is effectively the heart of the search engine. It operates
    on a set of links that are either seeded into the system or discovered while crawling
    a previous set of links. As you can see in the preceding component model diagram,
    the crawler itself is in fact a package that encapsulates several other sub-components
    that operate in a pipeline-like kind of configuration. Let's examine the role
    of each one of those sub-components in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The link filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A naive crawler implementation would attempt to retrieve any links that are
    provided as input to it. But as we all know, the web is home to all sorts of content
    ranging from text or HTML documents to images, music, videos, and a wide variety
    of other types of binary data (for example, archives, ISOs, executables, and so
    on).
  prefs: []
  type: TYPE_NORMAL
- en: 'You would probably agree that attempting to download items that cannot be processed
    by the search engine would not only be a waste of resources but it would also
    incur additional running costs to the operator of the Links ''R'' Us service:
    us! Consequently, excluding such content from the crawler would be a beneficial
    cost reduction strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: This is where the *link filter* component comes into play. Before we try to
    fetch a remote link, the link filter will first attempt to identify the content
    at the other side and drop any links that do not seem to point to content that
    we can process.
  prefs: []
  type: TYPE_NORMAL
- en: The link fetcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All links that survive the link filter are consumed by the *link fetcher* component.
    As its name implies, this component is responsible for establishing an HTTP connection
    to each link target and retrieving any content returned by the server at the other
    end.
  prefs: []
  type: TYPE_NORMAL
- en: The fetcher meticulously processes the HTTP status code and any HTTP headers
    returned by remote servers. If the returned status code indicates that the content
    has been moved to a different location (that is, 301 or 302), the fetcher will
    automatically follow redirects until it reaches the content's final destination.
    It stands to reason that we would not want our fetcher to get stuck in an infinite
    redirect loop trying to crawl an incorrectly configured (or malicious) remote
    host. To this end, the crawler will need to maintain a redirect hop counter and
    abort the crawl attempt when it exceeds a particular value.
  prefs: []
  type: TYPE_NORMAL
- en: Another important HTTP header that the fetcher pays close attention to is the `Content-Type` header.
    This header is populated by the remote server and identifies the type (also known
    as MIME type) of data returned by the server. If the remote server replies with
    an unsupported content type header (for example, indicating an image or a JavaScript
    file), the fetcher should automatically drop the link and prevent it from reaching
    the next stages of the crawl pipeline, the **content extractor** and the **content
    indexer**.
  prefs: []
  type: TYPE_NORMAL
- en: The content extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **content extractor** attempts to identify and extract all text from a document
    downloaded from a remote server. For instance, if the link is pointed to a plaintext
    document, then the extractor would emit the document content as is. On the other
    hand, if the link pointed to an HTML document, the extractor would strip off any
    HTML elements and emit the text-only portion of the document.
  prefs: []
  type: TYPE_NORMAL
- en: The emitted content is sent off to the **content indexer** component so it can
    be tokenized and update the Links 'R' Us full-text search index.
  prefs: []
  type: TYPE_NORMAL
- en: The link extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last crawler component that we will be examining is the **link extractor**.
    It scans retrieved HTML documents and attempts to identify and extract all links
    present inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'Link extraction is unfortunately not a trivial task. While it''s true that
    the majority of links can be extracted via a bunch of regular expressions, there
    are a few edge-cases that require additional logic from our end, as in the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Relative links need to be converted into absolute links.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the document `<head>` section includes the `<base href="xxx">` tag, we need
    to parse it and use its content to rewrite relative links.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We might encounter links that *do not* specify a protocol. These special links
    begin with `//` and are commonly used when referencing content from a CDN or in
    HTTPS pages that include static resources from non-HTTPS sources (for example,
    images in a cart checkout page). When a web-browser encounters such links, it
    will automatically use the protocol from the current URL to fetch those links.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link extractor will transmit all newly discovered links to the *link graph* component
    so that existing graph connections can be updated and new ones created.
  prefs: []
  type: TYPE_NORMAL
- en: The content indexer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **content indexer** is yet another very important component for the Links
    'R' Us project. This component performs two distinct functions.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, the component maintains a full-text index for all documents retrieved
    by the crawler. Any new or updated document that is emitted by the **content extractor** component
    is propagated to the **content indexer** so that the index can be updated.
  prefs: []
  type: TYPE_NORMAL
- en: It stands to reason that having an index with no means of searching greatly
    diminishes its usefulness. To this end, the content indexer exposes mechanisms
    that allow other components to perform full-text searches against the index and
    to order the results according to retrieval date and/or PageRank score.
  prefs: []
  type: TYPE_NORMAL
- en: The link provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The** link provider** component periodically scrubs the link graph and collects
    a list of candidate links for a new crawl pass. Candidate links include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Recently discovered links that haven't been crawled yet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Links for which recent crawl attempts failed (for example, the crawler received
    a 404/NOT-FOUND response from the remote server)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Links that the crawler successfully processed in the past but need to be re-visited
    in case the content they point to has changed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given that the WWW is comprised of a mind-boggling number of pages (approximately
    6.16 billion as of January 2020), it makes sense to assume that, as our discovered
    graph of links grows over time, we will eventually reach a point where the set
    of links we need to crawl will exceed the available memory capacity of our compute
    nodes! This is why the link provider component employs a *streaming* approach:
    while the link scrubbing process is executing, any selected link candidate will
    be immediately passed along to the *crawler* component for further processing.'
  prefs: []
  type: TYPE_NORMAL
- en: The link graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **link graph** is responsible for keeping track not only of all links that
    the crawler has discovered so far but also of how they are connected. It exposes
    interfaces for other components to add or remove links from the graph and, of
    course, query the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several other system components depend on the interfaces exposed by the link
    graph component:'
  prefs: []
  type: TYPE_NORMAL
- en: The *link provider* queries the link graph to decide which links should be crawled
    next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *link extractor* sub-component of the crawler adds newly discovered links
    to the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *PageRank calculator* components require access to the entire graph's connectivity
    information so that it can calculate the PageRank score of each link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that I am not talking about a *single* interface but I am using the plural
    form: *interfaces*. This is deliberate as the link graph component is a prime
    candidate for implementing the **Command Query Responsibility Segregation** (**CQRS**)
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The CQRS pattern belongs to the family of architectural patterns. The key idea
    behind CQRS is to separate the write and read models exposed by a particular component
    so they can be optimized in isolation. **Commands** refer to operations that mutate
    the state of the model, whereas *queries* retrieve and return the current model
    state.
  prefs: []
  type: TYPE_NORMAL
- en: This separation allows us to execute different business logic paths for reads
    and writes, and, in effect, enables us to implement complex access patterns. For
    example, writes could be a synchronous process whereas reads might be asynchronous
    and provide a limited view over the data.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, the component could utilize separate data stores for writes
    and reads. Writes would eventually trickle into the read store but perhaps the
    read store data could also be augmented with external data obtained from other
    downstream components.
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank calculator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **PageRank** calculator implements an asynchronous, periodic process for
    re-evaluating the PageRank scores for each link in the Links 'R' Us graph.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting a new calculation pass, the PageRank component will first use
    the interfaces exposed by the link graph component to obtain a snapshot of the
    current state of the graph. This includes both the graph vertices (links destinations)
    and the edges (links) connecting them.
  prefs: []
  type: TYPE_NORMAL
- en: Once the PageRank values for each link have been calculated, the PageRank component
    will contact the text indexer component and annotate each indexed document with
    its updated PageRank score. This is an asynchronous process and does not otherwise
    interfere with any searches performed by the Links 'R' Us users.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that the Links 'R' Us project consists of multiple components, it would
    make sense for us to deploy monitoring infrastructure so that we can keep track
    of the health of each component. This way, we can identify components that exhibit
    elevated error rates or experience high load and need to be scaled up.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the primary role of the **metrics store** component. As you can see
    in the component diagram, all components in our design transmit metrics to the
    metrics collector and therefore depend on it. Of course, this is not a *hard* dependency:
    our system design should assume that the metrics collector could go offline at
    any given moment and make sure that none of the other components are affected
    should this occur in production.'
  prefs: []
  type: TYPE_NORMAL
- en: The frontend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of the **frontend** component is to render a simple, static HTML-based
    user interface that will facilitate the users'' interaction with the project.
    More specifically, the design of the frontend component will enable users to perform
    the following set of functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Directly submit new URLs for indexing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type a keyword or phrase-based search query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paginate the search results for a particular query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is important to note that, in our current design, the frontend component
    serves as the entry-point for making our project accessible by the outside world! Given
    that *none* of the other project components can be directly accessed by the end
    users, we could argue that the frontend also doubles as an *API gateway*, where
    each incoming API request is mapped to *one or more* calls to the internal system
    components. Besides the obvious security benefits of isolating our internal components
    from the rest of the world, the API gateway pattern provides the following set
    of additional benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: If some of the internal calls need to be asynchronous, the gateway can execute
    them in parallel and wait for them to complete before aggregating their responses
    and returning them to the user in a synchronous manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It enables us to decouple the way that our internal components communicate with
    each other from the mechanism that the outside world uses to interface with our
    system. This means that we can expose a RESTful API to the outside world while
    still retaining the flexibility to select the most suitable transport for each
    internal component (for example, REST, gRPC, or perhaps a message queue).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monolith or microservices? The ultimate question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before commencing development of the Links 'R' Us service, we need to decide
    whether our system components will be developed as parts of a big, monolithic
    service or whether we will just bite the bullet and implement a service-oriented
    architecture right from the start.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the concept of using microservices does indeed seem enticing from the
    outside, it comes with a lot of operational overhead. Besides the mental effort
    required for building and wiring all components together, we would additionally
    need to worry about questions like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How does each service get deployed? Are we doing rolling deployments? What about
    dark or test releases? How easy is it to roll back to a previous deployment when
    something goes wrong?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we going to use a container orchestration layer such as Kubernetes ^([6])?
    How does traffic get routed between services? Do we need to use a service mesh
    such as Istio ^([4]) or Linkerd ^([7])?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we monitor the health of our services? Furthermore, how can we collect
    the logs from all our services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are we going to handle service downtime? Do we need to implement circuit-breakers
    to prevent a problematic service from breaking upstream services that depend on
    it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sure, we are all aware of the shortcomings of monolithic designs but, on the
    other hand, we don't have any available data to justify the extra cost of splitting
    components into microservices from the start of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Weighing the pros and cons of each approach, it looks like the best course of
    action is to follow a hybrid approach! We will initially develop our components
    using a monolithic design. However, and this is the twist, each component will
    define an interface that other components will use to communicate with it.
  prefs: []
  type: TYPE_NORMAL
- en: To connect components without introducing any coupling between their concrete
    implementations, we will be making use of the *proxy* design pattern. Initially,
    we will be providing dummy proxy implementations that facilitate inter-component
    communication within the *same process*. This is, of course, functionally equivalent
    to directly wiring components together as we would normally do in a monolithic
    design.
  prefs: []
  type: TYPE_NORMAL
- en: As our system grows and evolves, we will eventually reach a point where we need
    to extract one or more components into standalone services. Using the preceding
    pattern, all we need to do is update our proxies to use the appropriate transport
    (for example, REST, gRPC, and message queues) for connecting components together
    without having to modify any of the existing component implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes the presentation of the Links 'R' Us project. I hope that, by
    this point, you have acquired a general understanding of what we are going to
    be building over the next few chapters. If you find yourself wondering about the
    technical implementation details associated with some of the project components,
    that's perfectly normal. The main purpose of this chapter was to introduce a high-level
    overview of the project. We will analyze the construction of each one of these
    components in *extensive* detail in the pages that follow!
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the concepts and code for the following chapters easier to follow,
    we will be splitting each chapter into two core parts:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first half of each chapter, we will be performing a deep dive into a
    particular technical topic, for example, a survey of popular types of databases
    (relational, NoSQL, and so on), how you can create pipelines in Go, how you can
    run graph operations at scale, what gRPC is and how you can use it, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second half of the chapter, we will be taking the concepts from the first
    half and applying them toward building one or more components of the Links 'R'
    Us project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus our attention on building one of the key
    components for the Links ''R'' Us project: a fully-functioning data persistence
    layer for storing the links discovered by the crawler and indexing the contents
    of each web page retrieved by the crawler.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between a functional and a non-functional requirement?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the main components of a user story.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What things could possibly go wrong in the Links 'R' Us scenario if we blindly
    crawl any link that a user submits to the system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the key components of an SLO.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of a UML component diagram?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon Elastic Compute Cloud: *Instance Metadata and User Data*: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apache Mesos: Program against your data center like it''s a single pool of
    resources: [https://mesos.apache.org](https://mesos.apache.org)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Brooks, Frederick P., Jr.: *The Mythical Man-Month (Anniversary Ed.)*. Boston,
    MA, USA: Addison-Wesley Longman Publishing Co., Inc., 1995 — [https://www.worldcat.org/title/mythical-man-month/oclc/961280727](https://www.worldcat.org/title/mythical-man-month/oclc/961280727)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Istio: *Connect, secure, control, and observe services*: [https://istio.io](https://istio.io)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ivn, Gbor and Grolmusz, Vince: *When the Web Meets the Cell: Using Personalized
    PageRank for Analyzing Protein Interaction Networks*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kubernetes: *Production-Grade Container Orchestration*: [https://kubernetes.io](https://kubernetes.io)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linkerd: *Ultralight service mesh for Kubernetes and beyond*: [https://linkerd.io](https://linkerd.io)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'MASSCAN: Mass IP port scanner; reserved IP exclusion list: [https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf](https://github.com/robertdavidgraham/masscan/blob/master/data/exclude.conf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Page, L.; Brin, S.; Motwani, R.; and Winograd, T.: *The PageRank Citation Ranking:
    Bringing Order to the Web*. In: Proceedings of the 7th International World Wide
    Web Conference. Brisbane, Australia, 1998, S. 161–172'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pop, Florin ; Dobre, Ciprian: *An Efficient PageRank Approach for Urban Traffic
    Optimization*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Swarm: *a Docker-native clustering system*: [https://github.com/docker/swarm](https://github.com/docker/swarm)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
