<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Persistence Layer</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"Database schemas are notoriously volatile, extremely concrete, and highly depended on. This is one reason why the interface between OO applications and databases is so difficult to manage, and why schema updates are generally painful."</div>
<div class="packt_quote CDPAlignRight CDPAlign">- Robert C. Martin <sup>[14]</sup></div>
<p>In this chapter, we will focus our attention on designing and implementing the data access layers for two of the Links 'R' Us components: the link graph and the text indexer. More specifically, in the pages that follow, we will do the following:</p>
<ul>
<li>Discuss and compare the different types of database technologies</li>
<li>Identify and understand the main reasons that necessitate the creation of a data access layer as an abstraction over the underlying database layer</li>
<li>Analyze the entities, relations, and query requirements for the link graph component, define a Go interface for the data layer, and build two alternative data layer implementations from scratch: a simple, in-memory store that we can use for testing purposes and a production-ready store backed by CockroachDB </li>
<li>Come up with a document model for indexing and searching web page contents and implement both an in-memory indexer (based on the popular bleve Go package) as well as a horizontally scalable variant based on Elasticsearch</li>
<li>Outline strategies for creating test suites that can be shared and reused across different data layer implementations</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The full code for the topics that will be discussed in this chapter have been published in this book's GitHub repository under the<span> </span><kbd>Chapter06</kbd> folder.</p>
<p class="mce-root"/>
<div class="packt_infobox">You can access this book's GitHub repository at <a href="https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang">https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang</a>.</div>
<p>To get you up and running as quickly as possible, each example project includes a<span> makefile</span><span> </span><span>that defines the following set of targets:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Makefile target</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td><kbd>deps</kbd></td>
<td>Install any required dependencies.</td>
</tr>
<tr class="even">
<td><kbd>test</kbd></td>
<td>Run all tests and report coverage.</td>
</tr>
<tr class="odd">
<td><kbd>lint</kbd></td>
<td>Check for lint errors.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As with all the other chapters in this book, you will need a fairly recent version of Go, which you can download from <a href="https://golang.org/dl">https://golang.org/dl</a><em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running tests that require CockroachDB</h1>
                </header>
            
            <article>
                
<p>To run the link graph tests that use CockroachDB as a backend, you will need to download a recent version of CockroachDB (v19.1.2 or newer) from <a href="https://www.cockroachlabs.com/get-cockroachdb">https://www.cockroachlabs.com/get-cockroachdb</a>.</p>
<p>After downloading and unpacking the CockroachDB archive, you can spin up a CockroachDB instance for your tests by changing to the folder where the archive was extracted and run the following set of commands:</p>
<pre>cockroach start --insecure --advertise-addr 127.0.0.1:26257.<br/>cockroach sql --insecure -e 'CREATE DATABASE linkgraph;'</pre>
<p>The link graph tests for the CockroachDB backend examine the contents of the <kbd>CDB_DSN</kbd> environment variable by looking for a valid <strong>data source name</strong> (<strong>DSN</strong>) for accessing the CockroachDB instance. If the environment variable is empty or not defined, all the CockroachDB tests will be automatically skipped.</p>
<p>Assuming you followed the preceding instructions to start a local CockroachDB instance, you can execute the following command to define a suitable DSN prior to running the CockroachDB test suite:<span> </span></p>
<pre>export CDB_DSN='postgresql://root@localhost:26257/linkgraph?sslmode=disable'</pre>
<p class="mce-root"/>
<p>Finally, it is important to note that all the tests operate under the assumption that the database schema has been set up in advance. If you have just created the database, you can apply the required set of DB migrations by switching to your local checked-out copy of this book's source code repository and running <kbd>make run-cdb-migrations</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running tests that require Elasticsearch</h1>
                </header>
            
            <article>
                
<p><span>To run the link graph tests that use Elasticsearch as a backend, you will need to download a recent version of Elasticsearch (v7.2.0 or newer) from </span><a href="https://www.elastic.co/downloads/elasticsearch">https://www.elastic.co/downloads/elasticsearch</a><em>.</em></p>
<p>After downloading and unpacking the Elasticsearch archive, you can change to the location of the extracted files and start a local Elasticsearch instance (with a sane list of default configuration options) by running the following command: </p>
<pre>bin/elasticsearch</pre>
<p>The Elasticsearch tests obtain the list of Elasticsearch cluster endpoints to connect to by examining the contents of the <kbd>ES_NODES</kbd><span> environment variable.</span> <span>Assuming that you have started a local Elasticsearch instance by following the instructions above, you can define <kbd>ES_NODES</kbd> as follows</span>:<span> </span></p>
<pre>export ES_NODES='http://localhost:9200'</pre>
<p>As we will see in the following sections, the Elasticsearch indexer will be designed in a way that will allow the store to automatically define the schema for the indexed documents once it successfully establishes a connection to the Elasticsearch cluster. Consequently, there is no need for a separate migration step prior to running the Elasticsearch test suite.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring a taxonomy of database systems</h1>
                </header>
            
            <article>
                
<p>In the following sections, we will be presenting a list of the most popular DB technologies and analyze the pros and cons of each one. Based on our analysis, we will select the most appropriate type of database for implementing the link graph and the text indexer components of Links 'R' Us.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Key-value stores</h1>
                </header>
            
            <article>
                
<p>The first type of database technology that we will be examining is a key-value store. As the name implies, a key-value store database persists data as a collection of key-value pairs, where keys serve as unique identifiers for accessing stored data within a particular collection. By this definition, key-value stores are functionally equivalent to a hashmap data structure. Popular key-value store implementations include memcached<span> </span><sup><span class="citation">[15]</span></sup>, AWS DynamoDB<span> </span><sup><span class="citation">[8]</span></sup>, LevelDB<span> </span><span class="citation"><sup>[13]</sup>,</span><span> </span>and SSD-optimized RocksDB<span> </span><sup><span class="citation">[20]</span></sup>.</p>
<p>The basic set of operations supported by key-value stores are<span> </span><em>insertions</em>, <em>deletions</em>, and <em>lookups</em>. However, some popular key-value store implementations also provide support for<span> </span><em>range queries</em>, which allow clients to iterate an<span> </span><em>ordered</em><span> </span>list of key-value pairs between two particular keys. As far as keys and values are concerned, the majority of key-value store implementations do not enforce any constraints on their contents. This means that any kind of data (for example, strings, integers, or even binary blobs) can be used as a key.</p>
<p>The data access patterns that are used by key-value stores make data partitioning across multiple nodes much easier compared to other database technologies. This property allows key-value stores to scale horizontally so as to accommodate increased traffic demand.</p>
<p>Let's examine some common use cases where key-value stores are generally considered to be a great fit:</p>
<ul>
<li>Caches! We can use a key-value store as a general-purpose cache for all sorts of things. We could, for instance, cache web pages for a CDN service or store the results of frequently used database queries to reduce the response time for a web application.</li>
<li>A distributed store for session data: Imagine for a moment that we operate a high-traffic website. To handle the traffic, we would normally spin up a bunch of backend servers and place them behind a load balancer. Unless our load balancer had built-in support for sticky sessions (always sending requests from the same user to the same backend server), each request would be handled by a different backend server. This could cause issues with stateful applications as they require access to the session data associated with each user. If we tagged each user request with a unique per-user ID, we could use that as a key and retrieve the session data from a key-value store.</li>
<li>A storage layer for a database system. The properties of key-value stores make them a very attractive low-level primitive for implementing more sophisticated types of databases. For example, relational databases such as CockroachDB<span> </span><sup><span class="citation">[5]</span></sup><span> </span>and NoSQL databases such as Apache Cassandra<span> </span><sup><span class="citation">[2]</span></sup><span> </span>are prime examples of systems built on top of key-value stores.</li>
</ul>
<p class="mce-root"/>
<p>The main caveat of key-value stores is that we cannot efficiently search <em>within</em> the stored data without introducing some kind of auxiliary data structure to facilitate the role of an index.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relational databases</h1>
                </header>
            
            <article>
                
<p>The idea of relational databases was introduced by E. F. Codd in 1970<span> </span><sup><span class="citation">[6]</span></sup>. The main unit of data organization in a relational database is referred to as a<span> </span><strong>table</strong>. Each table is associated with a<span> </span><strong>schema</strong><span> </span>that defines the names and data types for each table <strong>column</strong>.</p>
<p>Within a table, each data record is represented by a<span> </span><strong>row</strong><span> </span>that is, in turn, identified by a<span> </span><strong>primary key</strong>, a tuple of column values that must be <em>unique</em> among all the table rows. Table columns may also reference records that exist in other tables. This type of column is typically referred to as a<span> </span><strong>foreign key</strong>.</p>
<p>The standardized way to access and query relational databases is via the use of an <em>English-like</em> <strong>structured query language</strong> (<strong>SQL</strong>), which is actually a subset of various domain-specific languages:</p>
<ul>
<li>A data<span> </span><em>definition</em><span> </span>language, which includes commands for managing the database schema; for example, creating, altering, or dropping tables, indexes, and constraints</li>
<li>A data<span> </span><em>manipulation</em><span> </span>language, which supports a versatile set of commands for inserting, deleting, and, of course, querying the database contents</li>
<li>A data<span> </span><em>control</em><span> </span>language, which provides a streamlined way to control the level of access that individual users have to the database</li>
<li>A<span> </span><em>transaction control</em><span> </span>language, which allows database users to start, commit, or abort database transactions</li>
</ul>
<p>One of the most important features of relational databases is the concept of transactions. A transaction can be thought of as a wrapper around a sequence of SQL statements that ensures that either<span> </span><em>all</em><span> </span>of them will be applied or<span> </span><em>none</em><span> </span>of them will be applied. To ensure that transactions work reliably in the presence of errors or faults (for example, loss of power or network connectivity) <em>and</em> that their outcomes are always deterministic when multiple transactions execute concurrently, relational databases must be compliant with a set of properties that are commonly referred to with the acronym<span> </span>ACID. Let's go over what <strong>ACID</strong> stands for:</p>
<ul>
<li><strong>Atomicity</strong>: Transactions are applied completely or not at all.</li>
<li><strong>Consistency</strong>: The contents of a transaction is not allowed to bring the database into an invalid state. This means that the database system must validate each of the statements included in a transaction against the constraints (for example, primary, foreign, or unique keys) that have been defined on the tables that are about to be modified.</li>
<li><strong>Isolation</strong>: Each transaction must execute in total isolation from other transactions. If multiple transactions are executing concurrently, the end result should be equivalent to running each transaction one after the other.</li>
<li><strong>Durability</strong>: Once a transaction has been committed, it will remain committed, even if the database system is restarted or the nodes it runs on experience loss of power.</li>
</ul>
<p>In terms of performance, relational databases such as PostgreSQL<span> </span><sup><span class="citation">[18]</span></sup><span> </span>and MySQL<span> </span><sup><span class="citation">[17]</span></sup><span> </span>are generally easy to scale vertically. S<span>witching to</span><span> a beefier CPU and/or adding more memory to your database server is more or less a standard operating procedure for increasing the <strong>queries per second</strong> (<strong>QPS</strong>) or <strong>transactions per second</strong> (<strong>TPS</strong>) that the DB can handle. On the other hand, scaling relational databases horizontally is much harder and typically depends on the type of workload you have.</span></p>
<p>For<span> </span><em>write-heavy</em><span> </span>workloads, we usually resort to techniques such as data sharding. Data sharding allows us to split (partition) the contents of one or more tables into multiple database nodes. This partitioning is achieved by means of a per-row<span> </span><strong>shard key</strong><span>, </span>which dictates which node is responsible for storing each row of the table. One caveat of this approach is that it introduces additional complexity at query time. While writes are quite efficient, reads are not trivial as the database might need to query <em>each</em> individual node and then aggregate the results together in order to answer even a simple query such as<span> </span><kbd>SELECT COUNT(*) FROM X</kbd>.</p>
<p>On the other hand, if our workloads are<span> </span><em>read-heavy</em>, horizontal scaling is usually achieved by spinning up<span> </span><em>read-replicas</em><span>, </span>which mirror updates to one or more<span> </span><em>primary</em><span> </span>nodes. Writes are always routed to the primary nodes while reads are handled by the read-replicas (ideally) or even by the primaries if the read-replicas cannot be reached.</p>
<p>While relational databases are a great fit for transactional workloads and complex queries, they are not the best tool for querying hierarchical data with arbitrary nesting or for modeling graph-like structures. Moreover, as the volume of stored data exceeds a particular threshold, queries take increasingly longer to run. Eventually, a point is reached where reporting queries that used to execute in real-time can only be processed as offline batch jobs. As a result, companies with high-volume data processing needs have been gradually shifting their focus toward NoSQL databases.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NoSQL databases</h1>
                </header>
            
            <article>
                
<p>NoSQL databases have met a sharp rise in popularity over the last couple of years. Their key value propositions are as follows:</p>
<ul>
<li>They are well suited for crunching massive volumes of data.</li>
<li>By design, NoSQL database systems can effortlessly scale both vertically and horizontally. As a matter of fact, most NoSQL database systems promise a linear increase in performance as more nodes are added to the database cluster.</li>
<li>More advanced NoSQL solutions can scale even across data centers and include support for automatically routing client requests to the nearest data center.</li>
</ul>
<p>However, as we all know, there is no such thing as a free lunch. To achieve this performance boost, NoSQL databases have to sacrifice something! Being distributed systems, NoSQL databases must adhere to the rules of the<span> </span><em>CAP theorem</em>.</p>
<div class="packt_infobox">The CAP theorem was proposed by Eric Brewer in 2000<span> </span><sup><span class="citation">[4]</span></sup><span> </span>and is one of the fundamental theorems that governs the operation of distributed systems. It states that networked shared data systems can only guarantee up to<span> </span><em>two</em><span> </span>of the following properties:
<ul>
<li><strong>Consistency</strong>: Each node in the system has the same view of the stored data. This implies that each read operation on a piece of data will always return the value of the last performed write.</li>
<li><strong>Availability</strong>: The system can still process read and write requests in a reasonable amount of time, even if some of the nodes are not online.</li>
<li><strong>Partition tolerance</strong>: If a network split occurs, some of the cluster nodes will become isolated and therefore unable to exchange messages with the remaining nodes in the cluster. However, the system should remain operational and the cluster should be able to reach a consistent state when the partitioned nodes rejoin the cluster.</li>
</ul>
</div>
<p>As shown in the following diagram, if we were to pair together two of the three fundamental properties of the CAP theorem, we can obtain a couple of interesting distributed system configurations:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4a225d36-b48f-4b7d-96e7-cc11680bfd0a.png" style="width:34.33em;height:33.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 1: The intersection of the three properties of the CAP theorem</div>
<p>Let's briefly analyze the behavior as to how each of these configurations reacts in the presence of errors:</p>
<ul>
<li><strong>Consistency <span>–</span> Partition (CP) tolerance</strong>: Distributed systems in this category typically use a voting protocol to ensure that the majority of nodes agree that they have the most recent version of the stored data; in other words, they reach a<span> </span><em>quorum</em>. This allows the system to recover from network partitioning events. However, if not enough nodes are available to reach quorum, the system will return an error to clients as data consistency is preferred over availability.</li>
<li><strong>Availability <span>–</span> Partition (AP) tolerance</strong>: This class of distributed systems favors availability over consistency. Even in the case of a network partition, an AP system will try to process read requests, although<span> </span><em>stale</em><span> </span>data may be returned to the clients.</li>
<li><strong>Consistency <span>–</span> Availability (CA)</strong>: In practice,<span> </span><em>all</em><span> </span>distributed systems are, to some extent, affected by network partitions. Therefore, a pure CA type of system is not really feasible unless, of course, we are talking about a single-node system. We could probably classify a single-node deployment of a traditional relational database as a CA system. </li>
</ul>
<p>At the end of the day, the choice of an appropriate NoSQL solution largely depends on your particular use case. What happens, though, if the use case requires all three of these properties? Are we simply out of luck?</p>
<p>Fortunately, over the years, several NoSQL solutions (for example, Cassandra<span> </span><sup><span class="citation">[2]</span></sup>) have evolved support for what is now referred to as<span> </span><strong>tunable consistency</strong>. Tunable consistency allows clients to specify their desired level of consistency on a<span> </span><em>per-query</em><span> </span>basis. For example, when creating a new user account, we would typically opt for strong consistency semantics. On the other hand, when querying the number of views of a popular video, we could dial down the desired level of consistency and settle for an approximate, eventually-consistent, value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document databases</h1>
                </header>
            
            <article>
                
<p>Document databases are specialized NoSQL databases that store, index, and query complex and possibly deeply nested <em>document-like</em> objects. All documents are stored within a <em>collection</em>, which is the equivalent of a table in a relational database. The key differentiation that makes document databases unique is that they do not enforce a particular schema (that is, they are schema-less) but rather<span> </span><em>infer</em><span> </span>the schema from the stored data. This design decision allows us to store <em>different</em> types of documents in the <em>same</em> collection. What's more, both the schema and contents of each individual document can evolve over time with no visible impact on the database's query performance.</p>
<p>Contrary to relational databases, which have standardized on SQL, document databases typically implement their own <strong>domain-specific language</strong> (<strong>DSL</strong>) for querying data. However, they also provide advanced primitives (for example, support for map-reduce) for calculating complex aggregations across multiple documents in a collection. This makes document databases a great fit for generating <strong>business intelligence</strong> (<strong>BI</strong>) and other types of analytics reports.</p>
<p class="mce-root"/>
<p>The list of document database systems is quite long, so I will just be listing some of the more popular (in my view) implementations: MongoDB<span> </span><sup><span class="citation">[16]</span></sup>, CouchDB<span> </span><span class="citation"><sup>[3]</sup>,</span><span> </span>and Elasticsearch<span> </span><sup><span class="citation">[9]</span></sup>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the need for a data layer abstraction</h1>
                </header>
            
            <article>
                
<p>Before we delve deeper into modeling the data layer for the link graph and text indexer components, we need to spend some time discussing the reasoning behind the introduction of a data layer abstraction.</p>
<p>First and foremost, the primary purpose of the data layer is to decouple our code from the underlying data store implementation. By programming against a well-defined and data store-agnostic interface, we ensure that our code remains clean, modular, and totally oblivious to the nuances of accessing each data store.</p>
<p>An extra benefit of this approach is that it offers us the flexibility to A/B test different data store technologies before we decide which one to use for our production systems. What's more, even if our original decision proves to be less than stellar in the long term (for example, service traffic exceeds the store's capability to scale vertically/horizontally), we can easily switch to a different system. This can be achieved by wiring in a new data store adapter implementation without the need to modify any of the higher levels of our services' implementation.</p>
<p>The final advantage of having such an abstraction layer has to do with<span> </span><em>testing</em>. By providing individual Go packages for each data store that we are interested in supporting, we can not only encapsulate the store-specific logic but can also write comprehensive test suites to test each store's behavior in total isolation from the rest of the code base. Once we are confident that the implementation behaves as expected, we can use any of the testing mechanisms (for example, mocks, stubs, and fake objects) that we outlined in <a href="d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml">Chapter 4</a>, <em>The Art of Testing</em>, to test other high-level components that require access to a data store without actually having to provision a real data store instance.</p>
<p>Initially, this might not seem to be a big benefit. However, for larger Go projects that spawn multiple packages, the cost of setting up, populating with fixtures, and finally cleaning a database<span> </span><em>between tests</em> can be quite high. Compared to using an in-memory data store implementation, tests against a real database not only take more time to run but may also prove to be quite flaky.</p>
<p class="mce-root"/>
<p>One common problem that you may have encountered in the past is potential DB access races for tests that belong to<span> </span><em>different packages</em><span> </span>but try to access and/or populate the<span> </span><em>same</em><span> </span>database instance concurrently. As a result, some of the DB-related tests may start randomly failing in a non-deterministic manner. And of course, by virtue of Murphy's law, such problems rarely crop up when testing locally, but rather have the tendency to manifest themselves when the continuous integration system runs the tests for the pull request you just submitted for review!</p>
<p>It is pretty easy to end up in such a messy situation if multiple packages from your code base have a strong coupling to the underlying database due to the fact that the<span> </span><kbd>go test</kbd><span> command </span>will, <em>by default</em>, run tests that belong to different packages <em>concurrently</em>. As a temporary workaround, you could force<span> </span><kbd>go test</kbd><span> </span>to serialize the execution of <em>all</em> the tests by providing the<span> </span><kbd>-parallel 1</kbd><span> </span>command-line flag. However, that option would severely increase the total execution time for your test suites and would be overkill for larger projects. Encapsulating the tests that require a real DB store instance into a single package and using mocks everywhere else is a clean and elegant solution for mitigating such problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing the data layer for the link graph component</h1>
                </header>
            
            <article>
                
<p>In the following sections, we will perform an extended analysis of the data models that are required for the operation of the link graph component. We will kick off our analysis by creating an <strong>Entity-Relationship</strong> (<strong>ER</strong>) diagram for the entities that compose the data access layer. Then, we will define an interface that fully describes the set of operations that the data access layer must support.</p>
<p>Finally,<span> we will design and build two alternative data access layer implementations (in-memory and CockroachDB-backed) that both satisfy the aforementioned interface. </span>To ensure that both implementations behave in exactly the same manner, we will also create a comprehensive, store-agnostic test suite and arrange for our test code to invoke it for each individual store implementation.</p>
<p>All the code that we will be discussing in the following sections can be found in the<span> </span><kbd>Chapter06/linkgraph</kbd><span> </span>folder in this book's GitHub repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an ER diagram for the link graph store</h1>
                </header>
            
            <article>
                
<p>The following diagram presents the ER diagram for the link graph data access layer. Given that the crawler retrieves web page links and discovers connections between websites, it makes sense for us to use a graph-based representation for our system modeling. As you can see, the ER diagram is comprised of two models:<span> </span><strong>Link</strong><span> </span>and<span> </span><strong>Edge</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/90c94c2c-00ed-4f53-9acd-2da303a48848.png" style="width:37.67em;height:9.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2:</span><span> </span>The ER diagram for the link graph component</div>
<p><span>L</span>ink<span> model instances </span>represent the set of web pages that have been processed or discovered by the crawler component. Its attribute set consists of an ID value for uniquely identifying each link, the URL associated with it, and a timestamp value indicating when it was last retrieved by the crawler. The preceding list constitutes the <em>bare minimum</em> set of attributes that are required for modeling the link graph for the Links 'R' Us project. In a real-world implementation, we would probably want to augment our link model with additional metadata, such as the following:</p>
<ul>
<li>The MIME type for the URL content (as indicated by the remote server) and its length in bytes.</li>
<li>The HTTP status code of the last crawl attempt. This is quite useful for retrying failed attempts or for dropping dead links from our graph.</li>
<li>A preferred (per-domain or per-link) <span>time window </span>for performing future crawl requests. As web crawlers tend to induce significant traffic spikes when fetching links from remote servers, this information can be used by our crawler to schedule its update cycle at off-peak times and thus minimize its impact on remote servers.</li>
</ul>
<p>Each<span> web page in the graph may contain <em>zero or more</em></span><span> </span>outgoing<span> </span><span>links to other web pages</span>. An Edge model instance<span> </span><span>represents a </span><strong>uni-directional</strong><span> connection between two</span><span> l</span>inks in the graph<span>. As shown in the preceding diagram, </span>the attribute set for the Edge model includes a unique ID for the edge itself, as well as the IDs of both the source and destination links. This modeling approach can also support <strong>bi-directional</strong> links (also known as backlinks) between web pages, with the minor caveat that they would need to be represented as two separate edge entries.</p>
<p class="mce-root"/>
<p>Moreover, the edge attribute set also contains a timestamp value that tracks the last time that the edge was visited by the crawler. A common challenge with graphs such as the WWW, whose structure changes at a very fast rate, is figuring out how to efficiently detect edge-related changes: new edges may appear and others may disappear at any point in time. Handling edge additions is a trivial task; all we need to do is <strong>upsert</strong> (insert or update if the entry already exists) an Edge model instance for every outgoing edge that's detected by the crawler. Handling edge <em>deletions, on the other hand,</em> is slightly more complicated.</p>
<p>The approach that we will be adopting for the crawler component will leverage the last update timestamp as the means of detecting whether an existing edge is <em>stale</em> and needs to be removed. Each time the crawler processes a link from the graph, it will perform the following actions:</p>
<ol>
<li> Upsert a Link model entry for each outgoing link.</li>
<li> Upsert an Edge model for each unique outgoing link, where we have the following:
<ul>
<li>The <kbd>origin</kbd> is always set to the link that is currently being processed.</li>
<li>The <kbd>destination</kbd> is each detected outgoing link.</li>
<li>The <kbd>updatedAt</kbd> timestamp is the current system time.</li>
</ul>
</li>
</ol>
<p>By following these steps, any links with the same<span> </span><kbd>(source, destination)</kbd><span> </span>tuple will have their<span> </span><kbd>UpdatedAt</kbd><span> </span>field refreshed while stale, old links will retain their previous<span> </span><kbd>UpdatedAt</kbd><span> </span>value. If we arrange for the crawler to record the exact time when it started crawling a particular page, we can simply delete all the edges whose<span> </span><em>source</em><span> </span>is the link that was just crawled and whose<span> </span><kbd>UpdatedAt</kbd><span> </span>value is older than the recorded timestamp.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Listing the required set of operations for the data access layer</h1>
                </header>
            
            <article>
                
<p>Following the SOLID design principles we discussed in the previous chapters, we will start designing the link graph data access layer by listing the operations (responsibilities, in SOLID terminology) that it needs to perform and then formally describe them by means of a Go interface.</p>
<p>For our particular use case, the link graph access layer must support the following set of operations:</p>
<ol type="1">
<li>Insert a link into the graph or update an existing link when the crawler discovers that its content has changed.</li>
</ol>
<ol start="2" type="1">
<li>Look up a link by its ID.</li>
<li>Iterate <em>all the links</em> present in the graph. This is the primary service that the link graph component must provide to the other components (for example, the crawler and <kbd>PageRank</kbd> calculator) that comprise the Links 'R' Us project.</li>
<li>Insert an edge into the graph or refresh the <kbd>UpdatedAt</kbd> value of an existing edge.</li>
<li>Iterate the list of edges in the graph. This functionality is required by the <kbd>PageRank</kbd> calculator component.</li>
<li>Delete stale links that originated from a particular link and were not updated during the last crawler pass.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining a Go interface for the link graph</h1>
                </header>
            
            <article>
                
<p>To satisfy the list of operations from the previous section, we shall define the<span> </span><kbd>Graph</kbd><span> </span>interface as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Graph <span class="kw">interface</span> {</a>
<a>    UpsertLink(link *Link) <span class="dt">error</span></a>
<a>    FindLink(id uuid.UUID) (*Link, <span class="dt">error</span>)</a>

<a>    UpsertEdge(edge *Edge) <span class="dt">error</span></a>
<a>    RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) <span class="dt">error</span></a>

<a>    Links(fromID, toID uuid.UUID, retrievedBefore time.Time) (LinkIterator, <span class="dt">error</span>)</a>
<a>    Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (EdgeIterator, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>The first two methods allow us to upsert a<span> </span><kbd>Link</kbd><span> </span>model and retrieve it from the backing store if we are aware of its ID. In the following code, you can see the definition of the<span> </span><kbd>Link</kbd><span> </span>type, whose fields match the ones from the ER diagram:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Link <span class="kw">struct</span> {<br/></a><a>    ID          uuid.UUID</a>
<a>    URL         <span class="dt">string<br/></span></a><a>    RetrievedAt time.Time</a>
<a>}</a></pre></div>
<p>Each link is assigned a unique ID (a V4 UUID, to be precise) and contains two fields: the URL for accessing the web page and a timestamp field that keeps track of the last time that the link's content was retrieved by the crawler.</p>
<p class="mce-root"/>
<p>The next two methods from the<span> </span><kbd>Graph</kbd><span> </span>interface allow us to manipulate the edges of the graph. Let's begin by examining the definition of the<span> </span><kbd>Edge</kbd><span> </span>type:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Edge <span class="kw">struct</span> {</a>
<a>    ID        uuid.UUID</a>
<a>    Src       uuid.UUID</a>
<a>    Dst       uuid.UUID</a>
<a>    UpdatedAt time.Time</a>
<a>}</a></pre></div>
<p>Similar to links, edges are also assigned their own unique ID (also a V4 UUID). In addition, the<span> </span><kbd>Edge</kbd><span> </span>model tracks the following:</p>
<ul>
<li>The ID of both the source and destination links that form the edge</li>
<li>The timestamp when it was last updated</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partitioning links and edges for processing the graph in parallel</h1>
                </header>
            
            <article>
                
<p>As you have probably noticed by their signatures, the<span> </span><kbd>Links</kbd><span> </span>and<span> </span><kbd>Edges</kbd><span> </span>methods are designed to return an<span> </span><em>iterator</em><span> so that they can access</span> a filtered subset of the graph's vertices and edges. More specifically, they do the following:</p>
<ul>
<li>The<span> </span><kbd>Links</kbd><span> </span>method returns a set of links whose ID belongs to the<span> </span><kbd>[fromID, toID)</kbd><span> </span>range<span> </span><em>and</em><span> </span>their last retrieval time before the provided timestamp.</li>
<li>The<span> </span><kbd>Edges</kbd><span> </span>method returns the set of edges whose<span> </span><em>origin vertex IDs</em><span> </span>belong to the<span> </span><kbd>[fromID, toID)</kbd><span> </span>range and their last update time is before the provided timestamp.</li>
</ul>
<p>At this point, we need to spend some time and elaborate on the reasoning behind the design of these methods. We could argue that, at some point, the link graph will grow large enough so that in order to process it in an efficient manner, we will eventually have to split it into chunks and process each chunk in parallel. To this end, our design must anticipate this need and include a mechanism for grouping links and edges into partitions based on their individual IDs. <span>Given a </span><kbd>[fromID, toID)</kbd><span> range, all graph implementations will use the following logic to select which link and edge model instances to return via the iterator:</span></p>
<ul>
<li>Return links whose ID is within the <kbd>[fromID, toID)</kbd> range.</li>
<li>Return edges for which the <em>origin link's ID</em> is <span>within the </span><kbd>[fromID, toID)</kbd><span> range. In other words, edges always belong to the same partition as their origin links.</span></li>
</ul>
<p class="mce-root"/>
<p>It is important to note that while the preceding method signatures accept a UUID range as their input, the implementation of a suitable partitioning scheme for calculating the UUID ranges themselves will be the<em> responsibility of the caller</em>. The <kbd>Links</kbd> and <kbd>Edges</kbd> methods will happily accept any UUID range that's provided by the caller as long as it is valid.</p>
<p>In <a href="bd9d530b-f50e-4b81-a6c1-95b31e79b8c6.xhtml">Chapter 10</a>, <em>Building, Packaging, and Deploying Software</em>, we will explore the use of the<span> </span><kbd>math/big</kbd><span> </span>package to facilitate the carving of the UUID space into non-overlapping regions that can then be fed into the aforementioned store methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating Links and Edges</h1>
                </header>
            
            <article>
                
<p>Since there is no upper bound in the number of links or edges that can be potentially returned by calls to the<span> </span><kbd>Links</kbd><span> </span>and<span> </span><kbd>Edges</kbd><span> </span>methods, we will be implementing the <em>iterator</em> design<em> </em>pattern and lazily fetch Link and Edge models on demand. The<span> </span><kbd>LinkIterator</kbd><span> </span>and<span> </span><kbd>EdgeIterator</kbd><span> </span>types, which are returned by these methods, are interfaces themselves. This is intentional as their internal implementation details will obviously depend on the database technology that we select for the link graph persistence layer. Here is how they are defined:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// LinkIterator is implemented by objects that can iterate the graph links.</span></a>
<a><span class="kw">type</span> LinkIterator <span class="kw">interface</span> {</a>
<a>    Iterator</a>

<a>    <span class="co">// Link returns the currently fetched link object.</span></a>
<a>    Link() *Link</a>
<a>}</a>

<a><span class="co">// EdgeIterator is implemented by objects that can iterate the graph edges.</span></a>
<a><span class="kw">type</span> EdgeIterator <span class="kw">interface</span> {</a>
<a>    Iterator</a>

<a>    <span class="co">// Edge returns the currently fetched edge objects.</span></a>
<a>    Edge() *Edge</a>
<a>}</a></pre></div>
<p>Both of the preceding interfaces define a<span> </span><em>getter</em><span> method </span>for retrieving the<span> </span><kbd>Link</kbd><span> </span>or<span> </span><kbd>Edge</kbd><span> </span>instance that the iterator is currently pointing at. The common logic between the two iterators has been extracted into a separate interface called <kbd>Iterator</kbd>, which both of the interfaces embed. The definition of the <kbd>Iterator</kbd> interface is as follows: </p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Iterator <span class="kw">interface</span> {</a>
<a>    <span class="co">// Next advances the iterator. If no more items are available or an</span></a>
<a>    <span class="co">// error occurs, calls to Next() return false.</span></a>
<a>    Next() <span class="dt">bool</span></a>

<a>    <span class="co">// Error returns the last error encountered by the iterator.</span></a>
<a>    Error() <span class="dt">error</span></a>

<a>    <span class="co">// Close releases any resources associated with an iterator.</span></a>
<a>    Close() <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>To iterate a list of edges or links, we must obtain an iterator from the graph and run our business logic within a <kbd>for</kbd> loop:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// 'linkIt' is a link iterator</span></a>
<a><span class="kw">for</span> linkIt.Next(){</a>
<a>    link := linkIt.Link()</a>
<a>    <span class="co">// Do something with link...</span></a>
<a>}</a>

<a><span class="kw">if</span> err := linkIt.Error(); err != <span class="ot">nil</span> {</a>
<a>    <span class="co">// Handle error...</span></a>
<a>}</a></pre></div>
<p>Calls to<span> </span><kbd>linkIt.Next()</kbd><span> </span>will return false when the following occurs:</p>
<ul>
<li>We have iterated all the available links</li>
<li>An error occurs (for example, we lost connection to the database)</li>
</ul>
<p>As a result, we don't need to check whether an error occurred inside the loop <span>– we </span>only need to check<span> </span><em>once</em><span> </span>after exiting the for loop. This pattern yields cleaner-looking code and is actually used in various places within the Go standard library, such as the <kbd>Scanner</kbd><span> type from the <kbd>bufio</kbd> package.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Verifying graph implementations using a shared test suite</h1>
                </header>
            
            <article>
                
<p>As we mentioned in the previous sections, we will be building both an in-memory and a database-backed implementation of the<span> </span><kbd>Graph</kbd><span> </span>interface. To this end, we need to come up with a set of comprehensive tests to ensure that both implementations behave in exactly the same manner.</p>
<p>One way to achieve this is to write the tests for the first implementation and then duplicate them for each additional implementation that we may introduce in the future. However, this approach doesn't really scale well: what if we modify the<span> </span><kbd>Graph</kbd><span> </span>interface in the future? We would need to track down and update a whole bunch of tests that might be scattered across different packages.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A much better, and cleaner, approach would be to come up with a shared, implementation-agnostic test suite and then just wire it to each underlying graph implementation. I opted for this approach as it reduces the amount of maintenance that's required, while at the same time allowing us to run<span> </span><em>exactly the same set of tests</em><span> </span>against all implementations: a fairly efficient way of detecting regressions when we change one of our implementations.</p>
<p>But if the test suite is shared, where should it live so that we can include it in all implementation-specific test suites? The answer is to encapsulate the suite into its own dedicated testing package that our regular test code can import and use where it's needed.</p>
<p>The<span> </span><kbd>SuiteBase</kbd><span> </span>definition<span> </span>lives<span> </span>in the<span> </span><kbd><span>Chapter06/linkgraph/graph/</span>graphtest</kbd> package and depends on the <kbd>gocheck</kbd><span> </span><sup><span class="citation">[11]</span></sup><span> </span>framework, which we introduced in <a href="d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml">Chapter 4</a>, <em>The Art of Testing</em>. The suite includes the following groups of tests:</p>
<ul>
<li><strong>Link/Edge upsert tests</strong>: These tests are designed to verify that we can insert new edges/links into the graph and that they are assigned a valid, unique ID.</li>
<li><strong>Concurrent link/edge iterator support</strong>: These tests ensure that no data races occur when the code concurrently accesses the graph's contents via multiple iterator instances.</li>
<li><strong>Partitioned iterator tests</strong>: These tests verify that if we split our graph into N partitions and assign an iterator to each partition, each iterator will receive a unique set of links/edges (that is, no item will be listed in more than one partition) and that all the iterators will process the full set of links/edges present in the graph. Additionally, the edge iterator tests ensure that each edge appears in the same partition as its source link.</li>
<li><strong>Link lookup tests</strong>: A simple set of tests that verify the graph implementation's behavior when looking up existing or unknown link IDs.</li>
<li><strong>Stale edge removal tests</strong>: A set of tests that verify that we can successfully delete stale edges from the graph using an<span> </span><em>updated-before-X</em><span> </span>predicate.</li>
</ul>
<p>To create a test suite for a <em>new</em> graph implementation, all we have to do is to define a new test suite that does the following:</p>
<ul>
<li>Embeds<span> </span><kbd>SuiteBase</kbd></li>
<li>Provides a suite setup helper that creates the appropriate graph instance and invokes the<span> </span><kbd>SetGraph</kbd><span> </span>method that's exposed by<span> </span><kbd>SuiteBase</kbd><span> </span>so that we can wire it to the base test suite before running any of the preceding tests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing an in-memory graph store</h1>
                </header>
            
            <article>
                
<p>The in-memory graph implementation will serve as a gentle introduction to writing a complete graph store implementation. By virtue of maintaining the graph in memory, this implementation is simple, self-contained, and safe for concurrent access. This makes it an ideal candidate for writing unit tests that require access to the link graph component.</p>
<p>Let's take a look at its implementation, starting with the definition of the<span> </span><kbd>InMemoryGraph</kbd><span> </span>type:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> edgeList []uuid.UUID</a>

<a><span class="kw">type</span> InMemoryGraph <span class="kw">struct</span> {</a>
<a>    mu sync.RWMutex</a>

<a>    links <span class="kw">map</span>[uuid.UUID]*graph.Link</a>
<a>    edges <span class="kw">map</span>[uuid.UUID]*graph.Edge</a>

<a>    linkURLIndex <span class="kw">map</span>[<span class="dt">string</span>]*graph.Link</a>
<a>    linkEdgeMap  <span class="kw">map</span>[uuid.UUID]edgeList</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>InMemoryGraph</kbd><span> </span>struct defines two maps (<kbd>links</kbd><span> </span>and<span> </span><kbd>edges</kbd>) that maintain the set of<span> </span><kbd>Link</kbd><span> </span>and<span> </span><kbd>Edge</kbd><span> </span>models that have been inserted into the graph. To accelerate ID-based lookups, both maps use the model IDs as their key.</p>
<p>Going back to our ER diagram, we can see that link URLs are also expected to be unique. To this end, the in-memory graph also maintains an auxiliary map (<kbd>linkURLIndex</kbd>) where keys are the URLs that are added to the graph and values are pointers to link models. We will go through the details of how this particular map is used when we examine the implementation of the<span> </span><kbd>UpsertLink</kbd><span> </span>method in the next section.</p>
<p>Another type of query that we should be able to answer <em>efficiently</em> in order to implement the<span> </span><kbd>Edges</kbd><span> </span>and<span> </span><kbd>RemoveStaleEdges</kbd><span> </span>methods is: <em>find the list of edges that originate from a particular link</em>. This is achieved by defining yet another auxiliary map called<span> </span><kbd>linkEdgeMap</kbd>. This map associates link IDs with a slice of edge IDs that correspond to the edges <em>originating</em> from it.</p>
<p>Finally, to ensure that our implementation is safe for concurrent access, the struct definition includes a <kbd>sync.RWMutex</kbd> field. In contrast to the regular <kbd>sync.Mutex</kbd>, which provides single reader/writer semantics, <kbd>sync.RWMutex</kbd> supports <em>multiple concurrent readers</em> and thus provides much better throughput guarantees for <em>read-heavy</em> workloads.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upserting links</h1>
                </header>
            
            <article>
                
<p>Let's begin our tour of the in-memory graph implementation by taking a look at how the<span> </span><kbd>UpsertLink</kbd><span> </span>method is implemented. Since an upsert operation will always modify the graph, the method will acquire a<span> </span><em>write</em><span> </span>lock so that we can apply any modifications in an atomic fashion. The method contains two distinct code paths.</p>
<p>If the link to be upserted does not specify an ID, we treat it as an insert attempt<span> </span><em>unless</em><span> </span>we have<span> </span><em>already</em><span> </span>added another link with the same<span> </span>URL. In the latter case, we silently convert the insert into an <em>update</em> operation while making sure that we always retain the most recent<span> </span><kbd>RetrievedAt</kbd><span> </span>timestamp:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">if</span> link.ID == uuid.Nil {</a>
<a>    link.ID = existing.ID</a>
<a>    origTs := existing.RetrievedAt</a>
<a>    *existing = *link</a>
<a>    <span class="kw">if</span> origTs.After(existing.RetrievedAt) {</a>
<a>        existing.RetrievedAt = origTs</a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a>

<a><span class="co">// Omitted: insert new link into the graph (see next block of code)...</span></a></pre></div>
<p>Once we verify that we need to create a new entry for the link, we must assign a unique ID to it before we can insert it into the graph. This is achieved by means of a small for loop where we keep generating new UUID values until we obtain one that is unique. Since we are using V4 (random) UUIDs for our implementation, we are more or less guaranteed to obtain a unique value on our first attempt. The presence of the for loop guarantees that our code behaves correctly, even in the highly unlikely case of UUID collisions:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Insert new link into the graph</span></a>
<a><span class="co">// Assign new ID and insert link</span></a>
<a><span class="kw">for</span> {</a>
<a>    link.ID = uuid.New()</a>
<a>    <span class="kw">if</span> s.links[link.ID] == <span class="ot">nil</span> {</a>
<a>        <span class="kw">break</span></a>
<a>    }</a>
<a>}</a>

<a>lCopy := <span class="bu">new</span>(graph.Link)</a>
<a>*lCopy = *link</a>
<a>s.linkURLIndex[lCopy.URL] = lCopy</a>
<a>s.links[lCopy.ID] = lCopy</a>
<a><span class="kw">return</span> <span class="ot">nil</span></a></pre></div>
<p class="mce-root"/>
<p>Once we have generated an ID for the link, we can make a<span> </span><em>copy</em><span> </span>of link that's provided by the caller to ensure that no code outside of our implementation can modify the graph data. Then, we insert the link into the appropriate map structures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upserting edges</h1>
                </header>
            
            <article>
                
<p>The edge upsert logic in<span> </span><kbd>UpsertEdge</kbd><span> </span>has a lot of things in common with the<span> </span><kbd>UpsertLink</kbd><span> </span>implementation we examined in the previous section. The first thing we need to do is acquire the write lock and verify that the source and destination links for the edge actually exist:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>s.mu.Lock()</a>
<a><span class="kw">defer</span> s.mu.Unlock()</a>

<a>_, srcExists := s.links[edge.Src]</a>
<a>_, dstExists := s.links[edge.Dst]</a>
<a><span class="kw">if</span> !srcExists || !dstExists {</a>
<a>    <span class="kw">return</span> xerrors.Errorf(<span class="st">"upsert edge: %w"</span>, graph.ErrUnknownEdgeLinks)</a>
<a>}</a></pre></div>
<p>Next, we scan the set of edges that originate from the specified source link and check whether we can find an<span> </span><em>existing</em><span> </span>edge to the same destination. If that happens to be the case, we simply update the entry's<span> </span><kbd>UpdatedAt</kbd><span> </span>field and copy its contents back to the provided<span> </span><kbd>edge</kbd><span> </span>pointer. This ensures that the<span> </span><kbd>entry</kbd><span> </span>value that's provided by the caller has both its<span> </span><kbd>ID</kbd><span> </span>and<span> </span><kbd>UpdatedAt</kbd><span> </span>synced with the values contained in the store:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Scan edge list from source</span></a>
<a><span class="kw">for</span> _, edgeID := <span class="kw">range</span> s.linkEdgeMap[edge.Src] {</a>
<a>    existingEdge := s.edges[edgeID]</a>
<a>    <span class="kw">if</span> existingEdge.Src == edge.Src &amp;&amp; existingEdge.Dst == edge.Dst {</a>
<a>        existingEdge.UpdatedAt = time.Now()</a>
<a>        *edge = *existingEdge</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span></a>
<a>    }</a>
<a>}</a></pre></div>
<p>If the preceding loop does not produce a match, we create and insert a new edge to the store. As you can see in the following code snippet, we follow the same methodology that we did for link insertions. First, we allocate a new, unique ID for the edge and populate its<span> </span><kbd>UpdatedAt</kbd><span> </span>value. Then, we create a <em>copy</em> of the provided<span> </span><kbd>Edge</kbd><span> </span>object and insert it into the store's<span> </span><kbd>edges</kbd><span> </span>map:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">for</span> {</a>
<a>    edge.ID = uuid.New()</a>
<a>    <span class="kw">if</span> s.edges[edge.ID] == <span class="ot">nil</span> {</a>
<a>        <span class="kw">break</span></a>
<a>    }</a>
<a>}</a>

<a>edge.UpdatedAt = time.Now()</a>
<a>eCopy := <span class="bu">new</span>(graph.Edge)</a>
<a>*eCopy = *edge</a>
<a>s.edges[eCopy.ID] = eCopy</a>

<a><span class="co">// Append the edge ID to the list of edges originating from the edge's source link.</span></a>
<a>s.linkEdgeMap[edge.Src] = <span class="bu">append</span>(s.linkEdgeMap[edge.Src], eCopy.ID)</a>
<a><span class="kw">return</span> <span class="ot">nil</span></a></pre></div>
<p>Finally, before returning, there is a last bit of book-keeping that we need to perform: we need to add the new link to the edge list that originates from the specified source link. To this end, we index the<span> </span><kbd>linkEdgeMap</kbd><span> </span>using the source link ID as a key and append the ID of the newly inserted edge to the appropriate edge list.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking up links</h1>
                </header>
            
            <article>
                
<p>Looking up links is a fairly trivial operation. All we need to do is acquire a<span> </span><em>read</em><span> </span>lock, look up the link by its ID, and do either of the following things:</p>
<ul>
<li>Return the link back to the caller</li>
<li>Return an error if no link with the provided ID was found</li>
</ul>
<p>The link lookup logic is outlined in the following code snippet:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *InMemoryGraph) FindLink(id uuid.UUID) (*graph.Link, <span class="dt">error</span>) {</a>
<a>    s.mu.RLock()</a>
<a>    <span class="kw">defer</span> s.mu.RUnlock()</a>

<a>    link := s.links[id]</a>
<a>    <span class="kw">if</span> link == <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find link: %w"</span>, graph.ErrNotFound)</a>
<a>    }</a>

<a>    lCopy := <span class="bu">new</span>(graph.Link)</a>
<a>    *lCopy = *link</a>
<a>    <span class="kw">return</span> lCopy, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since we want to ensure that no external code can modify the graph's contents without invoking the<span> </span><kbd>UpsertLink</kbd><span> </span>method, the<span> </span><kbd>FindLink</kbd><span> </span>implementation always returns a <em>copy</em> of the link that is stored in the graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating links/edges</h1>
                </header>
            
            <article>
                
<p>To obtain an iterator for the graph links or edges, users need to invoke the<span> </span><kbd>Links</kbd><span> </span>or<span> </span><kbd>Edges</kbd><span> </span>methods. Let's take a look at how the<span> </span><kbd>Links</kbd><span> </span>method is implemented:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *InMemoryGraph) Links(fromID, toID uuid.UUID, retrievedBefore time.Time) (graph.LinkIterator, <span class="dt">error</span>) {</a>
<a>    from, to := fromID.String(), toID.String()</a>

<a>    s.mu.RLock()</a>
<a>    <span class="kw">var</span> list []*graph.Link</a>
<a>    <span class="kw">for</span> linkID, link := <span class="kw">range</span> s.links {</a>
<a>        <span class="kw">if</span> id := linkID.String(); id &gt;= from &amp;&amp; id &lt; to &amp;&amp; link.RetrievedAt.Before(retrievedBefore) {</a>
<a>            list = <span class="bu">append</span>(list, link)</a>
<a>        }</a>
<a>    }</a>
<a>    s.mu.RUnlock()</a>

<a>    <span class="kw">return</span> &amp;linkIterator{s: s, links: list}, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>In the preceding implementation, we obtain a<span> </span><em>read</em><span> </span>lock and then proceed to iterate all the links in the graph, searching for the ones that belong to the<span> </span><kbd>[fromID, toID)</kbd><span> </span>partition range <em>and</em> whose<span> </span><kbd>RetrievedAt</kbd><span> </span>value is less than the specified<span> </span><kbd>retrievedBefore</kbd><span> </span>value. Any links that satisfy this predicate are appended to the<span> </span><kbd>list</kbd><span> </span>variable.</p>
<p>To figure out whether a link ID belongs to the specified partition range, we convert it into a string and then rely on string comparisons to verify that it is either equal to <kbd>fromID</kbd> or falls between the two ends of the partition range. Obviously, performing string conversions and comparisons is not as efficient as directly comparing the underlying byte representation of the UUID values. However, since this particular implementation is meant to be used just for debugging purposes, we can focus on keeping the code simple rather than worrying about its performance.</p>
<p class="mce-root"/>
<p>Once we have finished iterating all the links, we create a new<span> </span><kbd>linkIterator</kbd><span> </span>instance and return it to the user. Now, let's examine how the iterator is implemented, starting with its type definition:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> linkIterator <span class="kw">struct</span> {</a>
<a>    s *InMemoryGraph</a>

<a>    links    []*graph.Link</a>
<a>    curIndex <span class="dt">int</span></a>
<a>}</a></pre></div>
<p>As you can see, the iterator stores a pointer to the in-memory graph, a list of<span> </span><kbd>Link</kbd><span> </span>models to iterate, and an index for keeping track of the iterator's offset within the list.</p>
<p>The implementation of the iterator's<span> </span><kbd>Next</kbd><span> </span>method is quite trivial:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *edgeIterator) Next() <span class="dt">bool</span> {</a>
<a>    <span class="kw">if</span> i.curIndex &gt;= <span class="bu">len</span>(i.links) {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>
<a>    i.curIndex++</a>
<a>    <span class="kw">return</span> <span class="ot">true</span></a>
<a>}</a></pre></div>
<p>Unless we have already reached the end of the list of links, we advance<span> </span><kbd>curIndex</kbd><span> </span>and return true to indicate that more data is available for retrieval via a call to the<span> </span><kbd>Link</kbd><span> </span>method, whose implementation is listed as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *linkIterator) Link() *graph.Link {</a>
<a>    i.s.mu.RLock()</a>
<a>    link := <span class="bu">new</span>(graph.Link)</a>
<a>    *link = *i.links[i.curIndex<span class="dv">-1</span>]</a>
<a>    i.s.mu.RUnlock()</a>
<a>    <span class="kw">return</span> link</a>
<a>}</a></pre></div>
<p>Keep in mind that the<span> </span><kbd>Link</kbd><span> </span>model instances associated with this iterator are maintained by the in-memory graph and may potentially be <em>shared</em> with other iterator instances. As a result, while one go-routine may consuming links from the iterator, another go-routine may be modifying their contents. To avoid data races, whenever the user invokes the iterator's<span> </span><kbd>Link</kbd><span> </span>method, we obtain a<span> </span><em>read</em><span> </span>lock on the link graph. While holding the lock, we can safely fetch the next link and make a copy, which is then returned to the caller.</p>
<p>Finally, let's take a look at the implementation of the<span> </span><kbd>Edges</kbd><span> </span>method. The logic is quite similar to<span> </span><kbd>Links</kbd><span>, but </span>with a minor difference in the way we populate the list of edges that belong to the requested partition:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *InMemoryGraph) Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (graph.EdgeIterator, <span class="dt">error</span>) {</a>
<a>    from, to := fromID.String(), toID.String()</a>
<a>    s.mu.RLock()</a>
<a>    <span class="kw">var</span> list []*graph.Edge</a>
<a>    <span class="kw">for</span> linkID := <span class="kw">range</span> s.links {</a>
<a>        <span class="kw">if</span> id := linkID.String(); id &lt; from || id &gt;= to {</a>
<a>            <span class="kw">continue</span></a>
<a>        }</a>
<a>        <span class="kw">for</span> _, edgeID := <span class="kw">range</span> s.linkEdgeMap[linkID] {</a>
<a>            <span class="kw">if</span> edge := s.edges[edgeID]; edge.UpdatedAt.Before(updatedBefore) {</a>
<a>                list = <span class="bu">append</span>(list, edge)</a>
<a>            }</a>
<a>        }</a>
<a>    }</a>
<a>    s.mu.RUnlock()</a>
<a>    <span class="kw">return</span> &amp;edgeIterator{s: s, edges: list}, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>As we mentioned in the <em>Partitioning links and edges for processing the graph in paralle</em>l section, each edge belongs to the same partition as the link it originates from. Therefore, in the preceding implementation, we begin by iterating the set of links in the graph and skip the ones that do not belong to the partition we need. Once we have located a link belonging to the requested partition range, we iterate the list of edges that originate from it (via the<span> </span><kbd>linkEdgeMap</kbd><span> </span>field) and append any edges that satisfy the<span> </span><em>updated-before-X</em><span> </span>predicate to the<span> </span><kbd>list</kbd><span> </span>variable.</p>
<p>The content of the<span> </span><kbd>list</kbd><span> </span>variable is then used to create a new<span> </span><kbd>edgeIterator</kbd><span> </span>instance, which is then returned to the caller. The<span> </span><kbd>edgeIterator</kbd><span> </span>is implemented in more or less the same way as the<span> </span><kbd>linkIterator</kbd><span>,</span> so we will attempt to save some space by not including its full implementation here. You can easily look it up by visiting this book's GitHub repository.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stale edges</h1>
                </header>
            
            <article>
                
<p>The last bit of functionality that we need to explore is the<span> </span><kbd>RemoveStaleEdges</kbd><span> </span>method. The caller invokes it with the ID of a link (the origin) and an<span> </span><kbd>updatedBefore</kbd><span> </span>value:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *InMemoryGraph) RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) <span class="dt">error</span> {</a>
<a>    s.mu.Lock()</a>
<a>    <span class="kw">defer</span> s.mu.Unlock()</a>

<a>    <span class="kw">var</span> newEdgeList edgeList</a>
<a>    <span class="kw">for</span> _, edgeID := <span class="kw">range</span> s.linkEdgeMap[fromID] {</a>
<a>        edge := s.edges[edgeID]</a>
<a>        <span class="kw">if</span> edge.UpdatedAt.Before(updatedBefore) {</a>
<a>            <span class="bu">delete</span>(s.edges, edgeID)</a>
<a>            <span class="kw">continue</span></a>
<a>        }</a>
<a>        newEdgeList = <span class="bu">append</span>(newEdgeList, edgeID)</a>
<a>    }</a>
<a>    s.linkEdgeMap[fromID] = newEdgeList</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>As with other operations that mutate the graph's contents, we need to acquire a<span> </span><em>write</em><span> </span>lock. Then, we iterate the list of edges that originate from the specified source link and ignore the ones whose<span> </span><kbd>UpdatedAt</kbd><span> </span>value is less than the specified<span> </span><kbd>updatedBefore</kbd><span> </span>argument. Any edge that survives the culling is added to a<span> </span><kbd>newEdgeList</kbd><span>, </span>which becomes the new list of outgoing edges for the specified source link.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a test suite for the graph implementation</h1>
                </header>
            
            <article>
                
<p>Before we conclude our tour of the in-memory graph implementation, we need to spend some time authoring a test suite that will execute the shared verification suite against the store implementation we just created. This can be achieved with only a handful of lines, as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> _ = gc.Suite(<span class="bu">new</span>(InMemoryGraphTestSuite))</a>

<a><span class="kw">type</span> InMemoryGraphTestSuite <span class="kw">struct</span> {</a>
<a>    graphtest.SuiteBase</a>
<a>}</a>

<a><span class="kw">func</span> (s *InMemoryGraphTestSuite) SetUpTest(c *gc.C) {</a>
<a>    s.SetGraph(NewInMemoryGraph())</a>
<a>}</a>

<a><span class="co">// Register our test-suite with go test.</span></a>
<a><span class="kw">func</span> Test(t *testing.T) { gc.TestingT(t) }</a></pre></div>
<p>Since we are working with a pure, in-memory implementation, we can cheat and recreate the graph before running each test by providing a<span> </span><kbd>SetUpTest</kbd><span> </span>method that the <kbd>gocheck</kbd> framework will automatically invoke for us when running the test suite.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling across with a CockroachDB-backed graph implementation</h1>
                </header>
            
            <article>
                
<p>While the in-memory graph implementation is definitely a great asset for running our unit tests or even for spinning up small instances of the Links 'R' Us system for demonstration or end-to-end testing purposes, it's not really something that we would actually want to use in a production-grade system.</p>
<p>First and foremost, the data in the in-memory store will not persist across service restarts. Even if we could somehow address this limitation (for example, by creating periodic snapshots of the graph to disk), the best we can do is scale our graph up: for example, we can run the link graph service on a machine with a faster CPU and/or more memory. But that's about it; as we anticipate the graph size eventually outgrowing the storage capacity of a single node, we need to come up with a more efficient solution that can scale across multiple machines.</p>
<p>To this end, the following sections will explore a second graph implementation that utilizes a database system that can support our scaling requirements. While there are undoubtedly quite a few DBMS out there that can satisfy our needs, I have decided to base the graph implementation on CockroachDB<span> </span><sup><span class="citation">[5]</span></sup><span> </span>for the following set of reasons:</p>
<ul>
<li>It can easily scale horizontally just by increasing the number of nodes available to the cluster. CockroachDB clusters can automatically rebalance and heal themselves when nodes appear or go down. This property makes it ideal for our use case!</li>
<li>CockroachDB is fully ACID-compliant and supports distributed SQL transactions.</li>
<li>The SQL flavor supported by CockroachDB is compatible with the PostgreSQL syntax, which many of you should already be familiar with.</li>
<li>CockroachDB implements the PostgreSQL wire protocol; this means that we do not require a specialized driver package to connect to the database but can simply use the battle-tested pure-Go <span>Postgres</span><span> </span><sup><span class="citation">[19]</span></sup><span> </span>package to connect to the database.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dealing with DB migrations</h1>
                </header>
            
            <article>
                
<p>When creating a dependency on a DBMS, we need to introduce an external mechanism to assist us in managing the schema for the tables that we will be running queries against.</p>
<p>Following the recommended industry best practices, changes to our database schema need to be made in small, incremental steps that can be applied when deploying a new version of our software to production, or reverted if we decide to roll back a deployment due to the discovery of a bug.</p>
<p>For this particular project, we will be managing our database schema with the help of the <kbd>gomigrate</kbd> tool<span> </span><sup><span class="citation">[7]</span></sup>. This tool can work with most popular database systems (including CockroachDB) and provides a handy command-line tool that we can use to apply or revert DB schema changes. <kbd>gomigrate</kbd> expects database migrations to be specified as two separate files: one containing the SQL commands to apply the migration (the <em>up</em> path) and another to revert the migration (the <em>down</em> path). The standard format for migration file names uses the following pattern:<span> </span></p>
<pre>timestamp-description-{up/down}.sql </pre>
<p>The addition of a timestamp component ensures that <kbd>gomigrate</kbd> always picks up and applies the changes in the correct order.</p>
<p>To execute any required migrations, we need to invoke the <kbd>gomigrate</kbd> CLI tool and provide it with the following bits of information:</p>
<ul>
<li>A data source<strong> name</strong> (<strong>DSN</strong>) URL for the target database.</li>
<li>The path to the location of the migration files. The tool not only supports local paths but it can also pull migrations from GitHub, GitLab, AWS S3, and Google Cloud Storage.</li>
<li>A migration<span> </span><em>direction</em><span> </span>command. This is typically<span> </span><kbd>up</kbd><span> </span>to apply the migrations or<span> </span><kbd>down</kbd><span> </span>to revert them.</li>
</ul>
<p>You may be wondering: how does <kbd>gomigrate</kbd> ensure that migrations are only executed once? The answer is: by maintaining state! So, where is that state stored then? The first time you run the <kbd>gomigrate</kbd> tool against a database, it will create two additional tables that are used by the tool to keep track of which migrations it has applied so far. This makes the tool safe to run multiple times (for example, each time we deploy a new version of our software to production).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>All the required migrations for the link graph project live in the<span> </span><kbd>Chapter06/linkgraph/store/cdb/migrations</kbd><span> </span>folder. What's more, the top-level makefile includes a<span> </span><kbd>run-cdb-migrations</kbd><span> </span>target that will install (if missing) the <kbd>gomigrate</kbd> tool and automatically run any<span> </span><em>pending</em><span> </span>migrations. In fact, this command is leveraged by the CI system linked to this book's GitHub repository to bootstrap a test database before running the CockroachDB tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of the DB schema for the CockroachDB implementation</h1>
                </header>
            
            <article>
                
<p>Setting up the tables we need for the CockroachDB graph implementation is a fairly straightforward process. The following is a combined list of the SQL statements that will be applied when we run the included DB migrations:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">CREATE</span> <span class="kw">TABLE</span> <span class="cf">IF</span> <span class="kw">NOT</span> <span class="kw">EXISTS</span> links (</a>
<a>    <span class="kw">id</span> UUID <span class="kw">PRIMARY</span> <span class="kw">KEY</span> <span class="kw">DEFAULT</span> gen_random_uuid(),</a>
<a>    url STRING <span class="kw">UNIQUE</span>,</a>
<a>    retrieved_at <span class="dt">TIMESTAMP</span></a>
<a>);</a>

<a><span class="kw">CREATE</span> <span class="kw">TABLE</span> <span class="cf">IF</span> <span class="kw">NOT</span> <span class="kw">EXISTS</span> edges (</a>
<a>    <span class="kw">id</span> UUID <span class="kw">PRIMARY</span> <span class="kw">KEY</span> <span class="kw">DEFAULT</span> gen_random_uuid(),</a>
<a>    src UUID <span class="kw">NOT</span> <span class="kw">NULL</span> <span class="kw">REFERENCES</span> links(<span class="kw">id</span>) <span class="kw">ON</span> <span class="kw">DELETE</span> <span class="kw">CASCADE</span>,</a>
<a>    dst UUID <span class="kw">NOT</span> <span class="kw">NULL</span> <span class="kw">REFERENCES</span> links(<span class="kw">id</span>) <span class="kw">ON</span> <span class="kw">DELETE</span> <span class="kw">CASCADE</span>,</a>
<a>    updated_at <span class="dt">TIMESTAMP</span>,</a>
<a>    <span class="kw">CONSTRAINT</span> edge_links <span class="kw">UNIQUE</span>(src,dst)</a>
<a>);</a></pre></div>
<p>You probably noticed that, while building the in-memory graph implementation, we had to manually enforce some constraints. For example, we had to check the following:</p>
<ul>
<li>The link and edge IDs are unique</li>
<li>The URLs are unique</li>
<li>The source and destination link IDs for edges point to existing links</li>
<li>The<span> </span><kbd>(source, destination)</kbd><span> </span>tuple for edges is unique</li>
</ul>
<p>For the CockroachDB implementation, we can simply delegate those checks to the DB itself by introducing uniqueness and foreign-key constraints when defining the table schemas. A small caveat of this approach is that when a SQL statement execution attempt returns an error, we need to inspect its contents to detect whether a constraint validation occurred. If that happens to be the case, we can return a more meaningful, typed error such as<span> </span><kbd>graph.ErrUnknownEdgeLinks</kbd><span> </span>to the caller matching the behavior of the in-memory implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upserting links</h1>
                </header>
            
            <article>
                
<p>To upsert a link to the CockroachDB store, we will use an upsert-like SQL query that leverages the database's support for specifying an action to be applied when a conflict occurs:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">INSERT</span> <span class="kw">INTO</span> links (url, retrieved_at) <span class="kw">VALUES</span> ($<span class="dv">1</span>, $<span class="dv">2</span>) </a>
<a><span class="kw">ON</span> CONFLICT (url) DO <span class="kw">UPDATE</span> <span class="kw">SET</span> retrieved_at<span class="op">=</span><span class="fu">GREATEST</span>(links.retrieved_at, $<span class="dv">2</span>)</a>
<a><span class="kw">RETURNING</span> <span class="kw">id</span>, retrieved_at</a></pre></div>
<p>Basically, if we try to insert a link that has the same<span> </span><kbd>url</kbd><span> </span>as an existing link, the preceding conflict resolution action will ensure that we simply update the<span> </span><kbd>retrieved_at</kbd><span> </span>column to the maximum of the original value and the one specified by the caller. Regardless of whether a conflict occurs or not, the query will always return the row's<span> </span><kbd>id</kbd><span> </span>(existing or assigned by the DB) and the value for the<span> </span><kbd>retrieved_at</kbd><span> </span>column. The relevant<span> </span><kbd>UpsertLink</kbd><span> </span>method implementation is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) UpsertLink(link *graph.Link) <span class="dt">error</span> {</a>
<a>    row := c.db.QueryRow(upsertLinkQuery, link.URL, link.RetrievedAt.UTC())</a>
<a>    <span class="kw">if</span> err := row.Scan(&amp;link.ID, &amp;link.RetrievedAt); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"upsert link: %w"</span>, err)</a>
<a>    }</a>

<a>    link.RetrievedAt = link.RetrievedAt.UTC()</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>This method binds the fields from the provided model, which are bound to the<span> </span><kbd>upsertLinkQuery</kbd><span>, </span>and proceeds to execute it. Then, it scans the<span> </span><kbd>id</kbd><span> </span>and<span> </span><kbd>retrieved_at</kbd><span> </span>values that are returned by the query into the appropriate model fields.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upserting edges</h1>
                </header>
            
            <article>
                
<p>To upsert an edge, we will be using the following query:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">INSERT</span> <span class="kw">INTO</span> edges (src, dst, updated_at) <span class="kw">VALUES</span> ($<span class="dv">1</span>, $<span class="dv">2</span>, NOW())</a>
<a><span class="kw">ON</span> CONFLICT (src,dst) DO <span class="kw">UPDATE</span> <span class="kw">SET</span> updated_at<span class="op">=</span>NOW()</a>
<a><span class="kw">RETURNING</span> <span class="kw">id</span>, updated_at</a></pre></div>
<p>As you can see, the query includes a conflict resolution step for the case where we try to insert an edge with the same<span> </span><kbd>(src, dst)</kbd><span> </span>tuple. If that happens, we simply change the<span> </span><kbd>updated_at</kbd><span> </span>column value to the current timestamp.</p>
<p>Unsurprisingly, the code to upsert an edge to the CockroachDB store looks quite similar to the link upsert code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) UpsertEdge(edge *graph.Edge) <span class="dt">error</span> {</a>
<a>    row := c.db.QueryRow(upsertEdgeQuery, edge.Src, edge.Dst)</a>
<a>    <span class="kw">if</span> err := row.Scan(&amp;edge.ID, &amp;edge.UpdatedAt); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">if</span> isForeignKeyViolationError(err) {</a>
<a>            err = graph.ErrUnknownEdgeLinks</a>
<a>        }</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"upsert edge: %w"</span>, err)</a>
<a>    }</a>

<a>    edge.UpdatedAt = edge.UpdatedAt.UTC()</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>Once again, we bind the relevant fields to a query that we proceed to execute and update the provided edge model with the<span> </span><kbd>id</kbd><span> </span>and<span> </span><kbd>updated_at</kbd><span> </span>fields that were returned by the query.</p>
<p>The preceding code comes with a small twist! When we defined the schema for the edges table, we also specified a<span> </span><em>foreign-key</em><span> </span>constraint for the<span> </span><kbd>src</kbd><span> </span>and<span> </span><kbd>dst</kbd><span> </span>fields. Therefore, if we try to upsert an edge with an unknown source and/or destination ID, we will get an error. To check whether the error was actually caused by a foreign-key violation, we can use the following helper:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> isForeignKeyViolationError(err <span class="dt">error</span>) <span class="dt">bool</span> {</a>
<a>    pqErr, valid := err.(*pq.Error)</a>
<a>    <span class="kw">if</span> !valid {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>
<a>    <span class="kw">return</span> pqErr.Code.Name() == <span class="st">"foreign_key_violation"</span></a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>To match the behavior of the in-memory store implementation, if the error points to a foreign-key violation, we return the more user-friendly<span> </span><kbd>graph.ErrUnknownEdgeLinks</kbd><span> </span>error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking up links</h1>
                </header>
            
            <article>
                
<p>To look up a link by its ID, we will be using the following standard SQL selection query:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">SELECT</span> url, retrieved_at <span class="kw">FROM</span> links <span class="kw">WHERE</span> <span class="kw">id</span><span class="op">=</span>$<span class="dv">1</span><span class="ot">"</span></a></pre></div>
<p>The implementation of the<span> </span><kbd>FindLink</kbd><span> </span>method is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) FindLink(id uuid.UUID) (*graph.Link, <span class="dt">error</span>) {</a>
<a>    row := c.db.QueryRow(findLinkQuery, id)</a>
<a>    link := &amp;graph.Link{ID: id}</a>
<a>    <span class="kw">if</span> err := row.Scan(&amp;link.URL, &amp;link.RetrievedAt); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">if</span> err == sql.ErrNoRows {</a>
<a>            <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find link: %w"</span>, graph.ErrNotFound)</a>
<a>        }</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find link: %w"</span>, err)</a>
<a>    }</a>
<a>    link.RetrievedAt = link.RetrievedAt.UTC()</a>
<a>    <span class="kw">return</span> link, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>After executing the query, we create a new<span> </span><kbd>Link</kbd><span> model </span>instance and populate it with the returned link fields. If the selection query does not match any link, the SQL driver will return a<span> </span><kbd>sql.ErrNoRows</kbd><span> </span>error. The preceding code checks for this error and returns a user-friendly<span> </span><kbd>graph.ErrNotFound</kbd><span> </span>error to the caller.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating links/edges</h1>
                </header>
            
            <article>
                
<p>To select the links that correspond to a particular partition and whose retrieved timestamp is older than the provided value, we will use the following query:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">SELECT</span> <span class="kw">id</span>, url, retrieved_at <span class="kw">FROM</span> links <span class="kw">WHERE</span> <span class="kw">id</span> <span class="op">&gt;=</span> $<span class="dv">1</span> <span class="kw">AND</span> <span class="kw">id</span> <span class="op">&lt;</span> $<span class="dv">2</span> <span class="kw">AND</span> retrieved_at <span class="op">&lt;</span> $<span class="dv">3</span></a></pre></div>
<p>The implementation of the<span> </span><kbd>Links</kbd><span> </span>method is shown in the following listing:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) Links(fromID, toID uuid.UUID, accessedBefore time.Time) (graph.LinkIterator, <span class="dt">error</span>) {</a>
<a>    rows, err := c.db.Query(linksInPartitionQuery, fromID, toID, accessedBefore.UTC())</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"links: %w"</span>, err)</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;linkIterator{rows: rows}, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>As you can see, the method executes the query with the specified arguments and returns a<span> </span><kbd>linkIterator</kbd><span> </span>to consume the returned result set. The link CockroachDB iterator implementation is nothing more than a wrapper on top of the<span> </span><kbd>sql.Rows</kbd><span> </span>value that's returned by the SQL query. This is what the<span> </span><kbd>Next</kbd><span> </span>method's implementation looks like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *linkIterator) Next() <span class="dt">bool</span> {</a>
<a>    <span class="kw">if</span> i.lastErr != <span class="ot">nil</span> || !i.rows.Next() {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>

<a>    l := <span class="bu">new</span>(graph.Link)</a>
<a>    i.lastErr = i.rows.Scan(&amp;l.ID, &amp;l.URL, &amp;l.RetrievedAt)</a>
<a>    <span class="kw">if</span> i.lastErr != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>
<a>    l.RetrievedAt = l.RetrievedAt.UTC()</a>

<a>    i.latchedLink = l</a>
<a>    <span class="kw">return</span> <span class="ot">true</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Edges</kbd><span> </span>method uses the following query, which yields exactly the same set of results as the in-memory implementation:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">SELECT</span> <span class="kw">id</span>, src, dst, updated_at <span class="kw">FROM</span> edges <span class="kw">WHERE</span> src <span class="op">&gt;=</span> $<span class="dv">1</span> <span class="kw">AND</span> src <span class="op">&lt;</span> $<span class="dv">2</span> <span class="kw">AND</span> updated_at <span class="op">&lt;</span> $<span class="dv">3</span><span class="ot">"</span></a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here's what the implementation of<span> </span><kbd>Edges</kbd><span> </span>looks like:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (graph.EdgeIterator, <span class="dt">error</span>) {</a>
<a>    rows, err := c.db.Query(edgesInPartitionQuery, fromID, toID, updatedBefore.UTC())</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"edges: %w"</span>, err)</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;edgeIterator{rows: rows}, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The implementation of the<span> </span><kbd>edgeIterator</kbd><span> </span>is quite similar to the<span> </span><kbd>linkIterator</kbd><span>, </span>so we will conserve some space and omit it. You can take a look at the complete iterator implementations by examining the source code in the <kbd>iterator.go</kbd> file, which can be found within the <kbd>Chapter06/linkgraph/store/cdb</kbd> package of this book's GitHub repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stale edges</h1>
                </header>
            
            <article>
                
<p>The last piece of functionality that we will be examining is the<span> </span><kbd>RemoveStaleEdges</kbd><span> </span>method, which uses the following query to delete edges that have not been updated after a particular point in time:</p>
<div class="sourceCode">
<pre class="sourceCode sql"><a><span class="kw">DELETE</span> <span class="kw">FROM</span> edges <span class="kw">WHERE</span> src<span class="op">=</span>$<span class="dv">1</span> <span class="kw">AND</span> updated_at <span class="op">&lt;</span> $<span class="dv">2</span></a></pre></div>
<p>Let's take a look at the<span> </span><kbd>RemoveStaleEdges</kbd><span> </span>method implementation:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *CockroachDBGraph) RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) <span class="dt">error</span> {</a>
<a>    _, err := c.db.Exec(removeStaleEdgesQuery, fromID, updatedBefore.UTC())</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"remove stale edges: %w"</span>, err)</a>
<a>    }</a>

<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>There' nothing out of the ordinary here; the code in the preceding snippet simply binds the arguments to the delete query and executes it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a test suite for the CockroachDB implementation</h1>
                </header>
            
            <article>
                
<p>To create and wire the test suite for the CockroachDB implementation, we will follow exactly the same steps that we did for the in-memory implementation. The first step is to define a test suite that embeds the shared<span> </span><kbd>graphtest.SuiteBase</kbd><span> </span>type and register it with <kbd>go test</kbd>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> _ = gc.Suite(<span class="bu">new</span>(CockroachDBGraphTestSuite))</a>

<a><span class="kw">type</span> CockroachDBGraphTestSuite <span class="kw">struct</span> {</a>
<a>    graphtest.SuiteBase</a>
<a>    db *sql.DB</a>
<a>}</a>

<a><span class="co">// Register our test-suite with go test.</span></a>
<a><span class="kw">func</span> Test(t *testing.T) { gc.TestingT(t) }</a></pre></div>
<p>Then, we need to provide a setup method for the test suite that will create a new CockroachDB graph instance and wire it to the base suite. Following the testing paradigm we discussed in <a href="d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml">Chapter 4</a><span>, </span><em>The Art of Testing</em><span>,</span> our test suite relies on the presence of an environment variable that should contain the DSN for connecting to the CockroachDB instance. If the environment variable is not defined, the entire test suite will be automatically skipped:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *CockroachDBGraphTestSuite) SetUpSuite(c *gc.C) {</a>
<a>    dsn := os.Getenv(<span class="st">"CDB_DSN"</span>)</a>
<a>    <span class="kw">if</span> dsn == <span class="st">""</span> {</a>
<a>        c.Skip(<span class="st">"Missing CDB_DSN envvar; skipping cockroachdb-backed graph test suite"</span>)</a>
<a>    }</a>

<a>    g, err := NewCockroachDBGraph(dsn)</a>
<a>    c.Assert(err, gc.IsNil)</a>
<a>    s.SetGraph(g)</a>

<a>    <span class="co">// keep track of the sql.DB instance so we can execute SQL statements </span></a>
<a>    <span class="co">// to reset the DB between tests!</span></a>
<a>    s.db = g.db</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>To ensure that all the tests work exactly as expected, one of our requirements is that each test in the suite is provided with a clean DB instance. To this end, we need to define a<span> </span><em>per-test</em><span> </span>setup method that empties all the database tables:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *CockroachDBGraphTestSuite) SetUpTest(c *gc.C) { s.flushDB(c) }</a>

<a><span class="kw">func</span> (s *CockroachDBGraphTestSuite) flushDB(c *gc.C) {</a>
<a>    _, err := s.db.Exec(<span class="st">"DELETE FROM links"</span>)</a>
<a>    c.Assert(err, gc.IsNil)</a>
<a>    _, err = s.db.Exec(<span class="st">"DELETE FROM edges"</span>)</a>
<a>    c.Assert(err, gc.IsNil)</a>
<a>}</a></pre></div>
<p>Finally, we need to provide a teardown method for the test suite. Once the test suite has finished executing, we truncate the DB tables once more and release the DB connection:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *CockroachDBGraphTestSuite) TearDownSuite(c *gc.C) {</a>
<a>    <span class="kw">if</span> s.db != <span class="ot">nil</span> {</a>
<a>        s.flushDB(c)</a>
<a>        c.Assert(s.db.Close(), gc.IsNil)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Note that flushing the database's contents during teardown is not mandatory. In my opinion, it's good practice to always do so just in case some other set of tests from a different package use the same DB instance but expect it to be initially empty.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing the data layer for the text indexer component</h1>
                </header>
            
            <article>
                
<p>In the following sections, we will perform an in-depth analysis of the text indexer component. We will identify the set of operations that the text indexer component must be able to support and formally encode them as a Go interface named<span> </span><kbd>Indexer</kbd>.</p>
<p>In a similar fashion to the link graph analysis, we will be constructing two concrete implementations of the<span> </span><kbd>Indexer</kbd><span> </span>interface: an in-memory implementation based on the popular bleve<span> </span><sup><span class="citation">[1]</span></sup><span> </span>package and a horizontally-scalable implementation using Elasticsearch<span> </span><sup><span class="citation">[9]</span></sup>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A model for indexed documents</h1>
                </header>
            
            <article>
                
<p>As the first step in our analysis of the indexer component, we will start by describing the document model that the<span> </span><kbd>Indexer</kbd><span> </span>implementations will index and search:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Document <span class="kw">struct</span> {</a>
<a>    LinkID uuid.UUID</a>

<a>    URL <span class="dt">string</span></a>

<a>    Title <span class="dt">string</span></a>
<a>    Content <span class="dt">string</span></a>

<a>    IndexedAt time.Time</a>
<a>    PageRank <span class="dt">float64</span></a>
<a>}</a></pre></div>
<p>All the documents must include a non-empty attribute called<span> </span><kbd>LinkID</kbd>. This attribute is a UUID value that connects a document with a link that's obtained from the link graph. In addition to the link ID, each document also stores the URL of the indexed document and allows us to not only display it as part of the search results but to also implement more advanced search patterns in future (for example, constraint searches to a particular domain).</p>
<p>The<span> </span><kbd>Title</kbd><span> </span>and<span> </span><kbd>Content</kbd><span> </span>attributes correspond to the value of the<span> </span><kbd>&lt;title&gt;</kbd><span> </span>element if the link points to an HTML page, whereas the<span> </span><kbd>Content</kbd><span> </span>attribute stores the block of text that was extracted by the crawler when processing the link. Both of these attributes will be indexed and made available for searching.</p>
<p>The<span> </span><kbd>IndexedAt</kbd><span> </span>attribute contains a timestamp that indicates when a particular document was last indexed, while the<span> </span><kbd>PageRank</kbd><span> </span>attribute keeps track of the<span> </span><kbd>PageRank</kbd><span> </span>score that will be assigned to each document by the <kbd>PageRank</kbd> calculator component. Since <kbd>PageRank</kbd> scores can be construed as a quality metric for each link, the text indexer implementations will attempt to optimize the returned result sets by sorting search matches<span> </span><em>both</em><span> </span>by their relevance to the input query and by their <kbd>PageRank</kbd> scores.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Listing the set of operations that the text indexer needs to support</h1>
                </header>
            
            <article>
                
<p>For the text indexer component use case, we need to be able to perform the following set of operations:</p>
<ol type="1">
<li>Add a document to the index or reindex an existing document when its content changes. This operation will normally be invoked by the crawler component.</li>
<li>Perform a lookup for a document by its ID.</li>
<li>Perform a full-text query and obtain an<span> </span><em>iterable</em><span> </span>list of results. The frontend component for our project will invoke this operation when the user clicks the search button and consume the returned iterator to present a paginated list of results to the end user.</li>
<li>Update the <kbd>PageRank</kbd> score for a particular document. This operation will be invoked by the <kbd>PageRank</kbd> calculator component when the <kbd>PageRank</kbd> score for a particular link needs to be updated.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the Indexer interface</h1>
                </header>
            
            <article>
                
<p>Similar to the approach we followed when we modeled the link graph component, we shall encapsulate the preceding list of operations into a Go interface called<span> </span><kbd>Indexer</kbd>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Indexer <span class="kw">interface</span> {</a>
<a>    Index(doc *Document) <span class="dt">error</span></a>
<a>    FindByID(linkID uuid.UUID) (*Document, <span class="dt">error</span>)</a>
<a>    Search(query Query) (Iterator, <span class="dt">error</span>)</a>
<a>    UpdateScore(linkID uuid.UUID, score <span class="dt">float64</span>) <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Search</kbd><span> </span>method expects a<span> </span><kbd>Query</kbd><span> </span>type instead of a simple string value as its input argument. This is by design; it offers us the flexibility to expand the indexer's query capabilities further down the road to support richer query semantics without having to modify the signature of the<span> </span><kbd>Search</kbd><span> </span>method. Here is the definition of the<span> </span><kbd>Query</kbd><span> </span>type:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Query <span class="kw">struct</span> {</a>
<a>    Type       QueryType</a>
<a>    Expression <span class="dt">string</span></a>
<a>    Offset     <span class="dt">uint64</span></a>
<a>}</a>

<a><span class="kw">type</span> QueryType <span class="dt">uint8</span></a>

<a><span class="kw">const</span> (</a>
<a>    QueryTypeMatch QueryType = <span class="ot">iota</span></a>
<a>    QueryTypePhrase</a>
<a>)</a></pre></div>
<p>The<span> </span><kbd>Expression</kbd><span> </span>field stores the search query that's entered by the end user. However, its interpretation by the indexer component can vary, depending on the value of the<span> </span><kbd>Type</kbd><span> </span>attribute. As proof of concept, we will only implement two of the most common types of searches:</p>
<ul>
<li>Searching for a list of keywords<span> </span><em>in any order</em></li>
<li>Searching for an<span> </span><em>exact</em><span> </span>phrase match</li>
</ul>
<p>In the future, we can opt to add support for other types of queries such as<span> </span><em>boolean-</em>,<span> </span><em>date-</em><span>, </span>or<span> </span><em>domain-based</em><span> </span>queries.</p>
<p>After executing a search query, the text indexer will return an<span> </span><kbd>Iterator</kbd><span> </span>interface instance that provides a simple API for consuming the search results. This is the definition of the<span> </span><kbd>Iterator</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Iterator <span class="kw">interface</span> {</a>
<a>    <span class="co">// Close the iterator and release any allocated resources.</span></a>
<a>    Close() <span class="dt">error</span></a>

<a>    <span class="co">// Next loads the next document matching the search query.</span></a>
<a>    <span class="co">// It returns false if no more documents are available.</span></a>
<a>    Next() <span class="dt">bool</span></a>

<a>    <span class="co">// Error returns the last error encountered by the iterator.</span></a>
<a>    Error() <span class="dt">error</span></a>

<a>    <span class="co">// Document returns the current document from the result set.</span></a>
<a>    Document() *Document</a>

<a>    <span class="co">// TotalCount returns the approximate number of search results.</span></a>
<a>    TotalCount() <span class="dt">uint64</span></a>
<a>}</a></pre></div>
<p>After obtaining an iterator instance, we can consume each search result using a simple <kbd>for</kbd> loop:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// 'docIt' is a search iterator</span></a>
<a><span class="kw">for</span> docIt.Next() {</a>
<a>    doc := docIt.Document()</a>
<a>    <span class="co">// Do something with doc...</span></a>
<a>}</a>

<a><span class="kw">if</span> err := docIt.Error(); err != <span class="ot">nil</span> {</a>
<a>    <span class="co">// Handle error...</span></a>
<a>}</a></pre></div>
<p>Calls to<span> </span><kbd>docIt.Next()</kbd><span> </span>will return false either when we have iterated all the results or an error has occurred. In a similar fashion to the link graph iterators we examined in the previous sections, we only need to check<span> </span><em>once</em><span> </span>for the presence of errors after exiting the iteration loop.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Verifying indexer implementations using a shared test suite</h1>
                </header>
            
            <article>
                
<p>In the next few pages, we will be constructing two completely different Indexer implementations. In a similar fashion to the link graph component, we will again devise a shared test suite that will help us verify that both implementations behave in exactly the same way.</p>
<p class="mce-root">The<span> </span><kbd>SuiteBase</kbd><span> </span>definition for our shared indexer tests can be found in<span> the <kbd>Chapter06/textindexer/index/indextest</kbd> package and depends on the <kbd>gocheck</kbd> <span class="citation"><sup>[11]</sup></span> framework that we introduced in <a href="d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml">Chapter 4</a>, <em>The Art of Testing</em>.</span> The suite defines tests for the following groups of index operations:</p>
<ul>
<li><strong>Document indexing tests</strong>: These tests are designed to verify that the indexer component successfully processes valid documents and rejects any document that does not define the required set of document attributes (for example, it includes an empty link ID).</li>
<li><strong>Document lookup tests</strong>: These tests validate that we can look up a previously indexed document via its link ID and that the returned document model is identical to the document that was passed and indexed.</li>
<li><strong>Keyword search tests</strong>: A series of tests designed to verify that keyword searches yield the correct set of documents.</li>
<li><strong>Exact phrase search tests</strong>: Yet another series of tests that verifies that exact phrase searches yield the correct set of documents.</li>
<li><kbd>PageRank</kbd> <strong>score update tests</strong>: These tests exercise the <kbd>PageRank</kbd> score update code path and verify that changes to the score values for indexed documents are reflected in the order of returned search results.</li>
</ul>
<p class="mce-root"/>
<p>To create a test suite for an actual indexer implementation, all we have to do is the following:</p>
<ul>
<li>Define a new test suite that embeds<span> </span><kbd>SuiteBase</kbd></li>
<li>Provide a suite setup helper that creates the appropriate indexer instance and then invokes the<span> </span><kbd>SetIndexer</kbd><span> </span>method exposed by<span> </span><kbd>SuiteBase</kbd><span> </span>to wire the indexer to the base test suite</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An in-memory Indexer implementation using bleve</h1>
                </header>
            
            <article>
                
<p>Our first attempt at implementing an in-memory indexer will be based on a popular full-text search package for Go called bleve<span> </span><sup><span class="citation">[1]</span></sup>. While bleve is primarily designed to store its index on disk, it also supports an in-memory index. This makes it an excellent candidate for running unit tests in isolation or for demonstration purposes if we don't want to spin up a much more resource-intensive option such as Elasticsearch.</p>
<p>The full source for the bleve-based Indexer implementation is available in the<span> </span><kbd>Chapter06/textindexer/store/memory</kbd> package in this book's GitHub repository. The definition of the<span> </span><kbd>InMemoryBleveIndexer</kbd><span> </span>type is pretty straightforward:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> InMemoryBleveIndexer <span class="kw">struct</span> {</a>
<a>    mu   sync.RWMutex</a>
<a>    docs <span class="kw">map</span>[<span class="dt">string</span>]*index.Document</a>

<a>    idx bleve.Index</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>idx</kbd><span> </span>field stores a reference to the bleve index. To speed up indexing, we don't pass the full<span> </span><kbd>Document</kbd><span> </span>model to bleve and instead make use of a more lightweight representation that only contains the three fields we need for performing searches: the title, content, and <kbd>PageRank</kbd> score.</p>
<p>An obvious caveat of this approach is that since bleve stores a partial view of the document data, we cannot recreate the original document from the result list returned by bleve after executing a search query. To solve this problem, the in-memory indexer maintains a map where keys are the document link IDs and values are<span> </span><em>immutable</em><span> </span>copies of the documents that are processed by the indexer. When processing a result list, the returned document IDs are used to index the map and to recover the original document. To ensure that the in-memory indexer is safe for concurrent use, access to the map is guarded with a read/write mutex.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing documents</h1>
                </header>
            
            <article>
                
<p>The implementation of the<span> </span><kbd>Index</kbd><span> </span>method for the in-memory indexer is outlined as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *InMemoryBleveIndexer) Index(doc *index.Document) <span class="dt">error</span> {</a>
<a>    <span class="kw">if</span> doc.LinkID == uuid.Nil {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"index: %w"</span>, index.ErrMissingLinkID)</a>
<a>    }</a>
<a>    doc.IndexedAt = time.Now()</a>
<a>    dcopy := copyDoc(doc)</a>
<a>    key := dcopy.LinkID.String()</a>
<a>    i.mu.Lock()</a>
<a>    <span class="kw">if</span> orig, exists := i.docs[key]; exists {</a>
<a>        dcopy.PageRank = orig.PageRank</a>
<a>    }</a>
<a>    <span class="kw">if</span> err := i.idx.Index(key, makeBleveDoc(dcopy)); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"index: %w"</span>, err)</a>
<a>    }</a>
<a>    i.docs[key] = dcopy</a>
<a>    i.mu.Unlock()</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>To guarantee that the only way to mutate an already-indexed document is via a reindex operation, the indexer is designed to work with immutable copies of the documents that are passed as arguments to the<span> </span><kbd>Index</kbd><span> </span>method. The<span> </span><kbd>copyDoc</kbd><span> </span>helper creates a copy of the original document that we can safely store in the internal document map.</p>
<p>To add a new document to the index or to reindex an existing document, we need to provide bleve with two parameters: a<span> </span><em>string-based</em><span> </span>document ID and the document to be indexed. The<span> </span><kbd>makeBleveDoc</kbd><span> </span>helper returns a partial, lightweight view of the original document that, as we mentioned in the previous section, only contains the fields we want to use as part of our search queries.</p>
<p>When updating an existing document, we don't want the index operation to mutate the <kbd>PageRank</kbd> score that has already been assigned to the document as this would interfere with how the search results are ordered. To this end, if a document already exists, we need to patch the lightweight document that we pass to bleve so that it reflects the correct <kbd>PageRank</kbd> value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking up documents and updating their PageRank score</h1>
                </header>
            
            <article>
                
<p>If we know a document's link ID, we can invoke the<span> </span><kbd>FindByID</kbd><span> </span>method to look up the indexed document. The implementation is pretty straightforward; we just acquire a read lock and lookup for the specified ID in the internal map maintained by the indexer. If a matching entry exists, we create a copy and return it to the caller:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *InMemoryBleveIndexer) FindByID(linkID uuid.UUID) (*index.Document, <span class="dt">error</span>) {</a>
<a>    <span class="kw">return</span> i.findByID(linkID.String())</a>
<a>}</a>

<a><span class="kw">func</span> (i *InMemoryBleveIndexer) findByID(linkID <span class="dt">string</span>) (*index.Document, <span class="dt">error</span>) {</a>
<a>    i.mu.RLock()</a>
<a>    <span class="kw">defer</span> i.mu.RUnlock()</a>

<a>    <span class="kw">if</span> d, found := i.docs[linkID]; found {</a>
<a>        <span class="kw">return</span> copyDoc(d), <span class="ot">nil</span></a>
<a>    }</a>

<a>    <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find by ID: %w"</span>, index.ErrNotFound)</a>
<a>}</a></pre></div>
<p>You may be wondering why the<span> </span><kbd>FindByID</kbd><span> </span>implementation converts the input UUID into a string and delegates the actual document look up to the unexported<span> </span><kbd>findByID</kbd><span> </span>method. In the previous section, we saw that when we request bleve to index a document, we need to provide a string-based ID for the document. Bleve will return that ID to us when the document is matched by a search query. As will become evident in the following section, by providing a<span> </span><kbd>findByID</kbd><span> </span>method that accepts the linkID as a string, we can <em>reuse</em> the document lookup code when iterating search results.</p>
<p>To update the <kbd>PageRank</kbd> score for an existing document, clients invoke the<span> </span><kbd>UpdateScore</kbd><span> </span>method, which expects a document's link ID and the updated <kbd>PageRank</kbd> score:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *InMemoryBleveIndexer) UpdateScore(linkID uuid.UUID, score <span class="dt">float64</span>) <span class="dt">error</span> {</a>
<a>    i.mu.Lock()</a>
<a>    <span class="kw">defer</span> i.mu.Unlock()</a>
<a>    key := linkID.String()</a>
<a>    doc, found := i.docs[key]</a>
<a>    <span class="kw">if</span> !found {</a>
<a>        doc = &amp;index.Document{LinkID: linkID}</a>
<a>        i.docs[key] = doc</a>
<a>    }</a>

<a>    doc.PageRank = score</a>
<a>    <span class="kw">if</span> err := i.idx.Index(key, makeBleveDoc(doc)); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> xerrors.Errorf(<span class="st">"update score: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>Updating <em>any</em> searchable document attribute requires a reindex operation. Consequently, the<span> </span><kbd>UpdateScore</kbd><span> </span>implementation will acquire a<span> </span><em>write</em><span> </span>lock and look up the document in the internal document map. If the document is found, its <kbd>PageRank</kbd> score will be updated<span> </span><em>in-place</em><span> </span>and the document will be passed to bleve for indexing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Searching the index</h1>
                </header>
            
            <article>
                
<p>The clients of the in-memory indexer submit search queries by invoking the<span> </span><kbd>Search</kbd><span> </span>method. The implementation of this method is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *InMemoryBleveIndexer) Search(q index.Query) (index.Iterator, <span class="dt">error</span>) {</a>
<a>    <span class="kw">var</span> bq query.Query</a>
<a>    <span class="kw">switch</span> q.Type {</a>
<a>    <span class="kw">case</span> index.QueryTypePhrase:</a>
<a>        bq = bleve.NewMatchPhraseQuery(q.Expression)</a>
<a>    <span class="kw">default</span>:</a>
<a>        bq = bleve.NewMatchQuery(q.Expression)</a>
<a>    }</a>

<a>    searchReq := bleve.NewSearchRequest(bq)</a>
<a>    searchReq.SortBy([]<span class="dt">string</span>{<span class="st">"-PageRank"</span>, <span class="st">"-_score"</span>})</a>
<a>    searchReq.Size = batchSize</a>
<a>    searchReq.From = q.Offset</a>
<a>    rs, err := i.idx.Search(searchReq)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"search: %w"</span>, err)</a>
<a>    }</a>
<a>    <span class="kw">return</span> &amp;bleveIterator{idx: i, searchReq: searchReq, rs: rs, cumIdx: q.Offset}, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The first thing that our implementation needs to do is check what type of query the caller asked us to perform and then invoke the appropriate bleve helper to construct a query from the caller-provided expression.</p>
<p class="mce-root"/>
<p><span>Next, the generated query is transformed into a new search request where we also ask bleve to order the results by <kbd>PageRank</kbd> and relevance in descending order. Bleve search results are always paginated. Consequently, in addition to any sorting preferences, we must also specify the number of results per page that we want bleve to return (the batch size). The search request object also allows us to control the offset in the result list by specifying a value for its</span><span> </span><kbd>From</kbd><span> </span><span>field.</span></p>
<p>The next step is to submit the search request to bleve and check for the presence of errors. If everything goes according to plan and no error is returned, the implementation creates a new iterator instance that the caller can use to consume the matched documents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating the list of search results</h1>
                </header>
            
            <article>
                
<p>The<span> </span><kbd>bleveIterator</kbd><span> </span>type implements the<span> </span><kbd>indexer.Iterator</kbd><span> </span>interface and is defined as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> bleveIterator <span class="kw">struct</span> {</a>
<a>    idx       *InMemoryBleveIndexer</a>
<a>    searchReq *bleve.SearchRequest</a>

<a>    cumIdx <span class="dt">uint64</span></a>
<a>    rsIdx  <span class="dt">int</span></a>
<a>    rs     *bleve.SearchResult</a>

<a>    latchedDoc *index.Document</a>
<a>    lastErr    <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>The iterator implementation keeps track of two pointers:</p>
<ul>
<li>A pointer to the in-memory indexer instance, which allows the iterator to access the stored documents when the iterator is advanced</li>
<li>A pointer to the executed search request, which the iterator uses to trigger new bleve searches once the current page of results has been consumed</li>
</ul>
<p>To track the position in the paginated search result list, the iterator also maintains two counters:</p>
<ul>
<li>A cumulative counter (<kbd>cumIdx</kbd>) that tracks the absolute position in the <em>global</em> result list</li>
<li>A counter (<kbd>rsIdx</kbd>) that tracks the position in the <em>current</em> page of results</li>
</ul>
<p class="mce-root"/>
<p>The <kbd>bleve.SearchResult</kbd> objects returned by bleve queries provide information about both the total number of matched results and the number of documents in the current result page. The iterator's <kbd>Next</kbd> method makes use of this information to decide whether the iterator can be advanced.</p>
<p>When the iterator's<span> </span><kbd>Next</kbd><span> </span>method is invoked, the implementation performs a quick check to see if an error has occurred or we have already iterated the full set of results. If that is the case,<span> </span><kbd>Next</kbd><span> </span>will return <kbd>false</kbd> to indicate that no more items are available. The latter check is facilitated by comparing the total result count reported by bleve to the<span> </span><kbd>cumIdx</kbd><span> </span>value that the iterator tracks within its internal state:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">if</span> it.lastErr != <span class="ot">nil</span> || it.rs == nil || it.cumIdx &gt;= it.rs.Total {</a>
<a>    <span class="kw">return</span> <span class="ot">false</span></a>
<a>}</a></pre></div>
<p>Our next course of action is to check whether we have exhausted the current page of results. This is facilitated by comparing the number of documents in the current result page to the value of the<span> </span><kbd>rsIdx</kbd><span> </span>counter. If all the documents in the <em>current</em> result page have been consumed and <em>no</em> additional result pages are available, the method returns <kbd>false</kbd> to indicate this to the caller.</p>
<p>Otherwise, the implementation automatically fetches the next pages of results by doing the following:</p>
<ol>
<li>Updating the stored search request so that the result offset points to the beginning of the <em>next</em> page</li>
<li>Executing a new bleve search request to obtain the next page of results</li>
<li>Resetting the<span> </span><kbd>rsIdx</kbd><span> </span>counter so that we can process the first result of the newly retrieved page</li>
</ol>
<p>The preceding steps are outlined in the following code snippet:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">if</span> it.rsIdx &gt;= it.rs.Hits.Len() {</a>
<a>    it.searchReq.From += it.searchReq.Size</a>
<a>    <span class="kw">if</span> it.rs, it.lastErr = it.idx.idx.Search(it.searchReq); it.lastErr != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span></a>
<a>    }</a>
<a>    it.rsIdx = <span class="dv">0</span></a>
<a>}</a>

<a>nextID := it.rs.Hits[it.rsIdx].ID</a>
<a><span class="kw">if</span> it.latchedDoc, it.lastErr = it.idx.findByID(nextID); it.lastErr != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">false</span></a>
<a>}</a>

<a>it.cumIdx++</a>
<a>it.rsIdx++</a>
<a><span class="kw">return</span> <span class="ot">true</span></a></pre></div>
<p>To latch the next document from the result set, we extract its ID from the bleve result and look up the full document by invoking the<span> </span><kbd>findByID</kbd><span> </span>method on the in-memory index. As we saw in the previous section, the document lookup code always returns a <em>copy</em> of the indexed document that we can safely cache within the iterator. Lastly, both position-tracking counters are incremented and a <kbd>true</kbd> value is returned to the caller to indicate that the iterator has been successfully advanced and that the next document can be retrieved via a call to the iterator's <kbd>Document</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a test suite for the in-memory indexer</h1>
                </header>
            
            <article>
                
<p>The test suite for the in-memory indexer implementation embeds the shared test suite we outlined in the <em>Verifying indexer implementations using a shared test suite</em> section. Since the suite depends on the <kbd>gocheck</kbd> framework, we need to add some extra code to register the suite with the <kbd>go test</kbd> framework:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> _ = gc.Suite(<span class="bu">new</span>(InMemoryBleveTestSuite))</a>

<a><span class="kw">type</span> InMemoryBleveTestSuite <span class="kw">struct</span> {</a>
<a>    indextest.SuiteBase</a>
<a>    idx *InMemoryBleveIndexer</a>
<a>}</a>

<a><span class="co">// Register our test-suite with go test.</span></a>
<a><span class="kw">func</span> Test(t *testing.T) { gc.TestingT(t) }</a></pre></div>
<p>To ensure that each test uses a clean index instance, the suite provides a per-test setup method that recreates the index before running each test:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *InMemoryBleveTestSuite) SetUpTest(c *gc.C) {</a>
<a>    idx, err := NewInMemoryBleveIndexer()</a>
<a>    c.Assert(err, gc.IsNil)</a>
<a>    s.SetIndexer(idx)<br/></a><a>    // Keep track of the concrete indexer implementation so we can clean up <br/>    // when tearing down the test<br/>    s.idx = idx<br/></a><a>}</a> <br/><a><span class="kw">func</span> (s *InMemoryBleveTestSuite) TearDownTest(c *gc.C) {</a><a> c.Assert(s.idx.Close(), gc.IsNil)</a> <a>}</a></pre></div>
<p>Since bleve index instances are held in memory, we also need to define a per-test teardown method to ensure that the index is closed and that any acquired resources are freed after each test completes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling across an Elasticsearch indexer implementation</h1>
                </header>
            
            <article>
                
<p>A caveat of the in-memory bleve-based indexer implementation is that we are more or less limited to running our index on a single node. This not only introduces a single point of failure to our overall system design but it also places a hard limit on the amount of search traffic that our service can handle.</p>
<p>We could definitely argue that we could try to scale our implementation horizontally. At the time of writing, bleve does not provide any built-in mechanism for running in distributed mode; we would need to roll out a custom solution from scratch. One approach would be to create a multi-master setup. The idea here would be to spin up multiple instances of our index service and place them behind a<span> </span><em>gateway service</em><span> </span>that allows clients to access the index via an API. When clients provide a document for indexing, the gateway will ask<span> </span><em>all</em><span> the </span>index instances to process the document and will only return to the caller when all the instances have successfully indexed the document. On the other hand, the gateway can delegate incoming search requests to any random index instance in the pool. Given that searching is a read-intensive type of workload, the preceding approach would <em>probably</em> work nicely. I say probably because there are quite a few things that could possibly go wrong with such an implementation.</p>
<p>Building distributed systems is hard; figuring out how they behave when faults occur is even harder. We would definitely be better off using an off-the-self solution that has been battle-tested in large-scale production systems; preferably one whose failure modes (discovered via a framework such as Jepsen<span> </span><sup><span class="citation">[12]</span></sup>) are known and well understood. To this end, we will be basing our second indexer implementation on Elasticsearch<span> </span><sup><span class="citation">[9]</span></sup>. Here are some of the benefits of using Elasticsearch:</p>
<ul>
<li>We can run Elasticsearch on our own infrastructure or use one of the commercially available managed Elasticsearch SaaS offerings.</li>
<li>Elasticsearch has built-in support for clustering and can scale horizontally.</li>
<li>It exposes a REST API and clients are available for most popular programming languages. The client list includes an official Go client<span> </span><sup><span class="citation">[21]</span></sup><span> that </span>we will be using for our indexer implementation.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new Elasticsearch indexer instance</h1>
                </header>
            
            <article>
                
<p>To create a new Elasticsearch search indexer, clients need to invoke the<span> </span><kbd>NewElasticSearchIndexer</kbd><span> </span>constructor and provide a list of elastic search nodes to connect to. Our implementation will use the official Go client for Elasticsearch, which is provided by the <kbd>go-elastic</kbd> package <sup>[21]</sup>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> NewElasticSearchIndexer(esNodes []<span class="dt">string</span>) (*ElasticSearchIndexer, <span class="dt">error</span>) {</a>
<a>    cfg := elasticsearch.Config{</a>
<a>        Addresses: esNodes,</a>
<a>    }</a>
<a>    es, err := elasticsearch.NewClient(cfg)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>
<a>    <span class="kw">if</span> err = ensureIndex(es); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;ElasticSearchIndexer{</a>
<a>        es: es,</a>
<a>    }, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>After creating a new go-elastic client, the constructor invokes the<span> </span><kbd>ensureIndex</kbd><span> </span>helper, which checks whether the Elasticsearch index (the equivalent of a table, in DB terminology) that we will be using for storing our documents already exists. If not, the helper will automatically create it for us using the following set of field mappings (table schema, in DB terminology):</p>
<div class="sourceCode">
<pre class="sourceCode json"><a><span class="fu">{</span></a>
<a>  <span class="dt">"mappings"</span> <span class="fu">:</span> <span class="fu">{</span></a>
<a>    <span class="dt">"properties"</span><span class="fu">:</span> <span class="fu">{</span></a>
<a>      <span class="dt">"LinkID"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"keyword"</span><span class="fu">},</span></a>
<a>      <span class="dt">"URL"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"keyword"</span><span class="fu">},</span></a>
<a>      <span class="dt">"Content"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"text"</span><span class="fu">},</span></a>
<a>      <span class="dt">"Title"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"text"</span><span class="fu">},</span></a>
<a>      <span class="dt">"IndexedAt"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"date"</span><span class="fu">},</span></a>
<a>      <span class="dt">"PageRank"</span><span class="fu">:</span> <span class="fu">{</span><span class="dt">"type"</span><span class="fu">:</span> <span class="st">"double"</span><span class="fu">}</span></a>
<a>    <span class="fu">}</span></a>
<a>  <span class="fu">}</span></a>
<a><span class="fu">}</span></a></pre></div>
<div class="packt_infobox">Providing field mappings is not strictly required by Elasticsearch! In fact, the indexing engine is quite capable of inferring the types of each document field simply by analyzing their contents. However, if we explicitly provide the field mapping on our end, we not only force Elasticsearch to use a <em>specific indexer implementation</em> for each field type but we can also individually configure and fine-tune the behavior of each field indexer.</div>
<p>The preceding JSON document defines the following set of mappings:</p>
<ul>
<li>The<span> </span><kbd>LinkID</kbd><span> </span>and<span> </span><kbd>URL</kbd><span> </span>fields specify a<span> </span><kbd>keyword</kbd><span> </span>field type. This type instructs Elasticsearch to index them as a blob of text and is suited for queries such as <kbd>find the document whose LinkID is X</kbd>.</li>
<li>The<span> </span><kbd>Content</kbd><span> </span>and<span> </span><kbd>Title</kbd><span> </span>fields specify a<span> </span><kbd>text</kbd><span> </span>field type. Elasticsearch will use a special indexer that allows us to perform full-text searches against these fields.</li>
<li>The<span> </span><kbd>IndexedAt</kbd><span> </span>and<span> </span><kbd>PageRank</kbd><span> </span>fields are parsed and stored as date and double values.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing and looking up documents</h1>
                </header>
            
            <article>
                
<p>To upsert a document to the index, we need to submit an update operation to the Elasticsearch cluster. The update request's contents is populated using the following block of code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>esDoc := makeEsDoc(doc)</a>
<a>update := <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"doc"</span>:           esDoc,</a>
<a>    <span class="st">"doc_as_upsert"</span>: <span class="ot">true</span>,</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>makeEsDoc</kbd><span> </span>helper converts the input<span> </span><kbd>indexer.Document</kbd><span> </span>instance into a representation that Elasticsearch can process. It is important to note that the mapped document does not include a <kbd>PageRank</kbd> score value, even if that is present in the original docs. This is intentional as we only allow <kbd>PageRank</kbd> scores to be mutated via a call to<span> </span><kbd>UpdateScore</kbd>. The<span> </span><kbd>doc_as_upsert</kbd><span> </span>flag serves as a hint to Elasticsearch that it should create the document if it does not exist, that is, it should treat the update request as an upsert operation.</p>
<p class="mce-root"/>
<p>After populating the update document, we just need to serialize it into JSON, execute a<span> </span><em>synchronous</em><span> </span>update, and check for any reported errors:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> buf bytes.Buffer</a>
<a>err := json.NewEncoder(&amp;buf).Encode(doc)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> xerrors.Errorf(<span class="st">"index: %w"</span>, err)</a>
<a>}</a>

<a>res, err := i.es.Update(indexName, esDoc.LinkID, &amp;buf, i.es.Update.WithRefresh(<span class="st">"true"</span>))</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> xerrors.Errorf(<span class="st">"index: %w"</span>, err)</a>
<a>}</a>

<a><span class="kw">var</span> updateRes esUpdateRes</a>
<a><span class="kw">if</span> err = unmarshalResponse(res, &amp;updateRes); err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> xerrors.Errorf(<span class="st">"index: %w"</span>, err)</a>
<a>}</a></pre></div>
<p>When performing any API call to Elasticsearch using the go-elastic client, errors can be reported in two different ways:</p>
<ul>
<li>The client returns an error and a<span> </span><kbd>nil</kbd><span> </span>response value. This can happen, for instance, if the DNS resolution for the Elasticsearch nodes fails or if the client can't connect to any of the provided node addresses.</li>
<li>Elasticsearch sends a JSON response that contains a structured error as its payload.</li>
</ul>
<p>To deal with the latter case, we can use the handy<span> </span><kbd>unmarshalResponse</kbd><span> </span>helper, which checks for the presence of errors in the response and returns them as regular Go error values.</p>
<p>What about document lookups? This operation is modeled as a search query where we try to match a single document with a specific link ID value. Like any other request to the Elasticsearch cluster, search queries are specified as JSON documents that are sent to the cluster via an HTTP POST request. The<span> </span><kbd>FindByID</kbd><span> </span>implementation creates the search query inline by defining a nested block of<span> </span><kbd>map[string]interface{}</kbd><span> </span>items which are then serialized via a JSON encoder instance:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> buf bytes.Buffer</a>
<a>query := <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"query"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>        <span class="st">"match"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>            <span class="st">"LinkID"</span>: linkID.String(),</a>
<a>        },</a>
<a>    },</a>
<a>    <span class="st">"from"</span>: <span class="dv">0</span>,</a>
<a>    <span class="st">"size"</span>: <span class="dv">1</span>,</a>
<a>}</a>
<a><span class="kw">if</span> err := json.NewEncoder(&amp;buf).Encode(query); err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find by ID: %w"</span>, err)</a>
<a>}</a></pre></div>
<p>At this point, I would like to point out that I only opted to use an inline, <em>type-less</em> approach to define the search query for simplicity. Ideally, instead of using maps, you would define nested structs for each portion of the query. Besides the obvious benefits of working with typed values, one other important benefit of working with structs is that we can switch to a much more efficient JSON encoder implementation that doesn't require the use of<span> </span><em>reflection</em>. One such example is easyjson<span> </span><sup><span class="citation">[10]</span></sup><span>, </span>which utilizes code generation to create efficient JSON encoder/decoders and promises a 4x-5x increase in speed over the JSON encoder implementation that ships with the Go standard library.</p>
<p>After our query has been successfully serialized to JSON, we invoke the<span> </span><kbd>runSearch</kbd><span> </span>helper, which submits the query to Elasticsearch. The helper will then unserialize the obtained response into a nested struct while at the same time checking for the presence of errors:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>searchRes, err := runSearch(i.es, query)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find by ID: %w"</span>, err)</a>
<a>}</a>

<a><span class="kw">if</span> <span class="bu">len</span>(searchRes.Hits.HitList) != <span class="dv">1</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"find by ID: %w"</span>, index.ErrNotFound)</a>
<a>}</a>

<a>doc := mapEsDoc(&amp;searchRes.Hits.HitList[<span class="dv">0</span>].DocSource)</a></pre></div>
<p>If everything goes according to plan, we will receive a single result. The obtained result is then passed to the<span> </span><kbd>mapEsDoc</kbd><span> </span>helper, which converts it back into a<span> </span><kbd>Document</kbd><span> </span>model instance, as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> mapEsDoc(d *esDoc) *index.Document {</a>
<a>    <span class="kw">return</span> &amp;index.Document{</a>
<a>        LinkID:    uuid.MustParse(d.LinkID),</a>
<a>        URL:       d.URL,</a>
<a>        Title:     d.Title,</a>
<a>        Content:   d.Content,</a>
<a>        IndexedAt: d.IndexedAt.UTC(),</a>
<a>        PageRank:  d.PageRank,</a>
<a>    }</a>
<a>}</a></pre></div>
<p>As you can see in the preceding snippet, the majority of the fields are just copied over to the document with the exception of the<span> </span><kbd>LinkID</kbd><span> </span>field, which must be parsed from a string representation into a UUID value first. The converted document is then returned to the caller of the<span> </span><kbd>FindByID</kbd><span> </span>method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing paginated searches</h1>
                </header>
            
            <article>
                
<p>As you might expect from a product whose primary job is searching within documents, Elasticsearch supports a plethora of different query types, ranging from keyword-based searches to complex geospatial or time-based queries. Unfortunately, the syntax for specifying queries varies slightly, depending on the type of query that we wish to perform. </p>
<p>It turns out that, for our particular use case, we can get away with using the same query syntax for both keyword- and phrase-based queries. All we need to do is convert the <kbd>QueryType</kbd> provided by the caller into an Elasticsearch-specific value that we can plug into a predefined search template. To achieve this, the indexer implementation makes <span>use of the <em>switch</em> block to convert the incoming query type into a value that Elasticsearch can recognize and interpret:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> qtype <span class="dt">string</span></a>
<a><span class="kw">switch</span> q.Type {</a>
<a><span class="kw">case</span> index.QueryTypePhrase:</a>
<a>    qtype = <span class="st">"phrase"</span></a>
<a><span class="kw">default</span>:</a>
<a>    qtype = <span class="st">"best_fields"</span></a>
<a>}</a></pre></div>
<p>We can then proceed to assemble our search query in the (quite verbose) format that's expected by Elasticsearch using a series of nested<span> </span><kbd>map[string]interface{}</kbd><span> </span>values, as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>query := <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"query"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>        <span class="st">"function_score"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>            <span class="st">"query"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>                <span class="st">"multi_match"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>                    <span class="st">"type"</span>:   qtype,</a>
<a>                    <span class="st">"query"</span>:  q.Expression,</a>
<a>                    <span class="st">"fields"</span>: []<span class="dt">string</span>{<span class="st">"Title"</span>, <span class="st">"Content"</span>},</a>
<a>                },</a>
<a>            },</a>
<a>            <span class="st">"script_score"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>                <span class="st">"script"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>                    <span class="st">"source"</span>: <span class="st">"_score + doc['PageRank'].value"</span>,</a>
<a>                },</a>
<a>            },</a>
<a>        },</a>
<a>    },</a>
<a>    <span class="st">"from"</span>: q.Offset,</a>
<a>    <span class="st">"size"</span>: batchSize,</a>
<a>}</a></pre></div>
<p>To handle pagination of the matched results, the query specifies both the page offset and the page size via the<span> </span><kbd>from</kbd><span> </span>and<span> </span><kbd>size</kbd><span> </span>query fields.</p>
<p>The preceding query template demonstrates another very useful Elasticsearch feature:<span> </span><strong>score boosting</strong>. By default, Elasticsearch sorts the returned documents in terms of their<span> </span><em>relevance</em><span> </span>to the submitted query. For some kinds of queries, the default built-in relevance score calculation algorithm may not yield a meaningful value for sorting (for example, all the documents contain the search keywords and are assigned the same relevance score). To this end, Elasticsearch provides helpers for manipulating or even completely overriding the relevance scores of matched documents.</p>
<p>Our particular query template specifies a custom script that calculates the effective relevance score by <strong>aggregating</strong> the matched document's PageRank score and the query relevance score calculated by Elasticsearch <span>(exposed via the </span><kbd><span>_</span>score</kbd><span> field)</span>. This little trick ensures that documents with a higher <kbd>PageRank</kbd> score always sort higher in the set of results.</p>
<p>Just as we did for the<span> </span><kbd>FindByID</kbd><span> implementation</span>, we once again invoke the<span> </span><kbd>runSearch</kbd><span> </span>helper to submit a search request to Elasticsearch and unserialize the first page of returned results. If the operation succeeds, a new <kbd>esIterator</kbd><span> </span>instance is created and returned to the caller so that the results of the search query can be consumed:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>searchRes, err := runSearch(i.es, query)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, xerrors.Errorf(<span class="st">"search: %w"</span>, err)</a>
<a>}</a>

<a><span class="kw">return</span> &amp;esIterator{es: i.es, searchReq: query, rs: searchRes, cumIdx: q.Offset}, <span class="ot">nil</span></a></pre></div>
<p>In a similar fashion to its in-memory sibling, t<span>he</span><span> </span><kbd>esIterator</kbd><span> implementation maintains its own set of global and per-page counters for keeping track of its position within the result set returned by Elasticsearch. Each time the iterator's</span><span> </span><kbd>Next</kbd><span> method </span><span>is invoked, the iterator checks if an error has occurred or whether all the search results have been consumed. If this happens to be the case, then the call to</span><span> </span><kbd>Next</kbd><span> </span><span>returns</span><span> </span><kbd>false</kbd><span> </span><span>to notify the caller that no more results are available. </span></p>
<p class="mce-root"/>
<p>If the iterator hasn't exhausted the current page of results yet, it does the following:</p>
<ul>
<li>Both internal position-tracking counters are incremented</li>
<li>The next available result is converted into a<span> </span><kbd>Document</kbd><span> </span>model via a call to the<span> </span><kbd>mapEsDoc</kbd><span> </span>helper (see the previous section) and latched inside the iterator object</li>
<li>A <kbd>true</kbd> value is returned to the caller to indicate that the next result is available for retrieval via a call to the iterator's <kbd>Document</kbd> method</li>
</ul>
<p>Otherwise, if the end of the current page of results has been reached and more results are available, the iterator adjusts the offset field of the last search query and sends out a new search request to obtain the next page of results. </p>
<p><span>In the interest of brevity, we will not be listing the source code for the </span><kbd>esIterator</kbd><span> implementation here since it is almost identical to the in-memory indexer implementation that we've already examined. You can take a look at the fully documented source code for the iterator by opening the</span> <kbd>iterator.go</kbd> <span>file in this <kbd>Chapter06/textindexer/store/es</kbd> package, which is available in this book's GitHub repository.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the PageRank score for a document</h1>
                </header>
            
            <article>
                
<p>To update the <kbd>PageRank</kbd> score for an existing document, we need to construct an update request payload that the go-elastic client will submit to the Elasticsearch cluster via an HTTP POST request. The update payload includes a map with the fields names and values that need to be updated.</p>
<p>To facilitate document updates, the go-elastic client exposes an<span> </span><kbd>Update</kbd><span> </span>method that expects the following set of arguments:</p>
<ul>
<li>The name of the index that contains the document to be updated</li>
<li>The ID of the document to be updated</li>
<li>The document update payload encoded as JSON</li>
</ul>
<p>The following code snippet illustrates how the update request is assembled and passed to the<span> </span><kbd>Update</kbd><span> </span>method:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> buf bytes.Buffer</a>
<a>update := <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>    <span class="st">"doc"</span>: <span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">interface</span>{}{</a>
<a>        <span class="st">"LinkID"</span>:   linkID.String(),</a>
<a>        <span class="st">"PageRank"</span>: score,</a>
<a>    },</a>
<a>    <span class="st">"doc_as_upsert"</span>: <span class="ot">true</span>,</a>
<a>}</a>
<a><span class="kw">if</span> err := json.NewEncoder(&amp;buf).Encode(update); err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> xerrors.Errorf(<span class="st">"update score: %w"</span>, err)</a>
<a>}</a></pre></div>
<p>If the caller of the<span> </span><kbd>UpdateScore</kbd><span> </span>method provides a document link ID that does not exist, we want to be able to create a placeholder document containing just the<span> </span><kbd>LinkID</kbd><span> </span>and<span> </span><kbd>PageRank</kbd><span> </span>scores. This is facilitated by including the<span> </span><kbd>doc_as_upsert</kbd><span> </span>flag to our update payload.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a test suite for the Elasticsearch indexer</h1>
                </header>
            
            <article>
                
<p>The Elasticsearch-backed indexer implementation defines its own go-check test suite that embeds the shared indexer test suite and provides setup and teardown methods that are specific to the Elasticsearch implementation.</p>
<p>Each the tests in the suite use the same <kbd>ElasticSearchIndexer</kbd> instance that is initialized once with the following suite setup method:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *ElasticSearchTestSuite) SetUpSuite(c *gc.C) {</a>
<a>    nodeList := os.Getenv(<span class="st">"ES_NODES"</span>)</a>
<a>    <span class="kw">if</span> nodeList == <span class="st">""</span> {</a>
<a>        c.Skip(<span class="st">"Missing ES_NODES envvar; skipping elasticsearch-backed index test suite"</span>)</a>
<a>    }</a>

<a>    idx, err := NewElasticSearchIndexer(strings.Split(nodeList, <span class="st">","</span>))</a>
<a>    c.Assert(err, gc.IsNil)</a>
<a>    s.SetIndexer(idx)</a>
<a>    // Keep track of the concrete indexer implementation so we can access <br/>    // its internals when setting up the test<br/>    s.idx = idx</a>
<a>}</a></pre></div>
<p>Given the fact that Elasticsearch is quite a resource-intensive application, it stands to reason that you might not be running it locally on your dev machine. In anticipation of this, the suite setup code will check for the presence of the<span> </span><kbd>ES_NODES</kbd><span> </span>environment variable, which contains a comma-delimited list of Elasticsearch nodes to connect to. If the variable is not defined, then the entire test suite will be automatically skipped.</p>
<p class="mce-root"/>
<p>To guarantee that the tests don't interfere with each other, it is important to provide each test with a blank Elasticsearch index. To this end, before each test runs, a per-test setup method drops the Elasticsearch index and, by extension, any documents that were added to the index by the previous test runs:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (s *ElasticSearchTestSuite) SetUpTest(c *gc.C) {</a>
<a>    <span class="kw">if</span> s.idx.es != <span class="ot">nil</span> {</a>
<a>        _, err := s.idx.es.Indices.Delete([]<span class="dt">string</span>{indexName})</a>
<a>        c.Assert(err, gc.IsNil)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The remainder of the test suite code is responsible for registering the suite with the go-check framework and adding the appropriate hooks so that the suite can run when<span> </span><kbd>go test</kbd><span> </span>is invoked.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started laying the groundwork for the Links 'R' Us system by defining a data layer abstraction for the link graph and the text indexer components. Furthermore, as proof that our abstraction layer does indeed make it easy to swap the underlying implementation, we provided two compatible and fully testable implementations for each of the components.</p>
<p>In the next chapter, we will discuss strategies and patterns for building efficient data processing pipelines using Go and implement the web scraping component of the Links 'R' Us project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>What are the key differences between a relational database and a NoSQL database? Provide an example use case where a relational database would be a better fit than a NoSQL database and vice versa.</li>
<li>How would you scale a relational database system for a read-heavy and a write-heavy workload?</li>
<li>What is the CAP theorem and is it important when choosing which NoSQL implementation to use?</li>
</ol>
<ol start="4" type="1">
<li>Why is it important to provide an abstraction layer between our business logic and the underlying database?</li>
<li>How would you go about adding a new method to the<span> </span><kbd>Indexer</kbd><span> </span>interface we discussed in the last part of this chapter?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li>A modern text indexing library for Go. Available at:<span> </span><a href="https://github.com/blevesearch/bleve">https://github.com/blevesearch/bleve</a>.</li>
<li>Apache Cassandra: Manage massive amounts of data, fast, without losing sleep. Available at:<span> </span><a href="http://cassandra.apache.org">http://cassandra.apache.org</a>.</li>
<li>Apache CouchDB. Available at:<span> </span><a href="https://couchdb.apache.org">https://couchdb.apache.org</a>.</li>
<li><span class="smallcaps">Brewer, Eric A.</span>: <em>Towards Robust Distributed Systems.</em> In:<span> </span>Symposium on <strong>Principles of Distributed Computing</strong> (<strong>PODC</strong>), 2000.</li>
<li>CockroachDB: Ultra-resilient SQL for global business. Available at:<span> </span><a href="https://www.cockroachlabs.com">https://www.cockroachlabs.com</a>.</li>
<li><span class="smallcaps">Codd, E. F.</span>: <em>A Relational Model of Data for Large Shared Data Banks.</em> In:<span> </span>Commun. ACM<span> </span>Bd. 13. New York, NY, USA, ACM (1970), Nr. 6, S. 377–387.</li>
<li>Database migrations. CLI and Golang library. Available at:<span> </span><a href="https://github.com/golang-migrate/migrate">https://github.com/golang-migrate/migrate</a>.</li>
<li>DynamoDB: Fast and flexible NoSQL database service for any scale. Available at:<span> </span><a href="https://aws.amazon.com/dynamodb">https://aws.amazon.com/dynamodb</a>.</li>
<li>Elasticsearch: Open Source Search and Analytics. Available at:<span> </span><a href="https://www.elastic.co/">https://www.elastic.co/</a>.</li>
<li>Fast JSON serializer for golang. Available at:<span> </span><a href="https://github.com/mailru/easyjson">https://github.com/mailru/easyjson</a>.</li>
<li>gocheck: rich testing for the Go language. Available at:<span> </span><a href="http://labix.org/gocheck">http://labix.org/gocheck</a>.</li>
<li>Jepsen: Breaking distributed systems so you don't have to. Available at:<span> </span><a href="https://github.com/jepsen-io/jepsen">https://github.com/jepsen-io/jepsen</a>.</li>
<li>LevelDB: A fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Available at:<span> </span><a href="https://github.com/google/leveldb">https://github.com/google/leveldb</a>.</li>
<li><span class="smallcaps">Martin, Robert C.</span>:<span> </span>Clean Architecture: <em>A Craftsman's Guide to Software Structure and Design,</em><span> </span>Robert C. Martin Series. Boston, MA : Prentice Hall, 2017 — ISBN <a href="https://worldcat.org/isbn/978-0-13-449416-6">978-0-13-449416-6</a>.</li>
</ol>
<ol start="15">
<li>memcached: A distributed memory object caching system. Available at:<span> </span><a href="https://memcached.org">https://memcached.org</a>.</li>
<li>MongoDB: The most popular database for modern apps. Available at:<span> </span><a href="https://www.mongodb.com">https://www.mongodb.com</a>.</li>
<li>MySQL: The world's most popular open source database. Available at:<span> </span><a href="https://www.mysql.com">https://www.mysql.com</a>.</li>
<li>PostgreSQL: The world's most advanced open source relational database. Available at:<span> </span><a href="https://www.postgresql.org">https://www.postgresql.org</a>.</li>
<li>Pure Go Postgres driver for database/SQL. Available at:<span> </span><a href="https://github.com/lib/pq">https://github.com/lib/pq</a>.</li>
<li>RocksDB: An embeddable persistent key-value store for fast storage. Available at:<span> </span><a href="https://rocksdb.org">https://rocksdb.org</a>.</li>
<li>The official Go client for Elasticsearch. Available at:<span> </span><a href="https://github.com/elastic/go-elasticsearch">https://github.com/elastic/go-elasticsearch</a>.</li>
</ol>


            </article>

            
        </section>
    </body></html>