["```go\ncockroach start --insecure --advertise-addr 127.0.0.1:26257.\ncockroach sql --insecure -e 'CREATE DATABASE linkgraph;'\n```", "```go\nexport CDB_DSN='postgresql://root@localhost:26257/linkgraph?sslmode=disable'\n```", "```go\nbin/elasticsearch\n```", "```go\nexport ES_NODES='http://localhost:9200'\n```", "```go\ntype Graph interface {\n UpsertLink(link *Link) error\n FindLink(id uuid.UUID) (*Link, error)\n\n UpsertEdge(edge *Edge) error\n RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) error\n\n Links(fromID, toID uuid.UUID, retrievedBefore time.Time) (LinkIterator, error)\n Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (EdgeIterator, error)\n}\n```", "```go\ntype Link struct {\n ID          uuid.UUID\n URL         string RetrievedAt time.Time\n}\n```", "```go\ntype Edge struct {\n ID        uuid.UUID\n Src       uuid.UUID\n Dst       uuid.UUID\n UpdatedAt time.Time\n}\n```", "```go\n// LinkIterator is implemented by objects that can iterate the graph links.\ntype LinkIterator interface {\n Iterator\n\n // Link returns the currently fetched link object.\n Link() *Link\n}\n\n// EdgeIterator is implemented by objects that can iterate the graph edges.\ntype EdgeIterator interface {\n Iterator\n\n // Edge returns the currently fetched edge objects.\n Edge() *Edge\n}\n```", "```go\ntype Iterator interface {\n // Next advances the iterator. If no more items are available or an\n // error occurs, calls to Next() return false.\n Next() bool\n\n // Error returns the last error encountered by the iterator.\n Error() error\n\n // Close releases any resources associated with an iterator.\n Close() error\n}\n```", "```go\n// 'linkIt' is a link iterator\nfor linkIt.Next(){\n link := linkIt.Link()\n // Do something with link...\n}\n\nif err := linkIt.Error(); err != nil {\n // Handle error...\n}\n```", "```go\ntype edgeList []uuid.UUID\n\ntype InMemoryGraph struct {\n mu sync.RWMutex\n\n links map[uuid.UUID]*graph.Link\n edges map[uuid.UUID]*graph.Edge\n\n linkURLIndex map[string]*graph.Link\n linkEdgeMap  map[uuid.UUID]edgeList\n}\n```", "```go\nif link.ID == uuid.Nil {\n link.ID = existing.ID\n origTs := existing.RetrievedAt\n *existing = *link\n if origTs.After(existing.RetrievedAt) {\n existing.RetrievedAt = origTs\n }\n return nil\n}\n\n// Omitted: insert new link into the graph (see next block of code)...\n```", "```go\n// Insert new link into the graph\n// Assign new ID and insert link\nfor {\n link.ID = uuid.New()\n if s.links[link.ID] == nil {\n break\n }\n}\n\nlCopy := new(graph.Link)\n*lCopy = *link\ns.linkURLIndex[lCopy.URL] = lCopy\ns.links[lCopy.ID] = lCopy\nreturn nil\n```", "```go\ns.mu.Lock()\ndefer s.mu.Unlock()\n\n_, srcExists := s.links[edge.Src]\n_, dstExists := s.links[edge.Dst]\nif !srcExists || !dstExists {\n return xerrors.Errorf(\"upsert edge: %w\", graph.ErrUnknownEdgeLinks)\n}\n```", "```go\n// Scan edge list from source\nfor _, edgeID := range s.linkEdgeMap[edge.Src] {\n existingEdge := s.edges[edgeID]\n if existingEdge.Src == edge.Src && existingEdge.Dst == edge.Dst {\n existingEdge.UpdatedAt = time.Now()\n *edge = *existingEdge\n return nil\n }\n}\n```", "```go\nfor {\n edge.ID = uuid.New()\n if s.edges[edge.ID] == nil {\n break\n }\n}\n\nedge.UpdatedAt = time.Now()\neCopy := new(graph.Edge)\n*eCopy = *edge\ns.edges[eCopy.ID] = eCopy\n\n// Append the edge ID to the list of edges originating from the edge's source link.\ns.linkEdgeMap[edge.Src] = append(s.linkEdgeMap[edge.Src], eCopy.ID)\nreturn nil\n```", "```go\nfunc (s *InMemoryGraph) FindLink(id uuid.UUID) (*graph.Link, error) {\n s.mu.RLock()\n defer s.mu.RUnlock()\n\n link := s.links[id]\n if link == nil {\n return nil, xerrors.Errorf(\"find link: %w\", graph.ErrNotFound)\n }\n\n lCopy := new(graph.Link)\n *lCopy = *link\n return lCopy, nil\n}\n```", "```go\nfunc (s *InMemoryGraph) Links(fromID, toID uuid.UUID, retrievedBefore time.Time) (graph.LinkIterator, error) {\n from, to := fromID.String(), toID.String()\n\n s.mu.RLock()\n var list []*graph.Link\n for linkID, link := range s.links {\n if id := linkID.String(); id >= from && id < to && link.RetrievedAt.Before(retrievedBefore) {\n list = append(list, link)\n }\n }\n s.mu.RUnlock()\n\n return &linkIterator{s: s, links: list}, nil\n}\n```", "```go\ntype linkIterator struct {\n s *InMemoryGraph\n\n links    []*graph.Link\n curIndex int\n}\n```", "```go\nfunc (i *edgeIterator) Next() bool {\n if i.curIndex >= len(i.links) {\n return false\n }\n i.curIndex++\n return true\n}\n```", "```go\nfunc (i *linkIterator) Link() *graph.Link {\n i.s.mu.RLock()\n link := new(graph.Link)\n *link = *i.links[i.curIndex-1]\n i.s.mu.RUnlock()\n return link\n}\n```", "```go\nfunc (s *InMemoryGraph) Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (graph.EdgeIterator, error) {\n from, to := fromID.String(), toID.String()\n s.mu.RLock()\n var list []*graph.Edge\n for linkID := range s.links {\n if id := linkID.String(); id < from || id >= to {\n continue\n }\n for _, edgeID := range s.linkEdgeMap[linkID] {\n if edge := s.edges[edgeID]; edge.UpdatedAt.Before(updatedBefore) {\n list = append(list, edge)\n }\n }\n }\n s.mu.RUnlock()\n return &edgeIterator{s: s, edges: list}, nil\n}\n```", "```go\nfunc (s *InMemoryGraph) RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) error {\n s.mu.Lock()\n defer s.mu.Unlock()\n\n var newEdgeList edgeList\n for _, edgeID := range s.linkEdgeMap[fromID] {\n edge := s.edges[edgeID]\n if edge.UpdatedAt.Before(updatedBefore) {\n delete(s.edges, edgeID)\n continue\n }\n newEdgeList = append(newEdgeList, edgeID)\n }\n s.linkEdgeMap[fromID] = newEdgeList\n return nil\n}\n```", "```go\nvar _ = gc.Suite(new(InMemoryGraphTestSuite))\n\ntype InMemoryGraphTestSuite struct {\n graphtest.SuiteBase\n}\n\nfunc (s *InMemoryGraphTestSuite) SetUpTest(c *gc.C) {\n s.SetGraph(NewInMemoryGraph())\n}\n\n// Register our test-suite with go test.\nfunc Test(t *testing.T) { gc.TestingT(t) }\n```", "```go\ntimestamp-description-{up/down}.sql \n```", "```go\nCREATE TABLE IF NOT EXISTS links (\n id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n url STRING UNIQUE,\n retrieved_at TIMESTAMP\n);\n\nCREATE TABLE IF NOT EXISTS edges (\n id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n src UUID NOT NULL REFERENCES links(id) ON DELETE CASCADE,\n dst UUID NOT NULL REFERENCES links(id) ON DELETE CASCADE,\n updated_at TIMESTAMP,\n CONSTRAINT edge_links UNIQUE(src,dst)\n);\n```", "```go\nINSERT INTO links (url, retrieved_at) VALUES ($1, $2) \nON CONFLICT (url) DO UPDATE SET retrieved_at=GREATEST(links.retrieved_at, $2)\nRETURNING id, retrieved_at\n```", "```go\nfunc (c *CockroachDBGraph) UpsertLink(link *graph.Link) error {\n row := c.db.QueryRow(upsertLinkQuery, link.URL, link.RetrievedAt.UTC())\n if err := row.Scan(&link.ID, &link.RetrievedAt); err != nil {\n return xerrors.Errorf(\"upsert link: %w\", err)\n }\n\n link.RetrievedAt = link.RetrievedAt.UTC()\n return nil\n}\n```", "```go\nINSERT INTO edges (src, dst, updated_at) VALUES ($1, $2, NOW())\nON CONFLICT (src,dst) DO UPDATE SET updated_at=NOW()\nRETURNING id, updated_at\n```", "```go\nfunc (c *CockroachDBGraph) UpsertEdge(edge *graph.Edge) error {\n row := c.db.QueryRow(upsertEdgeQuery, edge.Src, edge.Dst)\n if err := row.Scan(&edge.ID, &edge.UpdatedAt); err != nil {\n if isForeignKeyViolationError(err) {\n err = graph.ErrUnknownEdgeLinks\n }\n return xerrors.Errorf(\"upsert edge: %w\", err)\n }\n\n edge.UpdatedAt = edge.UpdatedAt.UTC()\n return nil\n}\n```", "```go\nfunc isForeignKeyViolationError(err error) bool {\n pqErr, valid := err.(*pq.Error)\n if !valid {\n return false\n }\n return pqErr.Code.Name() == \"foreign_key_violation\"\n}\n```", "```go\nSELECT url, retrieved_at FROM links WHERE id=$1\"\n```", "```go\nfunc (c *CockroachDBGraph) FindLink(id uuid.UUID) (*graph.Link, error) {\n row := c.db.QueryRow(findLinkQuery, id)\n link := &graph.Link{ID: id}\n if err := row.Scan(&link.URL, &link.RetrievedAt); err != nil {\n if err == sql.ErrNoRows {\n return nil, xerrors.Errorf(\"find link: %w\", graph.ErrNotFound)\n }\n return nil, xerrors.Errorf(\"find link: %w\", err)\n }\n link.RetrievedAt = link.RetrievedAt.UTC()\n return link, nil\n}\n```", "```go\nSELECT id, url, retrieved_at FROM links WHERE id >= $1 AND id < $2 AND retrieved_at < $3\n```", "```go\nfunc (c *CockroachDBGraph) Links(fromID, toID uuid.UUID, accessedBefore time.Time) (graph.LinkIterator, error) {\n rows, err := c.db.Query(linksInPartitionQuery, fromID, toID, accessedBefore.UTC())\n if err != nil {\n return nil, xerrors.Errorf(\"links: %w\", err)\n }\n\n return &linkIterator{rows: rows}, nil\n}\n```", "```go\nfunc (i *linkIterator) Next() bool {\n if i.lastErr != nil || !i.rows.Next() {\n return false\n }\n\n l := new(graph.Link)\n i.lastErr = i.rows.Scan(&l.ID, &l.URL, &l.RetrievedAt)\n if i.lastErr != nil {\n return false\n }\n l.RetrievedAt = l.RetrievedAt.UTC()\n\n i.latchedLink = l\n return true\n}\n```", "```go\nSELECT id, src, dst, updated_at FROM edges WHERE src >= $1 AND src < $2 AND updated_at < $3\"\n```", "```go\nfunc (c *CockroachDBGraph) Edges(fromID, toID uuid.UUID, updatedBefore time.Time) (graph.EdgeIterator, error) {\n rows, err := c.db.Query(edgesInPartitionQuery, fromID, toID, updatedBefore.UTC())\n if err != nil {\n return nil, xerrors.Errorf(\"edges: %w\", err)\n }\n\n return &edgeIterator{rows: rows}, nil\n}\n```", "```go\nDELETE FROM edges WHERE src=$1 AND updated_at < $2\n```", "```go\nfunc (c *CockroachDBGraph) RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) error {\n _, err := c.db.Exec(removeStaleEdgesQuery, fromID, updatedBefore.UTC())\n if err != nil {\n return xerrors.Errorf(\"remove stale edges: %w\", err)\n }\n\n return nil\n}\n```", "```go\nvar _ = gc.Suite(new(CockroachDBGraphTestSuite))\n\ntype CockroachDBGraphTestSuite struct {\n graphtest.SuiteBase\n db *sql.DB\n}\n\n// Register our test-suite with go test.\nfunc Test(t *testing.T) { gc.TestingT(t) }\n```", "```go\nfunc (s *CockroachDBGraphTestSuite) SetUpSuite(c *gc.C) {\n dsn := os.Getenv(\"CDB_DSN\")\n if dsn == \"\" {\n c.Skip(\"Missing CDB_DSN envvar; skipping cockroachdb-backed graph test suite\")\n }\n\n g, err := NewCockroachDBGraph(dsn)\n c.Assert(err, gc.IsNil)\n s.SetGraph(g)\n\n // keep track of the sql.DB instance so we can execute SQL statements \n // to reset the DB between tests!\n s.db = g.db\n}\n```", "```go\nfunc (s *CockroachDBGraphTestSuite) SetUpTest(c *gc.C) { s.flushDB(c) }\n\nfunc (s *CockroachDBGraphTestSuite) flushDB(c *gc.C) {\n _, err := s.db.Exec(\"DELETE FROM links\")\n c.Assert(err, gc.IsNil)\n _, err = s.db.Exec(\"DELETE FROM edges\")\n c.Assert(err, gc.IsNil)\n}\n```", "```go\nfunc (s *CockroachDBGraphTestSuite) TearDownSuite(c *gc.C) {\n if s.db != nil {\n s.flushDB(c)\n c.Assert(s.db.Close(), gc.IsNil)\n }\n}\n```", "```go\ntype Document struct {\n LinkID uuid.UUID\n\n URL string\n\n Title string\n Content string\n\n IndexedAt time.Time\n PageRank float64\n}\n```", "```go\ntype Indexer interface {\n Index(doc *Document) error\n FindByID(linkID uuid.UUID) (*Document, error)\n Search(query Query) (Iterator, error)\n UpdateScore(linkID uuid.UUID, score float64) error\n}\n```", "```go\ntype Query struct {\n Type       QueryType\n Expression string\n Offset     uint64\n}\n\ntype QueryType uint8\n\nconst (\n QueryTypeMatch QueryType = iota\n QueryTypePhrase\n)\n```", "```go\ntype Iterator interface {\n // Close the iterator and release any allocated resources.\n Close() error\n\n // Next loads the next document matching the search query.\n // It returns false if no more documents are available.\n Next() bool\n\n // Error returns the last error encountered by the iterator.\n Error() error\n\n // Document returns the current document from the result set.\n Document() *Document\n\n // TotalCount returns the approximate number of search results.\n TotalCount() uint64\n}\n```", "```go\n// 'docIt' is a search iterator\nfor docIt.Next() {\n doc := docIt.Document()\n // Do something with doc...\n}\n\nif err := docIt.Error(); err != nil {\n // Handle error...\n}\n```", "```go\ntype InMemoryBleveIndexer struct {\n mu   sync.RWMutex\n docs map[string]*index.Document\n\n idx bleve.Index\n}\n```", "```go\nfunc (i *InMemoryBleveIndexer) Index(doc *index.Document) error {\n if doc.LinkID == uuid.Nil {\n return xerrors.Errorf(\"index: %w\", index.ErrMissingLinkID)\n }\n doc.IndexedAt = time.Now()\n dcopy := copyDoc(doc)\n key := dcopy.LinkID.String()\n i.mu.Lock()\n if orig, exists := i.docs[key]; exists {\n dcopy.PageRank = orig.PageRank\n }\n if err := i.idx.Index(key, makeBleveDoc(dcopy)); err != nil {\n return xerrors.Errorf(\"index: %w\", err)\n }\n i.docs[key] = dcopy\n i.mu.Unlock()\n return nil\n}\n```", "```go\nfunc (i *InMemoryBleveIndexer) FindByID(linkID uuid.UUID) (*index.Document, error) {\n return i.findByID(linkID.String())\n}\n\nfunc (i *InMemoryBleveIndexer) findByID(linkID string) (*index.Document, error) {\n i.mu.RLock()\n defer i.mu.RUnlock()\n\n if d, found := i.docs[linkID]; found {\n return copyDoc(d), nil\n }\n\n return nil, xerrors.Errorf(\"find by ID: %w\", index.ErrNotFound)\n}\n```", "```go\nfunc (i *InMemoryBleveIndexer) UpdateScore(linkID uuid.UUID, score float64) error {\n i.mu.Lock()\n defer i.mu.Unlock()\n key := linkID.String()\n doc, found := i.docs[key]\n if !found {\n doc = &index.Document{LinkID: linkID}\n i.docs[key] = doc\n }\n\n doc.PageRank = score\n if err := i.idx.Index(key, makeBleveDoc(doc)); err != nil {\n return xerrors.Errorf(\"update score: %w\", err)\n }\n return nil\n}\n```", "```go\nfunc (i *InMemoryBleveIndexer) Search(q index.Query) (index.Iterator, error) {\n var bq query.Query\n switch q.Type {\n case index.QueryTypePhrase:\n bq = bleve.NewMatchPhraseQuery(q.Expression)\n default:\n bq = bleve.NewMatchQuery(q.Expression)\n }\n\n searchReq := bleve.NewSearchRequest(bq)\n searchReq.SortBy([]string{\"-PageRank\", \"-_score\"})\n searchReq.Size = batchSize\n searchReq.From = q.Offset\n rs, err := i.idx.Search(searchReq)\n if err != nil {\n return nil, xerrors.Errorf(\"search: %w\", err)\n }\n return &bleveIterator{idx: i, searchReq: searchReq, rs: rs, cumIdx: q.Offset}, nil\n}\n```", "```go\ntype bleveIterator struct {\n idx       *InMemoryBleveIndexer\n searchReq *bleve.SearchRequest\n\n cumIdx uint64\n rsIdx  int\n rs     *bleve.SearchResult\n\n latchedDoc *index.Document\n lastErr    error\n}\n```", "```go\nif it.lastErr != nil || it.rs == nil || it.cumIdx >= it.rs.Total {\n return false\n}\n```", "```go\nif it.rsIdx >= it.rs.Hits.Len() {\n it.searchReq.From += it.searchReq.Size\n if it.rs, it.lastErr = it.idx.idx.Search(it.searchReq); it.lastErr != nil {\n return false\n }\n it.rsIdx = 0\n}\n\nnextID := it.rs.Hits[it.rsIdx].ID\nif it.latchedDoc, it.lastErr = it.idx.findByID(nextID); it.lastErr != nil {\n return false\n}\n\nit.cumIdx++\nit.rsIdx++\nreturn true\n```", "```go\nvar _ = gc.Suite(new(InMemoryBleveTestSuite))\n\ntype InMemoryBleveTestSuite struct {\n indextest.SuiteBase\n idx *InMemoryBleveIndexer\n}\n\n// Register our test-suite with go test.\nfunc Test(t *testing.T) { gc.TestingT(t) }\n```", "```go\nfunc (s *InMemoryBleveTestSuite) SetUpTest(c *gc.C) {\n idx, err := NewInMemoryBleveIndexer()\n c.Assert(err, gc.IsNil)\n s.SetIndexer(idx) // Keep track of the concrete indexer implementation so we can clean up \n    // when tearing down the test\n    s.idx = idx } \nfunc (s *InMemoryBleveTestSuite) TearDownTest(c *gc.C) { c.Assert(s.idx.Close(), gc.IsNil) }\n```", "```go\nfunc NewElasticSearchIndexer(esNodes []string) (*ElasticSearchIndexer, error) {\n cfg := elasticsearch.Config{\n Addresses: esNodes,\n }\n es, err := elasticsearch.NewClient(cfg)\n if err != nil {\n return nil, err\n }\n if err = ensureIndex(es); err != nil {\n return nil, err\n }\n\n return &ElasticSearchIndexer{\n es: es,\n }, nil\n}\n```", "```go\n{\n \"mappings\" : {\n \"properties\": {\n \"LinkID\": {\"type\": \"keyword\"},\n \"URL\": {\"type\": \"keyword\"},\n \"Content\": {\"type\": \"text\"},\n \"Title\": {\"type\": \"text\"},\n \"IndexedAt\": {\"type\": \"date\"},\n \"PageRank\": {\"type\": \"double\"}\n }\n }\n}\n```", "```go\nesDoc := makeEsDoc(doc)\nupdate := map[string]interface{}{\n \"doc\":           esDoc,\n \"doc_as_upsert\": true,\n}\n```", "```go\nvar buf bytes.Buffer\nerr := json.NewEncoder(&buf).Encode(doc)\nif err != nil {\n return xerrors.Errorf(\"index: %w\", err)\n}\n\nres, err := i.es.Update(indexName, esDoc.LinkID, &buf, i.es.Update.WithRefresh(\"true\"))\nif err != nil {\n return xerrors.Errorf(\"index: %w\", err)\n}\n\nvar updateRes esUpdateRes\nif err = unmarshalResponse(res, &updateRes); err != nil {\n return xerrors.Errorf(\"index: %w\", err)\n}\n```", "```go\nvar buf bytes.Buffer\nquery := map[string]interface{}{\n \"query\": map[string]interface{}{\n \"match\": map[string]interface{}{\n \"LinkID\": linkID.String(),\n },\n },\n \"from\": 0,\n \"size\": 1,\n}\nif err := json.NewEncoder(&buf).Encode(query); err != nil {\n return nil, xerrors.Errorf(\"find by ID: %w\", err)\n}\n```", "```go\nsearchRes, err := runSearch(i.es, query)\nif err != nil {\n return nil, xerrors.Errorf(\"find by ID: %w\", err)\n}\n\nif len(searchRes.Hits.HitList) != 1 {\n return nil, xerrors.Errorf(\"find by ID: %w\", index.ErrNotFound)\n}\n\ndoc := mapEsDoc(&searchRes.Hits.HitList[0].DocSource)\n```", "```go\nfunc mapEsDoc(d *esDoc) *index.Document {\n return &index.Document{\n LinkID:    uuid.MustParse(d.LinkID),\n URL:       d.URL,\n Title:     d.Title,\n Content:   d.Content,\n IndexedAt: d.IndexedAt.UTC(),\n PageRank:  d.PageRank,\n }\n}\n```", "```go\nvar qtype string\nswitch q.Type {\ncase index.QueryTypePhrase:\n qtype = \"phrase\"\ndefault:\n qtype = \"best_fields\"\n}\n```", "```go\nquery := map[string]interface{}{\n \"query\": map[string]interface{}{\n \"function_score\": map[string]interface{}{\n \"query\": map[string]interface{}{\n \"multi_match\": map[string]interface{}{\n \"type\":   qtype,\n \"query\":  q.Expression,\n \"fields\": []string{\"Title\", \"Content\"},\n },\n },\n \"script_score\": map[string]interface{}{\n \"script\": map[string]interface{}{\n \"source\": \"_score + doc['PageRank'].value\",\n },\n },\n },\n },\n \"from\": q.Offset,\n \"size\": batchSize,\n}\n```", "```go\nsearchRes, err := runSearch(i.es, query)\nif err != nil {\n return nil, xerrors.Errorf(\"search: %w\", err)\n}\n\nreturn &esIterator{es: i.es, searchReq: query, rs: searchRes, cumIdx: q.Offset}, nil\n```", "```go\nvar buf bytes.Buffer\nupdate := map[string]interface{}{\n \"doc\": map[string]interface{}{\n \"LinkID\":   linkID.String(),\n \"PageRank\": score,\n },\n \"doc_as_upsert\": true,\n}\nif err := json.NewEncoder(&buf).Encode(update); err != nil {\n return xerrors.Errorf(\"update score: %w\", err)\n}\n```", "```go\nfunc (s *ElasticSearchTestSuite) SetUpSuite(c *gc.C) {\n nodeList := os.Getenv(\"ES_NODES\")\n if nodeList == \"\" {\n c.Skip(\"Missing ES_NODES envvar; skipping elasticsearch-backed index test suite\")\n }\n\n idx, err := NewElasticSearchIndexer(strings.Split(nodeList, \",\"))\n c.Assert(err, gc.IsNil)\n s.SetIndexer(idx)\n // Keep track of the concrete indexer implementation so we can access \n    // its internals when setting up the test\n    s.idx = idx\n}\n```", "```go\nfunc (s *ElasticSearchTestSuite) SetUpTest(c *gc.C) {\n if s.idx.es != nil {\n _, err := s.idx.es.Indices.Delete([]string{indexName})\n c.Assert(err, gc.IsNil)\n }\n}\n```"]