<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data-Processing Pipelines</h1>
                </header>
            
            <article>
                
<div class="packt_quote">"Inside every well-written large program is a well-written small program."</div>
<div class="packt_quote CDPAlignRight CDPAlign">- Tony Hoare</div>
<p>Pipelines are a fairly standard and used way to segregate the processing of data into multiple stages. In this chapter, we will be exploring the basic principles behind data-processing pipelines and present a blueprint for implementing generic, concurrent-safe, and reusable pipelines using Go primitives, such as channels, contexts, and go-routines.</p>
<p>In this chapter, you will learn about the following:</p>
<ul>
<li>Designing a generic processing pipeline from scratch using Go primitives</li>
<li>Approaches to modeling pipeline payloads in a generic way</li>
<li>Strategies for dealing with errors that can occur while a pipeline is executing</li>
<li>Pros and cons of synchronous and asynchronous pipeline design</li>
<li>Applying pipeline design concepts to building the Links 'R' Us crawler component</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The full code for the topics discussed in this chapter has been published to this book's GitHub repository under the<span> </span><kbd>Chapter07</kbd> folder.</p>
<div class="packt_infobox">You can access the GitHub repository that contains the code and all required resources for each of this book's chapters by going to <a href="https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang">https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang</a>.</div>
<p>To get you up and running as quickly as possible, each example project includes a<span> makefile</span><span> </span><span>that defines the following set of targets:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Makefile target</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="odd">
<td><kbd>deps</kbd></td>
<td>Install any required dependencies</td>
</tr>
<tr class="even">
<td><kbd>test</kbd></td>
<td>Run all tests and report coverage</td>
</tr>
<tr class="odd">
<td><kbd>lint</kbd></td>
<td>Check for lint errors</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As with all other book chapters, you will need a fairly recent version of Go, which you can download at<span> </span><a href="https://golang.org/dl">https://golang.org/dl</a><em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a generic data-processing pipeline in Go</h1>
                </header>
            
            <article>
                
<p>The following figure<span> </span>illustrates the high-level design of the pipeline that we will be building throughout the first half of this chapter:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c88d9228-c7fa-46e8-813d-1e7464d29804.png" style="width:57.42em;height:19.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">F<span>igure 1:</span><span> </span>A generic, multistage pipeline</div>
<p>Keep in mind that this is definitely not the only, or necessarily the best, way to go about implementing a data-processing pipeline. Pipelines are inherently application specific, so there is not really a one-size-fits-all guide for constructing efficient pipelines.</p>
<p class="mce-root"/>
<p>Having said that, the proposed design is applicable to a wide variety of use cases, including, but not limited to, the crawler component for the Links 'R' Us project. Let's examine the preceding figure<span> </span>in a bit more detail and identify the basic components that the pipeline comprises:</p>
<ul>
<li>The<span> </span><strong>input source</strong>: Inputs essentially function as data-sources that pump data into the pipeline. From this point onwards, we will be referring to this set of data with the term<span> </span><strong>payload</strong>. Under the hood, inputs facilitate the role of an<span> </span><strong>adapter</strong>, reading data typically available in an external system, such as a database or message queue, and converting it into a format that can be consumed by the pipeline.</li>
<li>One or more processing<span> </span><strong>stages</strong>: Each stage of the pipeline receives a payload as its input, applies a processing function to it, and passes the result to the stage that follows.</li>
<li>The<span> </span><strong>output sink</strong>: After stepping through each of the pipeline's stages, payloads eventually reach the output sink. In a similar fashion to input sources, sinks also work as<span> </span><strong>adapters</strong>, only this time the conversion works in reverse! Payloads are converted into a format that can be consumed by an external system.</li>
<li>An<span> </span><strong>error bus</strong>: The error bus provides a convenient abstraction that allows the pipeline components to report any errors that occur while the pipeline is executing.</li>
</ul>
<p>The full source code and tests for the pipeline are available at the book's GitHub repository under the<span> </span><kbd>Chapter07/pipeline</kbd><span> </span>folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Design goals for the pipeline package</h1>
                </header>
            
            <article>
                
<p>Let's quickly enumerate some of the design goals for the<span> </span><kbd>pipeline</kbd><span> </span>package that we will be building. The key principles that will serve as guides for the design decisions that we will be making are: simplicity, extensibility, and genericness.</p>
<p>First and foremost, our design should be able to adapt to different types of payloads. Keep in mind that payload formats are, in the majority of cases, dictated by the end user of the pipeline package. Consequently, the pipeline internals should not make any assumptions about the internal implementation details of payloads that traverse the various pipeline stages.</p>
<p class="mce-root"/>
<p>Secondly, the main role of a data-processing pipeline is to facilitate the flow of payloads between a source and a sink. In a similar manner to payloads, the endpoints of a pipeline are also provided by the end user. As a result, the pipeline package needs to define the appropriate abstractions and interfaces for allowing the end users to register their own source and sink implementations.</p>
<p>Moreover, the pipeline package should go beyond just allowing the end users to specify a processing function for each stage. Users should also be able to choose, on a per-stage basis, the strategy used by the pipeline for delivering payloads to processing functions. It stands to reason that the package should come with <em>batteries included–</em><span>that is, provide built-in implementations for the most common payload delivery strategies; however, the user should be given the flexibility to define their own custom strategies if the built-in ones are not sufficient for their particular use cases.</span></p>
<p>Finally, our implementation must expose simple and straightforward APIs for creating, assembling, and executing complex pipelines. Furthermore, the API dealing with the pipeline execution should not only provide users with the means to cancel long-running pipelines, but it should also provide a mechanism for capturing and reporting any errors that might occur while the pipeline is busy processing payloads.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling pipeline payloads</h1>
                </header>
            
            <article>
                
<p>The first and most crucial question we need to answer before we begin working on the pipeline package implementation is<span> </span><em>how can we describe pipeline payloads in a generic way using Go?</em></p>
<p>The kind of obvious answer to this question is to define payloads as empty interface values (an<span> </span><kbd>interface{}</kbd><span> </span>in Go terminology). The key argument in favor of this approach is that the pipeline internals shouldn't really care about payloads per se; all the pipeline needs to do is shuttle payloads between the various pipeline stages.</p>
<p>The interpretation of the payload contents (for example, by casting the input to a known type) should be the sole responsibility of the processing functions that execute at each stage. Given that the processing functions are specified by the end user of the pipeline, this approach would probably be a good fit for our particular requirements.</p>
<p>However, as Rob Pike quite eloquently puts it in one of his famous Go proverbs,<span> </span><kbd>interface{}</kbd> <em>says nothing</em>. There is quite a bit of truth in that statement. The empty interface conveys no useful information about the underlying type. As a matter of fact, if we were to follow the empty interface approach, we would be effectively disabling the Go compiler's ability to do static type checking of some parts of our code base!</p>
<p class="mce-root"/>
<p>On one hand, the use of empty interfaces is generally considered an antipattern by the Go community and is therefore a practice we would ideally want to avoid. On the other hand, Go has no support for generics, which makes it much more difficult to write code that can work with objects whose type is not known in advance. So, instead of trying to find a silver bullet solution to this problem, let's try to compromise: how about we try to enforce a set of common operations that all payload types must support and create a<span> </span><kbd>Payload</kbd><span> </span>interface to describe them? That would give us an extra layer of type-safety while still making it possible for pipeline processor functions to cast incoming payloads to the type they expect. Here is a possible definition for the<span> </span><kbd>Payload</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Payload is implemented by values that can be sent through a pipeline.</span></a>
<a><span class="kw">type</span> Payload <span class="kw">interface</span> {</a>
<a>    Clone() Payload</a>
<a>    MarkAsProcessed()</a>
<a>}</a></pre></div>
<p>As you can see, we expect that, regardless of the way that a payload is defined, it must be able to perform at least two simple (and quite common) operations:</p>
<ul>
<li><strong>Perform a deep-copy of itself</strong>: As we will see in one of the following sections, this operation will be required for avoiding data races when multiple processors are operating on the same payload concurrently.</li>
<li><strong>Mark itself as processed</strong>: Payloads are considered to be processed when they either reach the end of the pipeline (the sink) or if they are discarded at an intermediate pipeline stage. Having such a method invoked on payloads when they exit the pipeline is quite useful for scenarios where we are interested in collecting per-payload metrics (total processing time, time spent in the queue before entering the pipeline, and so on).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multistage processing</h1>
                </header>
            
            <article>
                
<p>The key concept behind pipelining is to break down a complex processing task into a series of smaller steps or<span> </span><strong>stages<span> </span></strong>that can be executed<span> </span><em>independently</em><span> </span>of each other and in a<span> </span><em>predefined order</em>. Multistage processing, as an idea, also seems to resonate quite well with the single-responsibility principle that we discussed in <a href="96fb70cb-8134-4156-bd3e-48ca53224683.xhtml">Chapter 2</a>, <em>Best Practices for Writing Clean and Maintainable Go Code</em>.</p>
<p>When assembling a multistage pipeline, the end user is expected to provide a set of functions, or<span> </span><strong>processors</strong>,<strong> </strong>that will be applied to incoming payloads as they flow through each stage of the pipeline. I will be referring to these functions with the notation<span> </span><em>F<sub>i</sub></em>, where<span> </span><em>i</em><span> </span>corresponds to a stage number.</p>
<p class="mce-root"/>
<p>Under normal circumstances, the output of each stage will be used as input by the stage that follows—that is <em>Output<sub>i</sub><span> </span>= F<sub>i</sub>( Output<sub>i-1</sub><span> </span>)</em>. Yet, one could definitely picture scenarios where we would actually like to discard a payload and prevent it from reaching any of the following pipeline stages.</p>
<p>For example, let's say we are building a pipeline to read and aggregate data from a CSV file. Unfortunately, the file contains some rows with garbage data that we must exclude from our calculations. To deal with cases like this, we can add a<span> </span><strong>filter stage</strong><span> </span>to the pipeline that inspects the contents of each row and drops the ones containing malformed data.</p>
<p>With the preceding cases in mind, we can describe a stage<span> </span><kbd>Processor</kbd><span> </span>interface as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Processor <span class="kw">interface</span> {</a>
<a>    <span class="co">// Process operates on the input payload and returns back a new payload</span></a>
<a>    <span class="co">// to be forwarded to the next pipeline stage. Processors may also opt</span></a>
<a>    <span class="co">// to prevent the payload from reaching the rest of the pipeline by</span></a>
<a>    <span class="co">// returning a nil payload value instead.</span></a>
<a>    Process(context.Context, Payload) (Payload, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>There is a small issue with the preceding definition that makes it a bit cumbersome to use in practice. Since we are talking about an interface, it needs to be implemented by a type such as a Go<span> </span>struct; however, one could argue that in many cases, all we really need is to be able to use a simple function, or a<span> </span><strong>closure</strong><strong> </strong>as our processor.</p>
<p>Given that we are designing a<span> </span><em>generic</em><span> </span>pipeline package, our aim should be to make its API as convenient as possible for the end users. To this end, we will also define an auxiliary type called <kbd>ProcessorFunc</kbd> that serves the role of a function<span> </span><em>adapter</em>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> ProcessorFunc <span class="kw">func</span>(context.Context, Payload) (Payload, <span class="dt">error</span>)</a>

<a><span class="co">// Process calls f(ctx, p).</span></a>
<a><span class="kw">func</span> (f ProcessorFunc) Process(ctx context.Context, p Payload) (Payload, <span class="dt">error</span>) {</a>
<a>    <span class="kw">return</span> f(ctx, p)</a>
<a>}</a></pre></div>
<p>If we have a function with the<span> </span>appropriate signature, we can cast it to a<span> </span><kbd>ProcessorFunc</kbd><span> </span>and automatically obtain a type that implements the<span> </span><kbd>Processor</kbd><span> </span>interface! If this trick seems vaguely familiar to you, chances are that you have already used it before if you have written any code that imports the<span> </span><kbd>http</kbd><span> </span>package and registers HTTP handlers. The<span> </span><kbd>HandlerFunc</kbd><span> </span>type from the<span> </span><kbd>http</kbd><span> </span>package uses exactly the same idea to convert user-defined functions into valid HTTP <kbd>Handler</kbd><span> </span>instances.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stageless pipelines – is that even possible?</h1>
                </header>
            
            <article>
                
<p>Should a pipeline definition include a minimum number of stages for it to be considered as valid? More specifically, should we be allowed to define a pipeline with<span> </span><em>zero</em><span> </span>stages? In my view, stages should be considered as an optional part of a pipeline definition. Remember that for a pipeline to function, it requires, at minimum, an input source and an output sink.</p>
<p>If we were to directly connect the input to the output and execute the pipeline, we would get the same result as if we had executed a pipeline with just a single stage whose<span> </span><kbd>Processor</kbd><span> </span>is an<span> </span><em>identity </em>function—that is, a function that always outputs the value passed to it as input. We could easily define such a function using the<span> </span><kbd>ProcessorFunc</kbd><span> </span>helper from the previous section:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>identityFn := ProcessorFunc( func(_ context.Context, p Payload) (Payload, error) {  return p, nil },)</a></pre></div>
<p>Is there a practical real-world use for this kind of pipeline? The answer is yes! Such a pipeline facilitates the role of an adapter for linking together two, potentially incompatible, systems and transferring data between them. For example, we could use this approach for reading events off a message queue and persisting them into a noSQL database for further processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategies for handling errors</h1>
                </header>
            
            <article>
                
<p>As a pipeline executes, each one of the components that comprise it may potentially encounter errors. Consequently, prior to implementing the internals of our pipeline package, we need to devise a strategy for detecting, collecting, and handling errors.</p>
<p>In the following sections, we will be exploring some alternative strategies for dealing with errors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accumulating and returning all errors</h1>
                </header>
            
            <article>
                
<p>One of the simplest strategies at our disposal involves the introduction of a mechanism for collecting and accumulating <em>all</em> errors emitted by any of the pipeline components while the pipeline is executing. Once the pipeline detects an error, it automatically discards the payload that triggered the error, but appends the captured error to a list of collected errors. The pipeline resumes its execution with the next payload till all payloads have been processed.</p>
<p class="mce-root"/>
<p>After the pipeline execution completes, any collected errors are returned back to the user. At this point, we have the option to either return a slice of Go error values or use a helper package, such as<span> </span><kbd>hashicorp/go-multierror</kbd><span> </span><sup><span class="citation">[6]</span></sup><span>, </span>which allows us to aggregate a list of Go error values into a container value that implements the<span> </span><kbd>error</kbd><span> </span>interface.</p>
<p>A great candidate for this type of error handling is pipelines where the processors implement best-effort semantics. For example, if we were building a pipeline to pump out events in a fire-and-forget manner, we wouldn't want the pipeline to stop if one of the events could not be published.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a dead-letter queue</h1>
                </header>
            
            <article>
                
<p>In some scenarios, the user of the pipeline package might be interested in obtaining a list of all the payloads that could not be processed by the pipeline because of the presence of errors.</p>
<p>The following points apply, depending on the application requirements:</p>
<ul>
<li>Detailed information about each error and the content of each failed payload can be logged out for further analysis</li>
<li>Failed payloads can be persisted to an external system (for example, via a messaging queue) so that they can be manually inspected and corrected (when feasible) by human operators</li>
<li>We could start a new pipeline run to process the payloads that failed during the previous run</li>
</ul>
<p>The concept of storing failed items for future processing is quite prevalent in event-driven architectures, and is typically referred to as the <strong>dead-letter queue</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Terminating the pipeline's execution if an error occurs</h1>
                </header>
            
            <article>
                
<p>One important caveat of the previous strategies is that they cannot be applied to <em>long-running</em> pipelines. Even if an error occurs, we will not find out about it until the pipeline completes. This could take hours, days, or even forever if the pipeline's input never runs out of data. An example of the latter case would be a pipeline whose input is connected to a message queue and blocks while waiting for new messages to arrive.</p>
<p>To deal with such scenarios, we could <em>immediately</em><span> </span>terminate the pipeline's execution when an error occurs and return the error back to the user. As a matter of fact, this is the error-handling strategy that we will be using in our pipeline implementation.</p>
<p class="mce-root"/>
<p>At first glance, you could argue that this approach is quite limiting compared to the other strategies we have discussed so far; however, if we dig a bit deeper, we will discover that this approach is better suited for a greater number of use cases, as it is versatile enough to emulate the behavior of the other two error-handling strategies.</p>
<p>To gain a better understanding of how this can be achieved, we first need to talk a bit about the nature of errors that might occur while a pipeline is executing. Depending on whether errors are fatal, we can classify them into two categories:</p>
<ul>
<li><strong>Nontransient<span> </span>errors</strong>: Such errors are considered to be fatal and applications cannot really recover from them. An example of a nontransient error would be running out of disk space while writing to a file.</li>
<li><strong>Transient<span> </span>errors</strong>: Applications can, and should, always attempt to recover from such errors, although this may not always be possible. This is usually achieved by means of some sort of retry mechanism. For instance, if the application loses its connection to a remote server, it can attempt to reconnect using an exponential back-off strategy. If a maximum number of retries is reached, then this becomes a nontransient error.</li>
</ul>
<p>The following is a simple example illustrating how a user can apply the decorator design pattern to wrap a <kbd>Processor</kbd> function and implement a retry mechanism that can distinguish between transient and nontransient errors:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> retryingProcessor(proc Processor, isTransient <span class="kw">func</span>(<span class="dt">error</span>) <span class="dt">bool</span>, maxRetries <span class="dt">int</span>) Processor {</a>
<a>    <span class="kw">return</span> ProcessorFunc(<span class="kw">func</span>(ctx context.Context, p Payload) (Payload, <span class="dt">error</span>) {</a>
<a>        <span class="kw">var</span> out Payload</a>
<a>        <span class="kw">var</span> err <span class="dt">error</span></a>
<a>        <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; maxRetries; i++ {</a>
<a>            <span class="kw">if</span> out, err = proc.Process(ctx, p); err != <span class="ot">nil</span> &amp;&amp; !isTransient(err) {</a>
<a>                <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>            }</a>
<a>        }</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    })</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>The <kbd>retryingProcessor</kbd> function wraps an existing <kbd>Processor</kbd><span> to provide support for automatic retries in the presence of errors. </span>Each time an error occurs, the function consults the<span> </span><kbd>isTransient</kbd><span> </span>helper function to decide whether the obtained error is transient and whether another attempt at processing the payload can be performed. Nontransient errors are considered to be nonrecoverable, and in such cases, the function will return the error to cause the pipeline to terminate. Finally, if the maximum number of retries is exceeded, the function treats the error as nontransient and bails out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synchronous versus asynchronous pipelines</h1>
                </header>
            
            <article>
                
<p>A critical decision that will influence the way we implement the core of the pipeline is whether it will operate in a synchronous or an asynchronous fashion. Let's take a quick look at these two modes of operation and discuss the pros and cons of each one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synchronous pipelines</h1>
                </header>
            
            <article>
                
<p>A synchronous pipeline essentially processes one payload at a time. We could implement such a pipeline by creating a <kbd>for</kbd> loop that does the following:</p>
<ul>
<li>Dequeues the next payload from the input source or exits the loop if no more payloads are available</li>
<li>Iterates the list of pipeline stages and invokes the<span> </span><kbd>Processor</kbd><span> </span>instance for each stage</li>
<li>Enqueues the resulting payload to the output source</li>
</ul>
<p>Synchronous pipelines are great for workloads where payloads must always be processed in <strong>first-in-first-out</strong> (<strong>FIFO</strong>) fashion, a quite common case for event-driven architectures which, most of the time, operate under the assumption that events are always processed in a specific order.</p>
<p class="mce-root"/>
<p>As an example, let's say that we are trying to construct an <strong>ETL</strong> (short for <strong>extract</strong>, <strong>transform</strong>, and <strong>load</strong>) pipeline for consuming an event-stream from an order-processing system, enriching some of the incoming events with additional information by querying an external system and finally transforming the enriched events into a format suitable for persisting into a relational database. The pipeline for this use-case can be assembled using the following two stages:</p>
<ul>
<li>The first stage inspects the event type and enriches it with the appropriate information by querying an external service</li>
<li>The second stage converts each enriched event into a sequence of SQL queries for updating one or more database tables</li>
</ul>
<p>By design, our processing code expects that an<span> </span><kbd>AccountCreated</kbd><span> </span>event must always precede an<span> </span><kbd>OrderPlaced</kbd><span> </span>event, which includes a reference (a UUID) to the account of the customer who placed the order. If the events were to be processed in the wrong order, the system might find itself trying to process<span> </span><kbd>OrderPlaced</kbd><span> </span>events before the customer records in the database have been created. While it is certainly possible to code around this limitation, it would make the processing code much more complicated and harder to debug when something goes wrong. A synchronous pipeline would enforce in-order processing semantics and make this a nonissue.</p>
<p>So what's the catch when using synchronous pipelines? The main issue associated with synchronous pipelines is<span> </span><em>low throughput</em>. If our pipeline consists of<span> </span><em>N</em><span> </span>stages and each stage takes<span> </span><em>1 time unit</em><span> </span>to complete, our pipeline would require<span> </span><em>N time-units</em><span> </span>to process and emit<span> </span><em>each</em><span> </span>payload. By extension, each time a stage is processing a payload, the remaining<span> </span><em>N-1 </em>stages are<span> </span><em>idling</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asynchronous pipelines</h1>
                </header>
            
            <article>
                
<p>In an asynchronous pipeline design, once a stage processes an incoming payload and emits it to the next stage, it can immediately begin processing the next available payload without having to wait for the currently processed payload to exit the pipeline, as would be the case in a synchronous pipeline design. This approach ensures that all stages are continuously kept busy processing payloads instead of idling.</p>
<p>It is important to note that asynchronous pipelines typically require some form of concurrency. A common pattern is to run each stage in a separate goroutine. Of course, this introduces additional complexity to the mix as we need to do the following:</p>
<ul>
<li>Manage the lifecycle of each goroutine</li>
<li>Make use of concurrency primitives, such as locks, to avoid data races</li>
</ul>
<p class="mce-root"/>
<p>Nevertheless, asynchronous pipelines have much better throughput characteristics compared to synchronous pipelines. This is the main reason why the pipeline package that we will be building in this chapter will feature an asynchronous pipeline implementation... with a small twist! Even though all pipeline components (input, output, and stages) will be running<span> </span><em>asynchronously</em>, end users will be interacting with the pipeline using a<span> </span><em>synchronous</em><span> </span>API.</p>
<p>A quick survey of the most popular Go <strong>software development kits</strong> (<strong>SDKs</strong>) out there will reveal a general consensus toward exposing synchronous APIs. From the perspective of the API consumer, synchronous APIs are definitely easier to consume as the end user does not need to worry about managing resources, such as Go channels, or writing complex<span> </span><kbd>select</kbd><span> </span>statements to coordinate reads and/or writes between channels. Contrast this approach with having an asynchronous API, where the end user would have to deal with an input, output, and error channel every time they wanted to execute a pipeline run!</p>
<p>As mentioned previously, the pipeline internals will be executing asynchronously. The typical way to accomplish this in Go would be to start a goroutine for each pipeline component and link the individual goroutines together by means of Go channels. The pipeline implementation will be responsible for fully managing the lifecycle of any goroutine it spins up, in a way that is totally transparent to the end user of the pipeline package.</p>
<div class="packt_tip">When working with goroutines, we must always be conscious about their individual lifecycles. A sound piece of advice is to never start a goroutine unless you know when it will exit and which conditions need to be satisfied for it to exit.<br/>
<br/>
Failure to heed this bit of advice can introduce goroutine leaks in long-running applications that typically require quite a bit of time and effort to track down.</div>
<p>Exposing a synchronous API for the pipeline package has yet another benefit that we haven't yet mentioned. It is pretty trivial for the end users of the pipeline package to wrap the synchronous API in a goroutine and make it asynchronous. The goroutine would simply invoke the blocking code and use a channel to signal the application code when the pipeline execution has completed.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a stage worker for executing payload processors</h1>
                </header>
            
            <article>
                
<p>One of the goals of the pipeline package is to allow the end users to specify a per-stage strategy for dispatching incoming payloads to the registered processor functions. In order to be able to support different dispatch strategies in a clean and extensible way, we are going to be introducing yet another abstraction, the<span> </span><kbd>StageRunner</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> StageRunner <span class="kw">interface</span> {</a>
<a>    Run(context.Context, StageParams)</a>
<a>}</a></pre></div>
<p>Concrete <kbd>StageRunner</kbd> implementations provide a<span> </span><kbd>Run</kbd><span> </span>method that implements the payload processing loop for a single stage of the pipeline. A typical processing loop consists of the following steps:</p>
<ol type="1">
<li>Receive the next payload from the previous stage or the input source, if this happens to be the first stage of the pipeline. If the upstream data source signals that it has run out of data, or the externally provided<span> </span><kbd>context.Context</kbd><span> </span>is cancelled, then the<span> </span><kbd>Run</kbd><span> </span>method should automatically return.</li>
<li>Dispatch the payload to a user-defined processor function for the stage. As we will see in the following sections, the implementation of this step depends on the dispatch strategy that is being used by the <kbd>StageRunner</kbd> implementation.</li>
<li>If the error processor returns an<span> </span><em>error</em>, enqueue the error to the shared error bus and return.</li>
<li>Push successfully processed payloads to the next pipeline stage, or the output sink, if this is the last stage of the pipeline.</li>
</ol>
<p>The preceding steps make it quite clear that<span> </span><kbd>Run</kbd><span> </span>is a<span> </span>blocking<span> </span>call. The pipeline implementation will start a goroutine for each stage of the pipeline, invoke the<span> </span><kbd>Run</kbd><span> </span>method of each registered<span> </span><kbd>StageRunner</kbd><span> instance, </span>and wait for it to return. Since we are working with goroutines, the appropriate mechanism for interconnecting them is to use Go channels. As both the goroutine and channel lifecycles are managed by the pipeline internals, we need a way to configure each<span> </span><kbd>StageRunner</kbd><span> </span>with the set of channels it will be working with. This information is provided to the<span> </span><kbd>Run</kbd><span> </span>method via its second argument. Here is the definition of the<span> </span><kbd>StageParams</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> StageParams <span class="kw">interface</span> {</a>
<a>    StageIndex() <span class="dt">int</span></a>

<a>    Input() &lt;-<span class="kw">chan</span> Payload</a>
<a>    Output() <span class="kw">chan</span>&lt;- Payload</a>
<a>    Error() <span class="kw">chan</span>&lt;- <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Input</kbd><span> </span>method returns a<span> </span><em>read-only</em><span> </span>channel that the worker will be watching for incoming payloads. The channel will be<span> </span><em>closed</em><span> </span>to indicate that no more data is available for processing. The<span> </span><kbd>Output</kbd><span> </span>method returns a<span> </span><em>write-only</em><span> </span>channel where the<span> </span><kbd>StageRunner</kbd> should publish the input payload after it has been successfully processed. On the other hand, should an error occur while processing an incoming payload, the<span> </span><kbd>Error</kbd><span> </span>channel returns a<span> </span><em>write-only</em><span> </span>channel where the error can be published. Finally, the<span> </span><kbd>StageIndex</kbd><span> </span>method returns the position of the stage in the pipeline that can be optionally used by<span> </span><kbd>StageRunner</kbd><span> </span>implementations to annotate errors.</p>
<p>In the following sections, we will be taking a closer look at the implementation of three very common payload dispatch strategies that we will be bundling with the pipeline package: FIFO, fixed/dynamic worker pools, and broadcasting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">FIFO</h1>
                </header>
            
            <article>
                
<p>As the name implies, when a stage operates in FIFO mode, it processes payloads sequentially, thereby maintaining their order. By creating a pipeline where <em>all</em> stages use FIFO dispatching, we can enforce synchronous-like semantics for data processing, but still retain the high throughput benefits associated with an asynchronous pipeline.</p>
<p>The<span> </span><kbd>fifo</kbd><span> </span>type is private within the<span> </span><kbd>pipeline</kbd><span> </span>package, but it can be instantiated via a call to the <span> </span><kbd>FIFO</kbd> function, which is outlined as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> fifo <span class="kw">struct</span> {</a>
<a>    proc Processor</a>
<a>}</a>

<a><span class="co">// FIFO returns a StageRunner that processes incoming payloads in a </span></a>
<a><span class="co">// first-in first-out fashion. Each input is passed to the specified </span></a>
<a><span class="co">// processor and its output is emitted to the next stage.</span></a>
<a><span class="kw">func</span> FIFO(proc Processor) StageRunner {</a>
<a>    <span class="kw">return</span> fifo{proc: proc}</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>Let's now take a look at the<span> </span><kbd>Run</kbd><span> </span>method implementation for the<span> </span><kbd>fifo</kbd><span> </span>type:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (r fifo) Run(ctx context.Context, params StageParams) {</a>
<a>    <span class="kw">for</span> {</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>            <span class="kw">return</span> <span class="co">// Asked to cleanly shut down</span></a>
<a>        <span class="kw">case</span> payloadIn, ok := &lt;-params.Input():</a>
<a>            <span class="kw">if</span> !ok {</a>
<a>                <span class="kw">return</span> <span class="co">// No more data available.</span></a>
<a>            }</a>
<a>            </a>
<a>            <span class="co">// Process payload, handle errors etc.</span></a>
<a>            <span class="co">// (see following listing)</span></a>
<a>        }</a>
<a>    }</a>
<a>}</a></pre></div>
<p>As you can see,<span> </span><kbd>Run</kbd><span> </span>is, by design, a blocking call; it runs an infinite for-loop with a single <kbd>select</kbd> statement. Within the <kbd>select</kbd> block, the code does the following:</p>
<ul>
<li>Monitors the provided context for cancellation and exits the main loop when the context gets cancelled (for example, if the user cancelled it or its timeout expired).</li>
<li>Attempts to retrieve the next payload from the input channel. If the input channel closes, the code exits the main loop.</li>
</ul>
<p>Once a new input payload has been received, the FIFO runner executes the following block of code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>payloadOut, err := r.proc.Process(ctx, payloadIn)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    wrappedErr := xerrors.Errorf(<span class="st">"pipeline stage %d: %w"</span>, params.StageIndex(), err)</a>
<a>    maybeEmitError(wrappedErr, params.Error())</a>
<a>    <span class="kw">return</span></a>
<a>}</a>
<a><span class="kw">if</span> payloadOut == <span class="ot">nil</span> {</a>
<a>   payloadIn.MarkAsProcessed()</a>
<a>    <span class="kw">continue</span></a>
<a>}</a>

<a><span class="kw">select</span> {</a>
<a><span class="kw">case</span> params.Output() &lt;- payloadOut:</a>
<a><span class="kw">case</span> &lt;-ctx.Done():</a>
<a>    <span class="kw">return</span>  <span class="co">// Asked to cleanly shut down</span></a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The input payload is first passed to the user-defined<span> </span><kbd>Processor</kbd><span> </span>instance. If the processor returns an error, the code annotates it with the current stage number and attempts to enqueue it to the provided error channel by invoking the<span> </span><kbd>maybeEmitError</kbd><span> </span>helper before exiting the worker:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// maybeEmitError attempts to queue err to a buffered error channel. If the</span></a>
<a><span class="co">// channel is full, the error is dropped.</span></a>
<a><span class="kw">func</span> maybeEmitError(err <span class="dt">error</span>, errCh <span class="kw">chan</span>&lt;- <span class="dt">error</span>) {</a>
<a>    <span class="kw">select</span> {</a>
<a>    <span class="kw">case</span> errCh &lt;- err: <span class="co">// error emitted.</span></a>
<a>    <span class="kw">default</span>: <span class="co">// error channel is full with other errors.</span></a>
<a>    }</a>
<a>}</a></pre></div>
<p>If the payload is processed without an error, then we need to check whether the processor returned a valid payload that we need to forward or a<span> </span><em>nil</em><span> </span>payload to indicate that the input payload should be discarded. Prior to discarding a payload, the code invokes its<span> </span><kbd>MarkAsProcessed</kbd><span> </span>method before commencing a new iteration of the main loop.</p>
<p>On the other hand, if the processor returns a valid payload, we attempt to enqueue it to the output channel with the help of a <kbd>select</kbd> statement that blocks until either the payload is written to the output channel or the context gets cancelled. In the latter case, the worker terminates and the payload is dropped to the floor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixed and dynamic worker pools</h1>
                </header>
            
            <article>
                
<p>Oftentimes, processor functions can take quite a bit of time to return. This could be either because the actual payload processing involves CPU-intensive calculations or simply because the function is waiting for an I/O operation to complete (for example, the processor function performed an HTTP request to a remote server and is waiting for a response).</p>
<p>If all stages were linked using the FIFO dispatch strategy, then slowly executing processors could cause the pipeline to stall. If<span> </span><em>out-of-order</em><span> </span>processing of payloads is not an issue, we can make much better use of the available system resources by introducing worker pools into the mix. Worker pools is a pattern that can significantly improve the throughput of a pipeline by enabling stages in order to process multiple payloads in<span> </span><em>parallel</em>.</p>
<p class="mce-root"/>
<p>The first worker pool pattern that we will be implementing is a<span> </span><strong>fixed<span> </span></strong>worker pool. This type of pool spins up a preconfigured number of workers and distributes incoming payloads among them. Each one of the pool workers implements the same loop as the FIFO<span> </span><kbd>StageRunner</kbd>. As the following code shows, our implementation actively exploits this observation and avoids duplicating the main loop code by creating a FIFO instance for each worker in the pool:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> fixedWorkerPool <span class="kw">struct</span> {</a>
<a>    fifos []StageRunner</a>
<a>}</a>

<a><span class="kw">func</span> FixedWorkerPool(proc Processor, numWorkers <span class="dt">int</span>) StageRunner {</a>
<a>    <span class="kw">if</span> numWorkers &lt;= <span class="dv">0</span> {</a>
<a>        <span class="bu">panic</span>(<span class="st">"FixedWorkerPool: numWorkers must be &gt; 0"</span>)</a>
<a>    }</a>
<a>    fifos := <span class="bu">make</span>([]StageRunner, numWorkers)</a>
<a>    <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; numWorkers; i++ {</a>
<a>        fifos[i] = FIFO(proc)</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;fixedWorkerPool{fifos: fifos}</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Run</kbd><span> </span>method shown in the following code spins up the individual pool workers, executes their <kbd>Run</kbd> method, and uses a<span> </span><kbd>sync.WaitGroup</kbd><span> to prevent it from returning until all the spawned worker goroutines terminate</span>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (p *fixedWorkerPool) Run(ctx context.Context, params StageParams) {</a>
<a>    <span class="kw">var</span> wg sync.WaitGroup</a>

<a>    <span class="co">// Spin up each worker in the pool and wait for them to exit</span></a>
<a>    <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">len</span>(p.fifos); i++ {</a>
<a>        wg.Add(<span class="dv">1</span>)</a>
<a>        <span class="kw">go</span> <span class="kw">func</span>(fifoIndex <span class="dt">int</span>) {</a>
<a>            p.fifos[fifoIndex].Run(ctx, params)</a>
<a>            wg.Done()</a>
<a>        }(i)</a>
<a>    }</a>

<a>    wg.Wait()</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In terms of wiring, things are pretty simple here. All we need to do is pass the incoming parameters, as-is, to each one of the FIFO instances. The effect of this wiring is as follows:</p>
<ul>
<li>All FIFOs are set up to read incoming payloads from the <em>same</em> input channel, which is connected to the previous pipeline stage (or input source). This approach effectively acts as a load balancer for distributing payloads to idle FIFOs.</li>
<li>All FIFOs output processed payloads to the <em>same</em> output channel, which is linked to the next pipeline stage (or output sink).</li>
</ul>
<p>Fixed worker pools are quite easy to set up, but come with a caveat: the number of workers must be specified<span> </span><em>in advance</em>! In some cases, coming up with a good value for the number of workers is really easy. For instance, if we know that the processor will be performing CPU-intensive calculations, we can ensure that our pipeline fully utilizes all available CPU cores by setting the number of workers equal to the result of the<span> </span><kbd>runtime.NumCPU()</kbd><span> </span>call. Sometimes, coming up with a good estimate for the number of workers is not that easy. A potential solution would be to switch to a<span> </span><em>dynamic</em><span> </span>worker pool.</p>
<p>The key difference between a static and a dynamic worker pool is that with the latter, the number of workers is not fixed but varies over time. This fundamental difference allows us to make better use of available resources by allowing the dynamic pool to automatically scale the number of workers <span>up or down </span><span>to adapt to variances in the throughput from the previous stages.</span></p>
<p>It goes without saying that we should always enforce an upper limit for the number of workers that can be spawned by the dynamic pool. Without such a limit in place, the number of goroutines spawned by the pipeline might grow out of control and cause the program to either grind to a halt or, even worse, to crash! To avoid this problem, the dynamic worker pool implementation presented in the following code uses a primitive known as a<span> </span><strong>token pool</strong>:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> dynamicWorkerPool <span class="kw">struct</span> {</a>
<a>    proc      Processor</a>
<a>    tokenPool <span class="kw">chan</span> <span class="kw">struct</span>{}</a>
<a>}</a>

<a><span class="kw">func</span> DynamicWorkerPool(proc Processor, maxWorkers <span class="dt">int</span>) StageRunner {</a>
<a>    <span class="kw">if</span> maxWorkers &lt;= <span class="dv">0</span> {</a>
<a>        <span class="bu">panic</span>(<span class="st">"DynamicWorkerPool: maxWorkers must be &gt; 0"</span>)</a>
<a>    }</a>
<a>    tokenPool := <span class="bu">make</span>(<span class="kw">chan</span> <span class="kw">struct</span>{}, maxWorkers)</a>
<a>    <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; maxWorkers; i++ {</a>
<a>        tokenPool &lt;- <span class="kw">struct</span>{}{}</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;dynamicWorkerPool{proc: proc, tokenPool: tokenPool}</a>
<a>}</a></pre></div>
<p>A token pool is modeled as a buffered<span> </span><kbd>chan struct{}</kbd><span>, </span>which is prepopulated with a number of tokens equal to the maximum number of concurrent workers that we wish to allow. Let's see how this primitive can be used to as a concurrency-control mechanism by breaking down the dynamic pool's<span> </span><kbd>Run</kbd><span> </span>method implementation into logical blocks:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (p *dynamicWorkerPool) Run(ctx context.Context, params StageParams) {</a>
<a>stop:</a>
<a>    <span class="kw">for</span> {</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>            <span class="kw">break</span> stop <span class="co">// Asked to cleanly shut down</span></a>
<a>        <span class="kw">case</span> payloadIn, ok := &lt;-params.Input():</a>
<a>            <span class="kw">if</span> !ok { <span class="kw">break</span> stop }</a>
<a>            <span class="co">// Process payload... (see listings below)</span></a>
<a>        }</a>
<a>    }</a>

<a>    <span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">cap</span>(p.tokenPool); i++ { <span class="co">// wait for all workers to exit</span></a>
<a>        &lt;-p.tokenPool</a>
<a>    }</a>
<a>}</a></pre></div>
<p>Similarly to the FIFO implementation, the dynamic pool executes an infinite for-loop containing a <kbd>select</kbd> statement; however, the code that deals with payload processing is quite different in this implementation. Instead of calling the payload processor code directly, we will just spin up a goroutine to take care of that task for us in the background, while the main loop attempts to process the next incoming payload.</p>
<p>Before a new worker can be started, we must first fetch a token from the pool. This is achieved via the following block of code that blocks until a token can be read off the channel or the provided context gets cancelled:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> token <span class="kw">struct</span>{}</a>
<a><span class="kw">select</span> {</a>
<a><span class="kw">case</span> token = &lt;-p.tokenPool:</a>
<a><span class="kw">case</span> &lt;-ctx.Done():</a>
<a>    <span class="kw">break</span> stop</a>
<a>}</a></pre></div>
<p>The preceding block of code serves as a choke point for limiting the number of concurrent workers. Once all tokens in the pool are exhausted, attempts to read off the channel will be blocked until a token is returned to the pool. So how do tokens get returned to the pool? To answer this question, we need to take a look at what happens<span> </span><em>after</em><span> </span>we successfully read a token from the pool:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">go</span> <span class="kw">func</span>(payloadIn Payload, token <span class="kw">struct</span>{}) {</a>
<a>    <span class="kw">defer</span> <span class="kw">func</span>() { p.tokenPool &lt;- token }()</a>
<a>    payloadOut, err := p.proc.Process(ctx, payloadIn)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        wrappedErr := xerrors.Errorf(<span class="st">"pipeline stage %d: %w"</span>, params.StageIndex(), err)</a>
<a>        maybeEmitError(wrappedErr, params.Error())</a>
<a>        <span class="kw">return</span></a>
<a>    }</a>
<a>    <span class="kw">if</span> payloadOut == <span class="ot">nil</span> {</a>
<a>        payloadIn.MarkAsProcessed()</a>
<a>        <span class="kw">return</span> <span class="co">// Discard payload</span></a>
<a>    }</a>
<a>    <span class="kw">select</span> {</a>
<a>    <span class="kw">case</span> params.Output() &lt;- payloadOut:</a>
<a>    <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>    }</a>
<a>}(payloadIn, token)</a></pre></div>
<p>This block of code is more or less the same as the FIFO implementation, with two small differences:</p>
<ul>
<li>It executes inside a goroutine.</li>
<li>It includes a<span> </span><kbd>defer</kbd><span> </span>statement to ensure that the token is returned to the pool once the goroutine completes. This is important as it makes the token available for reuse.</li>
</ul>
<p>The last bit of code that we need to discuss is the for-loop at the end of the<span> </span><kbd>Run</kbd><span> </span>method. To guarantee that the dynamic pool does not leak any goroutines, we need to make sure that any goroutines that were spawned while the method was running have terminated before<span> </span><kbd>Run</kbd><span> </span>can return. Instead of using a<span> </span><kbd>sync.WaitGroup</kbd>, we can achieve the same effect by simply draining the token pool. As we already know, workers can only run while holding a token; once the for-loop has extracted all tokens from the pool, we can safely return knowing that all workers have completed their work and their goroutines have been terminated.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">1-to-N broadcasting</h1>
                </header>
            
            <article>
                
<p>The 1-to-<em>N</em> broadcasting pattern allows us to support use cases where each incoming payload must to be processed in<span> </span>parallel<span> </span>by<span> </span><em>N</em> different<span> </span>processors, each one of which implements FIFO-like semantics.</p>
<p>The following code is the definition of the<span> </span><kbd>broadcast</kbd><span> </span>type and the<span> </span><kbd>Broadcast</kbd><span> </span>helper function that serves as its constructor:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> broadcast <span class="kw">struct</span> {</a>
<a>    fifos []StageRunner</a>
<a>}</a>

<a><span class="kw">func</span> Broadcast(procs ...Processor) StageRunner {</a>
<a>    <span class="kw">if</span> <span class="bu">len</span>(procs) == <span class="dv">0</span> {</a>
<a>        <span class="bu">panic</span>(<span class="st">"Broadcast: at least one processor must be specified"</span>)</a>
<a>    }</a>
<a>    fifos := <span class="bu">make</span>([]StageRunner, <span class="bu">len</span>(procs))</a>
<a>    <span class="kw">for</span> i, p := <span class="kw">range</span> procs {</a>
<a>        fifos[i] = FIFO(p)</a>
<a>    }</a>

<a>    <span class="kw">return</span> &amp;broadcast{fifos: fifos}</a>
<a>}</a></pre></div>
<p>As you can see, the variadic<span> </span><kbd>Broadcast</kbd><span> </span>function receives a list of<span> </span><kbd>Processor</kbd><span> </span>instances as arguments and creates a FIFO instance for each one. These FIFO instances are stored inside the returned<span> </span><kbd>broadcast</kbd><span> </span>instance and used within its<span> </span><kbd>Run</kbd><span> </span>method implementation, which we will be dissecting as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> wg sync.WaitGroup</a>
<a><span class="kw">var</span> inCh = <span class="bu">make</span>([]<span class="kw">chan</span> Payload, <span class="bu">len</span>(b.fifos))</a>
<a><span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">len</span>(b.fifos); i++ {</a>
<a>    wg.Add(<span class="dv">1</span>)</a>
<a>    inCh[i] = <span class="bu">make</span>(<span class="kw">chan</span> Payload)</a>
<a>    <span class="kw">go</span> <span class="kw">func</span>(fifoIndex <span class="dt">int</span>) {</a>
<a>        fifoParams := &amp;workerParams{</a>
<a>            stage: params.StageIndex(),</a>
<a>            inCh:  inCh[fifoIndex],</a>
<a>            outCh: params.Output(),</a>
<a>            errCh: params.Error(),</a>
<a>        }</a>
<a>        b.fifos[fifoIndex].Run(ctx, fifoParams)</a>
<a>        wg.Done()</a>
<a>    }(i)</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>Similar to the fixed worker pool implementation that we examined in the previous section, the first thing that we do inside<span> </span><kbd>Run</kbd><span> </span>is to spawn up a goroutine for each FIFO<span> </span><kbd>StageRunner</kbd><span> </span>instance. A<span> </span><kbd>sync.WaitGroup</kbd><span> </span>allows us to wait for all workers to exit before<span> </span><kbd>Run</kbd><span> </span>can return.</p>
<p>To avoid data races, the implementation for the broadcasting stage must intercept each incoming payload, <em>clone</em> it, and deliver a copy to each one of the generated FIFO processors. Consequently, the generated FIFO processor instances cannot be directly wired to the input channel for the stage, but must instead be configured with a dedicated input channel for reading . To this end, the preceding block of code generates a new <kbd>workerParams</kbd> value (an internal type to <span>the</span><span> </span><kbd>pipeline</kbd><span> </span><span>package that implements the</span><span> </span><kbd>StageParams</kbd><span> </span><span>interface) for each FIFO instance and supplies it as an argument</span><span> to its</span><span> </span><kbd>Run</kbd><span> </span><span>method</span><span>. Note that while each FIFO instance is configured with a separate input channel, they all share the same</span><span> </span>output and error channe<span>ls.</span></p>
<p>The next part of the<span> </span><kbd>Run</kbd><span> </span>method's implementation is the, by now familiar, main loop where we wait for the next incoming payload to appear:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>done:</a>
<a>    <span class="kw">for</span> {</a>
<a>        <span class="co">// Read incoming payloads and pass them to each FIFO</span></a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>            <span class="kw">break</span> done</a>
<a>        <span class="kw">case</span> payload, ok := &lt;-params.Input():</a>
<a>            <span class="kw">if</span> !ok {</a>
<a>                <span class="kw">break</span> done</a>
<a>            }</a>
<a>            </a>
<a>            <span class="co">// Clone payload and dispatch to each FIFO worker...</span></a>
<a>            <span class="co">// (see following listing)</span></a>
<a>        }</a>
<a>    }</a></pre></div>
<p>Once a new payload is received, the implementation writes a copy of the payload to the input channel for each FIFO instance, but the first one receives the original incoming payload:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">for</span> i := <span class="bu">len</span>(b.fifos) - <span class="dv">1</span>; i &gt;= <span class="dv">0</span>; i-- {</a>
<a>    <span class="kw">var</span> fifoPayload = payload</a>
<a>    <span class="kw">if</span> i != <span class="dv">0</span> {</a>
<a>        fifoPayload = payload.Clone()</a>
<a>    }</a>
<a>    <span class="kw">select</span> {</a>
<a>    <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>        <span class="kw">break</span> done</a>
<a>    <span class="kw">case</span> inCh[i] &lt;- fifoPayload:</a>
<a>        <span class="co">// payload sent to i_th FIFO</span></a>
<a>    }</a>
<a>}</a></pre></div>
<p>After publishing the payload to all FIFO instances, a new iteration of the main loop begins. The main loop keeps executing until either the input channel closes or the context gets cancelled. After exiting the main loop, the following sentinel block of code gets executed before<span> </span><kbd>Run</kbd><span> </span>returns:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Close input channels and wait for all FIFOs to exit</span></a>
<a><span class="kw">for</span> _, ch := <span class="kw">range</span> inCh {</a>
<a>    <span class="bu">close</span>(ch)</a>
<a>}</a>
<a>wg.Wait()</a></pre></div>
<p>In the preceding code snippet, we signal each one of the FIFO workers to shut down by closing their dedicated input channels. We then invoke the <kbd>Wait</kbd><span> </span><span>method of the </span><kbd>WaitGroup</kbd><span> </span><span>to wait for all FIFO workers to terminate.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the input source worker</h1>
                </header>
            
            <article>
                
<p>In order to begin a new pipeline run, users are expected to provide an input source that generates the application-specific payloads that drive the pipeline. All user-defined input sources must implement the<span> </span><kbd>Source</kbd><span> </span>interface, whose definition is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Source <span class="kw">interface</span> {</a>
<a>    Next(context.Context) <span class="dt">bool</span></a>
<a>    Payload() Payload</a>
<a>    Error() <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Source</kbd><span> </span>interface contains the standard set of methods that you would expect for any data source that supports iteration:</p>
<ul>
<li><kbd>Next</kbd><span> </span>attempts to advance the iterator. It returns<span> </span><kbd>false</kbd><span> </span>if either no more data is available or an error occurred.</li>
<li><kbd>Payload</kbd><span> </span>returns the a new <kbd>Payload</kbd> instance after a successful call to the iterator's <kbd>Next</kbd> method.</li>
<li><kbd>Error</kbd><span> </span>returns the last error encountered by the input.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To facilitate the asynchronous polling of the input source, the pipeline package will run the following <span> </span><kbd>sourceWorker</kbd><span> function inside a goroutine. </span>Its primary task is to iterate the data source and publish each incoming payload to the specified channel:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> sourceWorker(ctx context.Context, source Source, outCh <span class="kw">chan</span>&lt;- Payload, errCh <span class="kw">chan</span>&lt;- <span class="dt">error</span>) {</a>
<a>    <span class="kw">for</span> source.Next(ctx) {</a>
<a>        payload := source.Payload()</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> outCh &lt;- payload:</a>
<a>        <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>            <span class="kw">return</span> <span class="co">// Asked to shutdown</span></a>
<a>        }</a>
<a>    }</a>

<a>    <span class="co">// Check for errors</span></a>
<a>    <span class="kw">if</span> err := source.Error(); err != <span class="ot">nil</span> {</a>
<a>        wrappedErr := xerrors.Errorf(<span class="st">"pipeline source: %w"</span>, err)</a>
<a>        maybeEmitError(wrappedErr, errCh)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>sourceWorker</kbd><span> function </span>keeps running until a call to the source's<span> </span><kbd>Next</kbd><span> </span>method returns<span> </span><kbd>false</kbd>. Before returning, the worker implementation will check for any errors reported by the input source and publish them to the provided error channel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the output sink worker</h1>
                </header>
            
            <article>
                
<p>Of course, our pipeline would not be complete without an output sink! After all, payloads that travel through the pipeline do not disappear into thin air once they clear the pipeline; they must end up somewhere. So, together with an input source, users are expected to provide an output sink that implements the<span> </span><kbd>Sink</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Sink <span class="kw">interface</span> {</a>
<a>    <span class="co">// Consume processes a Payload instance that has been emitted out of</span></a>
<a>    <span class="co">// a Pipeline instance.</span></a>
<a>    Consume(context.Context, Payload) <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>In order to deliver processed payloads to the sink, the pipeline package will spawn a new goroutine and execute the<span> </span><kbd>sinkWorker</kbd><span> </span>function, whose implementation is as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> sinkWorker(ctx context.Context, sink Sink, inCh &lt;-<span class="kw">chan</span> Payload, errCh <span class="kw">chan</span>&lt;- <span class="dt">error</span>) {</a>
<a>    <span class="kw">for</span> {</a>
<a>        <span class="kw">select</span> {</a>
<a>        <span class="kw">case</span> payload, ok := &lt;-inCh:</a>
<a>            <span class="kw">if</span> !ok { <span class="kw">return</span> }</a>
<a>            <span class="kw">if</span> err := sink.Consume(ctx, payload); err != <span class="ot">nil</span> {</a>
<a>                wrappedErr := xerrors.Errorf(<span class="st">"pipeline sink: %w"</span>, err)</a>
<a>                maybeEmitError(wrappedErr, errCh)</a>
<a>                <span class="kw">return</span></a>
<a>            }</a>
<a>            payload.MarkAsProcessed()</a>
<a>        <span class="kw">case</span> &lt;-ctx.Done():</a>
<a>            <span class="kw">return</span> <span class="co">// Asked to shutdown</span></a>
<a>        }</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>sinkWorker</kbd><span> loop </span>reads payloads from the provided input channel and attempts to publish them to the provided<span> </span><kbd>Sink</kbd><span> </span>instance. If the <kbd>sink</kbd> implementation reports an error while consuming the payload, the<span> </span><kbd>sinkWorker</kbd><span> function </span>will publish it to the provided error channel before returning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together – the pipeline API</h1>
                </header>
            
            <article>
                
<p>After thoroughly describing the ins and outs of each individual pipeline component, it is finally time to bring everything together and implement an API that the end users of the pipeline package will depend on for assembling and executing their pipelines.</p>
<p>A new pipeline instance can be created by invoking the variadic<span> </span><kbd>New</kbd><span> </span>function from the <kbd>pipeline</kbd> package. As you can see in the following code listing, the construction function expects a list of<span> </span><kbd>StageRunner</kbd><span> </span>instances as arguments where each element of the list corresponds to a stage of the pipeline:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Pipeline <span class="kw">struct</span> {</a>
<a>    stages []StageRunner</a>
<a>}</a>

<a><span class="co">// New returns a new pipeline instance where input payloads will traverse</span></a>
<a><span class="co">// each one of the specified stages.</span></a>
<a><span class="kw">func</span> New(stages ...StageRunner) *Pipeline {</a>
<a>    <span class="kw">return</span> &amp;Pipeline{</a>
<a>        stages: stages,</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>Users can either opt to use the <kbd>StageRunner</kbd> implementations that we outlined in the previous sections (FIFO, <kbd>FixedWorkerPool</kbd>, <kbd>DynamicWorkerPool</kbd>, or <kbd>Broadcast</kbd>) and that are provided by the <kbd>pipeline</kbd> package or, alternatively, provide their own application-specific variants that satisfy the single-method <kbd>StageRunner</kbd> interface.</p>
<p>After constructing a new pipeline instance and creating a compatible input source/output sink, users can execute the pipeline by invoking the<span> </span><kbd>Process</kbd><span> </span>method on the pipeline instance that is obtained:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (p *Pipeline) Process(ctx context.Context, source Source, sink Sink) <span class="dt">error</span> {</a>
<a>    <span class="co">// ...</span></a>
<a>}</a></pre></div>
<p>The first argument to<span> </span><kbd>Process</kbd><span> </span>is a context instance that can be cancelled by the user to force the pipeline to terminate. Calls to the<span> </span><kbd>Process</kbd><span> </span>method will be blocked until one of the following conditions is met:</p>
<ul>
<li>The context is cancelled.</li>
<li>The source runs out of data and all payloads have been processed or discarded.</li>
<li>An error occurs in any of the pipeline components or the user-defined processor functions. In the latter case, an error will be returned back to the caller.</li>
</ul>
<p>Let's take a look at the implementation details of the<span> </span><kbd>Process</kbd><span> </span>method:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> wg sync.WaitGroup</a>
<a>pCtx, ctxCancelFn := context.WithCancel(ctx)</a>

<a><span class="co">// Allocate channels for wiring together the source, the pipeline stages</span></a>
<a><span class="co">// and the output sink. </span></a>
<a>stageCh := <span class="bu">make</span>([]<span class="kw">chan</span> Payload, <span class="bu">len</span>(p.stages)+<span class="dv">1</span>)</a>
<a>errCh := <span class="bu">make</span>(<span class="kw">chan</span> <span class="dt">error</span>, <span class="bu">len</span>(p.stages)+<span class="dv">2</span>)</a>
<a><span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">len</span>(stageCh); i++ {</a>
<a>    stageCh[i] = <span class="bu">make</span>(<span class="kw">chan</span> Payload)</a>
<a>}</a></pre></div>
<p>First of all, we create a new context (<kbd>pCtx</kbd>) that wraps the user-defined context, but also allows us to manually cancel it. The wrapped context will be passed to all pipeline components, allowing us to easily tear down the entire pipeline if we detect any error.</p>
<p class="mce-root"/>
<p>After setting up our context, we proceed to allocate and initialize the channels that we need to interconnect the various workers that we are about to spin up. If we have a total of<span> </span><em>N</em><span> </span>stages, then we need<span> </span><em>N</em>+1<span> </span>channels to connect everything together (including the source and sink workers). For instance, if<span> </span><em>no</em><span> </span>stages were specified when the pipeline was created, then we would still need one channel to connect the source to the sink.</p>
<p>The error channel functions as a<span> </span><em>shared error bus</em>. In the preceding code snippet, you can see that we are creating a<span> </span><em>buffered</em><span> </span>error channel with<span> </span><em>N</em>+2 slots. This provides enough space to hold a potential error for each one of the pipeline components (<em>N</em> stages and the source/sink workers).</p>
<p>In the following block of code, we start a goroutine whose body invokes the<span> </span><kbd>Run</kbd><span> </span>method of the<span> </span><kbd>StageRunner</kbd><span> </span>instance associated with each stage of the pipeline:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Start a worker for each stage</span></a>
<a><span class="kw">for</span> i := <span class="dv">0</span>; i &lt; <span class="bu">len</span>(p.stages); i++ {</a>
<a>    wg.Add(<span class="dv">1</span>)</a>
<a>    <span class="kw">go</span> <span class="kw">func</span>(stageIndex <span class="dt">int</span>) {</a>
<a>        p.stages[stageIndex].Run(pCtx, &amp;workerParams{</a>
<a>            stage: stageIndex,</a>
<a>            inCh:  stageCh[stageIndex],</a>
<a>            outCh: stageCh[stageIndex+<span class="dv">1</span>],</a>
<a>            errCh: errCh,</a>
<a>        })</a>
<a>        <span class="bu">close</span>(stageCh[stageIndex+<span class="dv">1</span>])</a>
<a>        wg.Done()</a>
<a>    }(i)</a>
<a>}</a></pre></div>
<p>As you probably noticed, the output channel of the<span> </span><em>n</em>th<span> </span>worker is used as the input channel for worker<span> </span><em>n</em>+1. Once the<span> </span><kbd>Run</kbd><span> </span>method for the<span> </span><em>n</em>th<span> </span>worker returns, it closes its output channel to signal to the next stage of the pipeline that no more data is available.</p>
<p>After starting the stage workers, we need to spawn two additional workers: one for the input source and one for the output sink:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>wg.Add(<span class="dv">2</span>)</a>
<a><span class="kw">go</span> <span class="kw">func</span>() {</a>
<a>    sourceWorker(pCtx, source, stageCh[<span class="dv">0</span>], errCh)</a>
<a>    <span class="bu">close</span>(stageCh[<span class="dv">0</span>])</a>
<a>    wg.Done()</a>
<a>}()</a>
<a><span class="kw">go</span> <span class="kw">func</span>() {</a>
<a>    sinkWorker(pCtx, sink, stageCh[<span class="bu">len</span>(stageCh)-<span class="dv">1</span>], errCh)</a>
<a>    wg.Done()</a>
<a>}()</a></pre></div>
<p class="mce-root"/>
<p>So far, our pipeline implementation has spawned quite a few goroutines. By this point, you may be wondering: how can we be sure that <em>all</em> of these goroutines will actually terminate?</p>
<p>Once the source worker runs out of data, the call to<span> </span><kbd>sourceWorker</kbd><span> </span>returns and we proceed to close the<span> </span><kbd>stageCh[0]</kbd><span> </span>channel. This triggers an avalanche effect that causes each stage worker to cleanly terminate. When the<span> </span><em>i</em>th<span> </span>worker detects that its input channel has been closed, it assumes that no more data is available and closes its own output channel (which also happens to be the<span> </span><em>i+1 </em>worker's input) before terminating. The <em>last</em> output channel is connected to the sink worker. Consequently, the sink worker will also terminate once the last stage worker closes its output.</p>
<p>This brings us to the final part of the<span> </span><kbd>Process</kbd><span> </span>method's implementation:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">go</span> <span class="kw">func</span>() {</a>
<a>    wg.Wait()</a>
<a>    <span class="bu">close</span>(errCh)</a>
<a>    ctxCancelFn()</a>
<a>}()</a>

<a><span class="co">// Collect any emitted errors and wrap them in a multi-error.</span></a>
<a><span class="kw">var</span> err <span class="dt">error</span></a>
<a><span class="kw">for</span> pErr := <span class="kw">range</span> errCh {</a>
<a>    err = multierror.Append(err, pErr)</a>
<a>    ctxCancelFn()</a>
<a>}</a>
<a><span class="kw">return</span> err</a></pre></div>
<p>As you can see in the preceding snippet, we spawn one final worker that serves the role of a <strong>monitor</strong>: it waits for all other workers to complete before closing the shared error channel and cancelling the wrapped context.</p>
<p>While all workers are happily running, the<span> </span><kbd>Process</kbd><span> </span>method is using the<span> </span><kbd>range</kbd><span> </span>keyword to iterate the contents of the error channel. If any error gets published to the shared error channel, it will be appended to the<span> </span><kbd>err</kbd><span> </span>value with the help of the<span> </span><kbd>hashicorp/multierror</kbd><span> </span>package<span> </span><sup><span class="citation">[6]</span></sup><span> </span>and the wrapped context will be cancelled to trigger a shutdown of the entire pipeline.</p>
<p>On the other hand, if no error occurs, the preceding  for-loop will block indefinitely until the channel is closed by the monitor worker. Since the error channel will only be closed once all other pipeline workers have terminated, the same range loop prevents the call to<span> </span><kbd>Process</kbd><span> </span>from returning until the pipeline execution completes, with or without an error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a crawler pipeline for the Links 'R' Us project</h1>
                </header>
            
            <article>
                
<p>In the following sections, we will be putting the generic pipeline package that we built to the test by using it to construct the crawler pipeline for the Links 'R' Us project!</p>
<p>Following the single-responsibility principle, we will break down the crawl task into a sequence of smaller subtasks and assemble the pipeline illustrated in the following figure. The decomposition into smaller subtasks also comes with the benefit that each stage processor can be tested in total isolation without the need to create a pipeline instance:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/781d5ab6-b8cb-4425-b390-d4334b3fd396.png" style="width:56.92em;height:31.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 2:</span><span> </span>The stages of the crawler pipeline that we will be constructing</div>
<p>The full code for the crawler and its tests can be found in the <kbd>Chapter07/crawler</kbd> package, which you can find at the book's GitHub repository.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the payload for the crawler</h1>
                </header>
            
            <article>
                
<p>First things first, we need to define the payload that will be shared between the processors for each stage of the pipeline:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> crawlerPayload <span class="kw">struct</span> {</a>
<a>    LinkID      uuid.UUID</a>
<a>    URL         <span class="dt">string</span></a>
<a>    RetrievedAt time.Time</a>

<a>    RawContent bytes.Buffer</a>

<a>    <span class="co">// NoFollowLinks are still added to the graph but no outgoing edges</span></a>
<a>    <span class="co">// will be created from this link to them.</span></a>
<a>    NoFollowLinks []<span class="dt">string</span></a>

<a>    Links       []<span class="dt">string</span></a>
<a>    Title       <span class="dt">string</span></a>
<a>    TextContent <span class="dt">string</span></a>
<a>}</a></pre></div>
<p>The first three fields,<span> </span><kbd>LinkID</kbd>,<span> </span><kbd>URL</kbd>, and<span> </span><kbd>RetrievedAt</kbd><span>, </span>will be populated by the input source. The remaining fields will be populated by the various crawler stages:</p>
<ul>
<li><kbd>RawContent</kbd><span> </span>is populated by the link fetcher</li>
<li><kbd>NoFollowLinks</kbd><span> </span>and<span> </span><kbd>Links</kbd><span> </span>are populated by the link extractor</li>
<li><kbd>Title</kbd><span> </span>and<span> </span><kbd>TextContent</kbd><span> </span>are populated by the text extractor</li>
</ul>
<p>Of course, in order to be able to use this payload definition with the pipeline package, it needs to implement the<span> </span><kbd>pipeline.Payload</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Payload <span class="kw">interface</span> {</a>
<a>    Clone() Payload</a>
<a>    MarkAsProcessed()</a>
<a>}</a></pre></div>
<p>Before we go about implementing these two methods on our payload type, let's take a small break and spend some time learning about the memory-allocation patterns for our application-specific pipeline. Given that our plan is to have the crawler executing as a long-running process and tentatively process a high volume of links, we need to consider whether memory allocations will have an impact on the crawler's performance.</p>
<p class="mce-root"/>
<p>While the pipeline is executing, the input source will allocate a new payload for each new link entering the pipeline. In addition, as we saw in <em>Figure<span> </span>2</em>,<span> </span>one extra copy will be made at the fork point where the payload is sent to the graph updater and text indexer stages. Payloads can either be discarded early on (for example, the link fetcher can filter links using a list of blacklisted file extensions) or eventually make their way to the output sink.</p>
<p>Consequently, we will be generating a large number of small objects that at some point need to be garbage-collected by the Go runtime. Performing a large number of allocations in a relatively short amount of time increases the pressure on the Go <strong>garbage collector</strong> (<strong>GC</strong>) and triggers more frequent GC pauses that affect the latency characteristics of our pipeline.</p>
<p>The best way to verify our theory is to capture a memory-allocation profile for a running crawler pipeline using the<span> </span><kbd>runtime/pprof</kbd><span> </span>package<span> </span><sup><span class="citation">[3]</span></sup><span> </span>and analyze it using the <kbd>pprof</kbd> tool. Using <kbd>pprof</kbd><span> </span><sup>[8]</sup><span> </span>is outside of the scope of this book, so this step is left as an exercise for the curious reader.</p>
<p>Now that we have a better understanding of the expected allocation patterns for our crawler, the next question is: what can we do about it? Fortunately for us, the<span> </span><kbd>sync</kbd><span> </span>package in the Go standard library includes the<span> </span><kbd>Pool</kbd><span> </span>type<span> </span><sup><span class="citation">[4]</span></sup><span>, </span>which is designed for exactly this use case!</p>
<p>The <kbd>Pool</kbd> type attempts to relieve the pressure on the garbage collector by amortizing the cost of allocating objects across multiple clients. This is achieved by maintaining a cache of allocated, but not used, instances. When a client requests a new object from the pool, they can either receive a cached instance or a newly allocated instance if the pool is empty. Once clients are done using the object they obtained, they must return it to the pool so it can be reused by other clients. Note that any objects <em>within</em> the pool that are not in use by clients are fair game for the garbage collector and can be reclaimed at any time.</p>
<p>Here is the definition of the pool that we will be using for recycling payload instances:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> (</a>
<a>    payloadPool = sync.Pool{</a>
<a>        New: <span class="kw">func</span>() <span class="kw">interface</span>{} { </a>
<a>            <span class="kw">return</span> <span class="bu">new</span>(crawlerPayload) </a>
<a>        },</a>
<a>    }</a>
<a>)</a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The<span> </span><kbd>New</kbd><span> </span>method will be automatically invoked by the underlying pool implementation to service incoming client requests when it has run out of cached items. As the zero value of the<span> </span><kbd>Payload</kbd><span> </span>type is already a valid payload, all we need to do is allocate and return a new<span> </span><kbd>Payload</kbd><span> </span>instance. Let's see how we can use the pool that we just defined to implement the<span> </span><kbd>Clone</kbd><span> </span>method for the payload:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (p *crawlerPayload) Clone() pipeline.Payload {</a>
<a>    newP := payloadPool.Get().(*Payload)</a>
<a>    newP.LinkID = p.LinkID</a>
<a>    newP.URL = p.URL</a>
<a>    newP.RetrievedAt = p.RetrievedAt</a>
<a>    newP.NoFollowLinks = <span class="bu">append</span>([]<span class="dt">string</span>(<span class="ot">nil</span>), p.NoFollowLinks...)</a>
<a>    newP.Links = <span class="bu">append</span>([]<span class="dt">string</span>(<span class="ot">nil</span>), p.Links...)</a>
<a>    newP.Title = p.Title</a>
<a>    newP.TextContent = p.TextContent</a>

<a>    _, err := io.Copy(&amp;newP.RawContent, &amp;p.RawContent)</a>
<a>    <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>        <span class="bu">panic</span>(fmt.Sprintf(<span class="st">"[BUG] error cloning payload raw content: %v"</span>, err))</a>
<a>    }</a>
<a>    <span class="kw">return</span> newP</a>
<a>}</a></pre></div>
<p>As you can see, a new payload instance is allocated from the pool and all fields from the original payload are copied over before it is returned to the caller. Finally, let's take a look at the<span> </span><kbd>MarkAsProcessed</kbd><span> </span>method implementation:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (p *crawlerPayload) MarkAsProcessed() {</a>
<a>    p.URL = p.URL[:<span class="dv">0</span>]</a>
<a>    p.RawContent.Reset()</a>
<a>    p.NoFollowLinks = p.NoFollowLinks[:<span class="dv">0</span>]</a>
<a>    p.Links = p.Links[:<span class="dv">0</span>]</a>
<a>    p.Title = p.Title[:<span class="dv">0</span>]</a>
<a>    p.TextContent = p.TextContent[:<span class="dv">0</span>]</a>
<a>    payloadPool.Put(p)</a>
<a>}</a></pre></div>
<p>When<span> </span><kbd>MarkAsProcessed</kbd><span> </span>is invoked, we need to clear the payload contents before returning it to the pool so it can be safely used by the next client that retrieves it.</p>
<p class="mce-root"/>
<p>One other thing to note is that we also employ a small optimization trick to reduce the total number of allocations that are performed while our pipeline is executing. We set the length of both of the slices and the byte buffer to<span> </span>zero without modifying their original capacities. The next time that a recycled payload is sent through the pipeline, any attempt to write to the byte buffer or append to one of the payload slices will reuse the already allocated space and only trigger a new memory allocation if additional space is required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a source and a sink for the crawler</h1>
                </header>
            
            <article>
                
<p>A prerequisite for executing the crawler pipeline is to provide an input source that conforms to the<span> </span><kbd>pipeline.Source</kbd><span> </span>interface and an output sink that implements<span> </span><kbd>pipeline.Sink</kbd>. We have discussed both these interfaces in the previous sections, but I am copying their definitions as follows for reference:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Source <span class="kw">interface</span> {</a>
<a>    Next(context.Context) <span class="dt">bool</span></a>
<a>    Payload() Payload</a>
<a>    Error() <span class="dt">error</span></a>
<a>}</a>

<a><span class="kw">type</span> Sink <span class="kw">interface</span> {</a>
<a>    Consume(context.Context, Payload) <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>In <a href="ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml">Chapter 6</a>, <em>Building a Persistence Layer</em>, we put together the interface of the link graph component and came up with two alternative, concrete implementations. One of the methods of the<span> </span><kbd>graph.Graph</kbd><span> </span>interface that is of particular interest at this point is<span> </span><kbd>Links</kbd>. The<span> </span><kbd>Links</kbd><span> </span>method returns a<span> </span><kbd>graph.LinkIterator</kbd><span>, </span>which allows us to traverse the list of links within a section (partition) of the graph or even the graph in its entirety. As a quick refresher, here is the list of methods included in the<span> </span><kbd>graph.LinkIterator</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> LinkIterator <span class="kw">interface</span> {</a>
<a>    Next() <span class="dt">bool</span></a>
<a>    Error() <span class="dt">error</span></a>
<a>    Close() <span class="dt">error</span></a>
<a>    Link() *Link</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>As you can see, the<span> </span><kbd>LinkIterator</kbd><span> </span>and the<span> </span><kbd>Source</kbd><span> </span>interfaces are quite similar to each other. As it turns out, we can apply the decorator design pattern (as shown in the following code) to wrap a<span> </span><kbd>graph.LinkIterator</kbd><span> </span>and turn it into an input source that is compatible with our pipeline!</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> linkSource <span class="kw">struct</span> {</a>
<a>    linkIt graph.LinkIterator</a>
<a>}</a>

<a><span class="kw">func</span> (ls *linkSource) Error() <span class="dt">error</span>              { <span class="kw">return</span> ls.linkIt.Error() }</a>
<a><span class="kw">func</span> (ls *linkSource) Next(context.Context) <span class="dt">bool</span> { <span class="kw">return</span> ls.linkIt.Next() }</a>
<a><span class="kw">func</span> (ls *linkSource) Payload() pipeline.Payload {</a>
<a>    link := ls.linkIt.Link()</a>
<a>    p := payloadPool.Get().(*crawlerPayload)</a>
<a>    p.LinkID = link.ID</a>
<a>    p.URL = link.URL</a>
<a>    p.RetrievedAt = link.RetrievedAt</a>
<a>    <span class="kw">return</span> p</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Error</kbd><span> </span>and<span> </span><kbd>Next</kbd><span> </span>methods are simply proxies to the underlying iterator object. The<span> </span><kbd>Payload</kbd><span> </span>method fetches a<span> </span><kbd>Payload</kbd><span> </span>instance from the pool and populates its fields from the<span> </span><kbd>graph.Link</kbd><span> </span>instance that was obtained via the iterator.</p>
<p>Things are much simpler as far as the output sink is concerned. After each payload goes through the<span> </span>link updater<span> </span>and<span> </span>text indexer<span> </span>stages, we have no further use for it! As a result, all we need to do is to provide a sink implementation that functions as a black hole:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> nopSink <span class="kw">struct</span>{}</a>

<a><span class="kw">func</span> (nopSink) Consume(context.Context, pipeline.Payload) <span class="dt">error</span> { </a>
<a>    <span class="kw">return</span> <span class="ot">nil</span> </a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Consume</kbd><span> </span>method simply ignores payloads and always returns a<span> </span><kbd>nil</kbd><span> </span>error. Once the call to<span> </span><kbd>Consume</kbd><span> </span>returns,  the pipeline worker automatically invokes the<span> </span><kbd>MarkAsProcessed</kbd><span> </span>method on the payload, which, as we saw in the previous section, ensures that the payload gets returned to the pool so it can be reused in the future.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fetching the contents of graph links </h1>
                </header>
            
            <article>
                
<p>The link fetcher serves as the first stage of the crawler pipeline. It operates on<span> </span><kbd>Payload</kbd><span> </span>values emitted by the input source and attempts to retrieve the contents of each link by sending out HTTP GET requests. The retrieved link web page contents are stored within the payload's<span> </span><kbd>RawContent</kbd><span> </span>field and made available to the following stages of the pipeline.</p>
<p>Let's now take a look at the definition of the<span> </span><kbd>linkFetcher</kbd><span> </span>type and its associated methods:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> linkFetcher <span class="kw">struct</span> {</a>
<a>    urlGetter URLGetter</a>
<a>    netDetector PrivateNetworkDetector</a>
<a>}</a>

<a><span class="kw">func</span> newLinkFetcher(urlGetter URLGetter, netDetector PrivateNetworkDetector) *linkFetcher {</a>
<a>    <span class="kw">return</span> &amp;linkFetcher{</a>
<a>        urlGetter: urlGetter,</a>
<a>        netDetector: netDetector,</a>
<a>    }</a>
<a>}</a>

<a><span class="kw">func</span> (lf *linkFetcher) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, <span class="dt">error</span>) {</a>
<a>    <span class="co">//...</span></a>
<a>}</a></pre></div>
<p>While the Go standard library comes with the<span> </span><kbd>http</kbd><span> </span>package that we could directly use to fetch the link contents, it is often a good practice to allow the intended users of the code to plug in their preferred implementation for performing HTTP calls. As the link fetcher is only concerned about making GET requests, we will apply the interface segregation principle and define a<span> </span><kbd>URLGetter</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// URLGetter is implemented by objects that can perform HTTP GET requests.</span></a>
<a><span class="kw">type</span> URLGetter <span class="kw">interface</span> {</a>
<a>    Get(url <span class="dt">string</span>) (*http.Response, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>This approach brings a few important benefits to the table. To begin with, it allows us to test the link fetcher code without the need to spin up a dedicated test server. While it is quite common to use the<span> </span><kbd>httptest.NewServer</kbd><span> </span>method to create servers for testing, arranging for the test server to return the right payload and/or status code for each individual test requires extra effort.</p>
<p class="mce-root"/>
<p>Moreover, having a test server available doesn't really help in scenarios where we expect the<span> </span><kbd>Get</kbd><span> </span>call to return an error and a nil<span> </span><kbd>http.Response</kbd>. This could be quite useful for evaluating how our code behaves in the presence of DNS lookup failures or TLS validation errors. By introducing this interface-based abstraction, we can use a package such as <kbd>gomock</kbd> <sup><span class="citation">[5]</span></sup><span> </span>to generate a compatible mock for our tests, as we illustrated in <a href="d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml">Chapter 4</a>, <em>The Art of Testing</em>.</p>
<p>Besides testing, this approach makes our implementation much more versatile! The end users of the crawler are now given the flexibility to either pass<span> </span><kbd>http.DefaultClient</kbd><span> if they prefer to use a sane default, </span>or to provide their own customized<span> </span><kbd>http.Client</kbd><span> </span>implementation, which can additionally deal with retries, proxies, and so on.</p>
<p>In <a href="6e4047ad-1fc1-4c3e-b90a-f27a62d06f17.xhtml">Chapter 5</a>, <em>The Links 'R' Us Project</em>, we discussed a list of potential security issues associated with automatically crawling links that are obtained through third-party resources that are outside of our control. The key takeaway from that discussion was that our crawler should never attempt to fetch links that belong to private network addresses, as that could lead in sensitive data ending up in our search index! To this end, the<span> </span><kbd>newLinkFetcher</kbd><span> </span>function also expects an argument that implements the<span> </span><kbd>PrivateNetworkDetector</kbd><span> </span>interface:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// PrivateNetworkDetector is implemented by objects that can detect whether a</span></a>
<a><span class="co">// host resolves to a private network address.</span></a>
<a><span class="kw">type</span> PrivateNetworkDetector <span class="kw">interface</span> {</a>
<a>    IsPrivate(host <span class="dt">string</span>) (<span class="dt">bool</span>, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>Chapter07/crawler/privnet</kbd><span> </span>package contains a simple private network detector implementation that first resolves hosts into an IP address and then checks whether the IP address belongs to any of the private network ranges defined by RFC1918<span> </span><sup><span class="citation">[7]</span></sup>.</p>
<p>Now that we have covered all of the important details surrounding the creation of a new<span> </span><kbd>linkFetcher</kbd><span> </span>instance, let's take a look at its internals. As expected by any component that we want to include in our pipeline,<span> </span><kbd>linkFetcher</kbd><span> </span>adheres to the<span> </span><kbd>pipeline.Processor</kbd><span> </span>interface. Let's break down the<span> </span><kbd>Process</kbd><span> </span>method into smaller chunks so we can analyze it further:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>payload := p.(*crawlerPayload)</a>

<a><span class="kw">if</span> exclusionRegex.MatchString(payload.URL) {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, <span class="ot">nil</span> <span class="co">// Skip URLs that point to files that cannot contain<br/>                    // html content.</span></a>
<a>}</a>

<a><span class="kw">if</span> isPrivate, err := lf.isPrivate(payload.URL); err != <span class="ot">nil</span> || isPrivate {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, <span class="ot">nil</span> <span class="co">// Never crawl links in private networks</span></a>
<a>}</a>

<a>res, err := lf.urlGetter.Get(payload.URL)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The first step is to cast the incoming<span> </span><kbd>pipeline.Payload</kbd><span> </span>value into the concrete<span> </span><kbd>*crawlerPayload</kbd><span> </span>instance that the input source injected into the pipeline. Next, we check the URL against a case-insensitive regular expression (its definition will be shown in the following section) designed to match file extensions that are known to contain binary data (for example, images) or text content (for example, loadable scripts, JSON data, and so on) that the crawler should ignore. If a match is found, the link fetcher instructs the pipeline to discard the payload by returning the values<span> </span><kbd>nil, nil</kbd>. The second and final precheck ensures that the crawler always ignores URLs that resolve to private network addresses. Finally, we invoke the provided<span> </span><kbd>URLGetter</kbd><span> </span>to retrieve the contents of the link.</p>
<p>Let's now see what happens after the call to the <kbd>URLGetter</kbd> returns:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>_, err = io.Copy(&amp;payload.RawContent, res.Body)</a>
<a>_ = res.Body.Close()</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>}</a>
<a><span class="kw">if</span> res.StatusCode &lt; <span class="dv">200</span> || res.StatusCode &gt; <span class="dv">299</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, <span class="ot">nil</span></a>
<a>}</a>
<a><span class="kw">if</span> contentType := res.Header.Get(<span class="st">"Content-Type"</span>); !strings.Contains(contentType, <span class="st">"html"</span>) {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, <span class="ot">nil</span></a>
<a>}</a>

<a><span class="kw">return</span> payload, <span class="ot">nil</span></a></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For GET requests that complete without an error, we copy the response body into the payload's<span> </span><kbd>RawContent</kbd><span> </span>field and then close the body to avoid memory leaks. Before allowing the payload to continue to the next pipeline stage, we perform two additional sanity checks:</p>
<ul>
<li>The response status code should be in the 2xx range. If not, we discard the payload rather than returning an error as the latter would cause the pipeline to terminate. Not processing a link is not a big issue; the crawler will be running periodically, so the crawler will revisit problematic links in the future.</li>
<li>The<span> </span><kbd>Content-Type</kbd><span> </span>header should indicate that the response contains an HTML document; otherwise, there is no point in further processing the response, so we can simply discard it.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting outgoing links from retrieved webpages</h1>
                </header>
            
            <article>
                
<p>The task of the link extractor is to scan the body of each retrieved HTML document and extract the<span> </span>unique set of links contained within it. Each <strong>uniform resource locator</strong> (<strong>URL</strong>) in a web page can be classified into one of the following categories:</p>
<ul>
<li><strong>URL with a network path reference</strong><span> </span><sup><span class="citation">[1]</span></sup>: This type of link is quite easy to identify as it<span> </span><em>does not</em><span> </span>include a URL scheme (for example, <kbd>&lt;img src="//banner.png"/&gt;</kbd>). When the web browser (or crawler, in our case) needs to access the link, it will substitute the protocol used to access the web page that contained it. Consequently, if the parent page was accessed via HTTPS, then the browser will also request the banner image over HTTPS.</li>
<li><strong>Absolute links</strong>: These links are fully qualified<span> </span>and are typically used to point at resources that are hosted on different domains.</li>
<li><strong>Relative links</strong>: As the name implies, these links are resolved relative to the current page URL. It is also important to note that web pages can opt to override the URL used for resolving relative links by specifying a<span> </span><kbd>&lt;base href="XXX"&gt;</kbd><span> </span>tag in their<span> </span><kbd>&lt;head&gt;</kbd><span> </span>section.</li>
</ul>
<p>By design, the link graph component only stores fully qualified links. Therefore, one of the key responsibilities of the link extractor is to resolve all relative links into absolute URLs. This is achieved via the<span> </span><kbd>resolveURL</kbd><span> </span>helper function, which is shown as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> resolveURL(relTo *url.URL, target <span class="dt">string</span>) *url.URL {</a>
<a>    tLen := <span class="bu">len</span>(target)</a>
<a>    <span class="kw">if</span> tLen == <span class="dv">0</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span></a>
<a>    } <span class="kw">else</span> <span class="kw">if</span> tLen &gt;= <span class="dv">1</span> &amp;&amp; target[<span class="dv">0</span>] == <span class="ch">'/'</span> {</a>
<a>        <span class="kw">if</span> tLen &gt;= <span class="dv">2</span> &amp;&amp; target[<span class="dv">1</span>] == <span class="ch">'/'</span> {</a>
<a>            target = relTo.Scheme + <span class="st">":"</span> + target</a>
<a>        }</a>
<a>    }</a>
<a>    <span class="kw">if</span> targetURL, err := url.Parse(target); err == <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> relTo.ResolveReference(targetURL)</a>
<a>    }</a>

<a>    <span class="kw">return</span> <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>resolveURL</kbd><span> </span>function is invoked using a parsed<span> </span><kbd>url.URL</kbd><span> </span>and a target path to resolve relative to it. Resolving relative paths is not a trivial process because of the number of rules specified in RFC 3986<span> </span><sup><span class="citation">[1]</span></sup>. Fortunately, the<span> </span><kbd>URL</kbd><span> </span>type provides the handy<span> </span><kbd>ResolveReference</kbd><span> </span>method that takes care of all the complexity for us. Before passing the target to the<span> </span><kbd>ResolveReference</kbd><span> </span>method, the code performs an extra check to detect network path references. If the target begins with a<span> </span><kbd>//</kbd><span> </span>prefix, the implementation will rewrite the target link by prepending the scheme from the provided<span> </span><kbd>relTo</kbd><span> </span>value.</p>
<p>Before we examine the link extractor's implementation, we need to define a few useful regular expressions that we will be using in the code:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">var</span> (</a>
<a>    exclusionRegex = regexp.MustCompile(<span class="st">`(?i)\.(?:jpg|jpeg|png|gif|ico|css|js)$`</span>)</a>
<a>    baseHrefRegex = regexp.MustCompile(<span class="st">`(?i)&lt;base.*?href\s*?=\s*?"(.*?)\s*?"`</span>)</a>
<a>    findLinkRegex = regexp.MustCompile(<span class="st">`(?i)&lt;a.*?href\s*?=\s*?"\s*?(.*?)\s*?".*?&gt;`</span>)</a>
<a>    nofollowRegex = regexp.MustCompile(<span class="st">`(?i)rel\s*?=\s*?"?nofollow"?`</span>)</a>
<a>)</a></pre></div>
<p>We will be using the preceding case-insensitive regular expressions to do the following:</p>
<ul>
<li>Skip extracted links that point to non-HTML content. Note that this particular regular expression instance is shared between this stage and the link fetcher stage.</li>
<li>Locate the<span> </span><kbd>&lt;base href="XXX"&gt;</kbd><span> </span>tag and capture the value in the<span> </span><kbd>href</kbd><span> </span>attribute.</li>
<li>Extract links from the HTML contents. The second regular expression is designed to locate the <span> </span><kbd>&lt;a href="XXX"&gt;</kbd><span> </span>elements and capture the value in the<span> </span><kbd>href</kbd><span> </span>attribute.</li>
<li><span>Identify links that should be inserted into the graph but shou</span>ld not be considered <span>when calculating the <kbd>PageRank</kbd> score for the page that links to them. Web masters can indicate such links by adding a </span><kbd>rel</kbd><span> attribute with the</span> <kbd>nofollow</kbd><span> value to the </span><kbd>&lt;a&gt;</kbd><span> tag. For instance, forum operators can add </span><kbd>nofollow</kbd><span> tags to links in posted messages to prevent users from artificially increasing the <kbd>PageRank</kbd> scores to their websites by cross-posting links to multiple forums.</span></li>
</ul>
<p>The following listing shows the definition of the<span> </span><kbd>linkExtractor</kbd><span> </span>type. Similar to the<span> </span><kbd>linkFetcher</kbd><span> </span>type, the<span> </span><kbd>linkExtractor</kbd><span> </span>also requires a<span> </span><kbd>PrivateNetworkDetector</kbd><span> </span>instance for further filtering extracted links:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> linkExtractor <span class="kw">struct</span> {</a>
<a>    netDetector PrivateNetworkDetector</a>
<a>}</a>

<a><span class="kw">func</span> newLinkExtractor(netDetector PrivateNetworkDetector) *linkExtractor {</a>
<a>    <span class="kw">return</span> &amp;linkExtractor{</a>
<a>        netDetector: netDetector,</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The business logic of the link extractor is encapsulated inside its<span> </span><kbd>Process</kbd><span> </span>method. As the implementation is a bit lengthy, we will once again split it into smaller chunks and discuss each chunk separately. Consider the following code block:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>payload := p.(*crawlerPayload)</a>
<a>relTo, err := url.Parse(payload.URL)</a>
<a><span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>}</a>

<a><span class="co">// Search page content for a &lt;base&gt; tag and resolve it to an abs URL.</span></a>
<a>content := payload.RawContent.String()</a>
<a><span class="kw">if</span> baseMatch := baseHrefRegex.FindStringSubmatch(content); <span class="bu">len</span>(baseMatch) == <span class="dv">2</span> {</a>
<a>    <span class="kw">if</span> base := resolveURL(relTo, ensureHasTrailingSlash(baseMatch[<span class="dv">1</span>])); base != <span class="ot">nil</span> {</a>
<a>        relTo = base</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>In order to be able to resolve any relative link we might encounter, we need a fully qualified link to use as a base. By default, that would be the incoming link URL that the code parses into a<span> </span><kbd>url.URL</kbd><span> </span>value. As we mentioned previously, if the page includes a valid<span> </span><kbd>&lt;base href="XXX"&gt;</kbd><span> </span>tag, we must resolve relative links using <em>that</em> instead.</p>
<p>To detect the presence of a<span> </span><kbd>&lt;base&gt;</kbd><span> </span>tag, we execute the<span> </span><kbd>baseHrefRegex</kbd><span> regular expression </span>against the page content. If we obtain a valid match,<span> </span><kbd>baseMatch</kbd><sup>[1]</sup><span> </span>will contain the<span> </span>value of the tag's<span> </span><kbd>href</kbd><span> </span>attribute. The captured value is then passed to the<span> </span><kbd>resolveURL</kbd><span> </span>helper and the resolved URL (if valid) is used to override the<span> </span><kbd>relTo</kbd><span> </span>variable.</p>
<p>The following block of code outlines the link extraction and deduplication steps:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>seenMap := <span class="bu">make</span>(<span class="kw">map</span>[<span class="dt">string</span>]<span class="kw">struct</span>{})</a>
<a><span class="kw">for</span> _, match := <span class="kw">range</span> findLinkRegex.FindAllStringSubmatch(content, <span class="dv">-1</span>) {</a>
<a>    link := resolveURL(relTo, match[<span class="dv">1</span>])</a>
<a>    <span class="kw">if</span> link == <span class="ot">nil</span> || !le.retainLink(relTo.Hostname(), link) {</a>
<a>        <span class="kw">continue</span></a>
<a>    }</a>

<a>    link.Fragment = <span class="st">""</span> <span class="co">// Truncate anchors</span></a>
<a>    linkStr := link.String()</a>
<a>    <span class="kw">if</span> _, seen := seenMap[linkStr]; seen || exclusionRegex.MatchString(linkStr) {</a>
<a>        <span class="kw">continue</span> <span class="co">// skip already seen links and links that do not contain HTML</span></a>
<a>    }</a>
<a>    seenMap[linkStr] = <span class="kw">struct</span>{}{}</a>
<a>    <span class="kw">if</span> nofollowRegex.MatchString(match[<span class="dv">0</span>]) {</a>
<a>        payload.NoFollowLinks = <span class="bu">append</span>(payload.NoFollowLinks, linkStr)</a>
<a>    } <span class="kw">else</span> {</a>
<a>        payload.Links = <span class="bu">append</span>(payload.Links, linkStr)</a>
<a>    }</a>
<a>}</a></pre></div>
<p>The<span> </span><kbd>FindAllStringSubmatch</kbd><span> </span>method returns a list of successive matches for a particular regular expression. The second argument to<span> </span><kbd>FindAllStringSubmatch</kbd><span> </span>controls the maximum number of matches to be returned. Therefore, by passing<span> </span><kbd>-1</kbd><span> </span>as an argument, we effectively ask the regular expression engine to return<span> </span><em>all</em><span> </span><kbd>&lt;a&gt;</kbd><span> </span>matches. We then iterate each matched link and resolve it into an absolute URL. The captured<span> </span><kbd>&lt;a&gt;</kbd><span> </span>tag contents and the resolved link are passed to the<span> </span><kbd>retainLink</kbd><span> </span>predicate, which returns<span> </span><kbd>false</kbd><span> </span>if the link must be skipped.</p>
<p class="mce-root"/>
<p>The final step of the processing loop entails the deduplication of links within the page. To achieve, this we will be using a map where link URLs are used as keys. Prior to checking the map for duplicate entries, we make sure to trim off the<span> </span>fragment part (also known as an HTML<span> </span><strong>anchor</strong>) of each link; after all, from the perspective of our crawler,<span> both </span><kbd>http://example.com/index.html#foo</kbd><span> </span>and<span> </span><kbd>http://example.com/index.html</kbd><span> </span>reference the same link. For each link that survives the <kbd>is-duplicate</kbd> check, we scan its<span> </span><kbd>&lt;a&gt;</kbd><span> </span>tag for the presence of a<span> </span><kbd>rel="nofollow"</kbd><span> </span>attribute. Depending on the outcome of the check, the link is appended either to the<span> </span><kbd>NoFollowLinks</kbd><span> </span>or the<span> </span><kbd>Links</kbd><span> </span>slice of the payload instance and is made available to the following stages of the pipeline.</p>
<p>The last part of code that we need to explore is the<span> </span><kbd>retainLink</kbd><span> </span>method implementation:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (le *linkExtractor) retainLink(srcHost <span class="dt">string</span>, link *url.URL) <span class="dt">bool</span> {</a>
<a>    <span class="kw">if</span> link == <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span> <span class="co">// Skip links that could not be resolved</span></a>
<a>    }</a>
<a>    <span class="kw">if</span> link.Scheme != <span class="st">"http"</span> &amp;&amp; link.Scheme != <span class="st">"https"</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span> <span class="co">// Skip links with non http(s) schemes</span></a>
<a>    }</a>
<a>    <span class="kw">if</span> link.Hostname() == srcHost {</a>
<a>                <span class="kw">return</span> <span class="ot">true</span> <span class="co">// No need to check for private network</span></a>
<a>    }</a>
<a>    <span class="kw">if</span> isPrivate, err := le.netDetector.IsPrivate(link.Host); err != <span class="ot">nil</span> || isPrivate {</a>
<a>        <span class="kw">return</span> <span class="ot">false</span> <span class="co">// Skip links that resolve to private networks</span></a>
<a>    }</a>
<a>    <span class="kw">return</span> <span class="ot">true</span></a>
<a>}</a></pre></div>
<p>As you can see from the preceding code, we perform two types of checks beforehand to decide whether a link should be retained or skipped:</p>
<ul>
<li>Links with a scheme other than HTTP or HTTPS should be skipped. Allowing other scheme types is a potential security risk! A malicious user could submit a web page containing links using <kbd>file://</kbd> URLs, which could possibly trick the crawler into reading (and indexing) files from the local filesystem.</li>
<li>We have already enumerated the security implications of allowing crawlers to access resources located at private network addresses. Therefore, any links pointing to private networks are automatically skipped.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting the title and text from retrieved web pages</h1>
                </header>
            
            <article>
                
<p>The next stage of the pipeline is responsible for extracting an index-friendly, text-only version of the web page contents and its title. The easiest way to achieve this is by stripping off any HTML tag in the page body and replacing consecutive whitespace characters with a single space.</p>
<p>A fairly straightforward approach would be to come up with a bunch of regular expressions for matching and then removing HTML tags. Unfortunately, the fact that HTML syntax is quite forgiving (that is, you can open a tag and never close it) makes HTML documents notoriously hard to properly clean up just with the help of regular expressions. Truth be told, to cover all possible edge cases, we need to use a parser that understands the structure of HTML documents.</p>
<p>Instead of reinventing the wheel, we will rely on the bluemonday<span> </span><sup><span class="citation">[2]</span></sup><span> </span>Go package for our HTML sanitization needs. The package exposes a set of configurable filtering policies that can be applied to HTML documents. For our particular use case, we will be using a strict policy (obtained via a call to the<span> </span><kbd>bluemonday.StrictPolicy</kbd><span> </span>helper) that effectively removes all HTML tags from the input document.</p>
<p>A small caveat is that bluemonday policies maintain their own internal state and are therefore not safe to use concurrently. Consequently, to avoid allocating a new policy each time we need to process a payload, we will be using a<span> </span><kbd>sync.Pool</kbd><span> </span>instance to recycle bluemonday policy instances. The pool will be initialized when a new<span> </span><kbd>textExtractor</kbd><span> </span>instance is created, as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> textExtractor <span class="kw">struct</span> {</a>
<a>    policyPool sync.Pool</a>
<a>}</a>

<a><span class="kw">func</span> newTextExtractor() *textExtractor {</a>
<a>    <span class="kw">return</span> &amp;textExtractor{</a>
<a>        policyPool: sync.Pool{</a>
<a>            New: <span class="kw">func</span>() <span class="kw">interface</span>{} {</a>
<a>                <span class="kw">return</span> bluemonday.StrictPolicy()</a>
<a>            },</a>
<a>        },</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>Let's take a closer look at the text extractor's<span> </span><kbd>Process</kbd><span> </span>method implementation:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (te *textExtractor) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, <span class="dt">error</span>) {</a>
<a>    payload := p.(*crawlerPayload)</a>
<a>    policy := te.policyPool.Get().(*bluemonday.Policy)</a>

<a>    <span class="kw">if</span> titleMatch := titleRegex.FindStringSubmatch(payload.RawContent.String()); <span class="bu">len</span>(titleMatch) == <span class="dv">2</span> {</a>
<a>        payload.Title = strings.TrimSpace(html.UnescapeString(repeatedSpaceRegex.ReplaceAllString(</a>
<a>            policy.Sanitize(titleMatch[<span class="dv">1</span>]), <span class="st">" "</span>,</a>
<a>        )))</a>
<a>    }</a>
<a>    payload.TextContent = strings.TrimSpace(html.UnescapeString(repeatedSpaceRegex.ReplaceAllString(</a>
<a>        policy.SanitizeReader(&amp;payload.RawContent).String(), <span class="st">" "</span>,</a>
<a>    )))</a>

<a>    te.policyPool.Put(policy)</a>
<a>    <span class="kw">return</span> payload, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>After obtaining a new bluemonday policy from the pool, we execute a regular expression to detect whether the HTML document contains a<span> </span><kbd>&lt;title&gt;</kbd><span> </span>tag. If a match is found, its content is sanitized and saved into the<span> </span><kbd>Title</kbd><span> </span>attribute of the payload. The same policy is also applied against the web page contents, but this time, the sanitized result is stored in the<span> </span><kbd>TextContent</kbd><span> </span>attribute of the payload.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inserting discovered outgoing links to the graph</h1>
                </header>
            
            <article>
                
<p>The next crawler pipeline stage that we will be examining is the graph updater. Its main purpose is to insert newly discovered links into the link graph and create edges connecting them to the web page they were retrieved from. Let's take a look at the definition of the<span> </span><kbd>graphUpdater</kbd><span> </span>type and its constructor:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> graphUpdater <span class="kw">struct</span> {</a>
<a>    updater Graph</a>
<a>}</a>
<a><span class="kw">func</span> newGraphUpdater(updater Graph) *graphUpdater {</a>
<a>    <span class="kw">return</span> &amp;graphUpdater{</a>
<a>        updater: updater,</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>The constructor expects an argument of the<span> </span><kbd>Graph</kbd><span> </span><span>type, </span><span>which is nothing more than an interface describing the methods needed for the graph updater to communicate with a link graph component:</span></p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Graph <span class="kw">interface</span> {</a>
<a>    UpsertLink(link *graph.Link) <span class="dt">error</span></a>
<a>    UpsertEdge(edge *graph.Edge) <span class="dt">error</span></a>
<a>    RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) <span class="dt">error</span></a>
<a>}</a></pre></div>
<p>The astute reader will probably notice that the preceding interface definition includes a subset of the methods from the similarly named interface in the<span> </span><kbd>graph</kbd><span> </span>package. This is a prime example of applying the interface-segregation principle to distill an existing, more open interface into the minimum possible interface that our code requires for it to function. Next, we will take a look at the implementation of the graph updater's<span> </span><kbd>Process</kbd><span> </span>method:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>payload := p.(*crawlerPayload)</a>

<a>src := &amp;graph.Link{</a>
<a>    ID:           payload.LinkID,</a>
<a>    URL:          payload.URL,</a>
<a>    RetrievedAt: time.Now(),</a>
<a>}</a>
<a><span class="kw">if</span> err := u.updater.UpsertLink(src); err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>}</a></pre></div>
<p>Before we iterate the list of discovered links, we first attempt to upsert the origin link from the payload to the graph by creating a new<span> </span><kbd>graph.Link</kbd><span> </span>object and invoking the graph's<span> </span><kbd>UpsertLink</kbd><span> </span>method. The origin link already exists in the graph, so all that the preceding upsert call does is update the timestamp for the<span> </span><kbd>RetrievedAt</kbd><span> </span>field.</p>
<p>The next step entails the addition of any discovered links with a no-follow <kbd>rel</kbd> attribute to the graph:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">for</span> _, dstLink := <span class="kw">range</span> payload.NoFollowLinks {</a>
<a>    dst := &amp;graph.Link{URL: dstLink}</a>
<a>    <span class="kw">if</span> err := u.updater.UpsertLink(dst); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>After processing all no-follow links, the graph updater iterates the slice of regular links and adds each one into the link graph together with a directed edge from the origin link to each outgoing link:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a>removeEdgesOlderThan := time.Now()</a>
<a><span class="kw">for</span> _, dstLink := <span class="kw">range</span> payload.Links {</a>
<a>    dst := &amp;graph.Link{URL: dstLink}</a>

<a>    <span class="kw">if</span> err := u.updater.UpsertLink(dst); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>

<a>    <span class="kw">if</span> err := u.updater.UpsertEdge(&amp;graph.Edge{Src: src.ID, Dst: dst.ID}); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>
<a>}</a></pre></div>
<p>All edges created or updated during this pass will be assigned an <kbd>UpdatedAt</kbd> value that is greater than or equal to the<span> </span><kbd>removeEdgesOlderThan</kbd><span> </span>value that we capture before entering the loop. We can then use the following block of code to remove any existing edges that were not touched by the preceding loop:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">if</span> err := u.updater.RemoveStaleEdges(src.ID, removeEdgesOlderThan); err != <span class="ot">nil</span> {</a>
<a>    <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>}</a></pre></div>
<p>To understand how the preceding process works, let's walk through a simple example. Assume that a<span>t time</span><span> </span><em>t<sub>0</sub></em><span>, the crawler processed a web page located at</span><span> </span><kbd>https://example.com</kbd><span>. At that particular point in time, the page contained outgoing links to</span><span> </span><kbd>http://foo.com</kbd><span> </span><span>and</span><span> </span><kbd>https://bar.com</kbd><span>. After the crawler completed its first pass, the link graph would contain the following set of edge entries:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Source</strong></td>
<td><strong>Destination</strong></td>
<td><strong>UpdatedAt</strong></td>
</tr>
<tr class="odd">
<td><kbd>https://example.com</kbd></td>
<td><kbd>http://foo.com</kbd><a href="http://foo.com"/></td>
<td>t<sub>0</sub></td>
</tr>
<tr class="even">
<td><kbd>https://example.com</kbd></td>
<td><kbd>https://bar.com</kbd></td>
<td>t<sub>0</sub></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Next, the crawler makes a new pass, this time at time<span> </span><em>t<sub>1</sub></em><span> </span>(where t<sub>1</sub> &gt; t<sub>0</sub>); however, the contents for the page located at <kbd>https://example.com</kbd> have now changed: the link to<span> </span><kbd><span>http://</span>foo.com</kbd><span> </span>is now <strong>gone</strong> and the page authors introduced a new link to<span> </span><kbd>https://baz.com</kbd>.</p>
<p class="mce-root"/>
<p>After we have updated the edge list and<span> </span>before we prune any stale edges, the edge entries in the link graph would look as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Source</strong></td>
<td><strong>Destination</strong></td>
<td><strong>UpdatedAt</strong></td>
</tr>
<tr class="odd">
<td><kbd>https://example.com</kbd></td>
<td><kbd>http://foo.com</kbd></td>
<td>t<sub>0</sub></td>
</tr>
<tr class="even">
<td><kbd>https://example.com</kbd></td>
<td><kbd>https://bar.com</kbd></td>
<td>t<sub>1</sub></td>
</tr>
<tr class="odd">
<td><kbd>https://example.com</kbd></td>
<td><kbd>https://baz.com</kbd></td>
<td>t<sub>1</sub></td>
</tr>
</tbody>
</table>
<p> </p>
<p>The prune step deletes all edges originating from<span> <em>https://</em></span><em>example.com</em><span> </span>that were last updated before<span> </span><em>t<sub>1</sub></em>. As a result, once the crawler completes its second pass, the final set of edge entries will look as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Source</strong></td>
<td><strong>Destination</strong></td>
<td><strong>UpdatedAt</strong></td>
</tr>
<tr class="odd">
<td><kbd><span>https://</span>example.com</kbd></td>
<td><kbd>bar.com</kbd></td>
<td>t<sub>1</sub></td>
</tr>
<tr class="even">
<td><kbd><span>https://</span>example.com</kbd></td>
<td><kbd>baz.com</kbd></td>
<td>t<sub>1</sub></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing the contents of retrieved web pages</h1>
                </header>
            
            <article>
                
<p>The last component in our pipeline is the text indexer. As the name implies, the text indexer is responsible for keeping the search index up to date by reindexing the content of each crawled web page.</p>
<p>In a similar fashion to the graph updater stage, we apply the single-responsibility principle and define the<span> </span><kbd>Indexer</kbd><span> </span>interface that gets passed to the text indexer component via its constructor:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Indexer is implemented by objects that can index the contents of webpages retrieved by the crawler pipeline.</span></a>
<a><span class="kw">type</span> Indexer <span class="kw">interface</span> {</a>
<a>    Index(doc *index.Document) <span class="dt">error</span></a>
<a>}</a>

<a><span class="kw">type</span> textIndexer <span class="kw">struct</span> {</a>
<a>    indexer Indexer</a>
<a>}</a>

<a><span class="kw">func</span> newTextIndexer(indexer Indexer) *textIndexer {</a>
<a>    <span class="kw">return</span> &amp;textIndexer{</a>
<a>        indexer: indexer,</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>The following code listing outlines the<span> </span><kbd>Process</kbd><span> </span>method implementation for the<span> </span><kbd>textIndexer</kbd><span> </span>type:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (i *textIndexer) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, <span class="dt">error</span>) {</a>
<a>    payload := p.(*crawlerPayload)</a>
<a>    doc := &amp;index.Document{</a>
<a>        LinkID:    payload.LinkID,</a>
<a>        URL:       payload.URL,</a>
<a>        Title:     payload.Title,</a>
<a>        Content:   payload.TextContent,</a>
<a>        IndexedAt: time.Now(),</a>
<a>    }</a>
<a>    <span class="kw">if</span> err := i.indexer.Index(doc); err != <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span>, err</a>
<a>    }</a>

<a>    <span class="kw">return</span> p, <span class="ot">nil</span></a>
<a>}</a></pre></div>
<p>Nothing out of the ordinary in the preceding code snippet: we create new<span> </span><kbd>index.Document</kbd><span> </span>instance and populate it with the title and content values provided by the text extractor stage of the pipeline. The document is then inserted into the search index by invoking the<span> </span><kbd>Index</kbd><span> </span>method on the externally provided<span> </span><kbd>Indexer</kbd><span> </span>instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assembling and running the pipeline</h1>
                </header>
            
            <article>
                
<p>Congratulations for making it this far! We have finally implemented all individual components that are required for constructing a pipeline for our crawler service. All that's left is to add a little bit of glue code to assemble the individual crawler stages into a pipeline and provide a simple API for running a full crawler pass. All this glue logic is encapsulated inside the<span> </span><kbd>Crawler</kbd><span> </span>type whose definition and constructor details are listed as follows:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">type</span> Crawler <span class="kw">struct</span> {</a>
<a>    p *pipeline.Pipeline</a>
<a>}</a>

<a><span class="co">// NewCrawler returns a new crawler instance.</span></a>
<a><span class="kw">func</span> NewCrawler(cfg Config) *Crawler {</a>
<a>    <span class="kw">return</span> &amp;Crawler{</a>
<a>        p: assembleCrawlerPipeline(cfg),</a>
<a>    }</a>
<a>}</a></pre></div>
<p class="mce-root"/>
<p>The<span> </span><kbd>Config</kbd><span> </span>type holds all required configuration options for creating a new crawler pipeline:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="co">// Config encapsulates the configuration options for creating a new Crawler.</span></a>
<a><span class="kw">type</span> Config <span class="kw">struct</span> {</a>
<a>    PrivateNetworkDetector PrivateNetworkDetector</a>
<a>    URLGetter URLGetter</a>
<a>    Graph Graph</a>
<a>    Indexer Indexer</a>

<a>    FetchWorkers <span class="dt">int</span></a>
<a>}</a></pre></div>
<p>The caller of the crawler's constructor is expected to provide the following configuration options:</p>
<ul>
<li>An object that implements the<span> </span><kbd>PrivateNetworkDetector</kbd><span> </span>interface, which will be used by the link fetcher and link extractor components to filter out links that resolve to private network addresses</li>
<li>An object that implements the<span> </span><kbd>URLGetter</kbd><span> </span>interface (for example,<span> </span><kbd>http.DefaultClient</kbd>), which the link fetcher will use to perform HTTP GET requests</li>
<li>An object that implements the<span> </span><kbd>Graph</kbd><span> </span>interface (for example, any of the link graph implementations from the previous chapter), which the graph updater component will use to upsert discovered links into the link graph</li>
<li>An object that implements the<span> </span><kbd>Indexer</kbd><span> </span>interface (for example, any of the indexer implementations from the previous chapter), which the text indexer component will use to keep the search index in sync</li>
<li>The size of the worker pool for executing the link fetcher stage of the pipeline</li>
</ul>
<p>The constructor code calls out to the<span> </span><kbd>assembleCrawlerPipeline</kbd><span> </span>helper function, which is responsible for instantiating each stage of the pipeline with the appropriate configuration options and calling out to <kbd>pipeline.New</kbd> to create a new pipeline instance :</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> assembleCrawlerPipeline(cfg Config) *pipeline.Pipeline {</a>
<a>    <span class="kw">return</span> pipeline.New(</a>
<a>        pipeline.FixedWorkerPool(</a>
<a>            newLinkFetcher(cfg.URLGetter, cfg.PrivateNetworkDetector),</a>
<a>            cfg.FetchWorkers,</a>
<a>        ),</a>
<a>        pipeline.FIFO(newLinkExtractor(cfg.PrivateNetworkDetector)),</a>
<a>        pipeline.FIFO(newTextExtractor()),</a>
<a>        pipeline.Broadcast(</a>
<a>            newGraphUpdater(cfg.Graph),</a>
<a>            newTextIndexer(cfg.Indexer),</a>
<a>        ),</a>
<a>    )</a>
<a>}</a></pre></div>
<p>As illustrated in <em>Figure<span> </span>2</em>, the first stage of the crawler pipeline uses a fixed-size worker pool that executes the link-fetcher processor. The output from this stage is piped into two sequentially connected FIFO stages that execute the link-extractor and text-extractor processors. Finally, the output of those FIFO stages is copied and broadcast to the graph updater and text indexer components in parallel.</p>
<p>The last piece of the puzzle is the<span> </span><kbd>Crawl</kbd><span> </span>method implementation, which constitutes the API for using the crawler from other packages:</p>
<div class="sourceCode">
<pre class="sourceCode go"><a><span class="kw">func</span> (c *Crawler) Crawl(ctx context.Context, linkIt graph.LinkIterator) (<span class="dt">int</span>, <span class="dt">error</span>) {</a>
<a>    sink := <span class="bu">new</span>(countingSink)</a>
<a>    err := c.p.Process(ctx, &amp;linkSource{linkIt: linkIt}, sink)</a>
<a>    <span class="kw">return</span> sink.getCount(), err</a>
<a>}</a></pre></div>
<p>The method accepts a context value, which can be cancelled at any time by the caller to force the crawler pipeline to terminate, as well as an iterator, which provides the set of links to be crawled by the pipeline. It returns the total number of links that made it to the pipeline sink.</p>
<p>On a side-note, the fact that<span> </span><kbd>Crawl</kbd><span> </span>creates new source and sink instances on each invocation, combined with the observation that none of the crawler stages maintains any internal state, makes<span> </span><kbd>Crawl</kbd><span> </span>safe to invoke concurrently!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we built from scratch our very own generic, extensible pipeline package using nothing more than the basic Go primitives. We have analyzed and implemented different strategies (FIFO, fixed/dynamic worker pools, and broadcasting) for processing data throughout the various stages of our pipeline. In the last part of the chapter, we applied everything that we have learned so far to implement a multistage crawler pipeline for the Links 'R' Us Project.</p>
<p class="mce-root"/>
<p class="mce-root">In summary, pipelines provide an elegant solution for breaking down complex data processing tasks into smaller and easier-to-test steps that can be executed in parallel to make better use of the compute resources available at your disposal. In the next chapter, we are going to take a look at a different paradigm for processing data that is organized as a graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>Why is it considered an antipattern to use<span> </span><kbd>interface{}</kbd><span> </span>values as arguments to functions and methods?</li>
<li>You are trying to design and build a complex data-processing pipeline that requires copious amounts of computing power (for example, face recognition, audio transcription, or similar). However, when you try to run it on your local machine, you realize that the resource requirements for some of the stages exceed the ones that are currently available locally. Describe how you could modify your current pipeline setup so that you could still run the pipeline on your machine, but arrange for some parts of the pipeline to execute on a remote server that you control.</li>
<li>Describe how you would apply the decorator pattern to log errors returned by the processor functions that you have attached to a pipeline.</li>
<li>What are the key differences between a synchronous and an asynchronous pipeline implementation?</li>
<li>Explain how dead-letter queues work and why you might want to use one in your application.</li>
<li>What is the difference between a fixed-size worker pool and a dynamic pool?</li>
<li>Describe how you would modify the Links 'R' Us crawler payload so that you can track the time each payload spent inside the pipeline.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ol>
<li><span class="smallcaps">Berners-Lee, T.</span><span> </span>;<span> </span><span class="smallcaps">Fielding, R.</span><span> </span>;<span> </span><span class="smallcaps">Masinter, L.,</span> RFC 3986, Uniform Resource Identifier (URI): Generic Syntax.</li>
<li>bluemonday: a fast golang HTML sanitizer (inspired by the OWASP Java HTML Sanitizer) to scrub user generated content of XSS: <a href="https://github.com/microcosm-cc/bluemonday">https://github.com/microcosm-cc/bluemonday</a></li>
</ol>
<ol start="3">
<li>Documentation for the Go pprof package:<span> </span><a href="https://golang.org/pkg/runtime/pprof">https://golang.org/pkg/runtime/pprof</a></li>
<li>Documentation for the Pool type in the sync package:<span> </span><a href="https://golang.org/pkg/sync/#Pool">https://golang.org/pkg/sync/#Pool</a></li>
<li>gomock: a mocking framework for the Go programming language:<span> </span><a href="https://github.com/golang/mock">https://github.com/golang/mock</a></li>
<li>go-multierror: a Go (golang) package for representing a list of errors as a single error:<span> </span><a href="https://github.com/hashicorp/go-multierror">https://github.com/hashicorp/go-multierror</a></li>
<li><span class="smallcaps">Moskowitz, Robert</span><span> </span>;<span> </span><span class="smallcaps">Karrenberg, Daniel</span><span> </span>;<span> </span><span class="smallcaps">Rekhter, Yakov</span><span> </span>;<span> </span><span class="smallcaps">Lear, Eliot</span><span> </span>;<span> </span><span class="smallcaps">Groot, Geert Jan de</span>: Address Allocation for Private Internets.</li>
<li>The Go blog: profiling Go programs: <a href="https://blog.golang.org/profiling-go-programs">https://blog.golang.org/profiling-go-programs</a></li>
</ol>


            </article>

            
        </section>
    </body></html>