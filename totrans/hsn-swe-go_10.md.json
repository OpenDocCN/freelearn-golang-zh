["```go\n// Payload is implemented by values that can be sent through a pipeline.\ntype Payload interface {\n Clone() Payload\n MarkAsProcessed()\n}\n```", "```go\ntype Processor interface {\n // Process operates on the input payload and returns back a new payload\n // to be forwarded to the next pipeline stage. Processors may also opt\n // to prevent the payload from reaching the rest of the pipeline by\n // returning a nil payload value instead.\n Process(context.Context, Payload) (Payload, error)\n}\n```", "```go\ntype ProcessorFunc func(context.Context, Payload) (Payload, error)\n\n// Process calls f(ctx, p).\nfunc (f ProcessorFunc) Process(ctx context.Context, p Payload) (Payload, error) {\n return f(ctx, p)\n}\n```", "```go\nidentityFn := ProcessorFunc( func(_ context.Context, p Payload) (Payload, error) {  return p, nil },)\n```", "```go\nfunc retryingProcessor(proc Processor, isTransient func(error) bool, maxRetries int) Processor {\n return ProcessorFunc(func(ctx context.Context, p Payload) (Payload, error) {\n var out Payload\n var err error\n for i := 0; i < maxRetries; i++ {\n if out, err = proc.Process(ctx, p); err != nil && !isTransient(err) {\n return nil, err\n }\n }\n return nil, err\n })\n}\n```", "```go\ntype StageRunner interface {\n Run(context.Context, StageParams)\n}\n```", "```go\ntype StageParams interface {\n StageIndex() int\n\n Input() <-chan Payload\n Output() chan<- Payload\n Error() chan<- error\n}\n```", "```go\ntype fifo struct {\n proc Processor\n}\n\n// FIFO returns a StageRunner that processes incoming payloads in a \n// first-in first-out fashion. Each input is passed to the specified \n// processor and its output is emitted to the next stage.\nfunc FIFO(proc Processor) StageRunner {\n return fifo{proc: proc}\n}\n```", "```go\nfunc (r fifo) Run(ctx context.Context, params StageParams) {\n for {\n select {\n case <-ctx.Done():\n return // Asked to cleanly shut down\n case payloadIn, ok := <-params.Input():\n if !ok {\n return // No more data available.\n }\n\n // Process payload, handle errors etc.\n // (see following listing)\n }\n }\n}\n```", "```go\npayloadOut, err := r.proc.Process(ctx, payloadIn)\nif err != nil {\n wrappedErr := xerrors.Errorf(\"pipeline stage %d: %w\", params.StageIndex(), err)\n maybeEmitError(wrappedErr, params.Error())\n return\n}\nif payloadOut == nil {\n payloadIn.MarkAsProcessed()\n continue\n}\n\nselect {\ncase params.Output() <- payloadOut:\ncase <-ctx.Done():\n return  // Asked to cleanly shut down\n}\n```", "```go\n// maybeEmitError attempts to queue err to a buffered error channel. If the\n// channel is full, the error is dropped.\nfunc maybeEmitError(err error, errCh chan<- error) {\n select {\n case errCh <- err: // error emitted.\n default: // error channel is full with other errors.\n }\n}\n```", "```go\ntype fixedWorkerPool struct {\n fifos []StageRunner\n}\n\nfunc FixedWorkerPool(proc Processor, numWorkers int) StageRunner {\n if numWorkers <= 0 {\n panic(\"FixedWorkerPool: numWorkers must be > 0\")\n }\n fifos := make([]StageRunner, numWorkers)\n for i := 0; i < numWorkers; i++ {\n fifos[i] = FIFO(proc)\n }\n\n return &fixedWorkerPool{fifos: fifos}\n}\n```", "```go\nfunc (p *fixedWorkerPool) Run(ctx context.Context, params StageParams) {\n var wg sync.WaitGroup\n\n // Spin up each worker in the pool and wait for them to exit\n for i := 0; i < len(p.fifos); i++ {\n wg.Add(1)\n go func(fifoIndex int) {\n p.fifos[fifoIndex].Run(ctx, params)\n wg.Done()\n }(i)\n }\n\n wg.Wait()\n}\n```", "```go\ntype dynamicWorkerPool struct {\n proc      Processor\n tokenPool chan struct{}\n}\n\nfunc DynamicWorkerPool(proc Processor, maxWorkers int) StageRunner {\n if maxWorkers <= 0 {\n panic(\"DynamicWorkerPool: maxWorkers must be > 0\")\n }\n tokenPool := make(chan struct{}, maxWorkers)\n for i := 0; i < maxWorkers; i++ {\n tokenPool <- struct{}{}\n }\n\n return &dynamicWorkerPool{proc: proc, tokenPool: tokenPool}\n}\n```", "```go\nfunc (p *dynamicWorkerPool) Run(ctx context.Context, params StageParams) {\nstop:\n for {\n select {\n case <-ctx.Done():\n break stop // Asked to cleanly shut down\n case payloadIn, ok := <-params.Input():\n if !ok { break stop }\n // Process payload... (see listings below)\n }\n }\n\n for i := 0; i < cap(p.tokenPool); i++ { // wait for all workers to exit\n <-p.tokenPool\n }\n}\n```", "```go\nvar token struct{}\nselect {\ncase token = <-p.tokenPool:\ncase <-ctx.Done():\n break stop\n}\n```", "```go\ngo func(payloadIn Payload, token struct{}) {\n defer func() { p.tokenPool <- token }()\n payloadOut, err := p.proc.Process(ctx, payloadIn)\n if err != nil {\n wrappedErr := xerrors.Errorf(\"pipeline stage %d: %w\", params.StageIndex(), err)\n maybeEmitError(wrappedErr, params.Error())\n return\n }\n if payloadOut == nil {\n payloadIn.MarkAsProcessed()\n return // Discard payload\n }\n select {\n case params.Output() <- payloadOut:\n case <-ctx.Done():\n }\n}(payloadIn, token)\n```", "```go\ntype broadcast struct {\n fifos []StageRunner\n}\n\nfunc Broadcast(procs ...Processor) StageRunner {\n if len(procs) == 0 {\n panic(\"Broadcast: at least one processor must be specified\")\n }\n fifos := make([]StageRunner, len(procs))\n for i, p := range procs {\n fifos[i] = FIFO(p)\n }\n\n return &broadcast{fifos: fifos}\n}\n```", "```go\nvar wg sync.WaitGroup\nvar inCh = make([]chan Payload, len(b.fifos))\nfor i := 0; i < len(b.fifos); i++ {\n wg.Add(1)\n inCh[i] = make(chan Payload)\n go func(fifoIndex int) {\n fifoParams := &workerParams{\n stage: params.StageIndex(),\n inCh:  inCh[fifoIndex],\n outCh: params.Output(),\n errCh: params.Error(),\n }\n b.fifos[fifoIndex].Run(ctx, fifoParams)\n wg.Done()\n }(i)\n}\n```", "```go\ndone:\n for {\n // Read incoming payloads and pass them to each FIFO\n select {\n case <-ctx.Done():\n break done\n case payload, ok := <-params.Input():\n if !ok {\n break done\n }\n\n // Clone payload and dispatch to each FIFO worker...\n // (see following listing)\n }\n }\n```", "```go\nfor i := len(b.fifos) - 1; i >= 0; i-- {\n var fifoPayload = payload\n if i != 0 {\n fifoPayload = payload.Clone()\n }\n select {\n case <-ctx.Done():\n break done\n case inCh[i] <- fifoPayload:\n // payload sent to i_th FIFO\n }\n}\n```", "```go\n// Close input channels and wait for all FIFOs to exit\nfor _, ch := range inCh {\n close(ch)\n}\nwg.Wait()\n```", "```go\ntype Source interface {\n Next(context.Context) bool\n Payload() Payload\n Error() error\n}\n```", "```go\nfunc sourceWorker(ctx context.Context, source Source, outCh chan<- Payload, errCh chan<- error) {\n for source.Next(ctx) {\n payload := source.Payload()\n select {\n case outCh <- payload:\n case <-ctx.Done():\n return // Asked to shutdown\n }\n }\n\n // Check for errors\n if err := source.Error(); err != nil {\n wrappedErr := xerrors.Errorf(\"pipeline source: %w\", err)\n maybeEmitError(wrappedErr, errCh)\n }\n}\n```", "```go\ntype Sink interface {\n // Consume processes a Payload instance that has been emitted out of\n // a Pipeline instance.\n Consume(context.Context, Payload) error\n}\n```", "```go\nfunc sinkWorker(ctx context.Context, sink Sink, inCh <-chan Payload, errCh chan<- error) {\n for {\n select {\n case payload, ok := <-inCh:\n if !ok { return }\n if err := sink.Consume(ctx, payload); err != nil {\n wrappedErr := xerrors.Errorf(\"pipeline sink: %w\", err)\n maybeEmitError(wrappedErr, errCh)\n return\n }\n payload.MarkAsProcessed()\n case <-ctx.Done():\n return // Asked to shutdown\n }\n }\n}\n```", "```go\ntype Pipeline struct {\n stages []StageRunner\n}\n\n// New returns a new pipeline instance where input payloads will traverse\n// each one of the specified stages.\nfunc New(stages ...StageRunner) *Pipeline {\n return &Pipeline{\n stages: stages,\n }\n}\n```", "```go\nfunc (p *Pipeline) Process(ctx context.Context, source Source, sink Sink) error {\n // ...\n}\n```", "```go\nvar wg sync.WaitGroup\npCtx, ctxCancelFn := context.WithCancel(ctx)\n\n// Allocate channels for wiring together the source, the pipeline stages\n// and the output sink. \nstageCh := make([]chan Payload, len(p.stages)+1)\nerrCh := make(chan error, len(p.stages)+2)\nfor i := 0; i < len(stageCh); i++ {\n stageCh[i] = make(chan Payload)\n}\n```", "```go\n// Start a worker for each stage\nfor i := 0; i < len(p.stages); i++ {\n wg.Add(1)\n go func(stageIndex int) {\n p.stages[stageIndex].Run(pCtx, &workerParams{\n stage: stageIndex,\n inCh:  stageCh[stageIndex],\n outCh: stageCh[stageIndex+1],\n errCh: errCh,\n })\n close(stageCh[stageIndex+1])\n wg.Done()\n }(i)\n}\n```", "```go\nwg.Add(2)\ngo func() {\n sourceWorker(pCtx, source, stageCh[0], errCh)\n close(stageCh[0])\n wg.Done()\n}()\ngo func() {\n sinkWorker(pCtx, sink, stageCh[len(stageCh)-1], errCh)\n wg.Done()\n}()\n```", "```go\ngo func() {\n wg.Wait()\n close(errCh)\n ctxCancelFn()\n}()\n\n// Collect any emitted errors and wrap them in a multi-error.\nvar err error\nfor pErr := range errCh {\n err = multierror.Append(err, pErr)\n ctxCancelFn()\n}\nreturn err\n```", "```go\ntype crawlerPayload struct {\n LinkID      uuid.UUID\n URL         string\n RetrievedAt time.Time\n\n RawContent bytes.Buffer\n\n // NoFollowLinks are still added to the graph but no outgoing edges\n // will be created from this link to them.\n NoFollowLinks []string\n\n Links       []string\n Title       string\n TextContent string\n}\n```", "```go\ntype Payload interface {\n Clone() Payload\n MarkAsProcessed()\n}\n```", "```go\nvar (\n payloadPool = sync.Pool{\n New: func() interface{} { \n return new(crawlerPayload) \n },\n }\n)\n```", "```go\nfunc (p *crawlerPayload) Clone() pipeline.Payload {\n newP := payloadPool.Get().(*Payload)\n newP.LinkID = p.LinkID\n newP.URL = p.URL\n newP.RetrievedAt = p.RetrievedAt\n newP.NoFollowLinks = append([]string(nil), p.NoFollowLinks...)\n newP.Links = append([]string(nil), p.Links...)\n newP.Title = p.Title\n newP.TextContent = p.TextContent\n\n _, err := io.Copy(&newP.RawContent, &p.RawContent)\n if err != nil {\n panic(fmt.Sprintf(\"[BUG] error cloning payload raw content: %v\", err))\n }\n return newP\n}\n```", "```go\nfunc (p *crawlerPayload) MarkAsProcessed() {\n p.URL = p.URL[:0]\n p.RawContent.Reset()\n p.NoFollowLinks = p.NoFollowLinks[:0]\n p.Links = p.Links[:0]\n p.Title = p.Title[:0]\n p.TextContent = p.TextContent[:0]\n payloadPool.Put(p)\n}\n```", "```go\ntype Source interface {\n Next(context.Context) bool\n Payload() Payload\n Error() error\n}\n\ntype Sink interface {\n Consume(context.Context, Payload) error\n}\n```", "```go\ntype LinkIterator interface {\n Next() bool\n Error() error\n Close() error\n Link() *Link\n}\n```", "```go\ntype linkSource struct {\n linkIt graph.LinkIterator\n}\n\nfunc (ls *linkSource) Error() error              { return ls.linkIt.Error() }\nfunc (ls *linkSource) Next(context.Context) bool { return ls.linkIt.Next() }\nfunc (ls *linkSource) Payload() pipeline.Payload {\n link := ls.linkIt.Link()\n p := payloadPool.Get().(*crawlerPayload)\n p.LinkID = link.ID\n p.URL = link.URL\n p.RetrievedAt = link.RetrievedAt\n return p\n}\n```", "```go\ntype nopSink struct{}\n\nfunc (nopSink) Consume(context.Context, pipeline.Payload) error { \n return nil \n}\n```", "```go\ntype linkFetcher struct {\n urlGetter URLGetter\n netDetector PrivateNetworkDetector\n}\n\nfunc newLinkFetcher(urlGetter URLGetter, netDetector PrivateNetworkDetector) *linkFetcher {\n return &linkFetcher{\n urlGetter: urlGetter,\n netDetector: netDetector,\n }\n}\n\nfunc (lf *linkFetcher) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, error) {\n //...\n}\n```", "```go\n// URLGetter is implemented by objects that can perform HTTP GET requests.\ntype URLGetter interface {\n Get(url string) (*http.Response, error)\n}\n```", "```go\n// PrivateNetworkDetector is implemented by objects that can detect whether a\n// host resolves to a private network address.\ntype PrivateNetworkDetector interface {\n IsPrivate(host string) (bool, error)\n}\n```", "```go\npayload := p.(*crawlerPayload)\n\nif exclusionRegex.MatchString(payload.URL) {\n return nil, nil // Skip URLs that point to files that cannot contain\n                    // html content.\n}\n\nif isPrivate, err := lf.isPrivate(payload.URL); err != nil || isPrivate {\n return nil, nil // Never crawl links in private networks\n}\n\nres, err := lf.urlGetter.Get(payload.URL)\nif err != nil {\n return nil, nil\n}\n```", "```go\n_, err = io.Copy(&payload.RawContent, res.Body)\n_ = res.Body.Close()\nif err != nil {\n return nil, err\n}\nif res.StatusCode < 200 || res.StatusCode > 299 {\n return nil, nil\n}\nif contentType := res.Header.Get(\"Content-Type\"); !strings.Contains(contentType, \"html\") {\n return nil, nil\n}\n\nreturn payload, nil\n```", "```go\nfunc resolveURL(relTo *url.URL, target string) *url.URL {\n tLen := len(target)\n if tLen == 0 {\n return nil\n } else if tLen >= 1 && target[0] == '/' {\n if tLen >= 2 && target[1] == '/' {\n target = relTo.Scheme + \":\" + target\n }\n }\n if targetURL, err := url.Parse(target); err == nil {\n return relTo.ResolveReference(targetURL)\n }\n\n return nil\n}\n```", "```go\nvar (\n exclusionRegex = regexp.MustCompile(`(?i)\\.(?:jpg|jpeg|png|gif|ico|css|js)$`)\n baseHrefRegex = regexp.MustCompile(`(?i)<base.*?href\\s*?=\\s*?\"(.*?)\\s*?\"`)\n findLinkRegex = regexp.MustCompile(`(?i)<a.*?href\\s*?=\\s*?\"\\s*?(.*?)\\s*?\".*?>`)\n nofollowRegex = regexp.MustCompile(`(?i)rel\\s*?=\\s*?\"?nofollow\"?`)\n)\n```", "```go\ntype linkExtractor struct {\n netDetector PrivateNetworkDetector\n}\n\nfunc newLinkExtractor(netDetector PrivateNetworkDetector) *linkExtractor {\n return &linkExtractor{\n netDetector: netDetector,\n }\n}\n```", "```go\npayload := p.(*crawlerPayload)\nrelTo, err := url.Parse(payload.URL)\nif err != nil {\n return nil, err\n}\n\n// Search page content for a <base> tag and resolve it to an abs URL.\ncontent := payload.RawContent.String()\nif baseMatch := baseHrefRegex.FindStringSubmatch(content); len(baseMatch) == 2 {\n if base := resolveURL(relTo, ensureHasTrailingSlash(baseMatch[1])); base != nil {\n relTo = base\n }\n}\n```", "```go\nseenMap := make(map[string]struct{})\nfor _, match := range findLinkRegex.FindAllStringSubmatch(content, -1) {\n link := resolveURL(relTo, match[1])\n if link == nil || !le.retainLink(relTo.Hostname(), link) {\n continue\n }\n\n link.Fragment = \"\" // Truncate anchors\n linkStr := link.String()\n if _, seen := seenMap[linkStr]; seen || exclusionRegex.MatchString(linkStr) {\n continue // skip already seen links and links that do not contain HTML\n }\n seenMap[linkStr] = struct{}{}\n if nofollowRegex.MatchString(match[0]) {\n payload.NoFollowLinks = append(payload.NoFollowLinks, linkStr)\n } else {\n payload.Links = append(payload.Links, linkStr)\n }\n}\n```", "```go\nfunc (le *linkExtractor) retainLink(srcHost string, link *url.URL) bool {\n if link == nil {\n return false // Skip links that could not be resolved\n }\n if link.Scheme != \"http\" && link.Scheme != \"https\" {\n return false // Skip links with non http(s) schemes\n }\n if link.Hostname() == srcHost {\n return true // No need to check for private network\n }\n if isPrivate, err := le.netDetector.IsPrivate(link.Host); err != nil || isPrivate {\n return false // Skip links that resolve to private networks\n }\n return true\n}\n```", "```go\ntype textExtractor struct {\n policyPool sync.Pool\n}\n\nfunc newTextExtractor() *textExtractor {\n return &textExtractor{\n policyPool: sync.Pool{\n New: func() interface{} {\n return bluemonday.StrictPolicy()\n },\n },\n }\n}\n```", "```go\nfunc (te *textExtractor) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, error) {\n payload := p.(*crawlerPayload)\n policy := te.policyPool.Get().(*bluemonday.Policy)\n\n if titleMatch := titleRegex.FindStringSubmatch(payload.RawContent.String()); len(titleMatch) == 2 {\n payload.Title = strings.TrimSpace(html.UnescapeString(repeatedSpaceRegex.ReplaceAllString(\n policy.Sanitize(titleMatch[1]), \" \",\n )))\n }\n payload.TextContent = strings.TrimSpace(html.UnescapeString(repeatedSpaceRegex.ReplaceAllString(\n policy.SanitizeReader(&payload.RawContent).String(), \" \",\n )))\n\n te.policyPool.Put(policy)\n return payload, nil\n}\n```", "```go\ntype graphUpdater struct {\n updater Graph\n}\nfunc newGraphUpdater(updater Graph) *graphUpdater {\n return &graphUpdater{\n updater: updater,\n }\n}\n```", "```go\ntype Graph interface {\n UpsertLink(link *graph.Link) error\n UpsertEdge(edge *graph.Edge) error\n RemoveStaleEdges(fromID uuid.UUID, updatedBefore time.Time) error\n}\n```", "```go\npayload := p.(*crawlerPayload)\n\nsrc := &graph.Link{\n ID:           payload.LinkID,\n URL:          payload.URL,\n RetrievedAt: time.Now(),\n}\nif err := u.updater.UpsertLink(src); err != nil {\n return nil, err\n}\n```", "```go\nfor _, dstLink := range payload.NoFollowLinks {\n dst := &graph.Link{URL: dstLink}\n if err := u.updater.UpsertLink(dst); err != nil {\n return nil, err\n }\n}\n```", "```go\nremoveEdgesOlderThan := time.Now()\nfor _, dstLink := range payload.Links {\n dst := &graph.Link{URL: dstLink}\n\n if err := u.updater.UpsertLink(dst); err != nil {\n return nil, err\n }\n\n if err := u.updater.UpsertEdge(&graph.Edge{Src: src.ID, Dst: dst.ID}); err != nil {\n return nil, err\n }\n}\n```", "```go\nif err := u.updater.RemoveStaleEdges(src.ID, removeEdgesOlderThan); err != nil {\n return nil, err\n}\n```", "```go\n// Indexer is implemented by objects that can index the contents of webpages retrieved by the crawler pipeline.\ntype Indexer interface {\n Index(doc *index.Document) error\n}\n\ntype textIndexer struct {\n indexer Indexer\n}\n\nfunc newTextIndexer(indexer Indexer) *textIndexer {\n return &textIndexer{\n indexer: indexer,\n }\n}\n```", "```go\nfunc (i *textIndexer) Process(ctx context.Context, p pipeline.Payload) (pipeline.Payload, error) {\n payload := p.(*crawlerPayload)\n doc := &index.Document{\n LinkID:    payload.LinkID,\n URL:       payload.URL,\n Title:     payload.Title,\n Content:   payload.TextContent,\n IndexedAt: time.Now(),\n }\n if err := i.indexer.Index(doc); err != nil {\n return nil, err\n }\n\n return p, nil\n}\n```", "```go\ntype Crawler struct {\n p *pipeline.Pipeline\n}\n\n// NewCrawler returns a new crawler instance.\nfunc NewCrawler(cfg Config) *Crawler {\n return &Crawler{\n p: assembleCrawlerPipeline(cfg),\n }\n}\n```", "```go\n// Config encapsulates the configuration options for creating a new Crawler.\ntype Config struct {\n PrivateNetworkDetector PrivateNetworkDetector\n URLGetter URLGetter\n Graph Graph\n Indexer Indexer\n\n FetchWorkers int\n}\n```", "```go\nfunc assembleCrawlerPipeline(cfg Config) *pipeline.Pipeline {\n return pipeline.New(\n pipeline.FixedWorkerPool(\n newLinkFetcher(cfg.URLGetter, cfg.PrivateNetworkDetector),\n cfg.FetchWorkers,\n ),\n pipeline.FIFO(newLinkExtractor(cfg.PrivateNetworkDetector)),\n pipeline.FIFO(newTextExtractor()),\n pipeline.Broadcast(\n newGraphUpdater(cfg.Graph),\n newTextIndexer(cfg.Indexer),\n ),\n )\n}\n```", "```go\nfunc (c *Crawler) Crawl(ctx context.Context, linkIt graph.LinkIterator) (int, error) {\n sink := new(countingSink)\n err := c.p.Process(ctx, &linkSource{linkIt: linkIt}, sink)\n return sink.getCount(), err\n}\n```"]