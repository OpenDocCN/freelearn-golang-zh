- en: Graph-Based Data Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Big data is at the foundation of all of the megatrends that are happening
    today, from social to mobile to the cloud to gaming."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Chris Lynch'
  prefs: []
  type: TYPE_NORMAL
- en: Ask any highly successful company out there and they will all unequivocally
    agree that data is a precious commodity. Companies use data to not only make informed
    short-term decisions that affect their day to day operations but also as a guide
    for shaping their strategy in the long term. In fact, in some industries (such
    as advertising), data *is* the product!
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, with the advent of cheap storage solutions, the collection of data
    has increased exponentially in comparison to the last few years. Furthermore,
    the rate of increase in storage requirements is expected to keep following an
    exponential curve well into the future.
  prefs: []
  type: TYPE_NORMAL
- en: While there are quite a few solutions for processing structured data (such as
    systems supporting map-reduce operations), they fall short when the data to be
    processed is organized as a *graph*. Running specialized algorithms against massive
    graphs is a fairly common use case for companies in the field of logistics or
    companies that operate social networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be focusing our attention on systems that process
    graphs at scale. More specifically, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the **Bulk Synchronous Parallel** (**BSP**) model for distributing
    computation across multiple nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the BSP model principles to create our very own graph processing system
    in Go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the graph system as a platform for solving graph-based problems such as
    shortest path and graph coloring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an iterative version of the PageRank algorithm for the Links 'R'
    Us project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full code for the topics that will be discussed in this chapter has been
    published in this book's GitHub repository under the `Chapter08` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access this book''s GitHub repository by visiting the following URL:
    [https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get you up and running as quickly as possible, each example project includes
    a Makefile that defines the following set of targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Makefile target** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `deps` | Install any required dependencies |'
  prefs: []
  type: TYPE_TB
- en: '| `test` | Run all tests and report coverage |'
  prefs: []
  type: TYPE_TB
- en: '| `lint` | Check for lint errors |'
  prefs: []
  type: TYPE_TB
- en: As with all other chapters in this book, you will need a fairly recent version
    of Go, which you can download at [https://golang.org/dl](https://golang.org/dl)*.*
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Bulk Synchronous Parallel model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can we efficiently run a graph algorithm against a massive graph? To be
    able to answer this question, we need to clarify what we mean by the word *massive*.
    Is a graph with 1 million nodes considered to be massive? How about 10 million,
    100 million, or even 1 billion nodes? The real question we should be asking ourselves
    is whether the graph can actually *fit in memory*. If the answer is yes, then
    we can simply buy (or rent from a cloud provider) a server with a beefy CPU, max
    out the amount of installed memory, and execute our graph-processing code on a
    single node.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, things get much more interesting when the answer to the preceding
    question is *no...* Congratulations; you can now claim that you work with big
    data! In such cases, traditional compute models are evidently inadequate; we need
    to start exploring alternative applications that are explicitly designed for out
    of core processing.
  prefs: []
  type: TYPE_NORMAL
- en: The BSP model is one of the most popular models for building systems that can
    process massive datasets by distributing calculations to a cluster of processing
    nodes. It was proposed in 1990 by Leslie Valiant ^([10]) as a novel and elegant
    approach for bridging together parallel hardware and software.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the heart of the BSP model lies the **BSP computer**. The BSP computer,
    which can be seen in the following diagram, is an abstract computer model made
    up of a collection of, potentially heterogeneous, processors that are interconnected
    via a computer network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f23d84d-39d0-4ac5-83e6-a90dc0962c8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The components that comprise the BSP computer model'
  prefs: []
  type: TYPE_NORMAL
- en: The BSP model itself is not particularly concerned with the network implementation
    details. In fact, the network is treated as a black box; the model can support
    any type of network as long as the network provides a mechanism for routing messages
    between processors.
  prefs: []
  type: TYPE_NORMAL
- en: Processors can not only access their own local memory, but they can also use
    the network link to exchange data with other processors. To this end, the BSP
    computer is effectively a *distributed memory* computer that can perform computations
    in parallel. However, this functionality comes with a catch! While access to local
    memory is fast, accessing a remote processor's memory is significantly slower
    as it involves an exchange of messages over the network link. Therefore, the BSP
    computer can be also characterized as a **non-uniform memory access** (**NUMA**)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: So, what kinds of programs can we run on a BSP computer? Algorithms or data
    processing operations that can be expressed as a *sequence of iteration steps*
    are generally a good fit for the BSP model. The BSP model uses the term *super-step*
    to refer to the execution of a single iteration of a user-defined program.
  prefs: []
  type: TYPE_NORMAL
- en: One thing that differentiates the BSP model from other concurrent programming
    models is that BSP achieves parallelism through the use of a technique referred
    to as **Single Program Multiple Data** (**SPMD**). Software engineers who are
    interested in writing programs for the BSP computer can do so as if they were
    writing a program for a single-core machine. The program simply receives a set
    of data as input, applies a processing function to it, and emits some output.
    In other words, software engineers are completely oblivious to the existence of
    individual processors and the network that connects them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before commencing the execution of the user''s program, the BSP computer transparently
    uploads the program to every single processor, splits the data to be processed
    into a set of partitions, and assigns each partition to one of the available processors.
    The model employs a rather cunning strategy to reduce computation latency: it
    breaks down each super-step into two phases or substeps: a **compute** step and
    a **communication** step. During the compute step, each processor executes a single
    iteration of the user''s program using the data that was assigned to the processor
    as input. Once *all* the processors have completed their individual computations,
    they can communicate through the network and – depending on the use case – compare,
    exchange, or aggregate the results of their individual computations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that each processor can perform computation work in parallel and independently
    from other processors, the BSP model makes use of **blocking barriers** to synchronize
    processors. The following diagram summarizes the way in which the BSP computer
    model executes programs as a sequence of super-steps that are isolated from each
    other via write barriers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c87d7d2-1148-4ae9-a9c8-29c43ae3efe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The BSP computer model executes programs as a sequence of super-steps
    that are isolated from each other via write barriers'
  prefs: []
  type: TYPE_NORMAL
- en: In Go parlance, a blocking barrier is equivalent to a `sync.WaitGroup`; the
    BSP computer waits for all the processors to reach the barrier before assigning
    them the next chunk of work.
  prefs: []
  type: TYPE_NORMAL
- en: In the last couple of years, interest in models such as BSP has spiked. This
    can largely be attributed to Google, which (excluding state-funded three-letter
    agencies) is the undisputed worldwide leader in big data processing. Google engineers
    incorporated several of the BSP model concepts into Pregel, an in-house solution
    for out-of-core graph processing. In 2010, Google published a paper ^([7]) detailing
    the design decisions and architecture behind Pregel. This publication paved the
    way for creating open source equivalents such as Stanford's GPS ^([4]) and Apache
    Giraph ^([1]). The latter is currently used at Facebook to analyze the social
    graph that's formed by the network's users and their connections.
  prefs: []
  type: TYPE_NORMAL
- en: Building a graph processing system in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no better way to gain a deeper understanding of the BSP model principles
    than to build, from scratch, our very own scalable Pregel-like graph processing
    system in Go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the design requirements for the system we will be building:'
  prefs: []
  type: TYPE_NORMAL
- en: Graphs will be represented as a collection of vertices and directed edges. Each
    vertex will be assigned a unique ID. In addition, both vertices and edges can
    optionally store a user-defined value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At every super-step, the system executes a user-defined compute function for
    *every* vertex in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute functions are allowed to inspect and modify the internal state of the
    vertex they are invoked on. They can also iterate the list of outgoing edges and
    exchange messages with other vertices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any outgoing messages that are produced during a super-step will be buffered
    and delivered to their intended recipients in the *following* super-step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system must be able to support both single- and multi-node (distributed)
    graph topologies. In a multi-node topology, each node is responsible for managing
    a subset of the graph vertices and their outgoing edges. While operating in a
    multi-node configuration, the system should provide a mechanism for relaying vertex
    messages between nodes (for example, over a network link).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will analyze each of these requirements in more
    detail and elaborate on how they can be implemented in Go. You can find the fully
    documented source code and test suites for the graph processing system from this
    chapter in the `Chapter08/bspgraph` folder in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Queueing and delivering messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the core ideas of the BSP model is that graph components communicate
    with each other by exchanging messages. The fact that each vertex in the graph
    can potentially receive multiple messages mandates the introduction of some sort
    of abstraction for storing or queuing incoming messages until they are ready to
    be processed by the intended recipient.
  prefs: []
  type: TYPE_NORMAL
- en: In the three sections that follow, we will kick off our design discussion by
    defining the required interfaces for modeling messages and queues. Then, we will
    take a stab at implementing a simple, concurrent-safe in-memory queue.
  prefs: []
  type: TYPE_NORMAL
- en: The Message interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It logically follows that the contents of messages that are exchanged between
    vertices heavily depend on the application or graph algorithm that we are trying
    to execute. Consequently, to avoid passing plain `interface{}` values around,
    we need to come up with a plausible interface for describing messages in a generic
    way. The `Message` interface, which lives in the `Chapter08/bspgraph/message`
    package, is an attempt at doing exactly that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you are probably dubious about the usefulness of having a `Type`
    method on this interface. Can this really be any better than simply using an `interface{}`?
  prefs: []
  type: TYPE_NORMAL
- en: If you recall our discussion of the BSP computer model, processors communicate
    with each other over network links. Before a message can be transmitted over the
    network, the sender must serialize it into a byte stream. On the receiving end,
    the byte stream is unserialized back into a message and delivered to the intended
    recipient.
  prefs: []
  type: TYPE_NORMAL
- en: The `Type` method is quite handy for supporting use cases where the sender and
    the receiver can exchange *different* types of messages over the same channel
    (for example, a TCP socket). At serialization time, the sender queries the type
    of the message and attaches this information as additional metadata to the serialized
    payload. The receiver can then decode the metadata and unserialize the payload's
    byte stream back to the appropriate type of message.
  prefs: []
  type: TYPE_NORMAL
- en: Queues and message iterators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Queues serve as buffers for storing incoming messages and making them available
    for consumption by compute functions. Users of the graph processing system can
    either make use of the built-in in-memory queue (see the next section) or inject
    their application-specific queue implementation as long as it adheres to the `Queue`
    interface, whose definition is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The methods on the `Queue` interface are pretty standard for any type of queue
    system. A call to `PendingMessages` reveals whether the queue is currently empty,
    while a call to `DiscardMessages` can be used to flush any stored messages. The
    `Enqueue` method can be used to append a new `Message` to the queue, while the
    `Messages` method returns an `Iterator` for accessing the list of already enqueued
    messages. Since iterator implementations are typically coupled to the underlying
    queue system, `Iterator` is also defined as an interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This interface follows exactly the same iterator pattern that you should be
    familiar with from the previous chapters. Calling `Next` advances the iterator
    and returns a Boolean value to indicate whether more messages are available. After
    a successful call to `Next`, the current message can be retrieved by calling `Message`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an in-memory, thread-safe queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the majority of applications, using an in-memory queue implementation such
    as the one presented here should suffice. Implementing support for other types
    of queue systems (for example, Kafka, nats-streaming, or even plain files) is
    left as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining the `inMemoryQueue` type and its constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the in-memory queue is nothing more than a slice of `Message`
    instances – a slot for storing the message that's being currently pointed to by
    an iterator and a `sync.Mutex` for serializing access to the list of messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will take a look at the implementation of `Enqueue` and `PendingMessages`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To enqueue a new message, we acquire the lock and then append the messages to
    the list. In a similar fashion, checking for pending messages is facilitated by
    obtaining the lock and checking whether the message list is empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last set of functions that we need to implement so that the type satisfies
    the `Queue` interface are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the preceding code block, the implementation of the `DiscardMessages`
    method uses a nifty trick: the message list is purged via a slice operation that
    *retains* the already allocated slice capacity but resets its length to zero.
    This allows us to reduce the number of memory allocations that need to be performed
    and, by extension, reduce the pressure on the Go garbage collector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the `Messages` method body is quite interesting in itself as the
    returned value implies that the `inMemoryQueue` type must *also* implement the
    `Iterator` interface! The following code shows the implementation of the relevant
    methods for satisfying the `Iterator` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: While most queue implementations use FIFO semantics, as you can easily tell
    by the `Message` method's implementation, the in-memory queue follows **last-in
    first-out** (**LIFO**) semantics. This is intentional; if we were to dequeue from
    the head of the list (for example, `q.msgs = q.msgs[1:]`), its capacity would
    decrease and we wouldn't be able to reuse the already allocated memory to append
    new messages in the future.
  prefs: []
  type: TYPE_NORMAL
- en: As the graph system that we are building is not required to provide any guarantee
    about the order of incoming messages, our in-memory queue implementation can be
    used as-is without any issue. Now that we have a solution for storing messages,
    we can go ahead and define the necessary structures that will represent the vertices
    and edges of our graph.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the vertices and edges of graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned when we discussed the requirements for the graph processing
    system, we need to come up with a model for describing the vertices and edges
    that comprise a graph. Moreover, we need to provide an API that we can use to
    insert new vertices and edges into the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Vertex and Edge types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Vertex` type encapsulates the state of each vertex that is part of a `Graph`
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting tidbit about the `Vertex` type definition is that we actually
    need to maintain two `message.Queue` instances. Any messages produced by compute
    function invocations while executing a super-step must be buffered so that they
    can be delivered to the intended recipient in the *following* super-step. To this
    end, our implementation will employ a double-buffering scheme. We will use one
    queue to hold the messages for the current super-step and another queue to buffer
    the messages for the next super-step. At the end of each super-step, we will swap
    the queues around so that the output queue from the previous super-step becomes
    the input queue for the following super-step and vice versa. To avoid having to
    physically swap the queue pointers for every vertex in the graph, we will rely
    on modulo arithmetic to select the input and output queues based on the current
    super-step number:'
  prefs: []
  type: TYPE_NORMAL
- en: The queue at index `super_step%2` holds the messages that should be consumed
    during the current super-step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The queue at index `(super_step+1)%2` buffers the messages for the next super-step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moving on, we shouldn''t allow users of the `bspgraph` package to directly
    mutate the internal state of vertices. Therefore, none of the `Vertex` fields
    are exported outside of the `bspgraph` package. Instead, we will define the following
    set of helper methods so that we can access and/or safely manipulate the state
    of a vertex instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each vertex is uniquely identified by a string-based ID that can be queried
    via a call to the `ID` method. In addition, vertices can optionally store a user-defined
    value that compute functions can read or write via the `Value` and `SetValue`
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s more, a vertex can be in one of the following two states: *active*
    or *inactive* state. All the vertices are initially marked as *active. *To conserve
    compute resources, the graph framework will only invoke compute functions on active
    vertices. If the compute method implementation decides that a particular vertex
    has reached a terminal state and no further calculations are required, it can
    opt to explicitly mark the vertex as inactive via a call to its `Freeze` method*.*
    However, should an inactive vertex receive a new message during a super-step,
    the graph framework will automatically mark it as active at the next super-step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `Edges` method returns a slice of `Edge` objects that correspond
    to the outgoing, directed edges originating from a particular vertex. The following
    code shows the definition of the `Edge` type and its helper methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `Vertex` type, edges can also store an optional user-defined
    value that can be read/written to via the `Value` and `SetValue` methods. Every
    edge has a destination vertex whose ID can be obtained via a call to the `DstID`
    method. As we will see in the *Sending and receiving messages* section, the vertex
    ID is the only piece of information that we need to be aware of in order to send
    a message to a particular vertex.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting vertices and edges into the graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Graph` type keeps track of all vertices that comprise the graph with the
    help of a map where keys are vertex IDs and values are `Vertex` instances. Besides
    the fact that the vertex map allows us to quickly lookup vertices by their ID
    – a very important feature for delivering incoming messages – it also provides
    an efficient mechanism (as opposed to using a slice) for *deleting* vertices if
    we ever wish to allow users to mutate the graph topology between super-steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'New vertices can be inserted into the graph via the `AddVertex` method. It
    expects two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: A unique vertex ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An initial value (which may also be `nil`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If a vertex with the same ID already exists, we simply override its stored initial
    value. Otherwise, a new `Vertex` instance must be allocated. The code populates
    its ID field, sets the vertex status to active, and invokes the configured (at
    graph construction time) queue factory to instantiate the two queues that we need
    in order to store incoming messages for the current and next super-steps. Finally,
    the new vertex instance is inserted into the map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the `AddEdge` method creates a new directed edge between two vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned in a *Defining the Vertex and Edge types* section, edges are
    *owned* by the vertices they originate from. Ergo, the `AddEdge` implementation
    must check whether the `srcID` can be resolved to an existing vertex. If the source
    vertex cannot be located, then an error is returned to the caller. Otherwise,
    a new edge is created and appended to the edge list of the source vertex.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while we expect the source vertex for the edge to be known locally,
    the same assumption cannot be made for the destination vertex. For instance, if
    the graph was spread across two nodes, the source vertex could be managed by the
    first node while the destination vertex could be managed by the second node.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing global graph state through data aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aggregators are a key component for implementing several graph-based algorithms
    that rely on sharing global state between vertices. They are concurrent-safe primitives
    that apply an aggregation operator to a set of values and make the result available
    to *all* the vertices at the next super-step.
  prefs: []
  type: TYPE_NORMAL
- en: Any kind of operator can be used to create an aggregator as long as it is commutative
    and associative. Aggregators are commonly used to implement counters, accumulators,
    or for keeping track of the minimum and/or maximum value of some quantity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming sections, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a generic interface for aggregators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augment our Graph type with helper methods for registering and looking up `Aggregator`
    instances by name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an example aggregator that accumulates `float64` values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the Aggregator interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Aggregator` interface describes the set of methods that must be implemented
    by Go types so that they can be used with our graph processing framework for data
    aggregation purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: One of my pet peeves is that the methods in the preceding interface definition
    use `interface{}` values. Unfortunately, this is one of the few cases where we
    cannot actually avoid the use of `interface{}` since the types of values that
    can be aggregated are implementation-specific.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we want to apply the aggregation operation to a new value, we can do
    so by invoking the `Aggregate` method. Furthermore, the current value can be retrieved
    via a call to the `Get` method. On the other hand, if we want to set the aggregator
    to a *specific* value (for example, reset a counter to zero), we can invoke the
    `Set` method. The `Type` method provides an identifier for the aggregator's type
    that can be used for serialization purposes (for example, if we want to take a
    snapshot of the graph's state).
  prefs: []
  type: TYPE_NORMAL
- en: The `Delta` method returns the *change* in the aggregator's value since the
    *last* time that either `Delta` or `Set` was called. This method is meant to be
    used in a distributed graph computation scenario (see [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph Processing Systems*) to reduce the values from individual
    local aggregators into a single global aggregated value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the `Delta` method is used, let''s picture a scenario where
    we deploy three nodes: a master and two workers. Our goal is to create a distributed
    counter whose value is synchronized with all the nodes prior to executing a new
    super-step. To achieve this, each node (including the master) defines a *local*
    aggregator instance that implements a simple counter. While executing a super-step,
    compute functions are only allowed access to the local counter of the worker they
    are executing on. The master node is not assigned any vertices. Instead, it is
    responsible for collecting the partial *deltas* from each worker, aggregating
    those into its own counter, and *broadcasting* the new total back to the workers.
    The workers then use the `Set` method to update their local counters to the new
    total.'
  prefs: []
  type: TYPE_NORMAL
- en: Registering and looking up aggregators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To facilitate efficient name-based aggregator lookups, `Graph` instances store
    aggregators in a map where the aggregator name is used as a key. New aggregator
    instances can be linked to a `Graph` instance through the `RegisterAggregator`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute functions that need access to a particular aggregator can invoke the
    `Aggregator` method to look up a registered aggregator instance by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In an effort to make it easier for clients to create snapshots of the graph's
    state, we will also be providing the auxiliary `Aggregators` method, which just
    returns a copy of the map that contains the complete set of registered aggregator
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a lock-free accumulator for float64 values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the `Chapter08/bspgraph/aggregator` package, you can find two concurrent-safe
    accumulator implementations that are designed to work with `int64` and `float64`
    values and can also double as distributed counters.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a mutex to guarantee concurrent access, both accumulators are
    implemented using compare and swap instructions. The int64-based version is pretty
    straightforward and can easily be implemented with the help of the functions provided
    by the `sync/atomic` package. The float64-based version, which we will be dissecting
    here, is more challenging (and fun!) since the `sync/atomic` package offers no
    support for dealing with floating-point values. To work around this limitation,
    we will import the `unsafe` package and employ a few *creative value casting tricks*
    to roll our very own set of atomic functions that can work with `float64` values!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by defining the `Float64Accumulator` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Float64Accumulator` type keeps track of two `float64` values: the first
    one holds the current sum while the latter keeps track of the last value that
    was reported by a call to the `Delta` method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define the necessary set of methods for satisfying the `Accumulator`
    interface. The first method that we will be defining is `Get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `loadFloat64` helper function is where all the magic happens. The
    trick that we will be using is based on the observation that a `float64` value
    takes exactly the same space in memory (8 bytes) as a `uint64` value. With the
    help of the `unsafe` package, we can cast a *pointer* to the `float64` value we
    want to read into a `*uint64` value and use the `atomic.LoadUint64` function to
    read it atomically as a raw `uint64` value. Then, we can use the handy `Float64frombits`
    function from the built-in `math` package to *interpret* the raw `uint64` value
    as a `float64`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s examine the implementation for `Aggregate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code snippet, we enter an infinite `for` loop
    where we fetch the current aggregator value, add the `float64` value that was
    passed to the method, and keep trying to execute a compare and swap operation
    until we succeed. Like we did previously, we exploit the observation that `float64`
    values take the same space in memory as an `uint64` and use `atomic.CompareAndSwapUint64`
    to perform the swap. This function expects `uint64` values as arguments, so this
    time, we leverage the `math.Float64bits` function to convert the `float64` values
    that we are working with into raw `uint64` values for the compare-and-swap operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply exactly the same methodology to implement the `Delta` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we enter an infinite for loop where we latch on to the current and
    previous values and then use a compare and swap operation to copy `curSum` to
    `prevSum`. Once the swap succeeds, we subtract the two latched values and return
    the result to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the set of methods for implementing our accumulator, we also need
    to provide an implementation for `Set`, which, as you see in the following code
    listing, is slightly more complicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The extra complexity arises from the fact that we need to perform two sequential
    compare and swap operations, both of which must succeed before we can exit the
    `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Sending and receiving messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned previously, vertices communicate with each other by exchanging
    messages. Sending the *same* message to all immediate neighbors of a particular
    vertex is an often recurring pattern in several graph algorithms. Let''s define
    a convenience method for handling this fairly common use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`BroadcastToNeighbors` simply iterates the list of edges for a particular vertex
    and attempts to send the message to each neighbor with the help of the `SendMessage`
    method. With the help of `SendMessage`, compute functions can send a message to
    any vertex in the graph, provided that its ID is known to them (for example, discovered
    through the use of a gossip protocol).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the implementation for `SendMessage`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: First things first, we need to look up the destination vertex in the graph's
    vertex map. If the lookup yields a valid `Vertex` instance, then we can enqueue
    the message so that it can be delivered to the vertex in the following super-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Things get a bit more interesting when the vertex lookup fails… A failed lookup
    can occur because of two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We are running in distributed mode and the vertex is managed by a *remote* graph
    instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vertex simply does not exist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To handle vertices that are potentially hosted remotely, the `Graph` type allows
    users of the `bspgraph` package to register a helper that can relay messages between
    remote graph instances. More specifically, these helpers:'
  prefs: []
  type: TYPE_NORMAL
- en: Are aware of the topology of a distributed graph (that is, the vertex ID ranges
    that are managed by each node in a cluster)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a mechanism for shuttling messages back and forth between the cluster
    nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User-defined relay helpers must implement the `Relayer` interface and can be
    registered with a graph instance through the `RegisterRelayer` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it easier for users to provide functions or closures as a suitable
    `Relayer` implementation, let''s also go ahead and define the `RelayerFunc` adapter,
    which converts a function with the appropriate signature into a `Relayer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If the destination vertex ID cannot be located by the graph and the user has
    registered a `Relayer` instance, `SendMessage` invokes its `Relay` method and
    checks the response for errors. If we get an error *other* than `ErrDestinationLocal`,
    we return the error as-is back to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: If the relay helper detects that the destination vertex ID should, in fact,
    be managed by the local graph instance, it will fail with the typed `ErrDestinationIsLocal`
    error to indicate this. In such a case, we assume that the vertex ID is invalid
    and return the typed `ErrInvalidMessageDestination` error to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing graph-based algorithms using compute functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order for a compute function to be used with the `bspgraph` package, it
    must adhere to the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The first argument to the compute function is a pointer to the `Graph` instance
    itself. This allows compute functions to use the graph API to query the current
    super-step number, look up aggregators, and send messages to vertices. The second
    argument is a pointer to the `Vertex` instance that the compute function is operating
    on, while the third and final argument is a `message.Iterator` for consuming the
    messages that were sent to the vertex during the *previous* super-step.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the system operates under the assumption that compute
    functions can be safely executed concurrently. The only runtime guarantee that's
    provided by the system is that at each super-step, compute functions will be executed
    for each vertex *exactly once*. Consequently, compute function implementations
    can use any of the `Vertex` methods without having to worry about data races and
    synchronization issues. Given that vertices effectively *own* the edges originating
    from them, the same data access principles also apply to any `Edge` instances
    that are obtained by invoking the `Edges` method on a vertex.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving vertical scaling by executing compute functions in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will shift our focus to the mechanism that's used to execute compute
    functions. A rather simplistic approach would be to use a `for` loop construct
    to iterate the vertices in the graph and invoke the compute function for each
    vertex in a sequential fashion. While the approach would undoubtedly work as expected,
    it would be a rather inefficient use of the compute resources that are at our
    disposal. Running compute functions sequentially would only make use of a single
    CPU core; that would be quite a waste given that machines with up to 64 cores
    are readily available from the majority of cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A much better alternative would be to fan out the execution of compute functions
    to a pool of workers. This way, compute functions can run in parallel and make
    full use of all the available CPU cores. The graph constructor initializes the
    pool of workers via a call to the `startWorkers` method, whose implementation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing that `startWorkers` does is create a set of channels that are
    needed to communicate with the workers in the pool. Let''s briefly talk about
    each channel''s purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vertexCh` is a channel that is polled by workers to obtain the next vertex
    to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errCh` is a buffered channel where workers publish any errors that may occur
    while invoking compute functions. The graph processing system implementation will
    treat all errors as *fatal*. Therefore, we only need room to store a single error
    value. When a worker detects an error, it will attempt to enqueue it to `errCh`;
    if the channel is full, another fatal error has already been written to it, so
    the new error can safely be ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are using a worker pool to execute compute functions in parallel, we
    need to introduce some sort of synchronization mechanism to detect when all the
    vertices have been processed. The `stepCompletedCh` channel allows workers to
    signal when the *last* enqueued vertex has been processed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The remainder of the `startWorkers` method is pretty straightforward: we start
    a go-routine for each worker and use a `sync.WaitGroup` to keep track of their
    completion status.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `step` method, as shown in the following code, is responsible for executing
    a single super-step. If the super-step completes without an error, `step` returns
    the number of vertices that were active during the super-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding block of code should be self-explanatory. First, we reset the
    `activeInStep` counter to zero and load the `pendingInStep` counter with the number
    of vertices in the graph. Then, the map that holds the set of the graph `Vertex`
    instances is iterated and each vertex value is written to `vertexCh` so that it
    can be picked up and processed by an idle worker.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the vertices have been enqueued, `step` waits for all vertices to be
    processed by the worker pool by performing a blocking read on `stepCompletedCh`.
    Before returning, the code checks whether an error has been enqueued to the error
    channel. If that happens to be the case, the error is dequeued and returned to
    the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the `stepWorker` method''s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The channel's `range` statement ensures that our worker will keep executing
    until the `vertexCh` is closed. After dequeuing the next vertex from `vertexCh`,
    the worker uses modulo arithmetic to select the message queue that contains the
    messages that should be consumed by the compute function during the current super-step.
  prefs: []
  type: TYPE_NORMAL
- en: Vertices are considered to be active if either their `active` flag is set or
    if their input message queue contains any undelivered messages. For any vertex
    deemed to be active, we set its `active` flag to `true` and atomically increment
    the `activeInStep` counter. As we will see in the following sections, several
    graph algorithms use the number of active vertices in a super-step as a predicate
    for deciding whether the algorithm has completed or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we invoke the registered compute function and check for any errors. If
    an error occurs, we invoke the `tryEmitError` helper to enqueue the error to `errCh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The last bit of housekeeping that we need to do within the `stepWorker` method
    is to call the queue's `DiscardMessages` method and flush any messages that were
    not consumed by the compute function that we executed in the previous step. This
    ensures that the queue is always empty and ready to store incoming messages for
    `superstep+2`.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether the vertex is active or not, the worker invokes the `atomic.AddInt64`
    function to *decrement* the `pendingInStep` counter and check whether it has reached
    zero. When that occurs, all the vertices for the current super-step have been
    processed and the worker writes an empty `struct{}` value to `stepCompletedCh`
    to unblock the `step` method and allow it to return.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating the execution of super-steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we performed a detailed analysis of the mechanism that's
    used by the `Graph` type to execute a single super-step. However, graph algorithms
    typically involve several super-steps. For the `bspgraph` package users to be
    able to run generic graph algorithms using the system we are building, they require
    more fine-grained control over the execution of a sequence of super-steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before executing a super-step, we need to reset the value of one or more aggregators.
    Likewise, after a super-step completes, we might be interested in examining or
    modifying an aggregator''s final value. Furthermore, each algorithm defines its
    own termination condition. For example, an algorithm might terminate when the
    following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: After a fixed number of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When all vertices in the graph become inactive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the value of some aggregator exceeds a threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To cater to such requirements, we need to introduce a high-level API that provides
    an orchestration layer for governing the execution of a sequential list of super-steps.
    This API is provided by the `Executor` type, whose definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'An `Executor` wraps a `Graph` instance and is parameterized with a set of user-defined
    `ExecutorCallbacks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `PreStep` and `PostStep` callbacks are invoked – if defined – before and
    after the execution of a new super-step. If the `PostStepKeepRunning` callback
    is defined, it will be automatically invoked by the `Executor` after `PostStep`.
    The callback is responsible for checking whether the termination condition for
    the algorithm has been met and to return `false` when no further super-steps need
    to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `NewExecutor` function serves as a constructor for creating new `Executor`
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid nil pointer dereference errors when trying to invoke undefined callbacks,
    the constructor uses the following helper to patch missing callbacks with a dummy
    no-op stub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The high-level interface that''s exposed by the `Executor` consists of the
    following set of methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Graph` method provides access to the `Graph` instance that''s linked to
    the `Executor`, while `Superstep` reports the *last* super-step that was executed.
    The `RunSteps` and `RunToCompletion` methods repeatedly execute super-steps until
    one of the following conditions is met:'
  prefs: []
  type: TYPE_NORMAL
- en: The context expires
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An error occurs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `PostStepKeepRunning` callback returns false
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of `numSteps` has been executed (only for `RunSteps`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both of these functions are simply proxies for the `run` method, whose implementation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `run` method enters a `for` loop that keeps running until the caller-provided
    `maxSteps` value becomes equal to zero. At the end of each iteration, `maxSteps`
    is decremented while the graph's `superstep` counter is incremented. However,
    if the caller specifies a *negative* value for `maxSteps` when invoking `run`,
    then the preceding loop is functionally equivalent to an infinite loop.
  prefs: []
  type: TYPE_NORMAL
- en: The `Executor` begins a new iteration by checking whether the provided context
    has been canceled and then proceeds to invoke the `PreStep` callback. Then, it
    executes a new super-step by invoking the `step` method of the wrapped `Graph`
    instance. Following that, it invokes the `PostStep` and `PostStepKeepRunning`
    callbacks. If any of the callbacks or the `step` method returns an error, then
    we break out of the loop and return the error back to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and managing Graph instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our graph processing system is nearly complete! To finalize our implementation,
    we need to define a constructor that will create new `Graph` instances and some
    auxiliary methods that will manage the graph's life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous sections, there are quite a few knobs for configuring
    `Graph` instances. Passing each individual configuration option as an argument
    to the graph's constructor is considered to be an anti-pattern – not to mention
    that we would have to bump the major version of our package every time we want
    to add a new configuration option; changing a constructor's signature is the very
    definition of a breaking change!
  prefs: []
  type: TYPE_NORMAL
- en: 'A much better solution is to define a typed configuration object and pass that
    as an argument to the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`QueueFactory` will be used by the `AddVertex` method to create the required
    message queue instances for each new vertex that is being added to the graph.
    The `ComputeFn` setting is used to specify the compute function that will be executed
    for each super-step. Finally, the `ComputeWorkers` option allows the end users
    of the package to fine-tune the size of the worker pool so that they can execute
    the provided compute function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding list of configuration options, only `ComputeFn` is required.
    Let''s create a validator helper to check a `GraphConfig` object and to populate
    missing fields with sane defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If a nil `QueueFactory` instance is provided by the caller, the validator code
    will use the in-memory implementation as a sane default. Furthermore, if an invalid
    number of compute workers is specified, the validator will fall back to using
    a single worker. Of course, doing so would effectively turn processing graph vertices
    for every super-step into a *sequential* operation. Nevertheless, this might prove
    to be a useful feature when the end users wish to debug misbehaving compute functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A new `Graph` instance can be created via the `NewGraph` constructor, which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The first thing that the constructor needs to do is run a validation check on
    the provided configuration options. With a valid configuration at hand, the code
    creates the `Graph` instance, plugs in the provided configuration options, and
    allocates the maps that are needed to hold the graph's `Vertex` and `Aggregator`
    instances. The last thing that the constructor needs to do before returning the
    new `Graph` instance to the caller is initialize the worker pool via a call to
    `startWorkers`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a new `Graph` instance, users can proceed with populating the
    graph vertices and edges, register aggregators, and make use of an `Executor`
    to orchestrate the execution of a particular graph-based algorithm. However, after
    a completed run, users may want to reuse a `Graph` instance to run the exact same
    algorithm again but this time to use a different graph layout. Let''s provide
    them with a `Reset` method to reset the graph''s internal state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: As you may recall, every vertex is associated with two message queue instances.
    The `message.Queue` interface defines a `Close` method that we must call to release
    any resources (for example, file handles, sockets, and so on) that are used by
    the underlying queue implementation. Lastly, to completely reset the graph's internal
    state, we can simply reset the graph's super-step counter to zero and recreate
    the maps that store the graph's vertices and aggregator instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the worker pool consists of a bunch of long-running go-routines,
    we must also provide a mechanism for managing their life cycle and, more importantly,
    to shut them down when we are done with the graph. The last method that we will
    be defining on the `Graph` type is `Close`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To force the worker pool to cleanly shut down and all its workers to exit, the
    `Close` method implementation closes `vertexCh`, which each worker polls for incoming
    vertex processing jobs. The code then blocks on a wait group until all the workers
    have exited. Before returning, we make a tail call to the `Reset` method to ensure
    that the per-vertex queue instances are properly closed.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the development of a framework that allows us to execute graph-based
    algorithms using the BSP computation model. Next, we will explore how we can leverage
    the framework to solve some real-world problems that involve graphs!
  prefs: []
  type: TYPE_NORMAL
- en: Solving interesting graph problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be examining three very popular graph-based problems
    that are good fits for the graph processing system we have just finished building.
  prefs: []
  type: TYPE_NORMAL
- en: After describing each problem in detail and listing some of its potential real-world
    applications, we will continue our discussion by presenting a *sequential* algorithm
    that can be used to solve it. Following that, we will come up with an equivalent
    parallel version of the same algorithm and encode it as a compute function that
    can be used with the `bspgraph` package.
  prefs: []
  type: TYPE_NORMAL
- en: Searching graphs for the shortest path
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we look around, we are bound to encounter a plethora of quite challenging
    problems that essentially boil down to finding a path or set of paths within a
    graph that minimize a particular cost function. Pathfinding has a multitude of
    real-world use cases, ranging from building efficient computer networks to logistics
    and even games!
  prefs: []
  type: TYPE_NORMAL
- en: The definition of a suitable cost function and its interpretation is typically
    application-specific.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the context of a map service, the cost associated with a graph
    edge could reflect the distance between two points or the time that's required
    to drive from one point to another due to traffic congestion. On the other hand,
    if we were talking about packet routing application, cost could represent the
    amount of money that the network operator would need to pay in order to use a
    peering connection with another provider.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, the sole focus of this section will be about finding the shortest
    paths within a graph. It goes without saying that the principles and algorithms
    we will be discussing can be applied, as-is, to any *type* of cost function as
    long as *lower* cost value for a graph edge indicates a *better* path through
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how we define the path origin and destination, we can classify
    shortest path queries into three general categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Point to point**: In a *point to point* search query, we are interested in
    locating the shortest path that connects *two points*. An interesting example
    of this type of search would be a real-time strategy game where the user selects
    a unit (the path origin) and subsequently clicks on the map location where they
    want the unit to move to (the path destination). The game engine searches for
    the shortest unobstructed path between the two points (typically by implementing
    an algorithm such as A* ^([5])) and navigates the unit along the path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point to multi-point**: A *point to multi-point* search query involves finding
    the shortest paths connecting a *single* graph vertex to *multiple* vertices.
    For example, a user of a map application might be interested in obtaining a list
    of coffee shops located within a particular radius from their current location
    sorted by distance. If we flip this query around, we can identify even more interesting
    use cases. For instance, a ride-hailing application is aware of both the users''
    and the drivers'' locations. A multi-point to point query would allow the application
    to dispatch the nearest driver to a user''s location and reduce the time-to-pickup.
    These types of queries can be efficiently answered using Dijkstra''s algorithm,
    one of the most widely-known graph algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-point to multi-point**: The third, and most complicated, pathfinding
    query category consists of *multi-point to multi-point* queries where we are effectively
    seeking the shortest path from each vertex to *all* the other vertices in the
    graph. Arguably, we could answer this kind of query by running Dijkstra''s algorithm
    for each vertex in the graph at the cost of a much longer total runtime, especially
    if the graph contains a large number of vertices. A much better alternative performance-wise
    for answering such queries would be to use a dynamic programming algorithm such
    as Floyd-Warshall ^([3]).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a stab at implementing Dijkstra's algorithm using the graph processing
    framework that we developed in the first half of this chapter. While the original
    version of Dijkstra's algorithm was meant to find the shortest path between two
    points, the variant that we will be working with is designed to locate the *shortest
    path tree*, that is, the shortest path from a point to all the other points in
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The sequential Dijkstra algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we adapt Dijkstra''s algorithm so that it works with our graph processing
    system, we need to have a clear idea of how the original algorithm works. The
    following snippet outlines an implementation of the sequential version of Dijkstra''s
    algorithm in pseudocode form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding implementation maintains two arrays, each one having a length
    equal to the number of vertices in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: The first array, `min_cost_via`, tracks the minimum cost (distance) for reaching
    the source vertex from the *i[th]* vertex in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `prev` array keeps track of the previous vertex in the optimal path leading
    from the source vertex to the *i[th]* vertex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At initialization time, we set all the entries in the `prev` array to `nil`.
    Additionally, all the entries in the `min_cost_via` array are initialized to a
    large number with the exception of the entry for the source vertex, whose entry
    is set to `0`. If we were implementing this algorithm in Go and path costs were
    `uint64` values, we would set the initial value to `math.MaxUint64`.
  prefs: []
  type: TYPE_NORMAL
- en: Dijkstra's algorithm is bootstrapped by placing all the graph vertices in a
    set called `Q`. The algorithm then executes a number of iterations equal to the
    number of vertices in the graph. At each iteration, we select vertex `u` from
    `Q` which has the *lowest* `min_cost_via` value and remove it from the set.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the algorithm examines each neighbor `v` of the selected vertex `u`. If
    a lower cost path from `v` to the source vertex can be constructed by passing
    through `u`, then we update the `min_cost_via` entry for `v` and make `u` the
    predecessor of the optimal path to `v`.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm completes when all the vertices in set `Q` have been processed.
    The shortest path from the source vertex to any other vertex in the graph can
    be reconstructed by starting at the destination vertex and following the `prev`
    array entries until we reach the source vertex.
  prefs: []
  type: TYPE_NORMAL
- en: What's more, we can slightly tweak the preceding algorithm to obtain the original
    variant that answers point to point queries. All we need to do is terminate the
    algorithm after processing the destination vertex for our query. Those of you
    who are familiar with, or have implemented, the A* algorithm in the past will
    definitely notice a lot of similarities between the two algorithms. In fact, Dijkstra's
    algorithm is a special case of the A* algorithm where no distance heuristic is
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging a gossip protocol to run Dijkstra in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dijkstra's algorithm is fairly straightforward to implement and its runtime
    can be sped up considerably with the introduction of specialized data structures
    (for example, min-heap or Fibonacci heap) for selecting the next vertex for each
    iteration. Let's take a look at how we can leverage the graph processing system
    that we have built to execute Dijkstra's algorithm in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: To break the sequential nature of the original algorithm, we will swap out the
    next vertex selection step and replace it with a *gossip protocol*. Whenever a
    vertex identifies a better path to it via another vertex, it will *broadcast*
    this information to all its neighbors by sending them a `PathCostMessage`. The
    neighbors would then process these messages during the *next* super-step, update
    their own min-distance estimates, and broadcast any better paths, if found, to
    their own neighbors. The key concept here is to trigger a wavefront of path updates
    throughout the graph that can be processed by each vertex in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to define the types for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The message that's exchanged by the vertices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the state for each vertex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `pathState` struct encodes the same kind information as the `min_cost_via`
    and `prev` arrays from the sequential version of the algorithm. The only difference
    is that each vertex maintains its own `pathState` instance, which is stored as
    the vertex value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s try to put together a compute function for the graph. As you may
    recall from the previous sections, compute functions receive the following input
    arguments: a pointer to the graph, the currently processed vertex, and an iterator
    for the messages that are sent to the vertex during the previous super-step. At
    super-step *0*, each vertex initializes its own internal state with the maximum
    possible distance value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, each vertex processes any path announcements from its neighbors and keeps
    track of the path announcement with the minimum cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the messages have been processed, we compare the cost of the best
    path from all the announcements to the cost of the best path we''ve seen so far
    by this vertex. If the vertex is already aware of a better path with a lower cost,
    we don''t really need to do anything. Otherwise, we update the local vertex state
    to reflect the new best path and send out a message to each of our neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Each outgoing `PathCostMessage` includes the cost of reaching each neighbor
    through the current vertex and is calculated by adding the cost for the next hop
    (the value associated with the outgoing edge) to the new minimum cost for reaching
    the current vertex.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether the best path to a vertex was updated or not, we always
    invoke the `Freeze` method on each vertex and mark it as processed. This means
    that the vertex will not be reactivated in a future super-step unless it receives
    a message from its neighbors. Eventually, all the vertices will figure out the
    optimal path to the source vertex and stop broadcasting cost updates to their
    neighbors. When this happens, all the vertices will end up in a frozen state and
    the algorithm will terminate.
  prefs: []
  type: TYPE_NORMAL
- en: We could definitely argue that this particular approach requires much more effort
    compared to the traditional sequential version. However, contrary to the sequential
    version of the algorithm, the parallel version can run efficiently on massive
    graphs that can be potentially distributed across multiple compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The full source code and tests for the shortest path calculator from this section
    can be found in this book's GitHub repository in the `Chapter08/shortestpath`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Graph coloring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next graph-based problem that we will be trying to solve using our graph
    processing system is **graph coloring**. The idea behind graph coloring is to
    assign a color to each vertex in the graph so that no adjacent vertices have the
    same color. The following diagram illustrates an example graph whose vertices
    have been colored with the optimal (minimum possible) number of colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/111cf89d-f996-467e-8f83-8016d6d5c74d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A graph with optimal coloring'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph coloring has numerous real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: It is frequently used by compilers to perform register allocation ^([2]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobile operators use graph coloring as the means for solving the frequency assignment
    problem ^([9]) where the goal is to assign frequencies from a limited frequency
    pool to a set of communication links so that there is no interference between
    links in the same vicinity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many popular puzzle games such as Sudoku can be modeled as a graph and solved
    with the help of a graph coloring variant where the allowed set of colors is fixed
    (k-color graph coloring).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sequential greedy algorithm for coloring undirected graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calculating the optimal graph coloring is known to be NP-hard. Therefore, researchers
    have proposed greedy algorithms that produce good enough, but not necessarily
    optimal, solutions. The sequential greedy algorithm listed here works with undirected
    graphs and guarantees an upper bound of *d+1* colors where *d* is the maximum
    out-degree (number of outgoing edges) for all the vertices in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm maintains an array called `C` that holds the assigned color for
    each vertex in the graph. During initialization, we set each entry of the `C`
    array to the value *0* to indicate that no color has been assigned to any of the
    graph vertices.
  prefs: []
  type: TYPE_NORMAL
- en: For each vertex `u` in the graph, the algorithm iterates its neighbor list and
    inserts the colors that have been already assigned to each neighbor into a map
    using the color value as a key. Next, the `assigned_color` variable is set to
    the lowest possible color value (in this case, *1*) and the `already_in_use` map
    is consulted to check whether that color is currently in use. If that happens
    to be the case, we increment the `assigned_color` variable and repeat the same
    steps until we finally land on a color value that is not in use. The unused color
    value is assigned to vertex `u` and the process continues until all the vertices
    have been colored.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting fact about the preceding algorithm is that we can tweak it slightly
    to add support for handling pre-colored graphs. In this type of graph, a subset
    of the vertices have been already assigned a color value and the goal is to assign
    non-conflicting colors to the remaining vertices. All we need to do is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Set `C[u]` to the already assigned color instead of *0* during initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When iterating the graph vertices, skip the ones that have already been colored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploiting parallelism for undirected graph coloring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel graph coloring algorithms are based on the observation that if we split
    the graph into multiple *independent sets of vertices*, we can color those in
    parallel without introducing any conflict.
  prefs: []
  type: TYPE_NORMAL
- en: An independent set is defined as the collection or set of vertices where no
    two vertices share an edge.
  prefs: []
  type: TYPE_NORMAL
- en: To develop a parallelized version of the sequential greedy graph coloring algorithm
    from the previous section, we will be relying on a simple, yet effective algorithm
    proposed by Jones and Plassmann ^([6]). Before diving into the implementation
    details, let's take a few minutes to explain how the algorithm generates independent
    sets and how can we guarantee that our compute function will avoid data races
    while accessing the graph.
  prefs: []
  type: TYPE_NORMAL
- en: At initialization time, each vertex is assigned a random token. During each
    super-step, every vertex that hasn't been colored yet compares its own token value
    to the value of every *uncolored* neighbor. In the highly unlikely case that two
    neighboring vertices have been assigned the same random token, we can use the
    vertex ID as an extra comparison predicate to break the tie. The vertex with the
    highest token value gets to choose the next color using the same steps as the
    sequential algorithm while the neighboring vertices remain idle, waiting for their
    turn.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of using tokens to enforce a coloring order guarantees that, at
    every super-step, we only color exactly one vertex from *each* independent set.
    At the same time, since connected vertices wait for their turn before they can
    pick a color, no data races can occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did with the shortest path implementation, we will begin by defining
    a type for holding the state of each vertex and a type that describes the messages
    that are exchanged between neighboring vertices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The `vertexState` struct keeps track of the token and colors that are assigned
    to the vertex. Furthermore, the `usedColors` map tracks colors that have already
    been assigned to the vertex neighbors. Vertices broadcast their state to each
    of their neighbors by exchanging `VertexStateMessage` instances. In addition to
    the token and color value, these messages also include a vertex ID. As we mentioned
    previously, the vertex ID is required for breaking ties while comparing token
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s break down the compute function for this algorithm into small chunks
    and examine each chunk in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: First things first, each super-step iteration begins by marking *all* the vertices
    as inactive. This way, vertices will only be reactivated in subsequent super-steps
    when a neighbor gets to pick a color and broadcast its selection. Consequently,
    once the last remaining vertex has been colored, no further messages will be exchanged
    and all the vertices will be marked as inactive. This observation will serve as
    the termination condition for the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Super-step *0* serves as an initialization step. During this step, we assign
    random tokens to all the vertices and have them *all* announce their initial state
    to their neighbors. If any of the vertices are pre-colored, their assigned colors
    will also be included in the broadcasted state update message. The `vertexState`
    type defines a handy helper method called `asMessage` which generates a `VertexStateMessage`,
    that can be sent to neighbors via the graph''s `BroadcastToNeighbors` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the input graph may potentially include vertices with no neighbors.
    If these vertices have not been already pre-colored, we simply assign them the
    first available color, which in our particular implementation is color *1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block of code processes state announcements from the vertex neighbors.
    Before iterating each state message, each vertex sets its local `pickNextColor`
    variable to `true`. Then, the message list is iterated and the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: If a neighbor has been assigned a color, we insert it into the `usedColors`
    map for the local vertex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of the neighbors has a higher token value or has the *same* token value
    but their ID string value is greater than the local vertex one, they have a higher
    priority for picking the next color. Therefore, the `pickNextColor` variable will
    be set to `false` for the local vertex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once all state announcements have been processed, we check the value of the
    `pickNextColor` variable. If the vertex is not allowed to pick the next color,
    it simply broadcasts its current state and waits for the next super-step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, the vertex gets to select the next color to be assigned to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Since some of the vertices in the graph might be pre-colored, our goal is to
    pick the *smallest* not used color for this vertex. To do this, we initialize
    a counter with the smallest allowed value and enter a loop: at each step, we check
    whether `usedColors` contains an entry for the `nextColor` value. If so, we increment
    the counter and try again. Otherwise, we assign `nextColor` to the vertex and
    broadcast our updated state to the neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: In case you are having concerns about the space requirements for keeping track
    of the used colors on a per-vertex basis, we can actually do much better if we
    don't need to support potentially pre-colored graphs. If that happens to be the
    case, each vertex only needs to keep track of the *maximum* color value that's
    assigned to its neighbors. At color selection time, the vertex picks `maxColor
    + 1` as its own color.
  prefs: []
  type: TYPE_NORMAL
- en: The full source code and tests for the graph coloring implementation from this
    section can be found in this book's GitHub repository in the `Chapter08/color`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating PageRank scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever someone hears the name Google, the first thing that would probably
    spring to mind is, of course, the widely popular search engine that made its appearance
    some time around 1997 and since then has consistently managed to eclipse all other
    competition in the search engine space.
  prefs: []
  type: TYPE_NORMAL
- en: The heart of Google's search engine technology is undoubtedly the patented PageRank
    algorithm, which was published in the 1999 paper by the Google co-founders, Larry
    Page and Sergey Brin ^([8]). Fortunately, the patent to the algorithm expired
    in June 2019; that's a great piece of news as it allows us to freely implement
    it for the Links 'R' Us project!
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank algorithm treats all indexed web pages as a massive, directed graph.
    Each page is represented as a vertex in the graph, while outgoing links from each
    page are represented as directed edges. All the pages in the graph are assigned
    what is referred to as a *PageRank score*. PageRank scores express the importance
    (ranking) of every page compared to all the other pages in the graph. The key
    premise of this algorithm is that if we were to sort the results of a keyword-based
    search by `both` keyword match relevance and by their PageRank score, we would
    be able to increase the quality of the results that are returned to the user performing
    the search.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore the formula for calculating PageRank
    scores and implement our very own PageRank calculator using the graph processing
    framework that we developed at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The model of the random surfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the score for each vertex in the graph, the PageRank algorithm
    utilizes the model of the **random surfer**. Under this model, a user performs
    an initial search and lands on a page from the graph. From that point on, users
    randomly select one of the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: They can click any outgoing link from the current page and navigate to a new
    page. Users choose this option with a predefined probability that we will be referring
    to with the term **damping factor**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, they can decide to run a new search query. This decision has
    the effect of *teleporting* the user to a random page in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PageRank algorithm works under the assumption that the preceding steps
    are repeated in perpetuity. As a result, the model is equivalent to performing
    a random walk of the web page graph. PageRank score values reflect the *probability*
    that a surfer lands on a particular page. By this definition, we expect the following
    to occur:'
  prefs: []
  type: TYPE_NORMAL
- en: Each PageRank score should be a value in the *[0, 1]* range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of all assigned PageRank scores should be exactly equal to 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An iterative approach to PageRank score calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To estimate the PageRank score for web page *P* from the graph, we need to
    take two factors into account:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of links leading to *P*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quality of the pages linking to *P*, as indicated by their own individual
    PageRank scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we only took the number of links into account, we would allow malicious users
    to game the system and artificially boost the score of a particular target page
    by creating a large number of links pointing at it. One way that this could be
    achieved would be, for instance, by cross-posting the same link to online forums.
    On the other hand, if we were to use the PageRank scores of the source pages to
    *weight* the incoming link contributions to the target page, pages with just a
    few incoming links from reputable sources (for example, major news outlets) would
    get a much better score than pages with a greater number of links but from sources
    that are not as popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out the score of a particular page, we need to be aware of the score
    of every page linking to it. To make matters even worse, pages might also link
    *back* to some or all pages that link to them. This sounds a bit like a chicken
    and egg problem! So, how does the calculation work? It turns out that we can actually
    calculate PageRank scores using an iterative algorithm that uses the following
    formula to calculate the PageRank score for page *P*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02e1279c-48f0-482e-a68f-059961edf01f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At step *0*, all the vertices in the graph are assigned an initial PageRank
    score of *1/N*, where *N* is the number of vertices in the graph. For the *i[th]*
    step, we calculate the PageRank score by taking the weighted sum of two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: The first term encodes the PageRank score contribution from a random page in
    the graph due to a teleport operation. According to the random surfer model, users
    can decide to stop clicking outgoing links from the page they are currently visiting
    and instead run a new query which lands them on page *P*. This is equivalent to
    creating a *one-off* connection to *P.* As a result, it transfers *1/N* units
    of PageRank to *P*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second term encodes the score contributions for every page in the graph
    that has an outgoing link to *P*. In the preceding equation, *LT(P)* represents
    the set of pages that link to page *P*. For each page *J* in that set, we calculate
    the PageRank contribution to *P* by dividing its accumulated PageRank score from
    step *i-1* by the number of outgoing links. Essentially, at each step, each and
    every page *evenly distributes* its PageRank score to all outgoing links.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the two terms refer to mutually exclusive events that occur with specific
    probabilities, we need to weigh each term by its respective probability to make
    sure that all PageRank scores add up to 1.0\. Given that users click outgoing
    links with a probability equal to the damping factor *d*, we need to multiply
    the first term by *1-d* and the second term by *d*.
  prefs: []
  type: TYPE_NORMAL
- en: Reaching convergence – when should we stop iterating?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By virtue of the fact that we are using an iterative formula to calculate PageRank
    scores, the more steps we execute, the more accurate results we will get. This
    raises an interesting question: how many steps do we need to execute so as to
    reach a desired level of accuracy for the calculated scores?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this question, we must come up with a suitable metric that will allow
    us to measure how close we are to reaching convergence. For this purpose, we will
    be calculating the **sum of absolute differences** (**SAD**) for PageRank scores
    between two subsequent iterations using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cccbc92-40e8-42c5-be1b-e9cdbfe37570.png)'
  prefs: []
  type: TYPE_IMG
- en: The intuition behind this metric is that while the first few iterations will
    cause significant, in absolute terms, changes to the PageRank scores, as we get
    closer to convergence, the magnitude of each subsequent change will keep decreasing
    and become zero as the number of iterations reaches infinity. Obviously, for our
    particular use case, we need to execute a finite number of steps. Consequently,
    we have to decide on a suitable threshold value (for example, 10^(-3)) and keep
    iterating until the SAD score drops below the target threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Web graphs in the real world – dealing with dead ends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding formula for calculating PageRank scores assumes that all the pages
    link to at *least* one page. In real life, this is not always the case! Let's
    consider the graph shown in the following diagram. Here, all the vertices are
    connected to each other with the exception of vertex **D**, which has incoming
    links but no outgoing links. In other words, **D** is a dead end!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e434f76d-ba21-47d9-8c53-b2c0e9895099.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4: An example graph where vertex D is a dead end
  prefs: []
  type: TYPE_NORMAL
- en: Would the presence of dead ends in the input graph cause problems with our PageRank
    score calculations? So far, we know that at each iteration of the PageRank algorithm,
    pages evenly distribute their currently accumulated PageRank score across all
    outgoing links. In the case of a dead-end scenario like the one shown in the preceding
    diagram, vertex **D** would keep receiving the PageRank scores from every other
    vertex in the graph but *never* distribute its own accumulated score since it
    has no outgoing links. Therefore, **D** will end up with a relatively high PageRank
    score while all the other vertices will end up with significantly lower scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'One strategy for mitigating this issue is to pre-process the graph, identify
    vertices that are dead ends, and exclude them from our PageRank calculations.
    However, dead-end elimination is far from a trivial task: the removal of an existing
    dead end might cause some of the vertices that used to link to it to transform
    into dead ends that also need to be removed and so on. As a result, implementing
    this solution and scaling it up to work with large graphs would be quite costly
    in terms of the required compute power.'
  prefs: []
  type: TYPE_NORMAL
- en: A much better alternative, and the one that we will be using for our Go implementation,
    would be to treat dead ends as having an implicit connection to *all* the other
    vertices in the graph. This approach mitigates the problem of skewed scores by
    distributing the accumulated PageRank scores from each dead end back to all the
    vertices in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The naive approach for implementing this strategy using our graph processing
    system would be to have each dead-end node broadcast a message that would transfer
    a quantity equal to *PR/N* to every other vertex in the graph. Obviously, this
    idea would never scale for large graphs, so we need to come up with something
    better… What if, instead of following a push-based approach, we switched to a
    pull-based approach? In fact, this is actually a great use case for leveraging
    the graph processing system''s support for aggregators! Rather than have each
    vertex with no outgoing links distribute its PageRank to every other vertex in
    the graph by means of message broadcasting, they can simply add their per-node
    contribution (the *PR/N* quantity) into an accumulator. We can then extend the
    PageRank formula with an extra term to take this *residual* PageRank into account
    when performing our calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f1dfb27-419a-4447-8a36-ea62f87005c5.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, `ResPR(i)` returns the residual PageRank accumulated
    while executing the *i[th]* step. Given that we treat dead ends as having outgoing
    links to every other node in the graph and that surfers click outgoing links with
    a probability equal to the damping factor *d*, we need to multiply the residual
    PageRank by the damping factor. This yields the preceding equation, which will
    form the basis for our PageRank score calculator.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an API for the PageRank calculator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's discuss how we can implement a PageRank calculator on top of the
    functionality provided by the graph processing framework we have built. The full
    source code and tests for the PageRank calculator can be found in this book's
    GitHub repository in the `Chapter08/pagerank` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Calculator` type is nothing more than a container for a `bspgraph.Graph`
    instance and a bunch of configuration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `executorFactory` points to the executor factory that will be used to create
    new `Executor` instances each time we want to execute the PageRank algorithm on
    a graph. By default, the `Calculator` constructor will use `bspgraph.NewExecutor`
    as the factory implementation, but users will be allowed to override it with a
    `SetExecutorFactory` helper method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: You might be curious why we allow the user to provide a custom factory for creating
    graph executors. The benefit of being able to specify a custom factory is that
    it permits us to intercept, inspect, and modify the `ExecutorCallbacks` object
    *before* it gets passed to the default factory. In [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph Processing Systems*, we will be leveraging this functionality
    to build a distributed version of the PageRank calculator. We will do all of this
    *without changing a single line of code* in the calculator implementation... Sounds
    impossible? Keep reading and all will be revealed in due course!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the definition of the `Config` type, which encapsulates all
    the required configuration options for executing the PageRank algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `validate` method, which is defined on the `Config` type, checks the validity
    of each configuration parameter and populates empty parameters with a sane default
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To create new `Calculator` instances, the clients populate a `Config` object
    that invokes the `NewCalculator` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Before proceeding any further, the constructor has to validate the provided
    set of configuration options. After the configuration object has been successfully
    validated, the next step is to create a new `bspgraph.Graph` instance and store
    it in a newly allocated `Calculator` instance. To instantiate the graph, we need
    to provide a compute function that is parameterized by the provided `DampingFactor`
    value. This is achieved with the help of the `makeComputeFunc` helper, which closes
    over the `dampingFactor` argument and makes it accessible by the returned closure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the underlying `bspgraph.Graph` instance is encapsulated in the `Calculator`
    type, we also need to provide a set of convenience methods so that we can add
    vertices or edges to the graph and access the raw `bspgraph.Graph` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, once the PageRank algorithm converges, users should be able to query
    the PageRank scores that have been assigned to each vertex in the graph. This
    is facilitated via a call to the `Scores` method. The method implementation invokes
    a user-defined visitor function for each vertex in the graph with the vertex ID
    and assigned PageRank score as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating a new `Calculator` instance and specifying the graph layout
    via calls to `AddVertex` and `AddEdge`, we are ready to execute the PageRank algorithm.
    To do so, we need to obtain a `bspgraph.Executor` instance by invoking the calculator''s
    `Executor` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The first task of the `Executor` method is to call the `registerAggregators`
    helper. This helper, whose implementation is outlined in the following code, is
    responsible for registering a set of aggregators that will be used by both the
    PageRank compute function and the executor callbacks that we will be defining
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a closer look at the role of each of these aggregators:'
  prefs: []
  type: TYPE_NORMAL
- en: '`link_count` keeps track of the total number of vertices in the graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`residual_0` and `residual_1` accumulate the residual PageRank quantities for
    even and odd super-steps. In case you are wondering, the reason why we need two
    accumulators is that, while calculating the PageRank scores for the *i[th]* step,
    we need to add the residual PageRank from the *previous* step and at the same
    time accumulate the residual PageRank for the *next* step. While executing the
    i[th] step, the compute function will read from the accumulator at index `i%2`
    and write to the accumulator at index `(i+1)%2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SAD` is yet another accumulator that tracks the sum of absolute PageRank score
    differences between two sequential super-steps. The algorithm will keep executing
    while the accumulator''s value is greater than the `MinSADForConvergence` configuration
    option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second responsibility of the `Executor` method is to define the appropriate
    set of callbacks for executing the PageRank algorithm and invoke the configured
    `ExecutorFactory` to obtain a new `bspgraph.Executor` instance, which is then
    returned to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PreStep` callback ensures that each of the required accumulators is set
    to a zero value prior to executing a new step. The handy `residualOutputAccName`
    helper function returns the name of the accumulator that will store the residual
    PageRank score to be used as input by the *next* super-step, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Once the executor successfully runs the compute function for each vertex in
    the graph, it invokes the `PreStepKeepRunning` callback, whose purpose is to decide
    whether a new super-step needs to be executed. The registered callback looks up
    the `SAD` aggregator's value, compares it to the configured threshold, and terminates
    the algorithm's execution once the value becomes less than the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a compute function to calculate PageRank scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have completed our brief tour of the `Calculator` API, it''s time
    to shift our focus to the most important part of the implementation: the *compute
    function*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of each super-step, vertices are expected to evenly distribute their
    PageRank score to their neighbors. Under our graph processing model, this task
    is facilitated by broadcasting a message. The `IncomingScoreMessage` type describes
    the payload for the exchanged messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: To bootstrap the calculator, we need to set the initial PageRank score for every
    vertex in the graph to the value *1/N*, where *N* is the number of vertices (pages)
    in the graph. An easy way to calculate *N* is to simply access the graph and count
    the number of vertices (for example, `len(g.Vertices())`). However, keep in mind
    that the end goal is to run the algorithm in a distributed fashion. In distributed
    mode, each worker node would only have access to a *subset* of the graph vertices.
    As a result, simply counting the vertices in the *local* graph instance would
    not produce a correct result.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, aggregators provide an elegant solution to our vertex counting
    problem that works for both single- and multi-node scenarios. Super-step *0* serves
    as our initialization step: every compute function invocation increments the value
    of the `page_count` aggregator. At the end of the super-step, the counter will
    contain the total number of vertices in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'For every other super-step, we apply the PageRank formula to estimate the new
    PageRank score for the current vertex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Before storing the new PageRank estimate for the current vertex, we calculate
    the absolute difference from the previous value and add it to the `SAD` aggregator,
    whose role is to track the *sum* of absolute score differences for the current
    super-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'If the vertex has no neighbors (that is, it is a dead end), our model assumes
    that it is *implicitly* connected to every other node in the graph. To ensure
    that the PageRank score for the vertex is evenly distributed to every other vertex
    in the graph, we add a quantity equal to `newScore`/`pageCount` to the residual
    PageRank aggregator that will be used as input in the following super-step. Otherwise,
    we need to evenly distribute the calculated PageRank score to the existing vertex
    neighbors. To achieve this, we send out a series of `IncomingScore` messages that
    contribute a quantity equal to `newScore`/`numOutLinks` to every neighbor at the
    next super-step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: That's basically it! The graph processing system we have developed made it quite
    easy to construct a fully functioning and vertically scalable PageRank calculator
    that can properly handle dead ends. All that remains is to hook it up to the link
    graph and text indexer components that we created in [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml),
    *Building a Persistence Layer*, and we are in business!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began this chapter by presenting the BSP model for building systems that
    can support out-of-core processing for massive datasets. Then, we applied the
    key principles of the BSP model so that we could create our own graph processing
    system that can execute user-defined compute functions for every vertex in the
    graph in parallel while taking advantage of all the available CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we explored a variety of graph-related problems
    and came up with parallel algorithms that can be efficiently executed against
    graphs of any size. In the last part of this chapter, we described the theory
    behind Google's PageRank algorithm and outlined the formulas for calculating PageRank
    scores in an iterative way. We leveraged the graph processing system to build
    a fully-fledged PageRank calculator that will form the basis for implementing
    the PageRank component for the Links 'R' Us project.
  prefs: []
  type: TYPE_NORMAL
- en: As we are getting closer and closer to completing the required components for
    our project, we need to plan ahead and design some APIs so that our components
    can exchange information between them. This is the main focus of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Giraph: An iterative graph processing system built for high scalability.
    URL: [https://giraph.apache.org/](https://giraph.apache.org/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chaitin, G. J.: *Register Allocation & Spilling via Graph Coloring.* In: Proceedings
    of the 1982 SIGPLAN Symposium on Compiler Construction, SIGPLAN ''82\. New York,
    NY, USA : ACM, 1982 — ISBN [0-89791-074-5](https://worldcat.org/isbn/0-89791-074-5),
    S. 98–105.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Floyd, Robert W.: *Algorithm 97: Shortest Path.* In: Commun. ACM Bd. 5\. New
    York, NY, USA, ACM (1962), Nr. 6, S. 345'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GPS: A graph processing system. URL: [http://infolab.stanford.edu/gps](http://infolab.stanford.edu/gps).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hart, P. E. ; Nilsson, N. J. ; Raphael, B.: *A Formal Basis for the Heuristic
    Determination of Minimum Cost Paths.* In: IEEE Transactions on Systems Science
    and Cybernetics Bd. 4 (1968), Nr. 2, S. 100–107.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jones, Mark T. ; Plassmann, Paul E.: *A Parallel Graph Coloring Heuristic.*
    In: SIAM J. Sci. Comput. Bd. 14\. Philadelphia, PA, USA, Society for Industrial;
    Applied Mathematics (1993), Nr. 3, S. 654–669.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Malewicz, Grzegorz ; Austern, Matthew H. ; Bik, Aart J. C ; Dehnert, James
    C. ; Horn, Ilan ; Leiser, Naty ; Czajkowski, Grzegorz: Pregel: *A System for Large-scale
    Graph Processing.* In: Proceedings of the 2010 ACM SIGMOD International Conference
    on Management of Data, SIGMOD ''10\. New York, NY, USA : ACM, 2010 — ISBN [978-1-4503-0032-2](https://worldcat.org/isbn/978-1-4503-0032-2),
    S. 135–146.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Page, Lawrence ; Brin, Sergey ; Motwani, Rajeev ; Winograd, Terry: *The PageRank
    Citation Ranking: Bringing Order to the Web.*(Technical Report Nr. 1999-66) :
    Stanford InfoLab; Stanford InfoLab, 1999\. – Previous number = SIDL-WP-1999-0120.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Park, Taehoon ; Lee, Chae Y.: *Application of the graph coloring algorithm
    to the frequency assignment problem.* In: Journal of the Operations Research Society
    of Japan Bd. 39 (1996), Nr. 2, S. 258–265.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Valiant, Leslie G.: *A Bridging Model for Parallel Computation.* In: Commun.
    ACM Bd. 33\. New York, NY, USA, ACM (1990), Nr. 8, S. 103–111.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
