- en: Building, Packaging, and Deploying Software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Kubernetes is the Linux of distributed systems."'
  prefs: []
  type: TYPE_NORMAL
- en: – Kelsey Hightower
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will guide you through the steps involved in dockerizing Go programs
    and will iterate the best practices for building the smallest possible container
    image for your applications. Following this, this chapter will focus on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin our tour of Kubernetes by comparing the different types of nodes
    that comprise a Kubernetes cluster and take a closer look at the function of the
    various services that make up Kubernetes' control plane. Moving forward, we will
    be describing a step-by-step walkthrough for setting up a Kubernetes cluster on
    your local development machine. The last part of this chapter is a practical application
    of everything you have learned so far. We will bring all the components that we
    created in the previous chapters together, join them with a fully functioning
    frontend, and create a monolithic version of Links 'R' Us that we will then deploy
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using intermediate build containers to compile static binaries for your Go applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the correct set of linker flags to ensure that Go binaries compile to
    the smallest possible size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The anatomy of the components that comprise a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different types of resource types supported by Kubernetes and their application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinning up a Kubernetes cluster on your local workstation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a monolithic version of the Links 'R' Us application using the components
    we developed in the previous chapters and deploying it on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full code for the topics that will be discussed in this chapter has been
    published to this book's GitHub repository under the `Chapter10` folder.
  prefs: []
  type: TYPE_NORMAL
- en: You can access this book's GitHub repository, which contains the code and all
    the required resources for the chapters in this book, by pointing your web browser
    to the following URL: [https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang](https://github.com/PacktPublishing/Hands-On-Software-Engineering-with-Golang).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get you up and running as quickly as possible, each example project includes
    a Makefile that defines the following set of targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Makefile target** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `deps` | Install any required dependencies. |'
  prefs: []
  type: TYPE_TB
- en: '| `test` | Run all tests and report coverage. |'
  prefs: []
  type: TYPE_TB
- en: '| `lint` | Check for lint errors. |'
  prefs: []
  type: TYPE_TB
- en: As with all the other chapters in this book, you will need a fairly recent version
    of Go, which you can download at [https://golang.org/dl](https://golang.org/dl)*.*
  prefs: []
  type: TYPE_NORMAL
- en: To run some of the code in this chapter, you will need to have a working Docker ^([5]) installation
    on your machine. Furthermore, a subset of the examples have been designed to run
    on Kubernetes ^([8]). If you don't have access to a Kubernetes cluster for testing,
    you can simply follow the instructions laid out in the following sections to set
    up a small cluster on your laptop or workstation.
  prefs: []
  type: TYPE_NORMAL
- en: Building and packaging Go services using Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the last few years, more and more software engineers started using systems
    such as Docker to containerize their applications. Containers offer a simple and
    clean way to execute an application without having to worry about the underlying
    hardware or operating system. In other words, the same container image can run
    on your local development machine, a VM on the cloud, or even on a bare-metal
    server located in your company's data center.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of containerization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other than portability, containerization offers a few more important benefits,
    both from a software engineering and DevOps perspective. To begin with, containers
    make it easy to deploy a new version of a piece of software and to effortlessly
    roll back the deployment if something goes wrong. Secondly, containerization introduces
    an extra layer of security; every application executes in complete isolation from
    not only other applications but also from the underlying host itself.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever a new container image is being built (for example, as part of a continuous
    integration pipeline), the target application gets packaged with an *immutable* copy
    of all the required dependencies for running it. As a result, when an engineer
    runs a particular container, they are guaranteed to run exactly the same binary
    as their other colleagues, whereas compiling and running the application locally
    could produce different results, depending on what compiler version or system
    libraries were installed on the development machine.
  prefs: []
  type: TYPE_NORMAL
- en: To take this a step further, apart from containerizing our applications, we
    can also containerize the tools that are used to build them. This allows us to
    create **hermetic** builds and paves the way for supporting repeatable builds,
    whose benefits we have already enumerated in [Chapter 3](bdd8b231-e9fa-4522-8497-66d77231b7f3.xhtml),
    *Dependency Management*.
  prefs: []
  type: TYPE_NORMAL
- en: When executing a hermetic build, the emitted binary artifact is not affected
    by any of the software or system libraries that are installed on the build machine.
    Instead, the build process uses pinned compiler and dependency versions to ensure
    that compiling the same snapshot (for example, a specific git SHA) of the code
    base will always produce the same, bit-by-bit identical binary.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will delve into the process of building Docker containers
    for your Go applications and explore a set of best practices for producing containers
    that are optimized for size.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for dockerizing Go applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go comes with built-in support for producing standalone, static binaries, making
    it an ideal candidate for containerization! Let's take a look at the best practices
    for building Docker containers for your Go applications.
  prefs: []
  type: TYPE_NORMAL
- en: Since static Go binaries tend to be quite large, we must take extra steps to
    ensure that the containers we build do not include any of the build tools (for
    example, the Go compiler) that are used at build time. Unless you are using a
    really old version of Docker, your currently installed version will most likely
    support a feature known as *build containers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A build container includes all the tools that are needed for compiling our
    Go application: the Go compiler and the Go standard library, git, tools for compiling
    protocol buffer definitions, and so on. We will be using the build compiler as
    an *intermediate* container for compiling and linking our application. Then, we
    will create a *new* container, copy the compiled binary over, and discard the
    build container.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how this process works, let''s examine the Dockerfile for building
    the Links ''R'' Us application that we will be building in the last part of this
    chapter. You can find the Dockerfile in the `Chapter10/linksrus` folder of this
    book''s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line specifies the container that we will be using as the base for
    our build container. We can reference this container within the Dockerfile using
    the `builder` alias. The rest of the commands from the preceding Dockerfile perform
    the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: The source files for the application are copied from the host into the build
    container. Note that we copy the **entire** book repository into the container
    to ensure that the `make deps` command can resolve all package imports from this
    book's repository and not try to download them from GitHub.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `make deps` command is invoked to fetch any external package dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the Go compiler is invoked to compile the application and place the
    resulting binary in a known location (in this case, `/go/bin/linksrus-monolith`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s zoom in and explain what actually happens when the `go build` command
    is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: The `GIT_SHA` environment variable is set to the short git SHA of the current
    commit. The `-X main.appSha=$GIT_SHA` linker flag overrides the value of the placeholder
    variable called `appSha` in the main package with the SHA value that we just calculated.
    We will be outputting the value of the `appSha` variable in the application logs
    to make it easy for operators to figure out which application version is currently
    deployed simply by tailing the logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CGO_ENABLED=0` environment variable notifies the Go compiler that we won't
    be invoking any C code from our program and allows it to optimize away quite a
    bit of code from the final binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-static` flag instructs the compiler to produce a static binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `-w` and `-s` flags instruct the Go linker to drop debug symbols
    (more specifically, the DWARF section and symbol information) from the final binary.
    This still allows you to get full stack traces in case of a panic but prevents
    you from attaching a debugger (for example, delve) to the binary. On the bright
    side, these flags will significantly reduce the total size of the final binary!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next section of the Dockerfile contains the steps for building the final
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we know that the Links 'R' Us application will most probably be making
    TLS connections, we need to ensure that the final container image ships with the
    CA certificates for trusted authorities around the world. This is achieved by
    installing the `ca-certificates` package. To complete the build, we *copy* the
    compiled binary from the **build** container into the final container.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a suitable base container for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, I chose to use *Alpine* as the base container for the
    application. So, why pick alpine over something more widely known, such as Ubuntu?
    The answer is size!
  prefs: []
  type: TYPE_NORMAL
- en: The Alpine Linux ^([1]) container is one of the smallest base containers you
    can find out there. It ships with a small footprint libc implementation (musl)
    and uses busybox as its shell. As a result, the total size of the alpine container
    is only 5 M, thus making it ideal for hosting our Go static binaries. Furthermore,
    it includes its own package manager (apk), which lets you install additional packages
    such as the ca-certificates or network tools while the final container is being
    built.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we don''t need this extra functionality, though? Is it possible to
    produce an application container that is even smaller? The answer is yes! We can
    use the special **scratch** container as our base container. As the name implies,
    the scratch container is literally empty... It has no root filesystem and only
    includes our application binary. However, it does come with a few caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: It does not include any CA certificates, nor is there any way to install them
    besides copying them over from an intermediate build container. However, if your
    application or microservice will only communicate with services in a private subnet
    using non-TLS connections, this might not be a problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container does not include a shell. This makes it impossible to actually
    SSH into a running container for debugging purposes (for example, to check that
    DNS resolution works or to grep through log files).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My recommendation is to always use a tiny container such as alpine or something
    similar instead of the scratch container.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should be able to apply the best practices we outlined in
    the previous sections and create space-efficient container images for your own
    Go applications. So, what''s next? The next step is, of course, to deploy and
    scale your applications. As you probably suspect, we won''t be doing this manually!
    Instead, we will be leveraging an existing, industrial-grade solution for managing
    containers at scale: Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: A gentle introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes ^([8]) is an open source platform for managing containerized workloads
    that was built from the start with future extensibility in mind. It was originally
    released by Google back in 2014 and it encompasses both their insights and best
    practices for running large-scale, production-grade applications. Nowadays, it
    has eclipsed the managed container offerings of the most popular cloud providers
    and is en route to becoming the *de facto* standard for deploying applications
    on-premises and in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Describing Kubernetes in detail is not within the scope of this book. Instead,
    the goal of the following sections is to provide you with a brief introduction
    to Kubernetes and distill some of its basic concepts into an easily digestible
    format that conveys enough information to allow you to spin up a test cluster
    and deploy the Links 'R' Us project to it.
  prefs: []
  type: TYPE_NORMAL
- en: Peeking under the hood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay, so we have already mentioned that Kubernetes will do the heavy lifting
    and manage different types of containerized workloads for you. But how does this
    work under the hood? The following diagram illustrates the basic components that
    comprise a Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/985bfbe5-58d9-4726-a105-e945048a2e56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A high-level overview of a Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster consists of two types of nodes: **masters** and **workers**.
    These can be either physical or virtual machines. The master nodes implement the
    control plane for your cluster, whereas the worker nodes pool their resources
    together (CPUs, memory, disk, or even GPUs) and execute the workloads that are
    assigned to them by the master.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every master node runs the following processes:'
  prefs: []
  type: TYPE_NORMAL
- en: The **kube-api-server**. You can think of this as an API gateway for allowing
    worker nodes and cluster operators to access the control plane for the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd** implements a key-value store where the cluster''s current state is
    persisted. It also provides a convenient API that allows clients to watch a particular
    key or set of keys and receive a notification when their values change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **scheduler** monitors the cluster state for incoming workloads and makes
    sure that every workload is assigned to one of the available worker nodes. If
    the workload requirements cannot be met by any of the worker nodes, the scheduler
    might opt to **reschedule** an existing workload to a different worker so as to
    make room for the incoming workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **cloud controller manager** handles all the necessary interactions with
    the underlying cloud substrate that hosts the cluster. Examples of such interactions
    include provisioning *cloud-specific* services, such as storage or load balancers,
    and creating or manipulating resources such as routing tables and DNS records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A production-grade Kubernetes cluster will typically be configured with multiple
    master nodes; the control plane manages the cluster state, so it is quite important
    for it to be *highly available*. In such a scenario, data will be automatically
    replicated across the master nodes and DNS-based load balancing will be used to
    access the kube-api-server gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at the internals of a worker node. Given that Kubernetes
    manages containers, a key requirement is that each worker node provides a suitable
    container runtime. As you've probably guessed, the most commonly used runtime
    is Docker; however, Kubernetes will happily work with other types of container
    runtime interfaces, such as containerd ^([4]) or rkt ^([12]).
  prefs: []
  type: TYPE_NORMAL
- en: Each and every workload that is scheduled on a particular worker node is executed
    in isolation within its container runtime. The minimum unit of work in Kubernetes
    is referred to as a **pod**. Pods consist of one or *more* container images that
    are executed on the same worker instance. While single-container pods are the
    most typical, multi-container pods are also quite useful. For instance, we could
    deploy a pod that includes nginx and a sidecar container that monitors an external
    configuration source and regenerates the nginx configuration as needed. An application
    can be horizontally scaled by creating additional pod instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The worker nodes also run the following processes:'
  prefs: []
  type: TYPE_NORMAL
- en: The **kubelet** agent connects to the master's **api-server** and watches for
    workload assignments to the worker node it is running on. It ensures that the
    required containers are always up and running by automatically restarting them
    if they suddenly die.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **kube-proxy** works like a network proxy. It maintains a set of rules that
    control the routing of internal (cluster) or external traffic to the pods that
    are currently executing on the worker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing the most common Kubernetes resource types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operators interact with the Kubernetes cluster by creating, deleting, or otherwise
    manipulating different types of resources via a CRUD-like interface. Let's take
    a brief look at some of the most common Kubernetes resource types.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite rare to come across an application that doesn't require any sort
    of configuration. While we could definitely hardcode the configuration settings
    when we create our pods, this is generally considered to be bad practice and frankly
    becomes a major source of frustration when we need to change a configuration setting
    (for example, the endpoint for a database) that is shared between multiple applications.
    To alleviate this problem, Kubernetes offers the *config map* resource. Config
    maps are collections of key-value pairs that can be injected into pods either
    as environment variables or mounted as plain text files. This approach allows
    us to manage configuration settings at a single location and avoid hardcoding
    them when creating pods for our applications. Kubernetes also provides the **secret** resource,
    which works in a similar fashion to a config map but is meant to be used for sharing
    sensitive information such as certificate keys and service credentials between
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: A **namespace** resource works as a virtual container for logically grouping
    other Kubernetes resources and controlling access to them. This is a very handy
    feature if multiple teams are using the same cluster for their deployments. In
    such a scenario, each team is typically assigned full access to their own namespace
    so that they cannot interfere with the resources that are deployed by other teams
    unless they are granted explicit access.
  prefs: []
  type: TYPE_NORMAL
- en: Once a pod dies, any data stored within any of its containers will be lost.
    To support use cases where we want to persist data across pod restarts or we simply
    want to share the same set of data (for example, the pages served by a web server)
    across multiple pod instances, Kubernetes provides the **persistent volume** (**PV**)
    and **persistent volume claim** (**PVC**) resources. A persistent volume is nothing
    more than a piece of block storage that is made available to the cluster. Depending
    on the substrate, it can either be manually provisioned by the cluster administrators
    or dynamically allocated on-demand by the underlying substrate (for example, an
    EBS volume when running on AWS). On the other hand, a persistent volume claim
    represents an operator's request for a block of storage with a particular set
    of attributes (for example, size, IOPS, and spinning disk or SSD). The Kubernetes
    control plane attempts to match the available volumes with the operator-specified
    claims and mount the volumes to the pods that reference each claim.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy a stateless application on Kubernetes, the recommended approach is
    to create a **deployment** resource. A deployment resource specifies a **template** for
    instantiating a single pod of the application and the desired number of replicas.
    Kubernetes continuously monitors the state of each deployment and attempts to
    synchronize the cluster state with the desired state by either creating new pods
    (using the template) or deleting existing pods when the number of active pods
    exceeds the requested number of replicas. Each pod in a deployment gets assigned
    a random hostname by Kubernetes and shares the *same PVC* with every other pod.
  prefs: []
  type: TYPE_NORMAL
- en: Many types of workloads, such as databases or message queues, require a stateful
    kind of deployment where pods are assigned stable and predictable hostnames and
    each individual pod gets its own PVC. What's more, these applications usually
    operate in a clustered configuration and expect nodes to be deployed, upgraded,
    and scaled in a particular sequence. In Kubernetes, this type of deployment is
    accomplished by creating a **StatefulSet**. Similar to a deployment resource,
    a StatefulSet also defines a pod template and a number of replicas. Each replica
    is assigned a hostname, which is constructed by concatenating the name of the
    StatefulSet and the index of each pod in the set (for example, web-0 and web-1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Being able to scale the number of deployed pods up and down is a great feature
    to have but not that useful unless other resources in the cluster can connect
    to them! To this end, Kubernetes supports another type of resource, called a **service**.
    Services come in two flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: A service can sit in front of a group of pods and act as a **load balancer**.
    In this scenario, the service is automatically assigned both an IP address and
    a DNS record to aid its discovery by clients. In case you are wondering, this
    functionality is implemented by the **kube-proxy** component that runs on each
    worker node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **headless** service allows you to implement a custom service discovery mechanism.
    These services are not assigned a cluster IP address and they are totally ignored
    by kube-proxy. However, these services create DNS records for the service and
    resolve to the address of every single pod behind the service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last Kubernetes resource that we will be examining is **ingresses**. Depending
    on its configuration, an ingress exposes HTTP or HTTPS endpoints for routing traffic
    from outside the cluster to particular services within the cluster. The common
    set of features that are supported by the majority of ingress controller implementations
    include TLS termination, name-based virtual hosts, and URL rewriting for incoming
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our overview of the most common Kubernetes resource types. Keep
    in mind that this is only the tip of the iceberg! Kubernetes supports many other
    resource types (for example, cron jobs) and even provides APIs that allow operators
    to define their own custom resources. If you want to learn more about Kubernetes
    resources, I would strongly recommend browsing the quite extensive set of Kubernetes
    documentation that is available online ^([8]).
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to easily set up your very own Kubernetes cluster on
    your laptop or workstation.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Kubernetes cluster on your laptop!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few years ago, experimenting with Kubernetes was more or less restricted to
    engineers who were either granted access to a test or dev cluster or they had
    the resources and knowledge that was required to bootstrap and operate their own
    cluster on the cloud. Nowadays, things are much simpler... In fact, you can even
    spin up a fully operational Kubernetes cluster on your laptop in just a couple
    of minutes!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some of the most popular, dev-friendly Kubernetes distributions
    that you can deploy on your development machine:'
  prefs: []
  type: TYPE_NORMAL
- en: K3S ^([7]) is a tiny (it's literally a 50 M binary!) distribution that allows
    you to run Kubernetes on resource-constrained devices. It provides binaries for
    multiple architectures, including ARM64/ARMv7\. This makes it a great candidate
    for running Kubernetes on Raspberry Pi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microk8s ^([9]) is a project by Canonical that promises zero-ops Kubernetes
    cluster setups. Getting a Kubernetes cluster up and running on Linux is as simple
    as running `snap install microk8s`. On other platforms, the recommended approach
    for installing microk8s is to use an application such as Multipass ^([11]) to
    spin up a VM and run the aforementioned command inside it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minikube ^([10]) is yet another distribution, this time by the Kubernetes authors.
    It can work with different types of hypervisors (for example, VirtualBox, Hyperkit,
    Parallels, VMware Fusion, or Hyper-V) and can even be deployed on bare metal (Linux
    only).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make it as easy as possible for you to set up your own Kubernetes cluster
    on your favorite OS and run the examples shown in the upcoming sections, we will
    be working exclusively with Minikube and use VirtualBox as our hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, make sure that you have downloaded and installed the following
    software:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker ^([5]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VirtualBox ^([13]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kubectl binary for your platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The helm [6] binary for your platform. Helm is a package manager for Kubernetes
    and we will be using it to deploy the CockroachDB and Elasticsearch instances
    for the Links 'R' Us project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest Minikube version for your platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With all the preceding dependencies in place, we are ready to bootstrap our
    Kubernetes cluster using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This command will create a virtual machine with 4 GB of RAM and deploy Kubernetes
    1.15.3 to it. It will also update the local configuration for kubectl so that
    it automatically connects to the cluster we have just provisioned. What's more,
    it will enable the **Container Networking Interface** (**CNI**) plugin for the
    cluster. In the next chapter, we will leverage this functionality to install a
    network security solution such as Calico ^([2]) or Cilium ^([3]) and define fine-grained
    network policies to lock down our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As our deployed services will be running inside Minikube's virtual machine,
    the only way to access them from the **host** machine is by provisioning an ingress
    resource. Luckily for us, Minikube provides a suitable ingress implementation
    as an add-on that we can activate by running `minikube addons enable ingress`. What's
    more, for our tests, we want to use a private Docker registry for pushing the
    Docker images that we will be building. Minikube ships with a private registry
    add-on that we can enable by running `minikube addons enable registry`.
  prefs: []
  type: TYPE_NORMAL
- en: However, by default, Minikube's private registry runs in insecure mode. When
    using insecure registries, we need to explicitly configure our local Docker daemon
    to allow connections to them; otherwise, we won't be able to push our images.
    The registry is exposed on port `5000` at the IP used by Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: You can find Minikube's IP address by running `minikube ip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, you can edit `/etc/docker/daemon.json`, merge in the following JSON
    block (replacing `$MINIKUBE_IP` with the IP we obtained with the `minikube ip` command),
    and then *restart the Docker daemon*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: On OS X and Windows, you can simply right-click on the Docker for desktop, select
    preferences, and then click on the Daemon tab to access the list of trusted insecure
    registries.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we need to do is install the required cluster resources so that
    we can use the helm package manager. We can do this by running `helm init`.
  prefs: []
  type: TYPE_NORMAL
- en: To save you some time, I have encoded all the preceding steps into a Makefile,
    which you can find in the `Chapter10/k8s` folder of this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: To bootstrap the cluster, install all the required add-ons, and configure helm,
    you can simply type `make bootstrap-minikube`.
  prefs: []
  type: TYPE_NORMAL
- en: That's it! We have a fully functioning Kubernetes cluster at our disposal. Now,
    we are ready to build and deploy a monolithic version of the Links 'R' Us project.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying a monolithic version of Links 'R' Us
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the moment of truth! In the following sections, we will leverage everything
    we have learned in this chapter to assemble all the Links 'R' Us components that
    we developed in the previous chapters into a monolithic application that we will
    then proceed to deploy on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the user stories from [Chapter 5](6e4047ad-1fc1-4c3e-b90a-f27a62d06f17.xhtml),
    *The Links ''R'' Us Project*, in order for our application to satisfy our design
    goals, it should provide the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: A periodically running, multi-pass crawler for scanning the link graph, retrieving
    links for indexing, and augmenting the graph with newly discovered links to be
    crawled during a future pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another periodically running service to recalculate and persist PageRank scores
    for the continuously expanding link graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A frontend for our end users to perform search queries and to submit website
    URLs for indexing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we haven't really discussed the frontend. Don't worry; we will be building
    a fully fledged frontend for our application in one of the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: As you've probably guessed, due to the number of components involved, the final
    application will undoubtedly include quite a bit of boilerplate code. Given that
    it is not feasible to include the full source code in this chapter, we will only
    focus on the most interesting parts. Nevertheless, you can find the documented
    source code for the entire application in the `Chapter10/linksrus` folder of this
    book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing computation across application instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In anticipation of the Links 'R' Us project becoming an overnight success and
    attracting a lot of traffic, especially after posting a link on sites such as
    Hacker News and Slashdot, we need to come up with a reasonable plan for scaling.
    Even though we are currently dealing with a monolithic application, we can always
    scale horizontally by spinning up additional instances. Moreover, as our link
    graph size grows, we will undoubtedly need additional compute resources for both
    our web crawlers and our PageRank calculator.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key benefits of using a container orchestration platform such as
    Kubernetes is that we can effortlessly scale up (or down) any deployed application.
    As we saw at the beginning of this chapter, a `Service` resource connected to
    an `Ingress` can act as a load balancer and distribute *incoming* traffic to our
    application. This transparently takes care of our frontend scaling issues with
    no additional development effort on our end.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, making sure that *each* application instance crawls a specific *subset* of
    the graph isn't straightforward as it requires application instances to coordinate
    with each other. This implies that we need to establish a communication channel
    between the individual instances. Or does it?
  prefs: []
  type: TYPE_NORMAL
- en: Carving the UUID space into non-overlapping partitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml), *Building a Persistence
    Layer*, we mentioned that the caller of the `Links` and `Edges` methods that are
    exposed by the link-graph component is responsible for implementing a suitable
    partitioning scheme and providing the appropriate UUID ranges as arguments to
    these methods. So, how can we go about implementing such a partitioning scheme?
  prefs: []
  type: TYPE_NORMAL
- en: Our approach exploits the observation that the link (and edge) IDs are, in fact,
    V4 (random) UUIDs and are therefore expected to be more or less evenly distributed
    in the massive (2^(128)) UUID space. Let's assume that the total number of workers
    (that is, the number of partitions) available to us is *N*. For the time being,
    we will treat the number of workers as being fixed and a priority known. In the
    following section, we will learn how to leverage the Kubernetes infrastructure
    to automatically discover this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out the range of UUIDs that the *M[th]* worker (where 0 <= M < N)
    needs to provide as arguments to the `Links` and `Edges` methods of the graph,
    we need to perform some calculations. First, we need to subdivide the 128-bit
    UUID space into *N* equally sized sections; in essence, each section will contain *C
    = 2^(128) / N* UUIDs. Consequently, to calculate the *M[th]* worker''s UUID range,
    we can use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a40a2bfe-c178-4e4a-8af1-34cffdf18d39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the number of workers (*N*) is *odd*, then we will not be able to divide
    the UUID space evenly; therefore, the **last** (N-1) section is treated in a special
    manner: it always extends to the **end** of the UUID space (the UUID value `ffffffff-ffff-ffff-ffff-ffffffffffff`).
    This ensures that we always cover the entire UUID space, regardless of whether *N* is
    odd or even!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rationale behind this type of split is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Most modern database systems tend to cache the primary key index in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They contain special optimized code paths for performing *range scans* on primary
    key ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of the preceding two properties makes this solution quite attractive
    for the read-heavy workloads that are performed by both the crawler and the PageRank
    calculator components. One small nuisance is that UUIDs are 128-bit values and
    Go does not provide scalar types for performing 128-bit arithmetic. Fortunately,
    the standard library provides the `math/big` package, which can perform arbitrary-precision
    arithmetic operations!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and create a helper that will take care of all these calculations
    for us. The `Range` helper implementation will live in a file called `range.go`,
    which is part of the `Chapter10/linksrus/partition` package (see this book''s
    GitHub repository). Its type definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For our particular application, we will provide two constructors for creating
    ranges. The first constructor creates a `Range` that spans the full UUID space
    and splits it into `numPartitions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the constructor delegates the creation of the range to the `NewRange` helper,
    whose implementation has been broken down into smaller snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Before we proceed, the code verifies that the provided UUID range is valid by
    making sure that the start UUID is smaller than the end UUID. To achieve this,
    we use the handy `bytes.Compare` function, which compares two byte slices and
    returns a value greater than or equal to zero if the two byte slices are either
    equal or the first byte slice is greater than the second. One caveat here is that
    the UUID type is defined as `[16]byte`, whereas the `bytes.Compare` function expects
    byte slices. However, we can easily convert each UUID into a byte slice using
    the convenience operator, `[:]`.
  prefs: []
  type: TYPE_NORMAL
- en: After the preliminary argument validation, we create an empty `big.Integer` value
    and use the cumbersome API of the `math/big` package to load it with the result
    of the `(end - start) + 1` expression. Once the value has been loaded, we divide
    it by the number of partitions that the caller provided as an argument to the
    function. This yields the `C` value from the formula we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following block of code uses a `for` loop to calculate and store the **end** UUID
    for each partition that is part of the range we are creating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned in the previous section, the end UUID for the last partition
    is always the maximum possible UUID value. For all the other partitions, we calculate
    the end by multiplying the size of each partition by the partition number, plus
    one. Once all the calculations have been completed, a new `Range` object is allocated
    and returned to the caller. In addition to the calculated end ranges, we also
    keep track of the start UUID for the range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to make the `Range` type easier to use from within the crawler service
    code, let''s define two auxiliary methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `Extends` method returns the start (inclusive) and end (exclusive) UUID
    value for the *entire* range. On the other hand, the `PartitionExtents` function returns
    the start and end UUID values for a specific *partition* within the range.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning a partition range to each pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the help of the `Range` type from the previous section, we now have the
    means to query the UUID range that''s assigned to every single partition. For
    our particular use case, the number of partitions is equal to the number of pods
    that we launch. However, one crucial bit of information that we are lacking is
    the partition number that''s assigned to each individual launched pod! Consequently,
    we now have two problems that we need to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the partition number of an individual pod?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the total number of pods?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we deploy our application as a StatefulSet with *N* replicas, every pod in
    the set will be assigned a hostname that follows the pattern `SET_NAME-INDEX`,
    where `INDEX` is a number from *0* to *N-1* that indicates the index of the pod
    in the set. All we need to do is read the pod's hostname from our application,
    parse the numeric suffix, and use that as the partition number.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to answering the second question would be to query the Kubernetes
    server API. However, this requires additional effort to set up (for example, create
    service accounts, RBAC records) – not to mention that it effectively locks us
    into Kubernetes! Fortunately, there is an easier way...
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to create a **headless** service for our application, it would automatically
    generate a set of SRV records that we can query and obtain the host for each individual
    pod that belongs to the service. The following diagram shows the results of running
    an SRV query from within a pod in the Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b560f6db-9058-4603-85ec-01efca0ad74e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2: The linksrus-headless service is associated with four pods whose hostnames
    are visible on the right-hand side
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the information displayed in the preceding screenshot, we could write
    a helper for figuring out the partition number and the total number of partitions
    for a running application instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To get the hostname, we invoke the `Hostname` function provided by the `os` package.
    Then, we split on the dash separator, extract the right-most part of the hostname,
    and use `ParseInt` to convert it into a number.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to get the SRV records, we use the `LookupSRV` function from the `net` package
    and pass the service name as the last argument. Then, we count the number of results
    to figure out the total number of pods in the set. One important thing to be aware
    of is that SRV record creation is not instantaneous! When the StatefulSet is initially
    deployed, it will take a bit of time for the SRV records to become available.
    To this end, if the SRV lookup does not yield any results, the code will return
    a typed error to let the caller know that they should try again later.
  prefs: []
  type: TYPE_NORMAL
- en: Building wrappers for the application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we intentionally designed the various Link 'R' Us components so that
    they are more or less decoupled from their input sources. For example, the crawler
    component from [Chapter 7](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml), *Data-Processing
    Pipelines*, expects an iterator that yields the set of links to be crawled, while
    the PageRank calculator component from [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    Graph*-Based Data Processing*, only provides convenience methods for creating
    the nodes and edges of the graph that are used by the PageRank algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate these components into a larger application, we need to provide
    a thin layer that implements two key functions:'
  prefs: []
  type: TYPE_NORMAL
- en: It connects each component with a suitable link graph and the text indexer data
    store implementation from [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml),
    *Building a Persistence Layer*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It manages the *refresh cycle* for each component (for example, triggering a
    new crawler pass or recalculating PageRank scores)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each service will be started from the main package of the Links ''R'' Us application
    and execute independently of other services. If any of the services exit due to
    an error, we want our application to cleanly shut down, log the error, and exit
    with the appropriate status code. This necessitates the introduction of a supervisor
    mechanism that will manage the execution of each service. Before we get to that,
    let''s start by defining an interface that each of our application services needs
    to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: No surprise there... The `Name` method returns the name of the service, which
    we can use for logging purposes. As you've probably guessed, the `Run` method
    implements the business logic for the service. Calls to `Run` are expected to
    block until either the provided context expires or an error occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The crawler service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The business logic for the crawler service is quite straightforward. The service
    uses a timer to sleep until the next update interval is due and then executes
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it queries the most recent information about partition assignments. This
    includes the partition number for the pod and the total number of partitions (pod
    count).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the partition count information from the previous step, a new full `Range` is
    created and the extents (UUID range) for the currently assigned partition number
    are calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the service obtains a link iterator for the calculated UUID range and
    uses it as a data source to drive the crawler component that we built in [Chapter
    7](51dcc0d4-2ba3-4db9-83f7-fcf73a33aa74.xhtml), *Data-Processing Pipelines*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The service constructor expects a configuration object that includes not only
    the required configuration options but also a set of interfaces that the service
    depends on. This approach allows us to test the service in total isolation by
    injecting mock objects that satisfy these interfaces. Here''s what the `Config` type
    for the crawler service looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be wondering why I chose to redefine the `GraphAPI` and `IndexAPI` interfaces inside this
    package instead of simply importing and using the original interfaces from the `graph` or
    `index` packages. This is, in fact, an application of the interface segregation
    principle! The original interfaces contain more methods than what we actually
    need for this service. For example, the following is the set of methods that the
    crawler requires to access the link graph and indexing documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'A very handy side effect of using the smallest possible interface definitions
    for the graph and index APIs is that these minimal interfaces also happen to be
    compatible with the gRPC clients that we created in the previous chapter. We will
    be exploiting this observation in the next chapter so that we can split our monolithic
    application into microservices! Now, let''s take a look at the rest of the configuration
    fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PartitionDetector` will be queried by the service to obtain its partition
    information. When running in Kubernetes, the detector will use the code from the
    previous section to discover the available partitions. Alternatively, a partition
    detector that always reports a single partition can be injected to allow us to
    run the application as a standalone binary on our development machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Clock` allows us to inject a fake clock instance for our tests. Just as we
    did in [Chapter 4](d279a0af-50bb-4af2-80aa-d18fedc3cb90.xhtml), *The Art of Testing*,
    we will be using the `juju/clock` package to mock time-related operations within
    our tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fand so onhWorkers` controls the number of workers that are used by the crawler
    component to retrieve links.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UpdateInterval` specifies how often the crawler should perform a new pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReIndexThreshold` is used as a filter when selecting the set of links to be
    crawled in the next crawler pass. A link will be considered for crawling when
    its *last retrieval time* is *older* than `time.Now() - ReIndexThreshold`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Logger` specifies an optional logger instance to use for log messages. We
    will talk more about structured logging in the next chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PageRank calculator service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a similar fashion to the crawler service, the PageRank service also wakes
    up periodically to recalculate the PageRank scores for every link in the graph.
    Under the hood, it uses the PageRank calculator component that we built in [Chapter
    8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml), *Graph-Based Data Processing*,
    to execute a complete pass of the PageRank algorithm. The service layer is responsible
    for populating the internal graph representation that's used by the calculator
    component, invoking it to calculate the updated PageRank scores, and updating
    the PageRank scores for every indexed document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service constructor also accepts a `Config` object that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pagerank` service package defines its own version of the `GraphAPI` and `IndexAPI` types.
    As shown in the following code, the method list for these interfaces is different
    from the one we used for the crawler service in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `ComputeWorkers` parameter is passed through to the PageRank calculator
    component and controls the number of workers that are used to execute the PageRank
    algorithm. On the other hand, the `UpdateInterval` parameter controls the score
    refresh frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it is not *currently* feasible to run the PageRank service in
    partitioned mode. As we saw in [Chapter 8](c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml),
    *Graph-Based Data Processing*, the calculator implementation operates under the
    assumption that every node in the graph can send messages to *every* other node
    in the graph and that all the vertices have access to the shared global state
    (aggregators). For the time being, we will use the detected partition information
    as a constraint to execute the service on a **single** pod (more specifically,
    the one assigned to partition 0). No need to worry, though! In [Chapter 12](67abdf43-7d4c-4bff-a17e-b23d0a900759.xhtml),
    *Building Distributed Graph-Processing Systems*, we will revisit this implementation
    and rectify all the aforementioned issues.
  prefs: []
  type: TYPE_NORMAL
- en: Serving a fully functioning frontend to users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, our little project can't really be complete without a proper frontend
    for our users! To build one, we will leverage the Go standard library's support
    for HTML templates (the `text/template` and `html/template` packages) to design
    a fully functioning static website for Links 'R' Us. For simplicity, all the HTML
    templates will be embedded into our application as *strings* and parsed into `text.Template` when
    the application starts. In terms of functionality, our frontend must implement
    a number of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it must implement an index/landing page where the user can enter a search
    query. Queries can either be keyword- or phrase-based. The index page should also
    include a link that can navigate users to another page where they can submit a
    website for indexing. The following screenshot shows a rendering of the index
    page template:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/066ea421-a3a2-4d39-862d-4f6372496d63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The landing page for Links ''R'' Us'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need a page where webmasters can manually submit their websites for
    indexing. As we mentioned previously, the index/landing page will include a link
    to the site submission page. The following screenshot shows what the rendered
    site submission page for indexing will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/416b6e86-061e-41af-967d-05be5b1dcec3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A form for manually submitting sites for indexing'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final, and obviously most important, page in our entire application is
    the search results page. As shown in the following screenshot, the results page
    renders a **paginated** list of websites matching the user''s search query. The
    header of the page includes a search text box that displays the currently searched
    term and allows users to change their search terms without leaving the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f8aab16-317e-4958-a397-50aefedf6ed1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The paginated list of search results'
  prefs: []
  type: TYPE_NORMAL
- en: 'The template for rendering the individual search result blocks, as portrayed
    in the preceding screenshot, consists of three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: A link to the web page. The link text will either display the title of the matched
    page or the link to the page itself, depending on whether the crawler was able
    to extract its title.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The URL to the matched web page in a smaller font.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of the page's contents where the matched keywords are highlighted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have defined all the necessary templates for rendering the pages
    that compose the Links 'R' Us frontend, we need to register a series of HTTP routes
    to allow our end users to access our service.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the endpoints for the frontend application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table lists the HTTP request types and endpoints that our frontend
    service needs to handle in order to implement all the features we described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Request Type** | **Path** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| GET | `/` | Displays the index page |'
  prefs: []
  type: TYPE_TB
- en: '| GET | `/search?q=TERM` | Displays the first page of results for TERM |'
  prefs: []
  type: TYPE_TB
- en: '| GET | `/search?q=TERM&offset=X` | Displays the results for TERM, starting
    from a particular offset |'
  prefs: []
  type: TYPE_TB
- en: '| GET | `/submit/site` | Displays the site submission form |'
  prefs: []
  type: TYPE_TB
- en: '| POST | `/submit/site` | Handles a site submission |'
  prefs: []
  type: TYPE_TB
- en: '| ANY | Any other path | Displays a 404 page |'
  prefs: []
  type: TYPE_TB
- en: 'To make our life easier, we will be using `gorilla/mux` as our preferred router.
    Creating the router and registering the endpoint handlers is as simple as using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To make the frontend service easier to test, the `Service` type stores a reference
    to the router. This way, we can use the `httptest` package primitives to perform
    HTTP requests directly at the mux without having to spin up any servers.
  prefs: []
  type: TYPE_NORMAL
- en: Performing searches and paginating results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Searching and paginating results is more or less a straightforward task for
    the frontend service. All our service needs to do is parse the search terms, offset
    from the request's query string, and invoke the `Query` method of the text indexer
    store that was passed as a configuration option when the service was instantiated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the service consumes the result iterator until it has either processed
    enough results to populate the results page or the iterator reaches the end of
    the result set. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The service creates a decorated model for each result that provides some convenience
    methods that will be called by the Go code blocks within the template. In addition,
    the `matchedDoc` type includes a `summary` field, which is populated with a short
    excerpt of the matched page's contents, with the search terms highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight search terms in the text summary, the keyword highlighter will
    wrap each term in a `<em>` tag. However, this approach requires the result page
    template to render summaries as **raw HTML**. Consequently, we must be very careful
    not to allow any other HTML tags in our result summaries as this would make our
    application vulnerable to **cross-site scripting** (**XSS**) attacks. While the
    crawler component strips all the HTML tags from crawled pages, it doesn't hurt
    to be a little paranoid and escape any HTML characters from the generated summaries
    before passing them through to our keyword highlighter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to render the navigation header and footer, we need to provide the
    page template with information about the current pagination state. The following
    code shows how the `paginationDetails` type is populated with the required bits
    of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: A *previous* result page link will always be rendered when the current result
    offset is greater than 0\. Unless we are moving back to the *first* result page,
    the link will always include an offset parameter. Similarly, the *next* result
    page link will be rendered as long as we haven't reached the end of the result
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Generating convincing summaries for search results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating a descriptive short summary that conveys enough information to the
    user about the contents of a web page that matched their query is quite a hard
    problem to solve. As a matter of fact, automatic summarization is an active research
    field for natural language processing and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Arguably, building such a system is outside the scope of this book. Instead,
    we will be implementing a much simpler algorithm that yields plausible summaries
    that should be good enough for our particular use case. Here is an outline of
    the algorithm''s steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the matched page's content into sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each sentence, calculate the ratio of matched keywords to the total number
    of words. This will serve as our quality metric for selecting and prioritizing
    the set of sentences to be included in the summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Skip any sentences where the ratio is zero; that is, they don't contain any
    of the searched keywords. These sentences are not really useful for our summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the remaining sentences to a list where each entry is a tuple of `{ordinal,
    text, matchRatio}`. The ordinal value refers to the location of the sentence in
    the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the list by *match ratio* in *descending* order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a second list for the sentence fragments that will be used for the
    summary and a variable that keeps track of the remaining characters for the summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate the sorted list; for each entry, do the following: If its length is *less* than
    the remaining summary characters, append the entry as is to the second list and
    subtract its length from the remaining characters, variable. If its length is *more* than
    the remaining summary character, truncate the sentence text to the remaining summary
    characters, append it to the second list, and *terminate* the iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the summary fragment list by *ordinal* in *ascending* order. This ensures
    that sentence fragments appear in the same order as the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate the sorted fragment list and concatenate the entries as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the ordinal of the current sentence is one more than the previous sentence's
    ordinal, they should be joined with a single period, just like they were connected
    together in the original text.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the sentences should be joined with an ellipsis since they belong
    to different parts of the text.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete Go implementation of the preceding algorithm is too long to list
    here, but if you're curious, you can take a look at it by visiting this book's
    GitHub repository and browsing the contents of the `summarizer.go` file, which
    you can find under the `Chapter10/linksrus/service/frontend` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Highlighting search keywords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have generated a summary for a matched document, we need to identify
    and highlight all the search keywords that are present within it. For this task,
    we will create a helper type named `matchHighlighter` that constructs a set of
    regular expressions for matching each search keyword and wrap it with a special
    HTML tag that our frontend template renders using a highlighted style.
  prefs: []
  type: TYPE_NORMAL
- en: 'The frontend creates a single `matchHighlighter` instance for the entire set
    of results by invoking the `newMatchHighlighter` function, which is listed in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The constructor receives the user's search terms as input and splits them into
    a list of words. Note that the search term will be enclosed in quotes if the user
    is searching for an exact phrase. Therefore, before passing the term string to `strings.Fields`, we
    need to trim any quotes at the beginning and end of the input string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, for each individual term, we compile a *case-insensitive* regular expression,
    which will be used by the `Highlight` method. This is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `Highlight` method simply iterates the list of regular expressions and wraps
    each match in a `<em>` tag that our result page template can style using CSS rules.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating the execution of individual services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have created three services for our monolith that all implement
    the `Service` interface. Now, we need to introduce a supervisor for coordinating
    their execution and making sure that they all cleanly terminate if any of them
    reports an error. Let''s define a new type so that we can model a group of services
    and add a helper `Run` method to manage their execution life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s break down the `Run` method''s implementation into smaller chunks
    and go through each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, first, we create a new cancelable context that wraps the one
    that was externally provided to us by the `Run` method caller. The wrapped context
    will be provided as an argument to the `Run` method of each individual service,
    thus ensuring that *all* the services can be canceled in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By the caller if, for instance, the provided context is canceled or expires
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the supervisor, if any of the services raise an error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will spin up a goroutine for each service in the group and execute
    its `Run` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If an error occurs, the goroutine will annotate it with the service name and
    write it to a buffered error channel before invoking the cancel function for the
    wrapped context. As a result, if any service fails, all the other services will
    be automatically instructed to shut down.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `sync.WaitGroup` helps us keep track of the currently running goroutines.
    As we mentioned previously, we are working with long-running services whose `Run` method
    only returns if the context is canceled or an error occurs. In either case, the
    wrapped context will expire so that we can have our service runner wait for this
    event to occur and then call the wait group''s `Wait` method to ensure that all
    the spawned goroutines have terminated before proceeding. The following code demonstrates
    how this is achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Before returning, we must check for the presence of errors. To this end, we
    close the error channel so that we can iterate it using a `range` statement. Closing
    the channel is safe since all the goroutines that could potentially write to it
    have already terminated. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding snippet, after closing the channel, the code dequeues
    and aggregates any reported errors and returns them to the caller. Note that a
    nil error value will be returned if no error has occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main package serves as the entry point for our application. It exposes
    the configuration options for the various services as command-line flags and takes
    care of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating the appropriate data store implementations for the link graph
    (memory or CockroachDB) and text indexer (memory or Elasticsearch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiating the various application services with the correct configuration
    options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `runMain` method implements the main loop of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the first line instantiates all the required
    services and adds them to a `Group`. Then, a new cancelable context is created
    and is used to invoke the group's (blocking) `Run` method.
  prefs: []
  type: TYPE_NORMAL
- en: Terminating the application in a clean way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, you are probably wondering: how does the application terminate?
    The answer is by receiving a signal from the operating system. The `signal` package
    in the Go standard library comes with a `Notify` function that allows an application
    to register for, and receive, notifications when the application receives a particular
    signal type. Common signal types include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SIGINT`, which is normally sent to a foreground application when the user
    presses* Ctrl *+ *C*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SIGHUP`, which many applications (for example, HTTP servers) hook and use
    as a trigger to reload their configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SIGKILL`, which is sent to an application before the operating system kills
    it. This particular signal cannot be caught.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SIGQUIT`, which is sent to a foreground application when the user presses
    *Ctrl*+ *_*. The Go runtime hooks this signal so that it can print the stacks
    for every running goroutine before terminating the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since our application will be running as a Docker container, we are only interested
    in handling `SIGINT` (sent by Kubernetes when the pod is about to shut down) and `SIGHUP` (for
    debug purposes). Since the preceding code blocks on the group''s `Run` method,
    we need to use a goroutine to watch for incoming signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Upon receiving one of the specified signals, we immediately invoke the cancellation
    function for the context and return. This action will cause all the services in
    the group to cleanly shut down and for the `svcGroup.Run` call to return, thus
    allowing `runMain` to also return and for the application to terminate.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerizing and starting a single instance of the monolith
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Chapter10/linksrus` package comes with a Dockerfile that includes the necessary
    steps for building a dockerized version of the monolithic application that you
    can then run either locally or deploy to Kubernetes using the guide in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: To create a Docker image for testing purposes, you can simply type `make dockerize` into
    the package directory. Alternatively, if you wish to build and push the generated
    images to a Docker registry, you can type `make dockerize-and-push`. The Makefile
    target assumes that you are running Minikube and have enabled the private registry
    add-on according to the instructions from the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tags for all the Docker images that are created by this Makefile will include
    the private registry URL as a prefix. For example, if the IP currently in use
    by Minikube is `192.168.99.100`, the generated image will be tagged as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`192.168.99.100/linksrus-monolith:latest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`192.168.99.100/linksrus-monolith:$GIT_SHA`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use a different private registry (for example, `localhost:32000`,
    if you're using microk8s), you can run `make PRIVATE_REGISTRY=localhost:32000
    dockerize-and-push` instead.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you want to push the images to the **public** Docker registry,
    you can invoke the command with an **empty** `PRIVATE_REGISTRY` environment variable
    with `make PRIVATE_REGISTRY= dockerize-and-push`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier for those of you who don''t want to spin up a Kubernetes
    cluster to test-drive the monolithic application, the application default command-line
    values will start the application in standalone mode:'
  prefs: []
  type: TYPE_NORMAL
- en: An in-memory store will be used for both the link graph and the text indexer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new crawler pass will be triggered every 5 minutes and a PageRank recalculation
    will occur every hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontend is exposed on port `8080`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The receding default settings make it easy to start the application either locally
    by running a command such as `go run main.go` or inside a Docker container by
    running `docker run -it --rm -p 8080:8080 $(minikube ip):5000/linksrus-monolith:latest`.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and scaling the monolith on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last part of this chapter, we will deploy the Links 'R' Us monolithic
    application on Kubernetes and put the partitioning logic to the test by scaling
    our deployment horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates what our final setup will look like. As you
    can see, we will be using Kubernetes namespaces to logically split the various
    components for our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b93c4d8a-8468-4f80-aa46-e849cbb671cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6: Deploying a monolithic version of Links 'R' Us on Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding diagram, we can see that the `linksrus-data` namespace will
    host our data stores, which will be configured in highly available mode. The CockroachDB
    cluster consists of multiple nodes that are hidden behind a Kubernetes service
    resource called `cdb-cockroachdb-public`. Our application can access the DB cluster
    via the service's DNS entry, `cdb-cockroachdb-public.linksrus-data`. The Elasticsearch
    cluster follows exactly the same pattern; it also exposes a service that we can
    use to reach the master nodes by connecting to `elasticsearch-master.linksrus-data:9200`.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the `linksrus` namespace is where our application will be
    deployed as a StatefulSet consisting of four replicas. The choice of the number
    of replicas is arbitrary and can be easily adjusted upward or downward at any
    point in time by reconfiguring the StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to query the SRV records for all the pods in the StatefulSet, we
    will create a **headless** Kubernetes service. This service makes it possible
    for us to use the partition discovery code that we described in *The crawler service* section.
    Before we can expose our frontend to the outside world, we need to create yet
    another Kubernetes service that will act as a load balancer for distributing incoming
    traffic to the pods in our StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: The final ingredient in our deploy recipe is an Ingress resource, which will
    allow our end users to access the frontend service over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every Kubernetes manifest that we will be working with in the following sections
    is available in the `Chapter10/k8s` folder of this book''s GitHub repository.
    Inside the same folder, you can find a Makefile with the following handy targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap-minikube`: Bootstraps a Kubernetes cluster using Minikube and installs
    all the required add-ons for deploying Links ''R'' Us'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deploy`: Deploys all the components for the Links ''R'' Us project, including
    the data stores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`purge`: Removes all the components that have been installed via `make deploy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dockerize-and-push`: Builds and pushes **all** the required container images
    for the Links ''R'' Us project'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the required namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create the required namespaces for the deployment, you need to switch to
    the `Chapter10/k8s` folder and apply the `01-namespaces.yaml` manifest by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the manifest, the new namespaces should show up when you run `kubectl
    get namespaces`. The following screenshot shows a list of the Kubernetes cluster
    namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75c397ac-2ad3-4e25-b8f8-44f0f52f42da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Listing the Kubernetes cluster namespaces'
  prefs: []
  type: TYPE_NORMAL
- en: The following steps entail the deployment of our database services, followed
    by the deployment of the monolithic Links 'R' Us application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying CockroachDB and Elasticsearch using Helm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up the CockroachDB and Elasticsearch cluster is quite tedious and involves
    applying quite a few manifests. Instead of doing this manually, we will actually
    cheat and deploy both data stores using the `helm` tool!
  prefs: []
  type: TYPE_NORMAL
- en: 'For CockroachDB, we can run the following command to deploy a three-node cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `cdb-settings.yaml` file that's referenced by the preceding command contains
    overrides for the default chart values, which restrict the spawned database instance
    to using 512 M of RAM and 100 M of disk space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `helm` charts for Elasticsearch are currently maintained in an external
    repository that must be registered with `helm` before we can proceed with the
    installation. Similar to CockroachDB, a settings override file is also provided
    that restricts the Elasticsearch master nodes to using 512 M of RAM and 300 M
    of disk space. The following command will take care of the Elasticsearch deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After running all of the preceding commands, you should be able to type `kubectl
    -n linksrus-data get pods` and see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eea6e389-2e3e-4817-866a-52f5b4298485.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Listing the pods in the linksrus-data namespace'
  prefs: []
  type: TYPE_NORMAL
- en: Once all the data store pods show up as *running*, we can deploy Links 'R' Us!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Links 'R' Us
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can create the Links ''R'' Us StatefulSet, there is one more aspect
    that we need to take care of: the CockroachDB instance is not aware of the schema
    for the link graph. Nothing to worry about... We can remedy this issue by spawning
    a one-off container that will create the database for the link graph and apply
    the schema migrations from [Chapter 6](ce489d62-aaa3-4fbb-b239-c9de3daa9a8f.xhtml),
    *Building a Persistence Layer*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the source code and Dockerfile for this container in the `Chapter10/cdb-schema` folder.
    Assuming that you are currently using Minikube for your cluster, you can run the
    following command in the preceding folder to create the Docker image and push
    it to the private registry exposed by Minikube:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving back to the manifests inside the `Chapter10/k8s` folder, you can apply
    the `02-cdb-schema.yaml` manifest to create a one-off Kubernetes `Job` that waits
    for the DB cluster to become available, ensures that the link-graph database and
    schema are up to date, and then exits. Here''s what the content of this YAML file
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can deploy the remaining Links 'R' Us resources by applying the `03-linksrus-monolith.yaml` manifest.
    If you haven't done so already, make sure that you run `make dockerize-and-push` in
    the `Chapter10/linksrus` folder prior to applying the manifest to make sure that
    Kubernetes can find the referenced container images.
  prefs: []
  type: TYPE_NORMAL
- en: The Makefile in the `k8s` folder also defines a `dockerize-and-push` target
    that can build and push **all** the required container images for running the
    Links 'R' Us demo from this section with a single command.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few seconds, you can type `kubectl -n linksrus get pods,statefulsets,services,ingresses` to
    get a list of all the resources we just deployed. The following screenshot shows
    the expected output of this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28e1cb08-0975-483f-98a1-78e90f99850c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Listing all the resources in the linksrus namespace'
  prefs: []
  type: TYPE_NORMAL
- en: Success! Our monolithic application has been deployed and connected to the data
    stores in the `linksrus-data` namespace. You can access the frontend service by
    pointing your browser to the IP address of your ingress. In the preceding output,
    I was using Minikube inside a VM and therefore the displayed ingress address is
    not accessible from the host. However, you can easily find out the public IP that
    was used by Minikube by running `minikube ip` and pointing your browser to it.
  prefs: []
  type: TYPE_NORMAL
- en: You can tail the logs of each individual pod in the StatefulSet using the `kubectl
    -n linksrus logs linksrus-monolith-instance-X -f` command, where *X* is a pod
    number from the set.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you can also tail the logs from *all* the pods in the set using the `kubectl
    -n linksrus logs -lapp=linksrus-monolith-instance -f` command.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to dockerize our Go applications in a way that
    yields container images with the smallest possible size. Then, we talked about
    the design philosophy and general architecture behind Kubernetes and elaborated
    on the different types of resources that you can create and manage on a Kubernetes
    cluster. In the last part of this chapter, we pieced together the first fully
    functioning version of the Links 'R' Us project and deployed it as a single monolithic
    application on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss the challenges and potential caveats involved
    when switching to a microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name some benefits of containerization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a master and a worker node in a Kubernetes cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a regular service and a headless service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What kind of Kubernetes resource would you use to share your OAuth2 client ID
    and secret with your frontend?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the difference between a deployment and a StatefulSet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alpine Linux: A security-oriented, lightweight Linux distribution based on
    musl libc and busybox. [https://alpinelinux.org](https://alpinelinux.org).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calico: Secure networking for the cloud-native era. [https://www.projectcalico.org](https://www.projectcalico.org).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cilium: API-aware networking and security. [https://cilium.io](https://cilium.io).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Containerd: An industry-standard container runtime with an emphasis on simplicity,
    robustness, and portability. [https://containerd.io](https://containerd.io).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Docker: Enterprise container platform. [https://www.docker.com](https://www.docker.com).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Helm: The package manager for Kubernetes. [https://helm.sh](https://helm.sh).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K3S: Lightweight Kubernetes. [https://k3s.io/](https://k3s.io/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kubernetes: Production-grade container orchestration. [https://www.kubernetes.io](https://www.kubernetes.io).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Microk8s: Zero-ops Kubernetes for workstations and edge / IoT. [https://microk8s.io](https://microk8s.io).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Minikube: Local Kubernetes, focused on application development and education. [https://minikube.sigs.k8s.io](https://minikube.sigs.k8s.io).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multipass: Orchestrates virtual Ubuntu instances. [https://multipass.run/](https://multipass.run/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'rkt: A security-minded, standards-based container engine. [https://coreos.com/rkt](https://coreos.com/rkt).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VirtualBox: A powerful x86 and AMD64/Intel64 virtualization product for enterprise
    as well as home use*.* [https://www.virtualbox.org](https://www.virtualbox.org).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
