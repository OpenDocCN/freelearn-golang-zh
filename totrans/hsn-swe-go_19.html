<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assessments</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1</h1>
                </header>
            
            <article>
                
<ol>
<li>Software engineering is defined as the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software.</li>
<li>Some of the key questions that a software engineer must be able to answer are as follows:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>What are the business use cases that the software needs to support?</li>
<li>What components comprise the system and how do they interact with each other?</li>
<li>What technologies will be used to implement the various system components?</li>
<li>How will the software be tested to ensure that its behavior matches the customer's expectations?</li>
<li>How does load affect the system's performance and what is the plan for scaling the system?</li>
</ul>
</li>
</ul>
<ol start="3">
<li>An SRE spends approximately half of their time on operations-related tasks such as dealing with support tickets, being on call, automating processes to eliminate human errors, and so on.</li>
<li>The waterfall model does not provide a detailed view of the processes that comprise each model step. In addition, it does not seem to support cross-cutting processes such as project management or quality control that run in parallel with the waterfall steps. A significant caveat of the waterfall model is that it operates under the assumption that all customer requirements are known in advance. The iterative enhancement model attempts to rectify these issues by executing small incremental waterfall iterations that allow the development team to adapt to changes in requirements.</li>
<li>According to the lean development model, the most common sources of waste are as follows:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>The introduction of non-essential changes when development is underway</li>
<li>Overly complicated decision-making processes for signing off new features</li>
<li>Unneeded communication between the various project stakeholders and the development teams</li>
</ul>
</li>
</ul>
<ol start="6">
<li>The team decides to focus on speedy delivery at the expense of code quality. As a result, the code base becomes more complex and defects start accumulating. Now, the team must dedicate a part of their development time to fixing bugs instead of working on the requested features. Consequently, the implementation stage becomes a bottleneck that reduces the efficiency of the entire development process.</li>
<li>The key responsibility of the Product Owner is to manage the backlog for the project. On the other hand, the Scrum Master is responsible for organizing and running the various Scrum events.</li>
<li>The retrospective serves as a feedback loop for incrementally improving the team's performance across sprints. The team members should be discussing both the things that went well during the last sprint as well as the things that didn't. The outcome of the retrospective should be a list of corrective actions to address the problems that were encountered during the sprint.</li>
<li>Automation is important as it reduces the potential for human error. In addition, it reduces the time that's needed to test and deploy changes to production. Measuring is equally important as it allows DevOps engineers to monitor production services and receive alerts when their behavior diverges from the expected norm.</li>
<li>The company is expected to operate in a high-risk environment. For one, the new gaming console depends on a piece of technology that is not available yet and is being developed by a third party. What's more, the market is already saturated: other, much larger competitors could also be working on their own next-gen console systems. The expected competitive advantage for ACME Gaming Systems may be rendered obsolete by the time their new system is released. This is yet another source of risk. Given the high risk that's involved, the spiral model with its risk assessment and prototyping processes would be the most sensible choice for developing the software that will power the new console.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 2</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>The following is what the SOLID acronym initials stand for:
<ul>
<li>Single responsibility</li>
<li>Open/Closed</li>
<li>Liskov substitution</li>
<li>Interface segregation</li>
<li>Dependency inversion</li>
</ul>
</li>
<li>The code conflates two responsibilities: retrieving/mutating the state of a document and creating a signature for the document's content. Furthermore, the proposed implementation is inflexible as it forces the use of a specific signing algorithm. To address this problem, we can remove the<span> </span><kbd>Sign</kbd><span> </span>method from the<span> </span><kbd>Document</kbd><span> </span>type and provide an external helper that can sign not only instances of<span> </span><kbd>Document</kbd><span> </span>but also any type that can export its content as a string:</li>
</ol>
<div class="sourceCode">
<pre style="padding-left: 60px" class="sourceCode go"><a><span class="kw">type</span> ContentProvider <span class="kw">interface</span> {</a>
<a>    Content() <span class="dt">string</span></a>
<a>}</a>

<a><span class="kw">type</span> ECDADocumentSigner <span class="kw">struct</span> {<span class="co">//...}</span></a>

<a><span class="kw">func</span> (s ECDADocumentSigner) Sign(pk *ecdsa.PrivateKey, contentProvider ContentProvider) (<span class="dt">string</span>, <span class="dt">error</span>) { <span class="co">//... }</span></a></pre></div>
<ol start="3" type="1">
<li>The idea behind the interface segregation principle is to provide clients with the smallest possible interface that satisfies their needs and thus avoid depending on interfaces that will not actually be used. In the provided example, the write method receives an<span> </span><kbd>*os.File</kbd><span> </span>instance as an argument. However, as the function implementation probably only needs to be able to<span> </span><em>write</em><span> </span>data to the file, we could achieve the same result by passing an<span> </span><kbd>io.Writer</kbd><span> </span>in the place of the<span> </span><kbd>*os.File</kbd> instance. Apart from breaking the dependency to the<span> </span><kbd>*os.File</kbd><span> </span>concrete type, this change will also allow us to reuse the implementation for any type that implements<span> </span><kbd>io.Writer</kbd><span> </span>(for example, sockets, loggers, or others).</li>
<li>The use of<span> </span><kbd>util</kbd><span> </span>as a package name is not a recommended practice due to the following reasons:
<ul>
<li>It provides little context as to the package's purpose and contents.</li>
<li>It can end up as the home for miscellaneous, possibly unrelated types and/or methods that would undoubtedly violate the single responsibility principle.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<ol start="5" type="1">
<li>Import cycles cause the Go compiler to emit compile-time errors when you attempt to compile and/or run your code.</li>
<li>Some of the advantages of using zero values when defining new Go types are as follows:
<ul>
<li>An explicit constructor is not required as Go will automatically assign the zero value to the fields of an object when it is allocated.</li>
<li>The types can be embedded into other types and used out-of-the-box without any further initialization (for example, embedding a<span> </span><kbd>sync.Mutex</kbd><span> </span>into a struct).</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>The purpose of software versioning is twofold. First, it allows software engineers to validate whether an external dependency can be safely upgraded without the risk of introducing issues to production systems. Secondly, being able to explicitly reference required software dependencies via their versions is a prerequisite for implementing the concept of repeatable builds.</li>
<li>A semantic version is a string that satisfies the following format:<span> </span><kbd>MAJOR.MINOR.PATCH</kbd>:
<ul>
<li>The major component is incremented when a breaking change is introduced to the software</li>
<li>The minor component is incremented when new functionality is introduced to the software in a backward-compatible way</li>
<li>The patch version is incremented when a backward-compatible fix is applied to the code</li>
</ul>
</li>
<li>In the first case, we would increment the<span> </span><strong>minor</strong><span> </span>version as the new API does not break backward compatibility. In the second case, we would increment the<span> </span><strong>major</strong><span> </span>version as the new required parameter breaks compatibility with older versions of the API. Finally, in the third case, we would increment the<span> </span><strong>patch</strong><span> </span>version.</li>
<li>One approach would be to tag each build with a unique, monotonically increasing number. Alternatively, we could annotate build artifacts with a timestamp that indicates when they were created.</li>
<li>The pros of vendoring are as follows:
<ul>
<li><span>The capability to run reproducible builds for current or older versions of a piece of software</span></li>
<li><span>Being able to access the required dependencies locally, even if they disappear from the place where they were originally hosted</span></li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<p style="padding-left: 60px" class="mce-root">The cons of vendoring are as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li class="mce-root">Engineers should monitor the change logs for their dependencies and manually upgrade them when security fixes become available.</li>
<li class="mce-root">If the authors of the vendored dependencies do not follow semantic versioning for their packages, upgrading a dependency can introduce breaking changes that must be addressed before it's able to compile our code.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Some differences between the dep tool and Go modules are as follows:
<ul>
<li>Go modules fully integrate with the various commands, such as<span> </span><kbd>go get</kbd><span>,</span><span> </span><kbd>go build</kbd><span>, </span><span>and</span><span> </span><kbd>go test</kbd><span>.</span></li>
<li>While the dep tool selects the<span> </span><strong>highest</strong><span> </span><span>common version for a package, Go modules select the</span><span> </span><strong>minimum</strong><span> </span><span>viable version.</span></li>
<li>Go modules support multi-versioned dependencies.</li>
<li>Go modules do away with the<span> </span><em>vendor</em><span> </span><span>folder that's used by the dep tool.</span></li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>A stub satisfies a particular interface and returns<span> </span><strong>canned</strong><span> </span>answers for every invocation to the methods it implements. Mocks allow us to specify the following in a declarative way:
<ul>
<li>The order and parameters of the expected set of method invocations</li>
<li>The set of values to be returned for each combination of inputs</li>
</ul>
</li>
</ol>
<ol start="2" type="1">
<li>A fake object provides a fully working implementation whose behavior matches the objects that they are meant to substitute. For example, instead of having our tests communicate with a real <strong>key-value</strong> (<strong>KV</strong>) store, we might inject a fake object that provides a compatible, in-memory implementation of the KV store's API.</li>
<li>A table-driven test consists of three main components:
<ul>
<li>A type that encapsulates the parameters for running the test and its expected outcome. In Go programs, this is typically facilitated using an anonymous struct.</li>
<li>A slice of test cases to evaluate.</li>
<li>The test runner. Here, a for loop that iterates the list of test cases invokes the code under test with the correct set of parameters and verifies that the obtained results match the expectations for each test case.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<ol start="4" type="1">
<li>The purpose of unit testing is to ensure that a particular unit of code (a function, method, or package), when exercised in<span> </span><strong>isolation</strong>, behaves according to a set of specifications. To this end, a unit test will typically use a mechanism such as stubs, mocks, or fake objects to replace any external dependencies of the code under test. On the other hand, integration tests are designed to exercise multiple units together so as to verify that they interoperate correctly.</li>
<li>Integration tests are designed to exercise multiple units together so as to verify that they interoperate correctly. In a similar fashion to unit tests, integration tests will oftentimes use a mechanism such as stubs, mocks, or fake objects as a substitute for external components (for example, databases, web servers, and so on). On the other hand, functional tests do not use any sort of mocking mechanism as their primary purpose is to test the behavior of the<span> </span><strong>complete</strong><span> </span>system.</li>
<li>The ambassador pattern injects a proxy between an application and a service it depends on. The proxy is typically run as a sidecar process alongside the application and exposes APIs to do the following:
<ul>
<li>Divert outgoing service calls to a different version of the service</li>
<li>Mock responses to outgoing service calls</li>
<li>Inject faults to requests or responses for testing purposes</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>Functional requirements outline the list of core functionalities that a system will implement, as well as the set of interactions between the system and any external actors. On the other hand, non-functional requirements list the mechanisms and metrics that we can use to ascertain whether a proposed design is a good fit for solving a particular problem.</li>
<li>A user story is comprised of the following two key components:
<ul>
<li>A requirement specification must always be expressed from the viewpoint of the actor interacting with the system</li>
<li>A set of acceptance criteria (also known as<span> the </span><em>definition of done</em>) for evaluating whether the story goals have been successfully met</li>
</ul>
</li>
</ol>
<ol start="3" type="1">
<li>An attacker could submit a carefully crafted link with a link-local address that would trick the crawler into making a call to the metadata API offered by the cloud provider hosting our project and subsequently caching the response to the search index. Moreover, the attacker could submit a URL<span> </span><kbd>file</kbd><span> </span>as its protocol type and cause the crawler to read a local file from the machine and leak its contents to the search index.</li>
</ol>
<ol start="4" type="1">
<li>A <strong>service-level objective</strong> (<strong>SLO</strong>) consists of the following parts:
<ul>
<li>A description of the thing being measured</li>
<li>The expected service level, specified as a percentage</li>
<li>The time period where the measurement takes place</li>
</ul>
</li>
</ol>
<ol start="5" type="1">
<li>A UML component diagram provides a high-level view of the core components that comprise a system and visualizes their dependencies in terms of implemented and required interfaces.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>Relational databases are a better fit for transactional workloads and for performing complex queries. They can scale horizontally using mechanisms such as data sharding but at the cost of requiring additional coordination for executing queries. On the other hand, NoSQL databases are best suited for crunching massive volumes of<span> </span><strong>denormalized</strong><span> </span>data. By design, NoSQL databases can efficiently scale horizontally (even across data centers), with many NoSQL offerings promising a linear increase in query performance as more nodes are added to the cluster. The main caveat of NoSQL databases is that they can only satisfy two facets of the <strong>CAP</strong> (<strong>consistency</strong>, <strong>availability</strong>, and <strong>partition tolerance</strong>) theorem.</li>
</ol>
<p style="padding-left: 60px">A relational database would be a great fit for systems that perform a large volume of concurrent transactions, such as the ones you would expect to find in a bank. On the other hand, a system that needs to process a large number of events for analytics purposes would probably benefit more from a NoSQL solution.</p>
<ol start="2" type="1">
<li>To scale a DBMS for a read-heavy workload, we would deploy multiple read replicas and update our applications to send read-only queries to the replicas and write queries to the master node. For a write-heavy workload, we would deploy multiple master nodes and enable data sharding so that writes can be efficiently distributed across the master nodes.</li>
<li>According to the CAP theorem, distributed systems can only satisfy two of the following properties: consistency, availability, and partition tolerance. When deciding on which NoSQL solution to use, we must identify which two of the CAP terms are the most important for our particular application (CP, AP, or CA) and then limit our search to those NoSQL offerings that satisfy our selected CAP requirements.</li>
</ol>
<ol start="4" type="1">
<li>Having an abstraction layer allows us to decouple the business logic from the underlying database system. This makes it much easier to switch to a different DBMS in the future, without having to update our business logic. Furthermore, testing our business logic code also becomes easier and faster as we can use a mechanism such as stubs, mocks, or fake objects to avoid running our tests against an actual database instance.</li>
<li>First, you would need to add the new method to the<span> </span><kbd>Indexer</kbd><span> </span>interface. Then, following a test-driven approach, you would need to add a test for the expected behavior of the new method to the<span> </span><kbd>SuiteBase</kbd><span> </span>type in the<span> </span><kbd>indextest</kbd><span> </span>package. Finally, you would need to visit all the types that adhere to the<span> </span><kbd>Indexer</kbd><span> </span>interface (in this case, the bleve and Elasticsearch indexers) and add an implementation for the new method.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>The Go<span> </span><kbd>interface{}</kbd><span> </span>type conveys no useful information about the underlying type. If we use it for representing an argument to a function or a method, we effectively bypass the compiler's ability to statically check the function/method arguments at compile-time and instead have to manually check whether the input can be safely cast into a supported known type.</li>
<li>Instead of running the compute-intensive stages locally, we can migrate them to a remote machine with enough computing resources. The respective local stages can then be replaced with a proxy that transmits the local payload data to the remote machine via a <strong>remote procedure call</strong> (<strong>RPC</strong>), waits for the results, and pushes them to the next local stage. The following diagram outlines the proposed solution:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/87496116-364e-46b7-b79d-45119ae937cc.png" style="width:51.25em;height:15.92em;"/></p>
<p class="mce-root"/>
<ol start="3" type="1">
<li>Each processor function must satisfy the<span> </span><kbd>Processor</kbd><span> </span>interface, whose definition is as follows:</li>
</ol>
<div class="sourceCode">
<pre style="padding-left: 60px" class="sourceCode go"><a><span class="kw">type</span> Processor <span class="kw">interface</span> {</a>
<a>    Process(context.Context, Payload) (Payload, <span class="dt">error</span>)</a>
<a>}</a></pre></div>
<p style="padding-left: 60px">In addition, we also defined the<span> </span><kbd>ProcessorFunc</kbd><span> </span>type, which acts as an adaptor for converting a function with a compatible signature into a type that implements the<span> </span><kbd>Processor</kbd><span> </span>interface.</p>
<p style="padding-left: 60px">For this particular use case, we can define a function that receives a<span> </span><kbd>Processor</kbd><span> </span>and a logger (for example, from the <kbd>logrus</kbd> package) instance and returns a new<span> </span><kbd>Processor</kbd><span> </span>that decorates the call to the<span> </span><kbd>Process</kbd><span> </span>method with additional logic that emits a log entry if an error occurs. The <kbd>makeErrorLoggingProcessor</kbd> function shows one of the possible ways of implementing this pattern:</p>
<div class="sourceCode">
<pre style="padding-left: 60px" class="sourceCode go"><a><span class="kw">func</span> makeErrorLoggingProcessor(proc Processor, logger *logrus.Logger) Processor {</a>
<a>    <span class="kw">return</span> ProcessorFunc(<span class="kw">func</span>(ctx context.Context, p Payload) (Payload, <span class="dt">error</span>) {</a>
<a>        out, err := proc.Process(ctx, p)</a>
<a>        <span class="kw">if</span> err != <span class="ot">nil</span> {</a>
<a>            logger.Error(err)</a>
<a>        }</a>
<a>        <span class="kw">return</span> out, err</a>
<a>    })</a>
<a>}</a></pre></div>
<ol start="4" type="1">
<li>A synchronous pipeline processes one payload at a time in <strong>first-in, first-out</strong> (<strong>FIFO</strong>) order and waits for it to exit the pipeline before processing the next available payload. As a result, if a single payload takes a long time to be processed, it effectively delays processing the payloads that are queued behind it. In an asynchronous pipeline, each stage operates asynchronously and can immediately begin processing the next payload as soon as the current payload has been sent to the next stage.</li>
<li>A dead-letter queue is a mechanism for deferring error handling for pipeline payloads to a later time. When the pipeline encounters an error while processing a payload, it appends the payload to the dead-letter queue, along with the error that occurred. The application can then introspect the contents of the dead-letter queue and decide how it wants to handle each error according to its business logic (for example, retry the failed payload, log or ignore the error, and so on).</li>
</ol>
<ol start="6" type="1">
<li>A fixed-size worker pool contains a predetermined number of workers that are created at the same time as the pool and remain active (even when they are idle) until the pool is destroyed. A dynamic pool is configured with lower and upper worker limits and can automatically grow or shrink on demand to accommodate changes in the rate of incoming payloads.</li>
<li>To measure the total time that each payload spent in the pipeline, we will modify the<span> </span><kbd>pipeline.Payload</kbd><span> </span>struct and add a new<span> </span><em>private</em><span> </span>field of the <kbd>time.Time</kbd><span> type </span>called<span> </span><kbd>processStartedAt</kbd>. This new field will be used to record the timestamp when the payload entered the pipeline. Next, we will modify the<span> </span><kbd>linkSource</kbd><span> </span>implementation to populate<span> </span><kbd>processStartedAt</kbd><span> </span>when it emits a new<span> </span><kbd>Payload</kbd>. Finally, we will update the (currently empty)<span> </span><kbd>Consume</kbd><span> </span>method of <kbd>nopSink</kbd><span> </span>to calculate the elapsed time via a call to<span> </span><kbd>time.Since</kbd>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>The BSP computer is an abstract computer model made up of a collection of potentially heterogeneous processors that are interconnected via a computer network. Processors can not only access their own local memory, but they can also use the network link to exchange data with other processors. In other words, the BSP computer is effectively a<span> </span><strong>distributed memory</strong><span> </span>computer that can perform computations in parallel.</li>
<li>The <strong>Single Program Multiple Data</strong> (<strong>SPMD</strong>) technique models distributed data processing tasks as a self-contained piece of software that runs on a single-core machine. The program receives a set of data as input, applies a processing function to it, and emits some output. Parallelism is then achieved by splitting the dataset into batches, launching multiple instances of the<span> </span><strong>same</strong><span> </span>program to process each batch in parallel, and combining the results.</li>
<li>A super-step is broken down into two phases, or sub-steps:
<ul>
<li>A compute step, where each processor executes (in parallel) a single iteration of the user's program using the data that was assigned to the processor as input.</li>
<li>A communication step that runs after<span> </span><strong>all</strong><span> the </span>processors complete the compute step. During this step, processors communicate through the network and compare, exchange, or aggregate the results of their individual computations.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<ol start="4" type="1">
<li>The following block of code demonstrates how we can create an aggregator to keep track of the minimum <kbd>int64</kbd> value we've seen so far. The use of an<span> </span><kbd>int64</kbd><span> </span>pointer allows us to detect whether<span> </span><em>any</em><span> </span>value has been seen so far (otherwise, the pointer will be<span> </span><kbd>nil</kbd>) and if so, the minimum value that's been seen by the<span> </span><kbd>Aggregate</kbd><span> </span>method. Atomic access to the <kbd>int64</kbd> value is enforced via the use of<span> </span><kbd>sync.Mutex</kbd>:</li>
</ol>
<div class="sourceCode">
<pre style="padding-left: 60px" class="sourceCode go"><a><span class="kw">type</span> MinInt64Aggregator <span class="kw">struct</span> {</a>
<a>    mu       sync.Mutex</a>
<a>    minValue *<span class="dt">int64</span></a>
<a>}</a>
<a><span class="kw">func</span> (a *MinInt64Aggregator) Aggregate(v <span class="kw">interface</span>{}) {</a>
<a>    a.mu.Lock()</a>
<a>    <span class="kw">if</span> intV := v.(<span class="dt">int64</span>); a.minValue == <span class="ot">nil</span> || intV &lt; *a.minValue {</a>
<a>        a.minValue = &amp;intV</a>
<a>    }</a>
<a>    a.mu.Unlock()</a>
<a>}</a>
<a><span class="kw">func</span> (a *MinInt64Aggregator) Set(v <span class="kw">interface</span>{}) {</a>
<a>    a.mu.Lock()</a>
<a>    intV := v.(<span class="dt">int64</span>)</a>
<a>    a.minValue = &amp;intV</a>
<a>    a.mu.Unlock()</a>
<a>}</a>
<a><span class="kw">func</span> (a *MinInt64Aggregator) Get() <span class="kw">interface</span>{} {</a>
<a>    a.mu.Lock()</a>
<a>    <span class="kw">defer</span> a.mu.Unlock()</a>
<a>    <span class="kw">if</span> a.minValue == <span class="ot">nil</span> {</a>
<a>        <span class="kw">return</span> <span class="ot">nil</span></a>
<a>    }</a>
<a>    <span class="kw">return</span> *a.minValue</a>
<a>}</a>
<a><span class="kw">func</span> (a *MinInt64Aggregator) Delta() <span class="kw">interface</span>{} { <span class="kw">return</span> a.Get() }</a>
<a><span class="kw">func</span> (a *MinInt64Aggregator) Type() <span class="dt">string</span> { <span class="kw">return</span> <span class="st">"MinInt64Aggregator"</span> }</a></pre></div>
<ol start="5" type="1">
<li>Under the random surfer model, a user performs an initial search and lands on a page from the link graph. From that point on, users randomly select one of the following two options:<br/>
<ul>
<li>They can click any outgoing link from the current page and navigate to a new page</li>
<li>Alternatively, they can decide to run a new search query</li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">The preceding steps continue in perpetuity.</p>
<p class="mce-root"/>
<ol start="6" type="1">
<li>A PageRank score reflects the probability that a random surfer lands on a particular web page. In other words, the score expresses the importance (ranking) of each web page relative to every other web page on the internet.</li>
<li>At each step of the PageRank algorithm, each link distributes its accumulated PageRank score to its outgoing links. Dead-ends receive the PageRank scores from pages that are linked to them but never redistribute them as they have no outgoing links. If we don't take steps to handle these problematic cases, the graph dead-ends will end up with a significantly higher (and incorrect) PageRank score compared to regular pages in the graph.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 9</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>The following table summarizes the CRUD endpoints for a user entity:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>HTTP Verb</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Path</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Expects (JSON)</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Returns (JSON)</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>HTTP Status</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Description</strong></td>
</tr>
<tr class="odd">
<td>POST</td>
<td><kbd>/users</kbd></td>
<td>A user entry</td>
<td>The new user entry and its ID</td>
<td>200 (success) or 201 (created)</td>
<td>Create a new user</td>
</tr>
<tr class="even">
<td>GET</td>
<td><kbd>/users</kbd></td>
<td>Nothing</td>
<td>An array with user entries</td>
<td>200 (success)</td>
<td>Get a list of users</td>
</tr>
<tr class="odd">
<td>GET</td>
<td><kbd>/users/:id</kbd></td>
<td>Nothing</td>
<td>The user with the specified ID</td>
<td>200 (success) or 404 (not found)</td>
<td>Get user by ID</td>
</tr>
<tr class="even">
<td>PUT</td>
<td><kbd>/users/:id</kbd></td>
<td>A user entry</td>
<td>The updated user entry</td>
<td>200 (success) or 404 (not found)</td>
<td>Update user by ID</td>
</tr>
<tr class="odd">
<td>PATCH</td>
<td><kbd>/users/:id</kbd></td>
<td>A<span> </span><em>partial</em><span> </span>user entry</td>
<td>The updated user entry</td>
<td>200 (success) or 404 (not found)</td>
<td>Update individual fields for a user by ID</td>
</tr>
<tr class="even">
<td>DELETE</td>
<td><kbd>/users/:id</kbd></td>
<td>Nothing</td>
<td>Nothing</td>
<td>200 (success) or 404 (not found)</td>
<td>Delete user by ID</td>
</tr>
</tbody>
</table>
<ol start="2" type="1">
<li>Basic authentication headers are transmitted as plaintext. By ensuring this information is transmitted over a TLS-encrypted channel, we prevent malicious actors from intercepting user credentials.</li>
<li>If a malicious adversary manages to install their own <strong>certificate authority</strong> (<strong>CA</strong>) on their targets' trusted certificate stores, they can mount a <strong>man-in-the-middle</strong> (<strong>MitM</strong>) attack and snoop on the TLS traffic between the target and any third party.</li>
</ol>
<ol start="4" type="1">
<li>In a three-legged OAuth2 flow, the following occurs:
<ol>
<li>A user visits service A and attempts to log in via service B.</li>
<li>The backend server for A generates an authorization URL for service B and redirects the user's browser to it. The generated URL includes the set of permissions that were requested by A and a URL that B should redirect the user to once they consent to granting access.</li>
<li>The user is redirected to service B and consents to the permissions that were requested by service A.</li>
<li>The user's browser is redirected to service A with an access code embedded in the URL.</li>
<li>The backend server for service A contacts service B and exchanges the access code with an access token.</li>
<li>Service A uses the token to access some resource (for example, user details) on service B on behalf of the user.</li>
</ol>
</li>
</ol>
<ol start="5" type="1">
<li>Protocol buffers are superior to JSON for request/response payloads for the following reasons:<br/>
<ul>
<li>They utilize a much more compact binary format to serialize payloads</li>
<li>Protocol buffer messages are strictly typed and support versioning</li>
<li>The protoc compiler can be used to generate the required code for working with protocol buffer messages in a variety of programming languages</li>
</ul>
</li>
</ol>
<ol start="6" type="1">
<li>gRPC supports the following RPC modes:<br/>
<ul>
<li><strong>Unary RPC</strong>: The client performs a request and receives a response.</li>
<li><strong>Server-streaming RPC</strong>: The client initiates an RPC connection to the server and receives a stream of responses from the server.</li>
<li><strong>Client-streaming RPC</strong>: The client initiates an RPC connection to the server and sends a stream of requests via the open connection. The server processes the requests and sends a single response.</li>
<li><strong>Bidirectional streaming RPC</strong>: The client and the server share a bidirectional channel where each side can asynchronously send and receive messages.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 10</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>Some of the benefits of containerization are as follows:<br/>
<ul>
<li>The same container image can run on a local development machine or a cloud instance</li>
<li>It is trivial to deploy a new version of a piece of software and perform a rollback if something goes wrong</li>
<li>It introduces an extra layer of security as applications are isolated from both the host and other applications</li>
</ul>
</li>
</ol>
<ol start="2" type="1">
<li>Master nodes implement the<span> </span><em>control plane</em><span> </span>of a Kubernetes cluster. Worker nodes pool their resources (CPUs, memory, disks, GPUs, and so on) and execute the workloads that have been assigned to them by the master nodes.</li>
<li>A regular Kubernetes service acts as a load balancer for distributing incoming traffic to a collection of pods. Regular services are reachable via the cluster IP address that's assigned to them by Kubernetes. A headless service provides the means for implementing a custom service discovery mechanism. It is not assigned a cluster IP address and DNS queries for it are returned the full list of pods behind the service.</li>
<li>Since both the OAuth2 client ID and secret are sensitive pieces of information, the recommended Kubernetes approach for sharing them with the frontend would be to create a secret resource.</li>
<li>A Kubernetes deployment creates a pod with non-predictable IDs, whereas a stateful set assigns predictable names that are constructed by concatenating the stateful set name and the pod ordinal (for example, web-0, web-1, and so on). Another difference is that while Kubernetes deployments spin up the required number of pods in parallel, a stateful set spins up pods sequentially.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 11</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>A microservice-based architecture brings a lot of benefits to the table. However, at the same time, it adds a lot of complexity to a system and requires additional effort to make it resilient against network issues, to monitor its internal state, and to debug issues when something goes wrong. Consequently, selecting this pattern for an MVP or PoC is often considered to be a form of premature optimization that likely introduces more issues than it solves.</li>
</ol>
<ol start="2" type="1">
<li>When the number of errors from a particular downstream service exceeds a particular threshold, the circuit breaker is tripped and all future requests automatically fail with an error. Periodically, the circuit breaker lets some requests go through and after a number of successful responses, the circuit breaker switches back to the open position, thereby allowing all the requests to go through.</li>
<li>Being able to trace requests as they travel through a system allows us to do the following:
<ul>
<li>Figure out how much time the request spends in each service and identify potential bottlenecks</li>
<li>Understand and map the dependencies between services</li>
<li>Pinpoint the root cause of issues that affect production systems</li>
</ul>
</li>
</ol>
<ol start="4" type="1">
<li>Log entries may contain sensitive information such as credit card numbers, security credentials, customer names, addresses, or social security numbers. Unless we actively sanitize these entries, this information will end up in the logs and could be potentially visible to entities (employees or third parties) that are not authorized to access this kind of information.</li>
<li>To collect logs from the pods running in a Kubernetes cluster, we can use one of the following strategies:
<ul>
<li>Use a daemon set to run a log collector on each Kubernetes node. The log collector digests the log files from each pod running on the node and ships them to a centralized log storage location.</li>
<li>Deploy a sidecar container in the same pod as the application whose logs we want to collect. The sidecar digests the application logs (which could be a single file or multiple files) and ships them to a centralized log storage location.</li>
<li>Ship logs directly from within the application.</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 12</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>In a leader-follower configuration, the nodes hold an election and elect a leader for the cluster. All reads and writes go through the cluster leader, while the other nodes monitor the leader and automatically hold a new election if the leader becomes unavailable. As the name implies, in a multi-master configuration, the cluster has several master nodes and each of the master nodes can serve both read and write requests. The master nodes implement some form of distributed consensus algorithm (Raft, Paxos, and so on) to ensure that they always share the same view of the cluster's state.</li>
</ol>
<ol start="2" type="1">
<li>When implementing the checkpoint strategy, workers are periodically asked by the master to persist their current state to durable storage. If this operation succeeds, a new checkpoint is created. If a worker crashes or becomes unavailable, the master will request for the remaining healthy workers to load their state from the last known checkpoint and resume executing the computation job from that point on.</li>
<li>The distributed barrier is a synchronization primitive that notifies the master node when all the workers have reached the same exact point of execution. This primitive is a prerequisite for executing compute jobs under the BSP model (see <a href="c505ec2d-0bd8-4edd-97e1-d06de2b326a5.xhtml">Chapter 8</a>, <em>Graph-Based Data Processing</em>), which requires that all the processors execute each super-step in the lockstep.</li>
<li>While the algorithm itself has completed without any errors, something might go wrong if one or more workers attempt to persist their results to durable storage. Consequently, a computation job can't really be considered as completed until<span> </span><strong>all</strong><span> the </span>workers have persisted the results of the computation.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 13</h1>
                </header>
            
            <article>
                
<ol type="1">
<li>A <strong>service-level indicator</strong> (<strong>SLI</strong>) is a type of metric that allows us to quantify the perceived quality of the service from the perspective of the end user (for example, metrics such as availability, throughput, and latency). A <strong>service-level objective</strong> (<strong>SLO</strong>) is the range of values for some SLIs that allow us to deliver a particular level of service to an end user or customer.</li>
<li>A <strong>service-level agreement</strong> (<strong>SLA</strong>) is an implicit or explicit contract between a service provider and one or more service consumers. The SLA outlines a set of SLOs that have to be met and the consequences (financial or not) for meeting and failing to meet them.</li>
<li>In a push-based metrics collection system, the metric-producing clients connect to the metrics collection and aggregation service over a TCP or UDP connection and publish their metrics. In a pull-based system, the metrics collection system, at its own leisure, connects to each client and collects (scrapes) any new metrics.</li>
<li>Due to the network security policies in place, the metrics collection service would not be able to establish a connection to any of the metrics producers in the locked-down subnet. However, the applications running on that subnet should still be able to access other subnets, including the one that the metrics collection service runs on. Consequently, the logical choice in such a situation is to use a push-based system.</li>
</ol>
<ol start="5" type="1">
<li>The value of a Prometheus counter can only increase, while the value of a Prometheus gauge can both increase and decrease.</li>
<li>A playbook is a short document that distills the best practices for resolving a particular type of problem. Having access to the playbook associated with a particular alert reduces the <strong>mean time to resolution</strong> (<strong>MTTR</strong>) as SREs can follow the playbook instructions to quickly diagnose the root cause of the problem and apply the recommended set of steps to fix it.</li>
</ol>


            </article>

            
        </section>
    </body></html>