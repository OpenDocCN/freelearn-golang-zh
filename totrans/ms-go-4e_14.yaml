- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Efficiency and Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every story has a villain. For developers, that villain is usually time. They
    must write code in a given time frame and ideally, the code must run as fast as
    possible. Most of the errors and bugs are a result of fighting with time constraints,
    both realistic and imaginary ones! So, this chapter is here to help you fight
    with the second aspect of time: efficiency and performance. For the first aspect
    of time, you need a good manager with technical skills.'
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the chapter is about benchmarking Go code using benchmark
    functions that measure the performance of a function or an entire program. Thinking
    that the implementation of a function is faster than a different implementation
    is not enough. We need to be able to prove it.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we talk about how Go manages memory and how careless Go code can
    introduce memory leaks. A memory leak in Go occurs when memory that is no longer
    needed is not properly released, causing the program’s memory usage to grow over
    time. Understanding the memory model is crucial for writing efficient, correct,
    and concurrent Go programs. In practice, when our code uses large amounts of memory,
    which is not usually the case, we need to take extra care with our code for better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we show how to use eBPF with Go. eBPF, which stands for Extended Berkeley
    Packet Filter, is a technology that enables programmability in the **Linux kernel**.
    It originated as an extension of the traditional **Berkeley Packet Filter** (**BPF**),
    which was designed for packet filtering within the kernel. eBPF, however, is a
    more general-purpose and flexible framework that allows the execution of user-supplied
    programs within the kernel space without requiring changes to the kernel itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buffered versus unbuffered file I/O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrongly defined benchmark functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory leaks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with eBPF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The section that follows is about benchmarking Go code, which helps you determine
    what is faster and what is slower in your code—this makes it a perfect place to
    begin your search for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarking measures the performance of a function or a program, allowing you
    to compare different implementations and understand the performance impact of
    code changes. Using that information, you can easily reveal the part of the code
    that needs to be rewritten to improve its performance. It goes without saying
    that you should not benchmark Go code on a busy machine that is currently being
    used for other, more important, purposes unless you have a very good reason to
    do so! Otherwise, you might interfere with the benchmarking process and get inaccurate
    results, but most importantly, you might generate performance issues on the machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, the load of the operating system plays a key role in the
    performance of your code. Let me tell you a story here: a Java utility I developed
    for a project performs lots of computations and finishes in 6,242 seconds (roughly
    1.7 hours) when running on its own. It took about a day for four instances of
    the same Java command line utility to run on the same Linux machine! If you think
    about it, running them one after the other would have been faster than running
    them at the same time!'
  prefs: []
  type: TYPE_NORMAL
- en: Go follows certain conventions regarding benchmarking (and testing). The most
    important convention is that the name of a benchmark function must begin with
    `Benchmark`. After the `Benchmark` word, we can put an underscore or an uppercase
    letter. Therefore, both `BenchmarkFunctionName()` and `Benchmark_functionName()`
    are valid benchmark functions, whereas `Benchmarkfunctionname()` is not. By convention
    such functions are put in files that end with `_test.go`. Once the benchmarking
    is correct, the `go test` subcommand does all the dirty work for you, which includes
    scanning all `*_test.go` files for special functions, generating a proper temporary
    `main` package, calling these special functions, getting the results, and generating
    the final output.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark functions use `testing.B` variables, whereas testing functions use
    `testing.T` variables. It is easy to remember.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from Go 1.17, we can shuffle the execution order of both tests and
    benchmarks with the help of the `shuffle` parameter (`go test -shuffle=on`). The
    `shuffle` parameter accepts a value (which is the seed for the random number generator)
    and can be useful when you want to replay an execution order. Its default value
    is off. **The logic behind this capability is that, sometimes, the order in which
    tests and benchmarks are executed affects their results**.
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection presents a simple benchmarking scenario where we try to
    optimize slice initializations.
  prefs: []
  type: TYPE_NORMAL
- en: A simple benchmark scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by presenting a scenario that tests the performance of two functions
    that do the same thing but with a different implementation. We want to be able
    to initialize slices with consecutive values that start from 0 and go up to a
    predefined value. So, given a slice named `mySlice`, `mySlice[0]` is going to
    have the value of `0`, `mySlice[1]` is going to have the value of `1`, and so
    on. The relevant code can be found inside `ch14/slices`, which contains two files
    named `initialize.go` and `initialize_test.go`. The Go code of `initialize.go`
    is presented in two parts. The first part is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we see the implementation of the desired functionality
    that uses `make()` to pre-allocate the desired memory space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part of `initialize.go` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In `InitSliceAppend()`, we see a different implementation of the desired functionality
    that starts with an empty slice and uses multiple `append()` calls to populate
    it. The purpose of the `main()` function is to naively test the functionality
    of `InitSliceNew()` and `InitSliceAppend()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the benchmarking functions is found in `initialize_test.go`
    and is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have two benchmarking functions that benchmark `InitSliceNew()` and
    `InitSliceAppend()`. The global parameter `t` is used to prevent Go from optimizing
    the `for` loops by preventing the return values of `InitSliceNew()` and `InitSliceAppend()`
    from being ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the benchmarking process takes place inside the `for` loop.
    This means that, when needed, we can declare new variables, open network connections,
    and so on, outside that `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now some important information regarding benchmarking: **each benchmark
    function is executed for at least one second by default**—this duration also includes
    the execution time of the functions that are called by a benchmark function. If
    the benchmark function returns in a time that is less than one second, the value
    of `b.N` is increased, and the function runs again as many times in total as the
    value of `b.N`. The first time the value of `b.N` is 1, then it becomes 2, then
    5, then 10, then 20, then 50, and so on. This happens because the faster the function,
    the more times Go needs to run it to get accurate results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking the code on a macOS M1 Max laptop produces the following kind
    of output—your output might vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are two important points here. First, the value of the `-bench` parameter
    specifies the benchmark functions that are going to be executed. The `.` value
    used is a regular expression that matches all valid benchmark functions. The second
    point is that if you omit the `-bench` parameter, no benchmark function is going
    to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: The generated output shows that `InitSliceNew()` is faster than `InitSliceAppend()`
    because `InitSliceNew()` was executed `255704` times, each time taking `79712
    ns`, whereas `InitSliceAppend()` was executed `86847` times, each time taking
    `143459 ns`. This makes perfect sense as `InitSliceAppend()` needs to allocate
    memory all the time—this means that both the length and the capacity of the slice
    change, whereas `InitSliceNew()` allocates the prerequisite memory once and for
    all.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how `append()` works is going to help you make sense of the results.
    If the underlying array has sufficient capacity, then the length of the resulting
    slice is increased by the number of elements appended and its capacity remains
    the same. This means that there are no new memory allocations. However, if the
    underlying array does not have sufficient capacity, a new array is created, which
    means that a new memory space is allocated, with a larger capacity. After that,
    the slice is updated to reference the new array, and its length and capacity are
    adjusted accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The subsection that follows shows a benchmarking technique that allows us to
    reduce the number of allocations.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking the number of memory allocations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this second benchmarking scenario, we are going to deal with a performance
    issue that has to do with the number of memory allocations that take place during
    the operation of a function. We present two versions of the same program to illustrate
    the differences between the slow version and the improved ones. All the relevant
    code can be found in two directories inside `ch14/alloc` named `base` and `improved`.
  prefs: []
  type: TYPE_NORMAL
- en: Initial version
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The purpose of the function that is being benchmarked is to write a message
    to a buffer. This version contains no optimizations. The code of this first version
    is found in `ch14/alloc/base`, which contains two Go source code files. The first
    one is named `allocate.go` and comes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `writeMessage()` function just writes the given message in a new buffer
    (`bytes.Buffer`). As we are only concerned about its performance, we do not deal
    with error handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second file, which is called `allocate_test.go`, contains benchmarks and
    comes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Benchmarking the code using the `-benchmem` command line flag, which also displays
    memory allocations, produces the following kind of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Each execution of the benchmarking function requires 50 memory allocations.
    This means that there is room for improvement regarding the number of memory allocations
    that are taking place. We are going to try to reduce them in the subsection that
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the number of memory allocations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we present three different functions that all implement
    the writing of a message to a buffer. However, this time, the buffer is given
    as a function parameter instead of being initialized internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code of the improved version is found in `ch14/alloc/improved` and contains
    two Go source code files. The first one is named `improve.go` and contains the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What we have here are three functions that all implement the writing of a message
    to a buffer. However, `writeMessageBuffer()` passes the buffer by value, whereas
    `writeMessageBufferPointer()` passes a pointer to the buffer variable. Lastly,
    `writeMessageBufferWriter()` uses an `io.Writer` interface variable, which also
    supports `bytes.Buffer` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second file is named `improve_test.go` and is going to be presented in
    three parts. The first part comes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the benchmarking function for `writeMessageBuffer()`. The buffer is
    allocated only once and is being used in all benchmarks by passing it to the relevant
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is the benchmarking function for `writeMessageBufferPointer()`. Once again,
    the used buffer is allocated only once and is being shared by all benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last part of `improve_test.go` contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see the use of `buffer.Reset()` in the two benchmarking functions.
    The purpose of the `buffer.Reset()` function is to reset the buffer so it has
    no content. The `buffer.Reset()` function has the same results as `buffer.Truncate(0)`.
    `Truncate(n)` discards all but the first `n` unread bytes from the buffer. We
    use `buffer.Reset()` thinking it might improve performance. However, this remains
    to be seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarking the improved version produces the following kind of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As presented by the results of the `BenchmarkWBuf()` benchmarking function,
    the use of a buffer as a parameter to the function does not automatically speed
    up the process, even if we share the same buffer during benchmarking. However,
    this is not the case with the remaining benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: The use of a pointer to the buffer saves us from having to copy the buffer before
    passing it to the function—this explains the results of the `BenchmarkWBufPointerNoReset()`
    function where we have no additional memory allocations. However, we still need
    to use 2,120 bytes per operation.
  prefs: []
  type: TYPE_NORMAL
- en: The output with the `-benchmem` command line parameter includes two additional
    columns. The fourth column shows the amount of memory that was allocated on average
    in each execution of the benchmark function. The fifth column shows the number
    of allocations used to allocate the memory value of the fourth column.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it turns out that resetting the buffer using `buffer.Reset()` after
    calling `writeMessageBufferPointer()` and `writeMessageBufferWriter()` speeds
    up the process. A possible explanation for that is that an empty buffer is easier
    to work with. So, when `buffer.Reset()` is used, we have both 0 memory allocations
    and 0 bytes per operation. As a result, `BenchmarkWBufPointerReset()` and `BenchmarkWBufWriterReset()`
    need 150.7 and 151.8 nanoseconds per operation, respectively, which is a huge
    speed up from the 1,056 and 337.1 nanoseconds per operation required by `BenchmarkWBuf()`
    and `BenchmarkWBufPointerNoReset()`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `buffer.Reset()` can be more efficient for one or a combination of the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reuse of allocated memory**: When you call `buffer.Reset()`, the underlying
    byte slice used by the `bytes.Buffer` is not deallocated. Instead, it is reused.
    The length of the buffer is set to zero, making the existing memory available
    for new data to be written.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced allocation overhead**: Creating a new buffer involves allocating
    a new underlying byte slice. This allocation comes with overhead, including managing
    the memory, updating the memory allocator’s data structures, and possibly invoking
    the garbage collector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding garbage collection**: Creating and discarding many small buffers
    can lead to increased pressure on the Garbage Collector, especially in scenarios
    with high-frequency buffer creation. By reusing the buffer with `Reset()`, you
    reduce the number of short-lived objects, potentially reducing the impact on garbage
    collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subject of the next section is benchmarking buffered writing.
  prefs: []
  type: TYPE_NORMAL
- en: Buffered versus unbuffered file I/O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to compare buffered and unbuffered operations
    while reading and writing files.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to test whether the size of the buffer plays a
    key role in the performance of write operations. The relevant code can be found
    in `ch14/io`. Apart from the relevant files, the directory includes a `testdata`
    directory, which was first seen in *Chapter 13*, *Fuzz Testing and Observability*,
    and is used for storing data related to the testing process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code of `table.go` is not presented here—feel free to look at it. The code
    of `table_test.go` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The reason for storing the return value of `Create()` in a variable named `err`
    and using another global variable named `ERR` afterward is tricky. We want to
    prevent the compiler from doing any optimizations that might exclude the function
    that we want to measure from being executed because its results are never used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Neither the signature nor the name of `benchmarkCreate()` makes it a benchmark
    function. This is a helper function that allows you to call `Create()`, which
    creates a new file on disk; its implementation can be found in `table.go`, with
    the proper parameters. Its implementation is valid, and it can be used by benchmark
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These are three correctly defined benchmark functions that all call `benchmarkCreate()`.
    Benchmark functions require a single `*testing.B` variable and return no values.
    In this case, the numbers at the end of the function name indicate the size of
    the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is the code that defines the array structures that are going to be used
    in the table tests. This saves us from having to implement (3x3=) 9 separate benchmark
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `b.Run()` method, which allows you to run one or more sub-benchmarks within
    a benchmark function, accepts two parameters. First, the name of the sub-benchmark,
    which is displayed on the screen, and second, the function that implements the
    sub-benchmark. This is an efficient way to run multiple benchmarks with the use
    of table tests and know their parameters. Just remember to define a proper name
    for each sub-benchmark because this is going to be displayed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the benchmarks generates the next output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The previous three lines are the results of the `BenchmarkBuffer4Create()`,
    `BenchmarkBuffer8Create()`, and `BenchmarkBuffer16Create()` benchmark functions,
    respectively, and indicate their performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The previous results are from the table tests with the 9 sub-benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So, what does this output tell us? First, the `-10` at the end of each benchmark
    function signifies the number of goroutines used for its execution, which is essentially
    the value of the `GOMAXPROCS` environment variable. Similarly, you can see the
    values of `GOOS` and `GOARCH`, which show the operating system and the architecture
    of the machine in the generated output. The second column in the output displays
    the number of times that the relevant function was executed. Faster functions
    are executed more times than slower functions. As an example, `BenchmarkBuffer4Create()`
    was executed `382740` times, while `BenchmarkBuffer16Create()` was executed `491230`
    times because it is faster! The third column in the output shows the average time
    of each run and is measured in nanoseconds per benchmark function execution (`ns/op`).
    The higher the value of the third column, the slower the benchmark function is.
    **A large value in the third column is an indication that a function might need
    to be optimized**.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to create benchmark functions to test the performance
    of our own functions to better understand potential bottlenecks that might need
    to be optimized. You might ask, how often do we need to create benchmark functions?
    The answer is simple. When something runs slower than needed and/or when you want
    to choose between two or more implementations.
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection shows how to compare benchmark results.
  prefs: []
  type: TYPE_NORMAL
- en: The benchstat utility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now imagine that you have benchmarking data, and you want to compare it with
    the results that were produced in another computer or with a different configuration.
    The `benchstat` utility can help you here. The utility can be found in the `https://pkg.go.dev/golang.org/x/perf/cmd/benchstat`
    package and can be downloaded using `go install golang.org/x/perf/cmd/benchstat@latest`.
    Go puts all binary files in `~/go/bin`, and `benchstat` is no exception to that
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: The `benchstat` utility replaces the `benchcmp` utility, which can be found
    at [https://pkg.go.dev/golang.org/x/tools/cmd/benchcmp](https://pkg.go.dev/golang.org/x/tools/cmd/benchcmp).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, imagine that we have two benchmark results for `table_test.go` saved in
    `r1.txt` and `r2.txt`—you should remove all lines from the `go test` output that
    do not contain benchmarking results, which leaves all lines that begin with `Benchmark`.
    You can use `benchstat` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can save the results from benchmarks by simply redirecting the generated
    output into a file. For example, you can run `go test -bench=. > output.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: If the value of the last column is `~`, as it happens to be here, it means that
    there was no significant change in the results. The previous output shows no differences
    between the two results. Discussing more about `benchstat` is beyond the scope
    of the book. Type `benchstat -h` to learn more about the supported parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The next section touches on a sensitive subject, which is incorrectly defined
    benchmark functions.
  prefs: []
  type: TYPE_NORMAL
- en: Wrongly defined benchmark functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should be very careful when defining benchmark functions because you might
    define them incorrectly. Look at the Go code of the following benchmark function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BenchmarkFibo()` function has a valid name and the correct signature.
    The bad news is that this benchmark function is logically wrong and is not going
    to produce any results. The reason for this is that as the `b.N` value grows in
    the way described earlier, the runtime of the benchmark function also increases
    because of the `for` loop. This fact prevents `BenchmarkFiboI()` from converging
    to a stable number, which prevents the function from completing and, therefore,
    returning any results. For analogous reasons, the next benchmark function is also
    wrongly implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, there is nothing wrong with the implementation of the following
    two benchmark functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Correct benchmark functions are a tool for identifying bottlenecks in your code
    that you should put in your own projects, especially when working with file I/O
    or CPU-intensive operations—as I am writing this, I have been waiting three days
    for a Python program to finish its operation to test the performance of the brute-force
    method of a mathematical algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Enough with benchmarking. The next section discusses the Go way of working with
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Go memory management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The subject of this section is Go memory management. We are going to begin
    by stating a fact that you should already be familiar with: Go sacrifices visibility
    and total control over memory management for the sake of simplicity and the use
    of the **Garbage Collector** (**GC**). Although the GC operation introduces an
    overhead to the speed of a program, it saves us from having to manually deal with
    memory, which is a huge advantage and saves us from lots of bugs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There exist two types of allocations that take place during program execution:
    *dynamic allocations* and *automatic allocations*. Automatic allocations are the
    allocations whose lifespan can be inferred by the compiler before the program
    starts its execution. For example, all local variables, the return arguments of
    functions, and function arguments have a given lifespan, which means that they
    can be automatically allocated by the compiler. All other allocations are performed
    dynamically, which also includes data that must be available outside the scope
    of a function.'
  prefs: []
  type: TYPE_NORMAL
- en: We continue our discussion on Go memory management by talking about the heap
    and the stack because this is where most of the allocations take place.
  prefs: []
  type: TYPE_NORMAL
- en: Heap and stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *heap* is the place where programming languages store global variables—the
    heap is where garbage collection takes place. The *stack* is the place where programming
    languages store temporary variables used by functions—each function has its own
    stack. As goroutines are located in user space, the Go runtime is responsible
    for the rules that govern their operation. Additionally, **each goroutine has
    its own stack,** whereas the heap is “shared” among goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic allocations take place in the heap, whereas automatic allocations are
    stored in the stack. The Go compiler performs a process that is called *escape
    analysis* to find out whether a memory needs to be allocated at the heap or should
    stay within the stack.
  prefs: []
  type: TYPE_NORMAL
- en: In C++, when you create new variables using the `new` operator, you know that
    these variables are going to the heap. This is not the case with Go and the use
    of `new()` and `make()` functions. In Go, the compiler decides where a new variable
    is going to be placed based on its size and the result of escape analysis. This
    is the reason that we can return pointers to local variables from Go functions.
    Although we have not seen `new()` in this book frequently, keep in mind that `new()`
    returns pointers to initialized memory.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know where the variables of a program are allocated by Go, you
    can use the `-m` `gc` flag with `go run`. This is illustrated in `allocate.go`—this
    is a regular program that needs no modifications in order to display the extra
    output as all details are handled by Go.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Running `allocate.go` generates the next output—the output is a result of the
    use of `-gcflags '-m'`, which modifies the generated executable. You should not
    create executable binaries that go to production with the use of `-gcflags` flags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `t escapes to heap` message means that `t` escapes the function. Put simply,
    it means that `t` is used outside of the function and does not have a local scope
    (because it is passed outside the function). However, this does not necessarily
    mean that the variable has moved to the heap. On other occasions, you can see
    the message `moved to heap`. This message shows that the compiler decided to move
    a variable to the heap because it might be used outside of the function. The `does
    not escape` message indicates that the relevant argument does not escape to the
    heap.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we should write our algorithms in order to use the stack instead of
    the heap, but this is impossible as stacks cannot allocate too-large objects and
    cannot store variables that live longer than a function. So, it is up to the Go
    compiler to decide.
  prefs: []
  type: TYPE_NORMAL
- en: The last two lines of the output consist of the output generated by the two
    `fmt.Println()` statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to get a more detailed output, you can use `-m` twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Although more detailed, I find this output too crowded. Usually, using `-m`
    just once reveals what is happening behind the scenes regarding the program heap
    and stack.
  prefs: []
  type: TYPE_NORMAL
- en: What you should remember is that the heap is where the largest amounts of memory
    are usually stored. In practice, this means that **measuring the heap size is
    usually enough for understanding and counting the memory usage of a Go process**.
    As a result, the Go GC spends most of its time working with the heap, which means
    the heap is the first element to be analyzed when we want to optimize the memory
    usage of a program.
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection discusses the main elements of the Go memory model.
  prefs: []
  type: TYPE_NORMAL
- en: The main elements of the Go memory model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to discuss the main elements of the Go memory
    model in order to have a better understanding of what is happening behind the
    scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Go memory model works with the following main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Program code**: Program code is memory mapped by the OS when the process
    is about to run, so Go has no control over that part. This kind of data is read-only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global data**: Global data is also memory mapped by the OS in read-only status.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uninitialized data**: Uninitialized data is stored in anonymous pages by
    the OS. By uninitialized data, we mean data such as the global variables of a
    package. Although we might not know their values before the program starts, we
    know that we are going to need to allocate memory for them when the program starts
    its execution. This kind of memory space is allocated once and is never freed.
    So, the GC has no control over it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heap**: As discussed earlier in this chapter, this is the heap used for dynamic
    allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacks**: These are the stacks used for automatic allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to know all the gory details of all these components of the
    Go memory model. What you need to remember is that problems arise when we either
    purposely or unintentionally put objects into the heap without letting the GC
    release them and, therefore, free their respective memory space. We are going
    to see cases related to memory leaks that have to do with slices and maps in a
    while.
  prefs: []
  type: TYPE_NORMAL
- en: There also exists an internal Go component that performs memory allocations
    called the *Go allocator*. It can dynamically allocate memory blocks in order
    for Go objects to work properly and it is optimized to prevent memory fragmentation
    and locking. The Go allocator is implemented and maintained by the Go team and,
    as such, its operation details can change.
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses memory leaks, which have to do with not properly
    freeing memory space.
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the subsections that follow, we are going to talk about **memory leaks in
    slices and maps**. A *memory leak* happens when a memory space is allocated without
    being completely freed afterward.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to begin with memory leaks caused by wrongly used slices.
  prefs: []
  type: TYPE_NORMAL
- en: Slices and memory leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we are going to showcase code that uses slices and produces
    memory leaks and then illustrate a way to avoid such memory leaks. One common
    scenario for memory leaks with slices involves holding a reference to a larger
    underlying array even after the slice is no longer needed. This prevents the GC
    from reclaiming the memory associated with the array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in `slicesLeaks.go` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `createSlice()` function creates a slice with a large underlying array,
    which means that it requires lots of memory. The `getValue()` function takes the
    first five elements of its input slice and returns those elements as a slice.
    However, it does that while referencing the original input slice, which means
    that that input slice cannot be freed by the GC. Yes, this is a problem!
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `slicesLeaks.go` with some extra command line arguments produces the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The output indicates that there is a leaking parameter. A *leaking parameter*
    means that this function somehow keeps its parameter alive after it returns—this
    is where the memory leak takes place. However, this does not mean it is being
    moved to the stack, as most *leaking parameters* are allocated on the heap.
  prefs: []
  type: TYPE_NORMAL
- en: 'An improved version of `slicesLeaks.go` can be found in `slicesNoLeaks.go`.
    The only difference is in the implementation of the `getValue()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This time we create a copy of the slice part that we want to return, which means
    that the function no longer references the initial slice. As a result, the GC
    is going to be allowed to free its memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `slicesNoLeaks.go` produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So, we get no message about leaking parameters, which means that the issue has
    been resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to discuss memory leaks and maps.
  prefs: []
  type: TYPE_NORMAL
- en: Maps and memory leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This subsection is about memory leaks introduced by maps as illustrated in
    `mapsLeaks.go`. The code in `mapsLeaks.go` is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `printAlloc()` is a helper function for printing information about the memory,
    whereas the `runtime.KeepAlive(m)` statement keeps a reference to `m` so that
    the map is not garbage collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `mapsLeaks.go` produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `make(map[int][128]byte)` statement allocates 111 KB of memory only. However,
    when we populate the map, it allocates 927,931 KB of memory. After that, we delete
    all the elements of the map, and we somehow expect the used memory to shrink.
    However, the empty map requires 600,767 KB of memory! The reason for that is that
    by design the number of buckets in a map cannot shrink. As a result, when we remove
    all maps elements, we do not reduce the number of existing buckets; we just zero
    the slots in the buckets.
  prefs: []
  type: TYPE_NORMAL
- en: However, using `m = nil` allows the GC to free the memory that was previously
    occupied by `m` and now only 119 KB of memory are allocated. As a result, giving
    `nil` values to unused objects is a good practice.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we are going to present a technique that can reduce memory allocations.
  prefs: []
  type: TYPE_NORMAL
- en: Memory pre-allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Memory pre-allocation* refers to the act of reserving memory space for data
    structures before they are needed. Although pre-allocating memory is not a panacea,
    it can be beneficial in certain situations to avoid frequent memory allocations
    and deallocations, which can lead to improved performance and reduced memory fragmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to consider pre-allocation when you have a good estimate of
    the required capacity or size, you expect a significant number of insertions or
    appends, or when you want to reduce memory reallocations and improve performance.
    However, pre-allocation makes more sense when dealing with large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the `main()` function of `preallocate.go` is presented
    in two parts. The first part comes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `make()` function is used to create a slice with a length
    of 0 and a capacity of 100\. This pre-allocates memory for the slice, and as elements
    are appended, the slice can grow without the need for repeated reallocation, which
    slows down the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As before, by providing an initial capacity, we reduce the chances of the map
    being resized frequently as elements are added, leading to more efficient memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses the use of eBPF from Go—as **eBPF is available on
    Linux only**, the presented code can be executed on Linux machines only.
  prefs: []
  type: TYPE_NORMAL
- en: Working with eBPF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BPF stands for Berkeley Packet Filter and eBPF for Extended BPF. BPF was introduced
    back in 1992 to improve the performance of packet capture tools. Back in 2013,
    Alexei Starovoitov did a major rewrite of BPF that was included in the Linux kernel
    in 2014 and replaced BPF. With this rewrite, BPF, which is now called eBPF, became
    more versatile and can be used for a variety of tasks other than network packet
    capture.
  prefs: []
  type: TYPE_NORMAL
- en: eBPF software can be programmed in BCC, `bpftrace`, or using LLVM. The LLVM
    compiler can compile BPF programs into BPF bytecode using a supported programming
    language such as C or the LLVM intermediate representation. As both ways are difficult
    to program because of the use of low-level code, using BCC or `bpftrace` makes
    things simpler for the developer.
  prefs: []
  type: TYPE_NORMAL
- en: What is eBPF?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is really difficult to describe precisely what eBPF can do because it has
    so many capabilities. It is much easier to describe how we can use eBPF. eBPF
    can be used in three main areas: networking, security, and observability. This
    section focuses on the observability capabilities (tracing) of eBPF.'
  prefs: []
  type: TYPE_NORMAL
- en: You can consider eBPF as a virtual machine located inside the Linux kernel that
    can execute eBPF commands, which is custom BPF code. So, eBPF makes the Linux
    kernel programmable to help you solve real-world problems. Keep in mind that eBPF
    (as well as all programming languages) does not solve problems on its own. eBPF
    just gives you the tools to solve your problems! eBPF programs are executed by
    the Linux kernel eBPF runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'In more detail, the key features and aspects of eBPF include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Programmability**: eBPF allows users to write and load small programs into
    the kernel, which can be attached to various hooks or entry points within the
    kernel code. These programs run in a restricted virtual machine environment, ensuring
    safety and security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-kernel execution**: eBPF programs are executed within the kernel in a
    secure way, making it possible to perform efficient and low-overhead operations
    directly in kernel space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic attach points**: eBPF programs can be attached to various hooks or
    attach points in the kernel, allowing developers to extend and customize kernel
    behavior dynamically. Examples include networking, tracing, and security-related
    hooks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability and tracing**: eBPF is widely used for observability and tracing
    purposes as it allows developers to instrument the kernel to gather insights into
    system behavior, performance, and interactions. Tools like `bpftrace` and `perf`
    use eBPF to provide advanced tracing capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Networking**: eBPF is extensively used in networking for tasks such as packet
    filtering, traffic monitoring, and load balancing. It enables the creation of
    efficient and customizable networking solutions without requiring modifications
    to the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance analysis**: eBPF provides a powerful framework for performance
    analysis and profiling. It allows developers and administrators to collect detailed
    information about system performance without significant overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main advantage of eBPF compared to traditional performance tools is that
    it is efficient, production-safe, and part of the Linux kernel. In practice, this
    means that we can use eBPF without the need to add or load any other components
    to the Linux kernel.
  prefs: []
  type: TYPE_NORMAL
- en: About observability and eBPF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most Linux applications are executed in user space, which is a layer without
    too many privileges. Although using user space is safer and more secure, it has
    restrictions and requires using system calls to ask the kernel for access to privileged
    resources. Even the simplest commands use a large amount of system calls when
    executed. In practice, this means that if we are able to observe the system calls
    of our applications, we can learn more information about the way they behave and
    operate.
  prefs: []
  type: TYPE_NORMAL
- en: When things operate as expected and the performance of our applications is good,
    we usually do not care much about performance and the executed system calls. But
    when things go wrong, we desperately need to understand more about the operation
    of our applications. Putting special code in the Linux kernel or developing a
    module in order to understand the operation of our applications is a difficult
    task that might require a long period of time. This is where observability and
    eBPF come into play. eBPF, its language, and its tools allow us to dynamically
    see what happens behind the scenes without the need to change the entire Linux
    operating system.
  prefs: []
  type: TYPE_NORMAL
- en: All you need to communicate with eBPF is a programming language that supports
    `libbpf` ([https://github.com/libbpf/libbpf](https://github.com/libbpf/libbpf)).
    Apart from C, Go also offers support for the `libbpf` library ([https://github.com/aquasecurity/libbpfgo](https://github.com/aquasecurity/libbpfgo)).
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection shows how to create an eBPF tool in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an eBPF tool in Go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As `gobpf` is an external Go package and the fact that, by default, all recent
    Go versions use modules, all source code should be put somewhere under `~/go/src`.
    The presented utility records the user ID of each user by tracing the `getuid(2)`
    system call and keeps a count for each user ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code of the `uid.go` utility is going to be presented in four parts. The
    first part comes with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If you are familiar with the C programming language, you should recognize that
    the `source` variable **holds C code**—this is the code that communicates with
    the Linux kernel to get the desired information. However, this code is called
    from a Go program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part of the utility is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this second part, we define a command line argument named `pid` and initialize
    a new eBPF module named `m`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third part of the utility contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `m.LoadUprobe("count")` statement loads the `count()` function. The handling
    of the probe is initiated with the `m.AttachUprobe()` call. The `AttachUprobe()`
    method says that we want to trace the `getuid(2)` system call using `Uprobe`.
    The `bpf.NewTable()` statement is what gives us access to the `counts` hash defined
    in the C code. Remember that the eBPF program is written in C code that is held
    in a `string` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last part of the utility contains the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The previous code uses channels and UNIX signal handling to block the program.
    Once *Ctrl* + *C* is pressed, the `sig` channel unblocks the program and prints
    the desired information with the help of the `table` variable. As the data in
    the `table` variable is in the binary format, we need to decode it using two `binary.LittleEndian.Uint64()`
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: In order to execute the program, you need a C compiler and the BPF libraries
    to be installed, which depends on your Linux variant. Please refer to your Linux
    variant documentation for instructions on how to install eBPF. If you have any
    issues running the program, ask in relevant forums.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `uid.go` creates the following kind of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You can use the code in `uid.go` as a template when writing your own eBPF utilities
    in Go.
  prefs: []
  type: TYPE_NORMAL
- en: The section that follows discusses the `rand.Seed()` function and why it is
    not necessary to use it, starting from Go version 1.20.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter of the book, we presented various advanced Go topics related
    to benchmarking, performance, and efficiency. Remember that benchmark results
    can be influenced by various factors such as hardware, compiler optimizations,
    and workload. It is important to **interpret the results carefully and rationally**
    while considering the specific conditions under which the benchmarks are run.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned that Go has automatic memory management, which means
    that the language runtime takes care of memory allocation and deallocation for
    you. The primary components of Go’s memory management are garbage collection,
    automatic memory allocation, and a runtime scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also presented a very powerful technology, eBPF. If you are using
    Linux machines, then you should definitely learn more about eBPF and how to use
    it with Go. The eBPF framework has gained popularity due to its versatility and
    the ability to address a wide range of use cases within the Linux kernel. When
    working with eBPF, you should first think like a system administrator, not as
    a programmer. Put simply, start by trying the existing eBPF tools instead of writing
    your own. However, if you have an actual issue that cannot be solved by existing
    eBPF tools, then you might need to start acting like a developer.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is about Go 1.21 and Go 1.22 and the changes they introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try to do the following exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: Create three different implementations of a function that copies binary files
    and benchmark them to find the faster one. Can you explain why this function is
    faster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a version of `BenchmarkWBufWriterReset()` that does not use `buffer.Reset()`
    and see how fast it performs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a really difficult task: Create a machine learning library in Go. Keep
    in mind that, behind the scenes, ML uses statistics and matrix operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'gobpf: [https://github.com/iovisor/gobpf](https://github.com/iovisor/gobpf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The smallest Go binary: [https://totallygamerjet.hashnode.dev/the-smallest-go-binary-5kb](https://totallygamerjet.hashnode.dev/the-smallest-go-binary-5kb
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Working with Go execution traces (gotraceui): [https://gotraceui.dev/](https://gotraceui.dev/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to troubleshoot memory leaks in Go with Grafana Pyroscope: [https://grafana.com/blog/2023/04/19/how-to-troubleshoot-memory-leaks-in-go-with-grafana-pyroscope/](https://grafana.com/blog/2023/04/19/how-to-troubleshoot-memory-leaks-in-go-with-grafana-pyroscope/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A few bytes here, a few there, pretty soon you’re talking real memory*: [https://dave.cheney.net/2021/01/05/a-few-bytes-here-a-few-there-pretty-soon-youre-talking-real-memory](https://dave.cheney.net/2021/01/05/a-few-bytes-here-a-few-there-pretty-soon-youre-talking-real-memory)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/FzuQbc8zd6](https://discord.gg/FzuQbc8zd6 )'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/QR_Code2286825896190168453.png)](https://discord.gg/FzuQbc8zd6 )'
  prefs: []
  type: TYPE_NORMAL
