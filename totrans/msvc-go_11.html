<html><head></head><body>
		<div id="_idContainer047">
			<h1 id="_idParaDest-151" class="chapter-number" lang="en-GB"><a id="_idTextAnchor152"/>11</h1>
			<h1 id="_idParaDest-152" lang="en-GB"><a id="_idTextAnchor153"/>Collecting Service Telemetry Data</h1>
			<p lang="en-GB">In the previous chapter, we explored the topic of service reliability and described various techniques for making your services more resilient to different types of errors. You learned that reliability-related work consists of making constant improvements in incident detection, mitigation, and <span class="No-Break" lang="">prevention techniques.</span></p>
			<p lang="en-GB">In this chapter, we are going to take a closer look at various types of service performance data, which is essential for setting up service health monitoring and debugging and automating service incident detection. You will learn how to collect service logs, metrics, and traces, and how to visualize and debug communication between your microservices using the distributed <span class="No-Break" lang="">tracing technique.</span></p>
			<p lang="en-GB">We will cover the <span class="No-Break" lang="">following topics:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang="">Telemetry overview</span></li>
				<li lang="en-GB">Collecting <span class="No-Break" lang="">service logs</span></li>
				<li lang="en-GB">Collecting <span class="No-Break" lang="">service metrics</span></li>
				<li lang="en-GB">Collecting <span class="No-Break" lang="">service traces</span></li>
			</ul>
			<p lang="en-GB">Now, let’s proceed to the overview of all the techniques that we are going to describe in <span class="No-Break" lang="">this chapter.</span></p>
			<h1 id="_idParaDest-153" lang="en-GB"><a id="_idTextAnchor154"/>Technical requirements</h1>
			<p lang="en-GB">To complete this chapter, you will need Go 1.11+ or above. You will also need the <span class="No-Break" lang="">following tools:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">grpcurl</strong></span><span class="No-Break" lang="">: </span><a href="https://github.com/fullstorydev/grpcurl"><span class="No-Break" lang="">https://github.com/fullstorydev/grpcurl</span></a></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">Jaeger</strong></span><span class="No-Break" lang="">: </span><a href="https://www.jaegertracing.io/"><span class="No-Break" lang="">https://www.jaegertracing.io/</span></a></li>
			</ul>
			<p lang="en-GB">You can find the GitHub code for this chapter <span class="No-Break" lang="">here: </span><a href="https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter11"><span class="No-Break" lang="">https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter11</span></a><span class="No-Break" lang="">.</span></p>
			<h1 id="_idParaDest-154" lang="en-GB"><a id="_idTextAnchor155"/>Telemetry overview</h1>
			<p lang="en-GB">In the introduction to this chapter, we mentioned that there are different types of service performance data, all of which are essential for service health monitoring and troubleshooting. These types of data are called <a id="_idIndexMarker536"/><strong class="bold" lang="">telemetry data</strong> and include <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Logs</strong>: Messages recorded <a id="_idIndexMarker537"/>by your services that provide insights into the operations they perform or errors <span class="No-Break" lang="">they encounter</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Metrics</strong>: Performance data <a id="_idIndexMarker538"/>produced by your services, such as the number of registered users, API request error rate, or percentage of free <span class="No-Break" lang="">disk space</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Traces</strong>: Data that shows <a id="_idIndexMarker539"/>how your services perform various operations, such as API requests, which other services they call, which internal operations they perform, and how long these <span class="No-Break" lang="">operations take</span></li>
			</ul>
			<p lang="en-GB">Telemetry data is <em class="italic" lang="">immutable</em>: it captures events that have already happened to the service and provides the results of various measurements, such as service API response latency. When different types of telemetry data are combined, they become a powerful source of information about <span class="No-Break" lang="">service behavior.</span></p>
			<p lang="en-GB">In this chapter, we are going to describe how to collect service telemetry data to monitor the health of services. There are two types of service health and <span class="No-Break" lang="">performance monitoring:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">White-box monitoring</strong>: Monitoring <a id="_idIndexMarker540"/>services while having access to different types of internally produced data. For example, you can monitor a server’s CPU utilization by viewing it in the system <span class="No-Break" lang="">monitoring application.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Black-box monitoring</strong>: Monitoring <a id="_idIndexMarker541"/>services using only externally available data and indicators. In this case, you don’t know or have access to data related to their structure or internal behavior. For example, if a service has a publicly available health check API, an external system can monitor its health by calling that API without having access to internal <span class="No-Break" lang="">service data.</span></li>
			</ul>
			<p lang="en-GB">Both types of monitoring are powered by collecting and continuously analyzing service performance data. In general, the more types of data you collect from your application, the more opportunities you get for extracting various types of information about its health and behavior. Let’s list some of the ways you can use the information about your <span class="No-Break" lang="">service performance:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Trend analysis</strong>: Detect <a id="_idIndexMarker542"/>any trends in your service <span class="No-Break" lang="">performance data:</span><ul><li lang="en-GB">Is your service health getting better or worse <span class="No-Break" lang="">over time?</span></li><li lang="en-GB">How does your API success <span class="No-Break" lang="">rate change?</span></li><li lang="en-GB">How many new users are you getting compared to the <span class="No-Break" lang="">previous day/month/year?</span></li></ul></li>
				<li lang="en-GB"><strong class="bold" lang="">Semantic graph capturing</strong>: Capture <a id="_idIndexMarker543"/>data on how your services communicate with each other and with any other components, such as databases, external APIs, and <span class="No-Break" lang="">message brokers.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Anomaly detection</strong>: Automatically <a id="_idIndexMarker544"/>detect anomalies in your service behavior, such as sudden drops in <span class="No-Break" lang="">API requests.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Event correlation</strong>: Detect relationships <a id="_idIndexMarker545"/>between various types of events, such as unsuccessful deployments and <span class="No-Break" lang="">service panics.</span></li>
			</ul>
			<p lang="en-GB">While <a id="_idIndexMarker546"/>observability opens lots of opportunities, it comes with the <span class="No-Break" lang="">following challenges:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Collecting large datasets</strong>: Real-time performance data often takes lots of space to store, especially if you have lots of services or if your services produce lots <span class="No-Break" lang="">of data.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Need for specific tooling</strong>: To collect, process, and visualize different types of data, such as logs, metrics, and traces, you need some extra tools. These tools often come at <span class="No-Break" lang="">a price.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Complex setup</strong>: Observability tooling and infrastructure are often difficult to configure. To access all data coming from multiple services, you need to set up the proper data collection, aggregation, data retention policies, and many <span class="No-Break" lang="">more strategies.</span></li>
			</ul>
			<p lang="en-GB">We are going to describe how to work with each type of telemetry data that you can collect in your microservices. For each type of data, we will provide some usage examples and describe the common ways of setting up the tooling for working with it. First, let’s proceed to look at service <span class="No-Break" lang="">log collection.</span></p>
			<h1 id="_idParaDest-155" lang="en-GB"><a id="_idTextAnchor156"/>Collecting service logs</h1>
			<p lang="en-GB"><strong class="bold" lang="">Logging</strong> is a technique<a id="_idIndexMarker547"/> that involves <a id="_idIndexMarker548"/>collecting real-time application performance data in the form of a time-ordered set of <a id="_idIndexMarker549"/>messages called a <strong class="bold" lang="">log</strong>. Here is an example of a <a id="_idIndexMarker550"/><span class="No-Break" lang="">service log:</span></p>
			<pre class="source-code" lang="en-GB">
2022/06/06 23:00:00 Service started
2022/06/06 23:00:01 Connecting to the database
2022/06/06 23:00:11 Unable to connect to the database: timeout error</pre>
			<p lang="en-GB">Logs can help us understand what was happening in the application at a particular moment in time. As you can see in the preceding example, the service started at 11 P.M. and began connecting to the database a second later, finally logging a timeout error 10 <span class="No-Break" lang="">seconds later.</span></p>
			<p lang="en-GB">Logs can provide lots of valuable insights about the component that emitted them, such as <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Order of operations</strong>: Logs can <a id="_idIndexMarker551"/>help us understand the logical sequence of operations performed by a service by showing when each operation <span class="No-Break" lang="">took place.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Failed operations</strong>: One of the <a id="_idIndexMarker552"/>most useful applications of logs is the ability to see the list of errors recorded by <span class="No-Break" lang="">a service.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Panics</strong>: If a service <a id="_idIndexMarker553"/>experiences an unexpected shutdown due to panic, a log can provide the relevant information, helping troubleshoot <span class="No-Break" lang="">the issue.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Debugging information</strong>: Developers <a id="_idIndexMarker554"/>can log various types of additional information, such as request parameters or headers, that can help when debugging <span class="No-Break" lang="">various issues.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Warnings</strong>: Logs can indicate <a id="_idIndexMarker555"/>various system-level warnings, such as low disk space, that can be used as notification mechanisms for preventing various types <span class="No-Break" lang="">of errors.</span></li>
			</ul>
			<p lang="en-GB">We used logs in the services that we created in <a href="B18865_02.xhtml#_idTextAnchor027"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 2</em></span></a> – our services have been logging some important status messages via the built-in log library. Here’s <span class="No-Break" lang="">an example:</span></p>
			<pre class="source-code" lang="en-GB">
log.Printf("Starting the metadata service on port %d", port)</pre>
			<p lang="en-GB">The built-in log library provides functionality for logging arbitrary text messages and panics. The output of the preceding operation would be <span class="No-Break" lang="">as follows:</span></p>
			<pre class="source-code" lang="en-GB">
2022/07/01 13:05:21 Starting the metadata service on port 8081</pre>
			<p lang="en-GB">By default, the <a id="_idIndexMarker556"/>log library records all logs to the <strong class="source-inline" lang="">stdout</strong> stream associated with the current process. But it is possible to set the output destination by calling the <strong class="source-inline" lang="">SetOutput</strong> function. This way, you can write your logs to files or send them over <span class="No-Break" lang="">the network.</span></p>
			<p lang="en-GB">Two types of functions are provided by the <a id="_idIndexMarker557"/>log library that can be used if a service experiences an unexpected or <span class="No-Break" lang="">non-recoverable error:</span></p>
			<ul>
				<li lang="en-GB"><strong class="source-inline" lang="">fatal</strong>: Functions with this prefix immediately stop the execution of the process after logging <span class="No-Break" lang="">the message.</span></li>
				<li lang="en-GB"><strong class="source-inline" lang="">panic</strong>: After logging the <a id="_idIndexMarker558"/>message, they call the Go <strong class="source-inline" lang="">panic</strong> function, writing the output of the associated error. The following is an example output of calling a <span class="No-Break" lang=""><strong class="source-inline" lang="">panic</strong></span><span class="No-Break" lang=""> function:</span><pre class="source-code" lang="en-GB">
2022/11/10 23:00:00 network unavailable</pre><pre class="source-code" lang="en-GB">
panic: network unavailable</pre></li>
			</ul>
			<p lang="en-GB">While the built-in log library provides a simple way of logging arbitrary text messages, it lacks some useful functionality that makes it easier to collect and process the service logs. Among the missing features is the ability to log events in popular serialization formats, such as JSON, which would simplify how message data is parsed. Another issue is that it lacks <strong class="source-inline" lang="">Error</strong> and <strong class="source-inline" lang="">Errorf</strong> functions, which could be used for explicitly logging errors. Since the built-in logging library only provides a <strong class="source-inline" lang="">Print</strong> function, it’s unclear by default whether the logged message indicates an error, a warning, <span class="No-Break" lang="">or neither.</span></p>
			<p lang="en-GB">Yet, the biggest missing piece in the built-in log library is the ability to perform <a id="_idIndexMarker559"/>structured logging. <strong class="bold" lang="">Structured logging</strong> is a technique that involves collecting log messages in the form of serialized structures, such as JSON records. A distinct feature of such structures, compared to arbitrary text strings, is that they can contain additional metadata in the form of fields – key-value records. This allows the service to represent the message metadata as any supported type, such as a number, string, or <span class="No-Break" lang="">serialized record.</span></p>
			<p lang="en-GB">The following snippet includes an example of a JSON-encoded <span class="No-Break" lang="">log structure:</span></p>
			<pre class="source-code" lang="en-GB">
{"level":"info", "time":"2022-09-04T20:10:10+1:00","message":"Service started", "service":"metadata"}</pre>
			<p lang="en-GB">As you may have noticed, in addition to using JSON as the output format, there are two additional features of the preceding <span class="No-Break" lang="">log format:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Log level</strong>: There is a field called <strong class="source-inline" lang="">level</strong> that specifies the type of a <span class="No-Break" lang="">log message.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Additional message fields</strong>: Our example includes a field called <strong class="source-inline" lang="">service</strong>, which is used to indicate the service that emitted <span class="No-Break" lang="">the message.</span></li>
			</ul>
			<p lang="en-GB">The output format <a id="_idIndexMarker560"/>described earlier allows us to decode the log messages much easier. It also helps us interpret their contents based on the log level and the additional message fields. Additional metadata can also be used for searching: for example, we can search for messages that have a particular <strong class="source-inline" lang="">service</strong> <span class="No-Break" lang="">field value.</span></p>
			<p lang="en-GB">Now, let’s focus on log-level metadata from the previous example. First, let’s review some of the common <span class="No-Break" lang="">log levels:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Info</strong>: Informational messages<a id="_idIndexMarker561"/> that do not indicate any error. An example of such a message is a log record indicating that the service successfully connected to <span class="No-Break" lang="">the database.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Error</strong>: Messages indicating <a id="_idIndexMarker562"/>errors, such as <span class="No-Break" lang="">network timeouts.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Warning</strong>: Messages indicating <a id="_idIndexMarker563"/>some potential issues, such as too many <span class="No-Break" lang="">open files.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Fatal</strong>: Messages indicating <a id="_idIndexMarker564"/>critical or non-recoverable errors, such as insufficient memory, that make executing the service <span class="No-Break" lang="">further impossible.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Debug</strong>: Messages that <a id="_idIndexMarker565"/>provide some additional context to the developers that can help troubleshoot various issues or get some additional insights for application performance. Collecting <strong class="source-inline" lang="">Debug</strong> messages is usually disabled by default, as they often generate a large amount <span class="No-Break" lang="">of data.</span></li>
			</ul>
			<p lang="en-GB">Log levels also help us interpret log messages. Consider the following unstructured message produced by the built-in log library, which does not include any <span class="No-Break" lang="">level information:</span></p>
			<pre class="source-code" lang="en-GB">
2022/06/06 23:00:00 Connection terminated</pre>
			<p lang="en-GB">Can you tell whether it’s a regular informational message indicating the regular behavior (for example, the service intentionally terminated a connection after performing some work), a warning, or an error? If this is an error, is it critical or not? Without the log level providing additional context, it is difficult to interpret <span class="No-Break" lang="">this message.</span></p>
			<p lang="en-GB">Another advantage of explicitly using log levels is the ability to enable or disable that ability to log specific types of levels. For example, logging <strong class="source-inline" lang="">Debug</strong> messages can be disabled under normal service conditions and enabled during troubleshooting. <strong class="source-inline" lang="">Debug</strong> messages often include much more information than regular ones, requiring more disk space and making it harder to navigate the other types of logs. Different logging libraries let us enable or disable specific levels, such as <strong class="source-inline" lang="">Debug</strong> or even <strong class="source-inline" lang="">Info</strong>, leaving only logs indicating warnings, errors, fatal errors, <span class="No-Break" lang="">and panics.</span></p>
			<p lang="en-GB">Let’s review some popular Go logging libraries and focus on choosing the one that we would use in <span class="No-Break" lang="">our microservices.</span></p>
			<h2 id="_idParaDest-156" lang="en-GB"><a id="_idTextAnchor157"/>Choosing the logging library</h2>
			<p lang="en-GB">In this section, we will <a id="_idIndexMarker566"/>describe some of the existing Go logging libraries and review their features. This section should help you choose the logging library that you will use in <span class="No-Break" lang="">your microservices.</span></p>
			<p lang="en-GB">First, let’s list the features that we would like to get from the <a id="_idIndexMarker567"/><span class="No-Break" lang="">logging library:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Structured logging</strong>: Supports logging structured messages that may include additional fields in a <span class="No-Break" lang="">key-value format.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Fast performance</strong>: Writing log messages should not have a noticeable impact on <span class="No-Break" lang="">service performance.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Log level support</strong>: Enforce inclusion of a log level in <span class="No-Break" lang="">message metadata.</span></li>
			</ul>
			<p lang="en-GB">The following is an additional feature that would be nice <span class="No-Break" lang="">to have:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Support formatting</strong>: We can write formatted strings in a <strong class="source-inline" lang="">Printf</strong>-like format (for example, support an <strong class="source-inline" lang="">Errorf</strong> function to log a <span class="No-Break" lang="">formatted error).</span></li>
			</ul>
			<p lang="en-GB">Now, let’s review some of the most popular Go logging libraries. When evaluating library performance, we will be using the logging library benchmark <span class="No-Break" lang="">data: </span><span class="No-Break" lang="">https://github.com/uber-go/zap#performance</span><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">The list of popular Go logging libraries includes <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Built-in Go log </strong><span class="No-Break" lang=""><strong class="bold" lang="">package</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://pkg.go.dev/log</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Officially <a id="_idIndexMarker568"/>supported by <a id="_idIndexMarker569"/>the Go development team, included in the <span class="No-Break" lang="">Go SDK</span></li><li lang="en-GB">Does not support structured logging and does not have built-in support for <span class="No-Break" lang="">log levels</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">zap</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/uber-go/zap</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Fastest <a id="_idIndexMarker570"/>performance among<a id="_idIndexMarker571"/> all logging <span class="No-Break" lang="">libraries reviewed</span></li><li lang="en-GB">Feature-rich and supports a fast minimalistic logger, as well as a slightly slower one with <span class="No-Break" lang="">additional features</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">zerolog</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/rs/zerolog</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB"><span class="No-Break" lang="">Fast performance</span></li><li lang="en-GB">Simple and <a id="_idIndexMarker572"/><span class="No-Break" lang="">elegant API</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">go-kit/log</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/go-kit/log</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Part of a <a id="_idIndexMarker573"/>larger <strong class="source-inline" lang="">go-kit</strong> toolkit for <span class="No-Break" lang="">microservice development</span></li><li lang="en-GB">Slightly slower than <strong class="source-inline" lang="">zerolog</strong> and <strong class="source-inline" lang="">zap</strong>, but faster than the other <span class="No-Break" lang="">logging libraries</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">apex/log</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/apex/log</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Has built-in <a id="_idIndexMarker574"/>support for various log storages, such as Elasticsearch, Graylog, and <span class="No-Break" lang="">AWS Kinesis</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">log15</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/inconshreveable/log15</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Feature-rich<a id="_idIndexMarker575"/> <span class="No-Break" lang="">logging toolkit</span></li><li lang="en-GB">Much slower than the other log <span class="No-Break" lang="">libraries reviewed</span></li></ul></li>
			</ul>
			<p lang="en-GB">The preceding list <a id="_idIndexMarker576"/>provides some high-level details about some of the popular Go logging libraries to help you choose the right one for your services. All libraries, except the built-in <strong class="source-inline" lang="">log</strong> package, provide the features that we need, including structured logging and log levels. Now, the question is, how do we select the best one <span class="No-Break" lang="">among them?</span></p>
			<p lang="en-GB">My personal opinion is that the <strong class="source-inline" lang="">zap</strong> library provides the most flexible and yet most performant solution to service logging problems. It allows us to use two separate loggers, called <strong class="source-inline" lang="">Logger</strong> and <strong class="source-inline" lang="">SugaredLogger</strong>. <strong class="source-inline" lang="">Logger</strong> can be used in high-performance applications, while <strong class="source-inline" lang="">SugaredLogger</strong> can be used when you need some extra features; we will review these features in the <span class="No-Break" lang="">next section.</span></p>
			<h2 id="_idParaDest-157" lang="en-GB"><a id="_idTextAnchor158"/>Using logging features</h2>
			<p lang="en-GB">Let’s start practicing and <a id="_idIndexMarker577"/>demonstrate how to use some features of the <strong class="source-inline" lang="">zap</strong> logging library that we picked in the previous section. First, let’s start with the basics and illustrate how to log a simple message that has the <strong class="source-inline" lang="">Info</strong> level and an additional metadata field called <strong class="source-inline" lang="">serviceName</strong>. The complete Go code for this example is <span class="No-Break" lang="">as follows:</span></p>
			<pre class="source-code" lang="en-GB">
package main
import "go.uber.org/zap"
func main() {
    logger, _ := zap.NewProduction()
    logger.Info("Started the service", zap.String("serviceName", "metadata"))
}</pre>
			<p lang="en-GB">We initialize the <strong class="source-inline" lang="">logger</strong> variable by calling the <strong class="source-inline" lang="">zap.NewProduction</strong> function, which returns a production-configured logger. This logger omits debug messages, uses JSON as the output format, and includes stack traces in the logs. Then, we create a structured log message by including a <strong class="source-inline" lang="">serviceName</strong> field by using the <strong class="source-inline" lang="">zap.String</strong> function, which can be used to log <span class="No-Break" lang="">string data.</span></p>
			<p lang="en-GB">The output of the preceding example is <span class="No-Break" lang="">as follows:</span></p>
			<pre class="source-code" lang="en-GB">
{"level":"info","ts":1257894000,"caller":"sandbox1575103092/prog.go:11","msg":"Started the service","serviceName":"metadata"}</pre>
			<p lang="en-GB">The <strong class="source-inline" lang="">zap</strong> library <a id="_idIndexMarker578"/>offers support for other types of Go primitives, such as <strong class="source-inline" lang="">int</strong>, <strong class="source-inline" lang="">long</strong>, <strong class="source-inline" lang="">bool</strong>, and many more. Corresponding functions for creating log field names follow the same naming format, such as <strong class="source-inline" lang="">Int</strong>, <strong class="source-inline" lang="">Long</strong>, and <strong class="source-inline" lang="">Bool</strong>. Additionally, <strong class="source-inline" lang="">zap</strong> includes a set of functions for the other built-in Go types, such as <strong class="source-inline" lang="">time.Duration</strong>. The following code shows an example of a <span class="No-Break" lang=""><strong class="source-inline" lang="">time.Duration</strong></span><span class="No-Break" lang=""> field:</span></p>
			<pre class="source-code" lang="en-GB">
logger.Info("Request timed out", zap.Duration("timeout", 10*time.Second))</pre>
			<p lang="en-GB">Let’s illustrate how to log arbitrary objects, such as structures. In <a href="B18865_02.xhtml#_idTextAnchor027"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 2</em></span></a>, we defined the <span class="No-Break" lang=""><strong class="source-inline" lang="">Metadata</strong></span><span class="No-Break" lang=""> structure:</span></p>
			<pre class="source-code" lang="en-GB">
// Metadata defines the movie metadata.
type Metadata struct {
    ID          string `json:"id"`
    Title       string `json:"title"`
    Description string `json:"description"`
    Director    string `json:"director"`
}</pre>
			<p lang="en-GB">Let’s assume that we want to log the entire structure for debugging purposes. One way of doing so is to use the <strong class="source-inline" lang="">zap.Stringer</strong> field. This field allows us to log any structure or interface with the <strong class="source-inline" lang="">String()</strong> function. We can define a <strong class="source-inline" lang="">String</strong> function for our <strong class="source-inline" lang="">Metadata</strong> structure <span class="No-Break" lang="">as follows:</span></p>
			<pre class="source-code" lang="en-GB">
func (m *Metadata) String() string {
    return fmt.Sprintf("Metadata{id=%s, title=%s, description=%s, director=%s}", m.ID, m.Title, m.Description, m.Director)
}</pre>
			<p lang="en-GB">Now, we can log the <strong class="source-inline" lang="">Metadata</strong> structure as a <span class="No-Break" lang="">log field:</span></p>
			<pre class="source-code" lang="en-GB">
logger.Debug("Retrieved movie metadata", zap.Stringer("metadata", metadata))</pre>
			<p lang="en-GB">The output would look <span class="No-Break" lang="">as follows:</span></p>
			<pre class="source-code" lang="en-GB">
{"level":"debug","msg":"Retrieved movie metadata","metadata":"Metadata{id=id, title=title, description=description, director=director}"}</pre>
			<p lang="en-GB">Now, let’s illustrate one <a id="_idIndexMarker579"/>more useful technique of using the <strong class="source-inline" lang="">zap</strong> library. If you want to include the same fields in multiple messages, you can re-initialize the logger by using the <strong class="source-inline" lang="">With</strong> function, as illustrated in the <span class="No-Break" lang="">following example:</span></p>
			<pre class="source-code" lang="en-GB">
logger = logger.With(zap.String("endpoint", "PutRating"), zap.String("ratingId", ratingID))
logger.Debug("Received a PutRating request")
// endpoint logic
logger.Debug("Processed a PutRating request")</pre>
			<p lang="en-GB">The results of both calls to the <strong class="source-inline" lang="">Debug</strong> function will now include both the <strong class="source-inline" lang="">endpoint</strong> and <span class="No-Break" lang=""><strong class="source-inline" lang="">ratingId</strong></span><span class="No-Break" lang=""> fields.</span></p>
			<p lang="en-GB">You can also use this technique when you create new service components in your code. In the following example, we are creating a sub-logger inside the <span class="No-Break" lang=""><strong class="source-inline" lang="">New</strong></span><span class="No-Break" lang=""> function:</span></p>
			<pre class="source-code" lang="en-GB">
func New(logger *zap.Logger, ctrl *rating.Controller) *Handler{ 
    return &amp;Handler{logger.With("component": "ratingController"), ctrl}
}</pre>
			<p lang="en-GB">This way, the newly created instance of a <strong class="source-inline" lang="">Handler</strong> structure will be initialized with a logger that includes the <strong class="source-inline" lang="">component</strong> field with the <strong class="source-inline" lang="">ratingController</strong> value in <span class="No-Break" lang="">each message.</span></p>
			<p lang="en-GB">Now that we have covered some of the primary service logging use cases, let’s discuss how to store logs in a <span class="No-Break" lang="">microservice environment.</span></p>
			<h2 id="_idParaDest-158" lang="en-GB"><a id="_idTextAnchor159"/>Storing microservice logs</h2>
			<p lang="en-GB">By default, logs of each <a id="_idIndexMarker580"/>service instance are written to the output stream of the process running it. This mechanism of log collection allows us to monitor service operations by continuously reading the data from the associated stream (<strong class="source-inline" lang="">stdout</strong> in most cases) on a host running a service instance. However, without any additional software, the log data would not be persisted, so you would not be able to read your previously recorded logs after a service restart or a <span class="No-Break" lang="">sudden crash.</span></p>
			<p lang="en-GB">Various software solutions allow us to store and query the log data in a multi-service environment. They help solve multiple <span class="No-Break" lang="">other problems:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Distributed log collection</strong>: If you have multiple services running on different hosts, you must collect the service logs on each host independently and send them for <span class="No-Break" lang="">further aggregation.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Centralized log storage</strong>: To be able to query the data that’s emitted by different services, you need to store it in a centralized way – all logs across all services should be accessible during the <span class="No-Break" lang="">query execution.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Data retention</strong>: Logging data usually takes a lot of disk space, and it often becomes too expensive to store it indefinitely for all your services. To solve this problem, you need to establish the right data retention policies for your services that will allow you to configure how long you can store the data for <span class="No-Break" lang="">each one.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Efficient indexing</strong>: To be able to quickly query your logging data, the logs need to be indexed and stored efficiently. Modern indexing software can help you query terabytes of log data in under <span class="No-Break" lang="">10 milliseconds.</span></li>
			</ul>
			<p lang="en-GB">Different tools help facilitate such log operations, such as Elasticsearch and Graylog. Let’s briefly review Elasticsearch to provide an example of an end-to-end log <span class="No-Break" lang="">management solution.</span></p>
			<p lang="en-GB"><strong class="bold" lang="">Elasticsearch</strong> is a <a id="_idIndexMarker581"/>popular open source search engine that was created in 2010 and quickly gained popularity as a scalable system for indexing and querying different types of structured data. While the primary use case of Elasticsearch is a full-text search, it can be efficiently used for storing and querying various types of structured data, such as service logs. Elasticsearch is also a part of the toolkit called the <strong class="bold" lang="">Elastic Stack</strong>, also called <strong class="bold" lang="">ELK</strong>, which<a id="_idIndexMarker582"/> includes some <span class="No-Break" lang="">other systems:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Logstash</strong>: A data <a id="_idIndexMarker583"/>processing pipeline that can collect, aggregate, and transform various types of data, such as <span class="No-Break" lang="">service logs</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Kibana</strong>: A user interface<a id="_idIndexMarker584"/> for accessing the data in Elasticsearch, providing convenient visualization and <span class="No-Break" lang="">querying features</span></li>
			</ul>
			<p lang="en-GB">The log collection pipeline in the Elastic Stack looks <span class="No-Break" lang="">like this:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/Figure_11.1_B18865.jpg" alt="Figure 11.1 – Logging pipeline in the Elastic Stack&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Logging pipeline in the Elastic Stack</p>
			<p lang="en-GB">In this flow chart, service logs <a id="_idIndexMarker585"/>are collected by Logstash and sent to Elasticsearch for indexing and storing. Then, users can access the logs and other data indexed in Elasticsearch using the <span class="No-Break" lang="">Kibana interface.</span></p>
			<p lang="en-GB">One of the key advantages of the Elastic Stack is that most of its tools are available for free and are open source. It is well-maintained and extremely popular in the developer community, making it easier to search for relevant documentation, get additional support, or find some additional tooling. It also has a set of libraries for all popular languages, allowing us to perform various types of queries and API calls to all components of <span class="No-Break" lang="">the pipeline.</span></p>
			<p lang="en-GB">The Go library for using the Elasticsearch API is called <strong class="source-inline" lang="">go-elasticsearch</strong> and can be found on GitHub <span class="No-Break" lang="">at </span><a href="https://github.com/elastic/go-elasticsearch"><span class="No-Break" lang="">https://github.com/elastic/go-elasticsearch</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">We are not going to cover the Elastic Stack in detail as it’s outside of the scope of this chapter, but you can get more familiar with the Elastic Stack by reading its official <span class="No-Break" lang="">documentation (</span><span class="No-Break" lang="">https://www.elastic.co/guide/index.html</span><span class="No-Break" lang="">).</span></p>
			<p lang="en-GB">Having covered some high-level details regarding some popular logging software, let’s move on to the next topic: describing the best practices <span class="No-Break" lang="">of logging.</span></p>
			<h2 id="_idParaDest-159" lang="en-GB"><a id="_idTextAnchor160"/>Logging best practices</h2>
			<p lang="en-GB">So far, we have <a id="_idIndexMarker586"/>covered the most important aspects of logging and described how to choose a logging library, as well as how to establish the logging infrastructure for collecting and analyzing data. Let’s describe some of the best practices for logging <span class="No-Break" lang="">service data:</span></p>
			<ul>
				<li lang="en-GB">Avoid using <span class="No-Break" lang="">interpolated strings.</span></li>
				<li lang="en-GB">Standardize your <span class="No-Break" lang="">log messages.</span></li>
				<li lang="en-GB">Periodically review your <span class="No-Break" lang="">log data.</span></li>
				<li lang="en-GB">Set up appropriate <span class="No-Break" lang="">log retention.</span></li>
				<li lang="en-GB">Identify the message source <span class="No-Break" lang="">in logs.</span></li>
			</ul>
			<p lang="en-GB">Let’s now cover each practice <span class="No-Break" lang="">in detail.</span></p>
			<h3 lang="en-GB">Avoid using interpolated strings</h3>
			<p lang="en-GB">One of the top logging anti-patterns is the usage of <strong class="bold" lang="">interpolated strings</strong> – messages that embed metadata inside text fields. Let’s take the following snippet of code as <span class="No-Break" lang="">an example:</span></p>
			<pre class="source-code" lang="en-GB">
logger.Infof("User %s successfully registered", userID)</pre>
			<p lang="en-GB">The problem with this code is that it merges two types of data into a single text message: an operation name (user registration) and a user identifier. Such messages make it harder to search and process log metadata: each time you need to extract <strong class="source-inline" lang="">userID</strong> from a log message, you would need to parse a string that <span class="No-Break" lang="">contains it.</span></p>
			<p lang="en-GB">Let’s update our example by following the structured logging approach, where we log additional metadata as <span class="No-Break" lang="">message fields:</span></p>
			<pre class="source-code" lang="en-GB">
logger.Infof("User successfully registered", zap.String("userId", userID))</pre>
			<p lang="en-GB">The updated version makes a big difference when you want to query your data. Now, you can query all log events that have <strong class="source-inline" lang="">User successfully registered</strong> text messages and easily access all user identifiers associated with them. Avoiding interpolated messages helps keep your log data easy to query and parse, simplifying all operations <span class="No-Break" lang="">with it.</span></p>
			<h3 lang="en-GB">Standardize your log messages</h3>
			<p lang="en-GB">In this section, we covered the benefits of log centralization and the advantages of querying the data across multiple services. But I would like to emphasize how it is important to standardize the format of log messages in a microservice environment. Sometimes, it is useful to execute log queries that span multiple services, API endpoint handlers, or other components. For example, you may need to perform the following types of queries on your <span class="No-Break" lang="">log data:</span></p>
			<ul>
				<li lang="en-GB">Get the distribution of timeout errors across <span class="No-Break" lang="">all services.</span></li>
				<li lang="en-GB">Get the daily count of errors for each <span class="No-Break" lang="">API endpoint.</span></li>
				<li lang="en-GB">Get distinct error messages across all <span class="No-Break" lang="">database repositories.</span></li>
			</ul>
			<p lang="en-GB">If your services log the data using different field names, you will not be able to easily gather such data using a common query function. On the opposite side, establishing the common field names helps ensure the log messages follow the same naming convention, simplifying any queries <span class="No-Break" lang="">you write.</span></p>
			<p lang="en-GB">To make sure the logs are <a id="_idIndexMarker587"/>emitted in the same way across all services and all components, you may follow <span class="No-Break" lang="">these tips:</span></p>
			<ul>
				<li lang="en-GB">Create a shared package that includes log field names as constants; take the <span class="No-Break" lang="">following example:</span><pre class="source-code" lang="en-GB">
package logging</pre><pre class="source-code" lang="en-GB">
const (</pre><pre class="source-code" lang="en-GB">
  FieldService  = "service"</pre><pre class="source-code" lang="en-GB">
  FieldEndpoint = "endpoint"</pre><pre class="source-code" lang="en-GB">
...</pre><pre class="source-code" lang="en-GB">
)</pre></li>
				<li lang="en-GB">To avoid forgetting to include some important field inside a certain structure, function, or set of functions, re-initialize the logger by setting the field as early as possible; take the <span class="No-Break" lang="">following example:</span><pre class="source-code" lang="en-GB">
func (h *Handler) PutRating(ctx context.Context, req *PutRatingRequest) (*PutRatingResponse, error) {</pre><pre class="source-code" lang="en-GB">
    logger := h.logger.With(logging.FieldEndpoint, "putRating")</pre><pre class="source-code" lang="en-GB">
    // Now we can make sure the endpoint field is set across all handler logic.</pre><pre class="source-code" lang="en-GB">
...</pre><pre class="source-code" lang="en-GB">
}</pre></li>
				<li lang="en-GB">Additionally, ensure that the root logger of your service is setting the service name so that all your service components will automatically collect this field <span class="No-Break" lang="">by default:</span><pre class="source-code" lang="en-GB">
func main() {</pre><pre class="source-code" lang="en-GB">
    logger, _ := zap.NewProduction()</pre><pre class="source-code" lang="en-GB">
    logger = logger.With(logging.FieldService, "rating")</pre><pre class="source-code" lang="en-GB">
    // Pass the initialized logger to all service components.</pre></li>
			</ul>
			<p lang="en-GB">The tips that we just provided should help you standardize the usage of common fields across all your service components, making it easier to query the logged data and aggregate it in <span class="No-Break" lang="">different ways.</span></p>
			<h3 lang="en-GB">Periodically review your log data</h3>
			<p lang="en-GB">Once you start collecting your service logs, it is important to periodically review them. Look out for the <span class="No-Break" lang="">following cases:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Make sure there is no PII data in logs</strong>: <strong class="bold" lang="">Personally identifiable information</strong> (<strong class="bold" lang="">PII</strong>), such as full names and SSNs, falls under <a id="_idIndexMarker588"/>many regulations and generally must not be stored in logs. Make sure that no component, such as an API handler or a repository component, emits any such data, even <span class="No-Break" lang="">for debugging.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Check that your service doesn’t emit extra debug data</strong>: Sometimes, developers log some additional data, such as request fields, to debug various issues. Check that no service is continuously emitting too many debug messages during a prolonged period, polluting the logs and using too much <span class="No-Break" lang="">disk space.</span></li>
			</ul>
			<h3 lang="en-GB">Set up appropriate log retention</h3>
			<p lang="en-GB">Log data often takes a lot of space to store. If it keeps growing in size without any additional actions being taken, you may end up using all your disk space and having to urgently clean up the old records. To prevent this, various log storage solutions allow you to configure <strong class="bold" lang="">retention policies</strong> for your data. For example, you can configure your log storage to keep the logs for some services for up to a few years, while limiting some other services to just a few days, depending on the requirements. Additionally, you can set some size constraints so that the logs of your services don’t exceed a predefined <span class="No-Break" lang="">size threshold.</span></p>
			<p lang="en-GB">Ensure you set retention policies for all types of your logs, avoiding situations when you need to clean up unneeded log <span class="No-Break" lang="">records manually.</span></p>
			<h3 lang="en-GB">Identify the message source in logs</h3>
			<p lang="en-GB">Imagine that you are viewing your system logs and notice the following <span class="No-Break" lang="">error event:</span></p>
			<pre class="source-code" lang="en-GB">
{"level":"error", "time":"2022-09-04T20:10:10+1:00","message":"Request timed out"}</pre>
			<p lang="en-GB">Can you understand the problem described in this event? The log record includes the <strong class="source-inline" lang="">Request timed out</strong> error message and has the <strong class="source-inline" lang="">error</strong> level, but it does not provide any meaningful context to us. Without any additional context, we can’t easily understand the problem that caused the <span class="No-Break" lang="">log event.</span></p>
			<p lang="en-GB">Providing the context of any log message is crucial for making it easy to work with the logs. This is especially important in a microservice environment, where similar operations can be performed by multiple services or components. It should always be easy to understand each message and have some reference to the component it is coming from. In this section, we already mentioned the practice of including some additional information, such as the name of the component, in a log event. Such metadata would generally include <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB">Name of <span class="No-Break" lang="">the service</span></li>
				<li lang="en-GB">Name of the component emitting the event (for example, <span class="No-Break" lang="">endpoint name)</span></li>
				<li lang="en-GB">Name of the <span class="No-Break" lang="">file (optional)</span></li>
			</ul>
			<p lang="en-GB">A more detailed version of the <a id="_idIndexMarker589"/>preceding log message looks <span class="No-Break" lang="">like this:</span></p>
			<pre class="source-code" lang="en-GB">
{"level":"error", "time":"2022-09-04T20:10:10+1:00","message":"Request timed out", "service":"rating", "component": "handler", "endpoint": "putRating", "file": "handler.go"}</pre>
			<p lang="en-GB">At this point, we have discussed the main topics related to logging and can move on to the next section, which describes another type of telemetry data – <span class="No-Break" lang="">metrics.</span></p>
			<h1 id="_idParaDest-160" lang="en-GB"><a id="_idTextAnchor161"/>Collecting service metrics</h1>
			<p lang="en-GB">In this section, we are <a id="_idIndexMarker590"/>going to describe another type of service telemetry data: <strong class="bold" lang="">metrics</strong>. To understand what metrics are and how they are different from log data, let’s start with an example. Imagine that you have a set of services providing APIs to their users, and you want to know how many times per second each API endpoint is called. How would you <span class="No-Break" lang="">do this?</span></p>
			<p lang="en-GB">One possible way of solving this problem is using logs. We could create a log event for each request, and then we would be able to count the number of events for each endpoint, aggregating them by second, minute, or in any other possible way. Such a solution would work until we get too many requests per endpoint and can’t log each one independently anymore. Let’s assume there is a service that processes more than a million requests per second. If we used logs to measure its performance, we would need to produce more than a million log events every second, generating lots <span class="No-Break" lang="">of data.</span></p>
			<p lang="en-GB">A more optimal solution to this problem would be to use some sort of value-based aggregation. Instead of storing the data representing each request separately, we could summarize the count of requests per second, minute, or hour, making the data more optimal <span class="No-Break" lang="">for storing.</span></p>
			<p lang="en-GB">The problem that we just described is a perfect use case for using metrics – real-time quantitative measurements of system performance, such as request rate, latency, or cumulative counts. Like logs, metrics are time-based – each record includes a timestamp representing a unique instant of time in the past. However, unlike log events, metrics are primarily used for storing individual values. In our example, the value of an endpoint request rate metric would be the count of requests <span class="No-Break" lang="">per second.</span></p>
			<p lang="en-GB">Metrics are generally represented as <strong class="bold" lang="">time series</strong> – sets of <a id="_idIndexMarker591"/>objects, called <strong class="bold" lang="">data points</strong>, containing <a id="_idIndexMarker592"/>the <span class="No-Break" lang="">following data:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang="">Timestamp</span></li>
				<li lang="en-GB">Value (most commonly, the value <span class="No-Break" lang="">is numerical)</span></li>
				<li lang="en-GB">An optional set of <strong class="bold" lang="">tags</strong>, defined as key-value pairs, that <a id="_idIndexMarker593"/>contain any <span class="No-Break" lang="">additional metadata</span></li>
			</ul>
			<p lang="en-GB">To help you better understand the use cases of using metrics, let’s define some common <span class="No-Break" lang="">metric types:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Counters</strong>: These are time series <a id="_idIndexMarker594"/>representing the value of a cumulative <a id="_idIndexMarker595"/>counter over time. An example would be the counter of service requests – each data point would include a timestamp and the count of requests at that <span class="No-Break" lang="">particular moment.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Gauges</strong>: These are time series representing the changes of a single scalar <a id="_idIndexMarker596"/>value over time. An example of a gauge is a dataset that contains the amount of free disk space on a server at different moments: each data point contains a single <span class="No-Break" lang="">numerical value.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Histograms</strong>: These are time series representing the distribution of <a id="_idIndexMarker597"/>some value against a predefined set of value ranges, called <strong class="bold" lang="">buckets</strong>. An example of a <a id="_idIndexMarker598"/>histogram metric is a dataset, containing the number of users for different <span class="No-Break" lang="">age groups.</span></li>
			</ul>
			<p lang="en-GB">Let’s focus on each metric type to help you understand their differences and the common use cases for <span class="No-Break" lang="">each one.</span></p>
			<p lang="en-GB">Counter metrics are generally used for measuring two types <span class="No-Break" lang="">of data:</span></p>
			<ul>
				<li lang="en-GB">Cumulative value over time (for example, the total number <span class="No-Break" lang="">of errors)</span></li>
				<li lang="en-GB">Change of the cumulative value over time (for example, the number of newly registered users <span class="No-Break" lang="">per hour)</span></li>
			</ul>
			<p lang="en-GB">The second use case is technically a different representation of the first one – if you know how many users you had at each moment in time, you can see how this value changes. Because of this, counters are often used to measure the rates of various events, such as API requests, <span class="No-Break" lang="">over time.</span></p>
			<p lang="en-GB">The following code snippet provides an example of a <strong class="source-inline" lang="">Counter</strong> interface in a <strong class="source-inline" lang="">tally</strong> metrics library (we’ll review this library later in <span class="No-Break" lang="">this chapter):</span></p>
			<pre class="source-code" lang="en-GB">
type Counter interface {
    // Inc increments the counter by a delta.
    Inc(delta int64)
}</pre>
			<p lang="en-GB">Unlike counters, gauges are used for storing unique values of measurements, such as the service’s available memory over time. Here is a gauge example from a <span class="No-Break" lang=""><strong class="source-inline" lang="">tally</strong></span><span class="No-Break" lang=""> library:</span></p>
			<pre class="source-code" lang="en-GB">
type Gauge interface {
    // Update sets the gauges absolute value.
    Update(value float64)
}</pre>
			<p lang="en-GB">Some of the other gauge use cases include <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB">Number of goroutines running by a <span class="No-Break" lang="">service instance</span></li>
				<li lang="en-GB">Number of <span class="No-Break" lang="">active connections</span></li>
				<li lang="en-GB">Number of <span class="No-Break" lang="">open files</span></li>
			</ul>
			<p lang="en-GB">Histograms are slightly different from <a id="_idIndexMarker599"/>counters and gauges. They require us to define a set of ranges that will be used to store the subsets of recorded data. The following are some examples of using <span class="No-Break" lang="">histogram metrics:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Latency tracking</strong>: You can track how long it takes to <a id="_idIndexMarker600"/>perform a certain service operation by creating a set of buckets representing various duration ranges. For example, your buckets could be 0–100 ms, 100–200 ms, 200–300 ms, and <span class="No-Break" lang="">so on.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Cohort tracking</strong>: You can track statistical data, such as the number of <a id="_idIndexMarker601"/>records in each group of values. For example, you can track how many users of each age subscribed to <span class="No-Break" lang="">your service.</span></li>
			</ul>
			<p lang="en-GB">Now that we have covered some high-level basics of metrics, let’s provide an overview of <span class="No-Break" lang="">storing metrics.</span></p>
			<h2 id="_idParaDest-161" lang="en-GB"><a id="_idTextAnchor162"/>Storing metrics</h2>
			<p lang="en-GB">Similar to logs, storing <a id="_idIndexMarker602"/>metrics in a microservice environment brings some <span class="No-Break" lang="">common challenges:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Collection and aggregation</strong>: Metrics need to be collected from all service instances and sent for further aggregation <span class="No-Break" lang="">and storage.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Aggregation</strong>: Collected data needs to be aggregated, so various types of metrics, such as counters, would contain the data coming from all service instances. For example, the counter measuring the total number of requests should summarize the data across all <span class="No-Break" lang="">service instances.</span></li>
			</ul>
			<p lang="en-GB">Let’s review some of the popular tools that provide <span class="No-Break" lang="">such features.</span></p>
			<h3 lang="en-GB">Prometheus</h3>
			<p lang="en-GB">Prometheus is a <a id="_idIndexMarker603"/>popular open source monitoring solution that provides mechanisms for collecting and querying service metrics, as well as setting up automated alerts for detecting various types of incidents. Prometheus gained popularity in the developer community due to its simple data model and being a very flexible model for data ingestion, which we are going to cover in <span class="No-Break" lang="">this section.</span></p>
			<p class="callout-heading" lang="en-GB">Note</p>
			<p class="callout" lang="en-GB">Did you know that Prometheus is written in Go? You can check its source code on its GitHub <span class="No-Break" lang="">page: </span><a href="https://github.com/prometheus/prometheus"><span class="No-Break" lang="">https://github.com/prometheus/prometheus</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">Prometheus supports three types of metrics – counters, gauges, and histograms. It stores each metric as a time series, similarly to the model that we described at the beginning of the <em class="italic" lang="">Collecting service metrics</em> section. Each metric contains the value, additional tags, called <strong class="bold" lang="">labels</strong>, and a <a id="_idIndexMarker604"/>name that can be used for <span class="No-Break" lang="">identifying it.</span></p>
			<p lang="en-GB">Once the data gets into the <a id="_idIndexMarker605"/>Prometheus time series storage, it is available for querying via its query language, called <strong class="bold" lang="">PromQL</strong>. <a id="_idIndexMarker606"/>PromQL allows us to fetch time series data using various functions that allow us to easily filter or exclude certain name and label combinations. The following is an example of a <span class="No-Break" lang="">PromQL query:</span></p>
			<pre class="source-code" lang="en-GB">
http_requests_total{environment="production",method!="GET"}</pre>
			<p lang="en-GB">In this example, the query fetches time series with the <strong class="source-inline" lang="">http_requests_total</strong> name, a label that contains the environment key and production value, and any value of a method label that is not equal <span class="No-Break" lang="">to </span><span class="No-Break" lang=""><strong class="source-inline" lang="">GET</strong></span><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">There is an official Prometheus Go client on GitHub that provides various mechanisms to get the metrics data into Prometheus, as well as to execute the PromQL queries. You can access it <span class="No-Break" lang="">here: </span><a href="https://github.com/prometheus/client_golang"><span class="No-Break" lang="">https://github.com/prometheus/client_golang</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">The documentation for instrumenting Go applications for using Prometheus can be found at the following <span class="No-Break" lang="">link: </span><a href="https://prometheus.io/docs/guides/go-application"><span class="No-Break" lang="">https://prometheus.io/docs/guides/go-application</span></a><span class="No-Break" lang="">.</span></p>
			<h3 lang="en-GB">Graphite</h3>
			<p lang="en-GB">Graphite is another <a id="_idIndexMarker607"/>popular monitoring tool that offers metric collection, aggregation, and querying functionality that is similar to Prometheus. Although it has been among the oldest service monitoring tools in the industry, it remains an extremely powerful instrument for working with service <span class="No-Break" lang="">metric data.</span></p>
			<p lang="en-GB">A typical Graphite installation consists of three <span class="No-Break" lang="">main components:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Carbon</strong>: A <a id="_idIndexMarker608"/>service that listens for time <span class="No-Break" lang="">series data</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Whisper</strong>: A <a id="_idIndexMarker609"/>time <span class="No-Break" lang="">series database</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Graphite-web</strong>: A web <a id="_idIndexMarker610"/>interface and an API for accessing the <span class="No-Break" lang="">metrics data</span></li>
			</ul>
			<p lang="en-GB">Graphite offers a quick integration with a data <a id="_idIndexMarker611"/>visualization tool called <strong class="bold" lang="">Grafana</strong>, which we are going to cover in <a href="B18865_13.xhtml#_idTextAnchor181"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 13</em></span></a> of this book. You can read more details about Graphite on its <span class="No-Break" lang="">website: </span><a href="https://graphiteapp.org"><span class="No-Break" lang="">https://graphiteapp.org</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">Now, let’s move on to the next section, where we will describe the popular libraries for emitting <span class="No-Break" lang="">service metrics.</span></p>
			<h2 id="_idParaDest-162" lang="en-GB"><a id="_idTextAnchor163"/>Popular Go metrics libraries</h2>
			<p lang="en-GB">There are some popular Go <a id="_idIndexMarker612"/>libraries for working with metrics that could help you ingest and query your time series metrics data. Let’s provide a brief overview of some <span class="No-Break" lang="">of them:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">Tally</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/uber-go/tally</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">A performant and <a id="_idIndexMarker613"/>minimalistic library for emitting <span class="No-Break" lang="">service metrics</span></li><li lang="en-GB">Built-in support for data ingestion in Prometheus, StatsD, <span class="No-Break" lang="">and M3</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">rcrowley/go-metrics</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/rcrowley/go-metrics</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">The Go port of a <a id="_idIndexMarker614"/>popular Java metric <span class="No-Break" lang="">library (</span><span class="No-Break" lang="">https://github.com/dropwizard/metrics</span><span class="No-Break" lang="">)</span></li><li lang="en-GB">Supports data ingestion into StatsD <span class="No-Break" lang="">and Graphite</span></li><li lang="en-GB">Has lots of integrations for exporting data to various observability systems, such as Datadog <span class="No-Break" lang="">and Prometheus</span></li></ul></li>
				<li lang="en-GB"><span class="No-Break" lang=""><strong class="bold" lang="">go-kit/metrics</strong></span><span class="No-Break" lang=""> (</span><span class="No-Break" lang="">https://github.com/go-kit/kit/metrics</span><span class="No-Break" lang="">):</span><ul><li lang="en-GB">Part of the <span class="No-Break" lang=""><strong class="source-inline" lang="">go-kit</strong></span><span class="No-Break" lang=""> toolkit</span></li><li lang="en-GB">Supports <a id="_idIndexMarker615"/>multiple metric storages, such as StatsD <span class="No-Break" lang="">and Graphite</span></li></ul></li>
			</ul>
			<p lang="en-GB">We will leave the decision of picking the metrics library for your services to you, as each library provides some useful features that you can leverage when developing your microservices. I am going to use the tally library in the examples throughout this chapter as it provides a simple and minimalistic API that can help illustrate the common metrics use cases. In the next section, we will review some of the use cases of using the metrics in the Go <span class="No-Break" lang="">microservice code.</span></p>
			<h2 id="_idParaDest-163" lang="en-GB"><a id="_idTextAnchor164"/>Emitting service metrics</h2>
			<p lang="en-GB">In this section, we will provide some <a id="_idIndexMarker616"/>examples of emitting and collecting service metrics while covering some common scenarios, such as measuring API request rates, operation latencies, and emitting gauge values. We will use the tally library in our examples, but you can implement this logic using all other popular <span class="No-Break" lang="">metric libraries.</span></p>
			<p lang="en-GB">First, let’s provide an example of how to initialize the tally library so that you can start using it in your service code. In the following example, we are initializing it using the StatsD client (you can use any other tool to collect <span class="No-Break" lang="">the metrics):</span></p>
			<pre class="source-code" lang="en-GB">
statter, err := statsd.NewBufferedClient("127.0.0.1:8125","stats", time.Second, 1440)
if err != nil {
    panic(err)
}
reporter := tallystatsd.NewReporter(statter, tallystatsd.Options{
    SampleRate: 1.0,
})
scope, closer := tally.NewRootScope(tally.ScopeOptions{
    Tags:     map[string]string{"service": "rating"},
    Reporter: reporter,
}, time.Second)</pre>
			<p lang="en-GB">In this example, we are <a id="_idIndexMarker617"/>creating a tally reporter that will submit the metrics to the data collector (StatsD in our use case) and create a <strong class="bold" lang="">scope</strong> – an <a id="_idIndexMarker618"/>interface for reporting the metrics data – that would automatically submit them <span class="No-Break" lang="">for collection.</span></p>
			<p lang="en-GB">Tally scopes are hierarchical: when we initialize the library, we create a <strong class="bold" lang="">root scope</strong>, which <a id="_idIndexMarker619"/>includes initial metadata in the form of key-value tags. All scopes that are created from it would include the parent metadata, preventing cases of missing tags during the <span class="No-Break" lang="">metric’s emission.</span></p>
			<p lang="en-GB">Once you get the scope, you can start reporting the metrics. The following example illustrates how to increment a counter metric by measuring the API request count, which would be automatically reported <span class="No-Break" lang="">by tally:</span></p>
			<pre class="source-code" lang="en-GB">
counter := scope.Counter("request_count")
counter.Inc(1)</pre>
			<p lang="en-GB">The <strong class="source-inline" lang="">Inc</strong> operation increments the value of the counter by <strong class="source-inline" lang="">1</strong>, and the updated value of the metric gets collected by tally automatically in the background. This does not affect the performance of the function that performs the <span class="No-Break" lang="">provided operations.</span></p>
			<p lang="en-GB">If you want to add some additional tags to the metric, you can use the <span class="No-Break" lang=""><strong class="source-inline" lang="">Tagged</strong></span><span class="No-Break" lang=""> function:</span></p>
			<pre class="source-code" lang="en-GB">
counter := scope.Tagged(map[string]string{"operation": "put"}).Counter("request_count")</pre>
			<p lang="en-GB">The following example illustrates how to update the gauge value. Let’s say we have a function that calculates the number of active users in the system and we want to report this value to the metrics storage. We can achieve this by using a gauge metric in the <span class="No-Break" lang="">following way:</span></p>
			<pre class="source-code" lang="en-GB">
gauge := scope.Gauge("active_user_count")
gauge.Update(userCount)</pre>
			<p lang="en-GB">Now, let’s provide an example of <a id="_idIndexMarker620"/>reporting a time duration. A common use case of this is reporting the latency of various operations. In the following example, we are reporting how long it takes to execute <span class="No-Break" lang="">our function:</span></p>
			<pre class="source-code" lang="en-GB">
func latencyTrackingExample(scope tally.Scope) {
    timer := scope.Timer("operation_latency")
    stopwatch := timer.Start()
    defer stopwatch.Stop()
    // Function logic.
}</pre>
			<p lang="en-GB">In our example, we are <a id="_idIndexMarker621"/>initializing the <strong class="source-inline" lang="">operation_latency</strong> timer and calling the <strong class="source-inline" lang="">Start</strong> function of it to start measuring operation latency. The <strong class="source-inline" lang="">Start</strong> function returns the instance of a <strong class="source-inline" lang="">Stopwatch</strong> interface, which includes the <strong class="source-inline" lang="">Stop</strong> function. This reports the time it takes since the stopwatch’s <span class="No-Break" lang="">start time.</span></p>
			<p lang="en-GB">When reporting the latency metrics, the tally library uses the default buckets, unless you provide their exact values. For example, when reporting the metrics to Prometheus, tally is using the following <span class="No-Break" lang="">bucket configuration:</span></p>
			<pre class="source-code" lang="en-GB">
func DefaultHistogramBuckets() []float64 {
    return []float64{
        ms,
        2 * ms,
        5 * ms,
        10 * ms,
        20 * ms,
        50 * ms,
        100 * ms,
        200 * ms,
        500 * ms,
        1000 * ms,
        2000 * ms,
        5000 * ms,
        10000 * ms,
    }
}</pre>
			<p lang="en-GB">Let’s provide an example of using a <a id="_idIndexMarker622"/>histogram with a set of predefined <span class="No-Break" lang="">numerical buckets:</span></p>
			<pre class="source-code" lang="en-GB">
histogram := scope.Histogram("user_age_distribution", tally.MustMakeLinearValueBuckets(0, 1, 130))
histogram.RecordValue(userAgeInYears)</pre>
			<p lang="en-GB">In our example, we are initializing the <a id="_idIndexMarker623"/>histogram metric using a set of predefined buckets from 0 to 130 and recording the value that matches the user age in years using it. Each bucket of the histogram will then contain the sum of <span class="No-Break" lang="">the values.</span></p>
			<p lang="en-GB">Now that we have provided some basic examples of emitting service metrics, let’s look at the best practices for working with <span class="No-Break" lang="">metrics data.</span></p>
			<h2 id="_idParaDest-164" lang="en-GB"><a id="_idTextAnchor165"/>Metrics best practices</h2>
			<p lang="en-GB">In this section, we will <a id="_idIndexMarker624"/>describe some of the best practices related to metric data collection. The list is not exhaustive, but should still be useful for setting up metric collection logic in <span class="No-Break" lang="">your services.</span></p>
			<h3 lang="en-GB">Keep tag cardinality in mind</h3>
			<p lang="en-GB">When you emit your metrics and add additional tags to time series data, keep in mind that most time series databases are not designed to store <strong class="bold" lang="">high-cardinality</strong> data, which can <a id="_idIndexMarker625"/>contain lots of possible tag values. For example, the following types of data should not be generally included in <span class="No-Break" lang="">service metrics:</span></p>
			<ul>
				<li lang="en-GB">Object identifiers, such as movie or <span class="No-Break" lang="">rating IDs</span></li>
				<li lang="en-GB">Randomly generated data, such as UUIDs (for example, <span class="No-Break" lang="">request UUIDs)</span></li>
			</ul>
			<p lang="en-GB">The reason for this is indexing is that each tag key-value combination must be indexed to make the time series searchable, and it becomes expensive to perform this when there are lots of distinct <span class="No-Break" lang="">tag values.</span></p>
			<p lang="en-GB">You can still use some low-cardinality data in metric tags. The following are some <span class="No-Break" lang="">possible examples:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang="">City ID</span></li>
				<li lang="en-GB"><span class="No-Break" lang="">Service name</span></li>
				<li lang="en-GB"><span class="No-Break" lang="">Endpoint name</span></li>
			</ul>
			<p lang="en-GB">Remember this tip to avoid reducing the throughput of your metrics pipeline and to ensure your services don’t emit user identifiers and other types of <span class="No-Break" lang="">high-cardinality metadata.</span></p>
			<h3 lang="en-GB">Standardize metric and tag names</h3>
			<p lang="en-GB">Imagine that you have <a id="_idIndexMarker626"/>hundreds of services, and each service follows different metric naming conventions. For example, one service could be using <strong class="source-inline" lang="">api_errors</strong> as the name of the API error counter metric, while the other could be using the <strong class="source-inline" lang="">api_request_errors</strong> name. If you wanted to compare the metrics for such services, you would need to remember which naming convention each service was using. Such metric discovery will always take time, reducing your ability to analyze <span class="No-Break" lang="">your data.</span></p>
			<p lang="en-GB">A much better solution is to standardize the names of common metrics and tags across all your services. This way, you can easily search and compare various performance indicators, such as service client and server error rates, API throughput, and request latency. In <a href="B18865_12.xhtml#_idTextAnchor171"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 12</em></span></a>, we will review some common performance indicators that you can use to monitor the health of <span class="No-Break" lang="">your services.</span></p>
			<h3 lang="en-GB">Set the appropriate retention</h3>
			<p lang="en-GB">Most time series databases are capable of storing large datasets of metrics due to efficient aggregation. Unlike logs that require you to store each record independently, metrics can be aggregated into a smaller dataset. For example, if you store the counter data, you can store the sums of the values instead of storing each value separately. Even with these optimizations, time series data can take a lot of disk space to store. Large companies can store terabytes of metrics data, so it becomes important to manage its size and set up data retention policies, similar to logs and other types of <span class="No-Break" lang="">telemetry data.</span></p>
			<p lang="en-GB">Metric storages, such as Prometheus, have a default retention time of 15 days, allowing you to change it in the settings. For example, to set the data retention time in Prometheus to 60 days, you can use the <span class="No-Break" lang="">following flag:</span></p>
			<pre class="source-code" lang="en-GB">
--storage.tsdb.retention.time=60d</pre>
			<p lang="en-GB">Limiting the storage retention time helps keep the size of the time series datasets under control, making it easier to manage the storage capacity and plan the infrastructure spending on <span class="No-Break" lang="">data storage.</span></p>
			<p lang="en-GB">Now that we’ve discussed the metrics data, let’s move on to the next section, which covers a powerful <span class="No-Break" lang="">technique: tracing.</span></p>
			<h1 id="_idParaDest-165" lang="en-GB"><a id="_idTextAnchor166"/>Tracing</h1>
			<p lang="en-GB">So far, we have covered two <a id="_idIndexMarker627"/>common types of observability data – logs and metrics. Having logs and metrics data in place is often sufficient for service debugging and troubleshooting. However, there is another type of data that is useful for getting insights into microservice communication and <span class="No-Break" lang="">data flows.</span></p>
			<p lang="en-GB">In this section, we are going to discuss <strong class="bold" lang="">distributed tracing</strong> – a <a id="_idIndexMarker628"/>technique that involves recording and analyzing interactions between different services and service components. The main idea behind distributed tracing is to automatically record all such interactions and provide a convenient way to visualize them. Let’s look at the following example, which illustrates a <a id="_idIndexMarker629"/>distributed tracing use case known as <span class="No-Break" lang="">call analysis:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/Figure_11.2_B18865.jpg" alt="Figure 11.2 – Tracing visualization example&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Tracing visualization example</p>
			<p lang="en-GB">Here, you can see the <a id="_idIndexMarker630"/>execution of a single <strong class="source-inline" lang="">GetMovieDetails</strong> request for our movie service. The data provides some insights into the <span class="No-Break" lang="">operation’s execution:</span></p>
			<ul>
				<li lang="en-GB">Soon after the request starts, two parallel calls come from the movie service; one to the metadata service and the other to the <span class="No-Break" lang="">rating service.</span></li>
				<li lang="en-GB">The call to the metadata service takes 100 milliseconds <span class="No-Break" lang="">to complete.</span></li>
				<li lang="en-GB">The call to the rating service takes 1,100 milliseconds to complete, spanning almost the entire request <span class="No-Break" lang="">processing time.</span></li>
			</ul>
			<p lang="en-GB">The data that we just extracted provided us with lots of valuable information for analyzing the movie service’s performance. First, it helped us understand how an individual request was handled by the movie service, as well as which sub-operations it performed. We can also see the duration of each operation and find out which one was slowing down the entire request. By using this data, we could troubleshoot endpoint performance issues, finding which components make a significant impact on <span class="No-Break" lang="">request processing.</span></p>
			<p lang="en-GB">In our example, we showed the interaction between just three services, but tracing tools allow us to analyze the behavior of systems that use tens and even hundreds of services simultaneously. The following are some other use cases that make <a id="_idIndexMarker631"/>tracing a powerful tool for <span class="No-Break" lang="">production debugging:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Error analysis</strong>: Tracing allows us to visualize the errors on complex call paths, such as chains of calls spanning lots of <span class="No-Break" lang="">different services.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Call path analysis</strong>: Sometimes, you may <a id="_idIndexMarker632"/>investigate issues in systems you are not very familiar with. Tracing data helps you visualize the call path of various operations, helping you understand the logic of the services without requiring you to analyze <span class="No-Break" lang="">their code.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Operation performance breakdown</strong>: Tracing <a id="_idIndexMarker633"/>allows us to see the duration of individual steps of a <span class="No-Break" lang="">long-running operation.</span></li>
			</ul>
			<p lang="en-GB">Let’s describe the tracing data model so that you can get familiar with its common terminology. The core element of tracing is a <strong class="bold" lang="">span</strong> – a <a id="_idIndexMarker634"/>record representing some logical operation, such as a service endpoint call. Each span has the <span class="No-Break" lang="">following properties:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang="">Operation name</span></li>
				<li lang="en-GB"><span class="No-Break" lang="">Start time</span></li>
				<li lang="en-GB"><span class="No-Break" lang="">End time</span></li>
				<li lang="en-GB">An optional set of tags providing some additional metadata related to the execution of the <span class="No-Break" lang="">associated operation</span></li>
				<li lang="en-GB">An optional set of <span class="No-Break" lang="">associated logs</span></li>
			</ul>
			<p lang="en-GB">Spans can be grouped into hierarchies to model relationships between different operations. For example, in <span class="No-Break" lang=""><em class="italic" lang="">Figure 11</em></span><em class="italic" lang="">.2</em>, our <strong class="source-inline" lang="">GetMovieDetails</strong> span included two child spans, representing <strong class="source-inline" lang="">GetMetadata</strong> and <span class="No-Break" lang=""><strong class="source-inline" lang="">GetAggregatedRating</strong></span><span class="No-Break" lang=""> operations.</span></p>
			<p lang="en-GB">Let’s explore how we can collect and use tracing data in our <span class="No-Break" lang="">Go applications.</span></p>
			<h2 id="_idParaDest-166" lang="en-GB"><a id="_idTextAnchor167"/>Tracing tools</h2>
			<p lang="en-GB">There are various tools for <a id="_idIndexMarker635"/>distributed tracing in a microservice environment. Among the most popular ones is Jaeger, which we will review in <span class="No-Break" lang="">this section.</span></p>
			<p class="callout-heading" lang="en-GB">Note</p>
			<p class="callout" lang="en-GB">There are multiple observability libraries and tools available for developers, including libraries for trace collection. To standardize the data model and make data interchangeable, there is an OpenTelemetry project that contains the specification of the tracing data model. You can get familiar with the project on its <span class="No-Break" lang="">website: </span><a href="https://opentelemetry.io"><span class="No-Break" lang="">https://opentelemetry.io</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">Jaeger is an <a id="_idIndexMarker636"/>open source distributed tracing tool that provides mechanisms for collecting, aggregating, and visualizing the tracing data. It offers a simple but highly flexible setup and a great user interface for accessing trace data. This is the reason why it quickly became one of the most popular observability tools across <span class="No-Break" lang="">the industry.</span></p>
			<p lang="en-GB">Jaeger is compatible with the OpenTelemetry specification, so it can be used in combination with any clients implementing the tracing specification, such as the Go SDK (https://opentelemetry.io/docs/instrumentation/go/). The OpenTelemetry SDK is currently the recommended way of emitting trace data from applications, so we are going to use it in our examples throughout <span class="No-Break" lang="">this section.</span></p>
			<p lang="en-GB">The general data flow for services using Jaeger looks <span class="No-Break" lang="">like this:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/Figure_11.3_B18865.jpg" alt="Figure 11.3 – Jaeger data flow&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Jaeger data flow</p>
			<p lang="en-GB">In the preceding diagram, a service using a Jaeger-compatible library is emitting traces to the Jaeger backend, which is storing them in the span storage. The data is accessible for querying and visualization via the <span class="No-Break" lang="">Jaeger UI.</span></p>
			<p class="callout-heading" lang="en-GB">Note</p>
			<p class="callout" lang="en-GB">Jaeger is another example of an observability tool written in Go. You can check out the Jaeger source code on its official GitHub <span class="No-Break" lang="">page: </span><a href="https://github.com/jaegertracing/jaeger"><span class="No-Break" lang="">https://github.com/jaegertracing/jaeger</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">You can find more information about the Jaeger project on its <span class="No-Break" lang="">website: </span><a href="https://www.jaegertracing.io"><span class="No-Break" lang="">https://www.jaegertracing.io</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">Let’s see some examples of instrumenting Go services to emit the tracing data. We will use the OpenTelemetry SDK in our examples to make our code compatible with different <span class="No-Break" lang="">tracing software.</span></p>
			<h2 id="_idParaDest-167" lang="en-GB"><a id="_idTextAnchor168"/>Collecting tracing data with the OpenTelemetry SDK</h2>
			<p lang="en-GB">In this section, we will show you how to emit tracing data in your <span class="No-Break" lang="">service code.</span></p>
			<p lang="en-GB">As we mentioned at the <a id="_idIndexMarker637"/>beginning of the <em class="italic" lang="">Tracing</em> section, the core <a id="_idIndexMarker638"/>benefit of distributed tracing is the ability to automatically capture data that shows how services and other network components communicate with each other. Unlike metrics, which measure the performance of individual operations, traces help to collect information on how each request or operation is handled across the entire network of nodes that report this data. To report the traces, service instances need to be <strong class="bold" lang="">instrumented</strong> so that they perform two <span class="No-Break" lang="">distinct roles:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Report the data on distributed operations</strong>: For each <strong class="bold" lang="">traceable operation</strong> – an operation spanning multiple components, such as network requests or database queries – an instrumented service should report spans. The report should contain the operation name, start time, and <span class="No-Break" lang="">end time.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Facilitate context propagation</strong>: The service should explicitly propagate context throughout the execution (if you are confused by this, please read the following paragraphs – this is the main trick behind <span class="No-Break" lang="">distributed tracing!).</span></li>
			</ul>
			<p lang="en-GB">We already defined a span earlier in this chapter, so let’s move to the second requirement for service instrumentation. What is context propagation and how do we perform it in the Go <span class="No-Break" lang="">microservice code?</span></p>
			<p lang="en-GB">Context propagation is a technique that involves explicitly passing an object, called a <strong class="bold" lang="">context</strong>, into other functions in the form of an argument. The context may contain arbitrary metadata, so passing it to another function helps propagate it further. That is, each function down the stream can either add new metadata to the context or access the metadata that already exists <span class="No-Break" lang="">in it.</span></p>
			<p lang="en-GB">Let’s illustrate context propagation via <span class="No-Break" lang="">a diagram:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/Figure_11.4_B18865.jpg" alt="Figure 11.4 – Context propagation example&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Context propagation example</p>
			<p lang="en-GB">In the <a id="_idIndexMarker639"/>previous flow chart, there is an HTTP <a id="_idIndexMarker640"/>request coming from <strong class="bold" lang="">Service A</strong> to <strong class="bold" lang="">Service B</strong> to process a payment. <strong class="bold" lang="">Service A</strong> includes an additional HTTP header called <strong class="source-inline" lang="">ctx.reqId</strong> for the request to pass the request identifier. <strong class="bold" lang="">Service B</strong> calls <strong class="bold" lang="">Service C</strong> to check the user details to verify whether the user is eligible for making a payment. Then, <strong class="bold" lang="">Service B</strong> passes the <strong class="source-inline" lang="">ctx.reqId</strong> header further to <strong class="bold" lang="">Service C</strong> so that all services can record the<a id="_idIndexMarker641"/> identifier of the request, for which they perform <span class="No-Break" lang="">the operations.</span></p>
			<p lang="en-GB">The example that we just provided illustrates context propagation between three services. This is achieved by including specific HTTP headers in requests, which provide additional metadata for request processing. There are multiple ways of propagating the data when executing various operations. We will start by looking at the regular Go <span class="No-Break" lang="">function calls.</span></p>
			<p lang="en-GB">We covered Go context propagation in <a href="B18865_02.xhtml#_idTextAnchor027"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 2</em></span></a> and mentioned the <strong class="source-inline" lang="">context</strong> package, which provides a type called <strong class="source-inline" lang="">context.Context</strong>. Passing the context between two Go functions of a single service is as easy as calling another function with an additional argument, as <span class="No-Break" lang="">shown here:</span></p>
			<pre class="source-code" lang="en-GB">
func ProcessRequest(ctx context.Context, ...) {
    return ProcessAnotherRequest(ctx, ...)
}</pre>
			<p lang="en-GB">In our example, we pass the context that we receive in our function into another one, propagating it throughout the execution chain. We can attach additional metadata to the context by using the <strong class="source-inline" lang="">WithValue</strong> function, as shown in the following <span class="No-Break" lang="">code block:</span></p>
			<pre class="source-code" lang="en-GB">
func ProcessRequest(ctx context.Context, ...) {
    newCtx = context.WithValue(ctx, someKey, someValue)
    return ProcessAnotherRequest(newCtx, ...)
}</pre>
			<p lang="en-GB">In this updated example, we are passing the modified context to the other function, which will include some additional <span class="No-Break" lang="">tracing metadata.</span></p>
			<p lang="en-GB">Now, let’s connect this knowledge with the core concept of tracing – a span. A span<a id="_idIndexMarker642"/> represents an individual operation, such as a network request, that can be related to some other operations, such as the other network calls that are made during the request’s execution. In our <strong class="source-inline" lang="">getMovieDetails</strong> example, the original request would be represented as a <strong class="bold" lang="">root span</strong> or a <a id="_idIndexMarker643"/>parent span. Its child spans represent the calls to the endpoints of metadata and rating services – both calls are made as a part of <strong class="source-inline" lang="">getMovieDetails</strong> request handling. To establish the relationship between the child and parent spans, we need to pass the identifier of the parent span to its children. We can do this by propagating it through the context of each function call, as we illustrated earlier. To make this easier to understand, let’s summarize the steps for collecting the trace data for a <span class="No-Break" lang="">Go function:</span></p>
			<ol>
				<li lang="en-GB">For the <a id="_idIndexMarker644"/>original function being traced, we generate <a id="_idIndexMarker645"/>a new parent <span class="No-Break" lang="">span object.</span></li>
				<li lang="en-GB">When the function makes calls to any other functions that need to be included in the trace (for example, network calls or database requests), we pass the parent span data to them as a part of the Go <span class="No-Break" lang=""><strong class="source-inline" lang="">context</strong></span><span class="No-Break" lang=""> argument.</span></li>
				<li lang="en-GB">When a<a id="_idIndexMarker646"/> function receives a context with <a id="_idIndexMarker647"/>some parent span metadata, we include the parent span ID in the span data associated with <span class="No-Break" lang="">the function.</span></li>
				<li lang="en-GB">All the functions in the chain should follow the same steps and, at the end of each execution, report the captured <span class="No-Break" lang="">span data.</span></li>
			</ol>
			<p lang="en-GB">Now, let’s demonstrate how to use this technique in Go applications. We are going to use the OpenTelemetry Go SDK in our examples, and use Jaeger as the data source of the <span class="No-Break" lang="">tracing data:</span></p>
			<ol>
				<li lang="en-GB" value="1">Let’s start with the configuration changes. Inside each service directory, update the <strong class="source-inline" lang="">cmd/config.go</strong> file to <span class="No-Break" lang="">the following:</span><pre class="source-code" lang="en-GB">
package main</pre><pre class="source-code" lang="en-GB">
type config struct {</pre><pre class="source-code" lang="en-GB">
    API    apiConfig    `yaml:"api"`</pre><pre class="source-code" lang="en-GB">
    Jaeger jaegerConfig `yaml:"jaeger"`</pre><pre class="source-code" lang="en-GB">
}</pre><pre class="source-code" lang="en-GB">
type apiConfig struct {</pre><pre class="source-code" lang="en-GB">
    Port int `yaml:"port"`</pre><pre class="source-code" lang="en-GB">
}</pre><pre class="source-code" lang="en-GB">
type jaegerConfig struct {</pre><pre class="source-code" lang="en-GB">
    URL string `yaml:"url"`</pre><pre class="source-code" lang="en-GB">
}</pre></li>
			</ol>
			<p lang="en-GB">The configuration that we just added will help us set the Jaeger URL for submitting the <span class="No-Break" lang="">trace data.</span></p>
			<ol>
				<li lang="en-GB" value="2">The next <a id="_idIndexMarker648"/>step is to update the <strong class="source-inline" lang="">configs/base.yaml</strong> file for <a id="_idIndexMarker649"/>each service so that it includes the Jaeger API URL property. We can do this by adding the following code at <span class="No-Break" lang="">the end:</span><pre class="source-code" lang="en-GB">
jaeger:</pre><pre class="source-code" lang="en-GB">
  url: http://localhost:14268/api/traces</pre></li>
				<li lang="en-GB">Let’s create a shared function that can be used in each service to initialize the tracing data provider. This is going to submit our traces to Jaeger. In our root <strong class="source-inline" lang="">pkg</strong> directory, create <a id="_idIndexMarker650"/>a directory called <strong class="source-inline" lang="">tracing</strong> and <a id="_idIndexMarker651"/>add a <strong class="source-inline" lang="">tracing.go</strong> file with the <span class="No-Break" lang="">following contents:</span><pre class="source-code" lang="en-GB">
package tracing</pre><pre class="source-code" lang="en-GB">
import (</pre><pre class="source-code" lang="en-GB">
    "go.opentelemetry.io/otel/exporters/jaeger"</pre><pre class="source-code" lang="en-GB">
    "go.opentelemetry.io/otel/sdk/resource"</pre><pre class="source-code" lang="en-GB">
    tracesdk "go.opentelemetry.io/otel/sdk/trace"</pre><pre class="source-code" lang="en-GB">
    semconv "go.opentelemetry.io/otel/semconv/v1.12.0"</pre><pre class="source-code" lang="en-GB">
)</pre><pre class="source-code" lang="en-GB">
// NewJaegerProvider returns a new jaeger-based tracing provider.</pre><pre class="source-code" lang="en-GB">
func NewJaegerProvider(url string, serviceName string) (*tracesdk.TracerProvider, error) {</pre><pre class="source-code" lang="en-GB">
    exp, err := jaeger.New(jaeger.WithCollectorEndpoint(jaeger.WithEndpoint(url)))</pre><pre class="source-code" lang="en-GB">
    if err != nil {</pre><pre class="source-code" lang="en-GB">
        return nil, err</pre><pre class="source-code" lang="en-GB">
    }</pre><pre class="source-code" lang="en-GB">
    tp := tracesdk.NewTracerProvider(</pre><pre class="source-code" lang="en-GB">
        tracesdk.WithBatcher(exp),</pre><pre class="source-code" lang="en-GB">
        tracesdk.WithResource(resource.NewWithAttributes(</pre><pre class="source-code" lang="en-GB">
            semconv.SchemaURL,</pre><pre class="source-code" lang="en-GB">
            semconv.ServiceNameKey.String(serviceName),</pre><pre class="source-code" lang="en-GB">
        )),</pre><pre class="source-code" lang="en-GB">
    )</pre><pre class="source-code" lang="en-GB">
    return tp, nil</pre><pre class="source-code" lang="en-GB">
}</pre></li>
			</ol>
			<p lang="en-GB">Here, we are<a id="_idIndexMarker652"/> initializing the Jaeger client and <a id="_idIndexMarker653"/>using it to create the OpenTelemetry trace data provider. The provider will automatically submit the trace data that we will collect throughout the <span class="No-Break" lang="">service execution.</span></p>
			<ol>
				<li lang="en-GB" value="4">The next step is to update the <strong class="source-inline" lang="">main.go</strong> file of each service. Add the <strong class="source-inline" lang="">go.opentelemetry.io/otel</strong> import to the <strong class="source-inline" lang="">imports</strong> block of the <strong class="source-inline" lang="">main.go</strong> file for each service, and add the following code block after the first <span class="No-Break" lang=""><strong class="source-inline" lang="">log.Printf</strong></span><span class="No-Break" lang=""> call:</span><pre class="source-code" lang="en-GB">
    tp, err := tracing.NewJaegerProvider(cfg.Jaeger.URL, serviceName)</pre><pre class="source-code" lang="en-GB">
    if err != nil {</pre><pre class="source-code" lang="en-GB">
        log.Fatal(err)</pre><pre class="source-code" lang="en-GB">
    }</pre><pre class="source-code" lang="en-GB">
    defer func() {</pre><pre class="source-code" lang="en-GB">
        if err := tp.Shutdown(ctx); err != nil {</pre><pre class="source-code" lang="en-GB">
            log.Fatal(err)</pre><pre class="source-code" lang="en-GB">
        }</pre><pre class="source-code" lang="en-GB">
    }()</pre><pre class="source-code" lang="en-GB">
    otel.SetTracerProvider(tp)</pre><pre class="source-code" lang="en-GB">
    otel.SetTextMapPropagator(propagation.TraceContext{})</pre></li>
			</ol>
			<p lang="en-GB">The last two lines of the code set the global OpenTelemetry trace provider to our Jaeger-based version. These lines also enable context propagation, which will allow us to transfer the tracing data between <span class="No-Break" lang="">the services.</span></p>
			<ol>
				<li lang="en-GB" value="5">To enable <a id="_idIndexMarker654"/>client-side context propagation, update<a id="_idIndexMarker655"/> the <strong class="source-inline" lang="">internal/grpcutil/grpcutil.go</strong> file to <span class="No-Break" lang="">the following:</span><pre class="source-code" lang="en-GB">
package grpcutil</pre><pre class="source-code" lang="en-GB">
import (</pre><pre class="source-code" lang="en-GB">
    "context"</pre><pre class="source-code" lang="en-GB">
    "math/rand"</pre><pre class="source-code" lang="en-GB">
    "go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc"</pre><pre class="source-code" lang="en-GB">
    "google.golang.org/grpc"</pre><pre class="source-code" lang="en-GB">
    "google.golang.org/grpc/credentials/insecure"</pre><pre class="source-code" lang="en-GB">
    "movieexample.com/pkg/discovery"</pre><pre class="source-code" lang="en-GB">
)</pre><pre class="source-code" lang="en-GB">
// ServiceConnection attempts to select a random service // instance and returns a gRPC connection to it.</pre><pre class="source-code" lang="en-GB">
func ServiceConnection(ctx context.Context, serviceName string, registry discovery.Registry) (*grpc.ClientConn, error) {</pre><pre class="source-code" lang="en-GB">
    addrs, err := registry.ServiceAddresses(ctx, serviceName)</pre><pre class="source-code" lang="en-GB">
    if err != nil {</pre><pre class="source-code" lang="en-GB">
        return nil, err</pre><pre class="source-code" lang="en-GB">
    }</pre><pre class="source-code" lang="en-GB">
    return grpc.Dial(</pre><pre class="source-code" lang="en-GB">
        addrs[rand.Intn(len(addrs))],</pre><pre class="source-code" lang="en-GB">
        grpc.WithTransportCredentials(insecure.NewCredentials()),</pre><pre class="source-code" lang="en-GB">
        grpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),</pre><pre class="source-code" lang="en-GB">
    )</pre><pre class="source-code" lang="en-GB">
}</pre></li>
			</ol>
			<p lang="en-GB">Here, we added an<a id="_idIndexMarker656"/> OpenTelemetry-based<a id="_idIndexMarker657"/> interceptor that injects the tracing data into <span class="No-Break" lang="">each request.</span></p>
			<ol>
				<li lang="en-GB" value="6">Inside the <strong class="source-inline" lang="">main.go</strong> file of each service, change the line containing the <strong class="source-inline" lang="">grpc.NewServer()</strong> call to the following one, to enable server-side <span class="No-Break" lang="">context propagation:</span><pre class="source-code" lang="en-GB">
srv := grpc.NewServer(grpc.UnaryInterceptor(otelgrpc.UnaryServerInterceptor()))</pre></li>
			</ol>
			<p lang="en-GB">The change that we just made is similar to the previous step, just for <span class="No-Break" lang="">server-side handling.</span></p>
			<ol>
				<li lang="en-GB" value="7">The last step is <a id="_idIndexMarker658"/>to make sure all the new libraries<a id="_idIndexMarker659"/> are included in our project by running the <span class="No-Break" lang="">following command:</span><pre class="source-code" lang="en-GB">
go mod tidy</pre></li>
			</ol>
			<p lang="en-GB">With that, our services have been instrumented with tracing code and emit span data on each <span class="No-Break" lang="">API request.</span></p>
			<p lang="en-GB">Let’s test our newly added code by running our services and making some requests <span class="No-Break" lang="">to them:</span></p>
			<ol>
				<li lang="en-GB" value="1">To be able to collect the tracing data, you will need to run Jaeger locally. You can do this by running the <span class="No-Break" lang="">following command:</span><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">docker run -d --name jaeger \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -e COLLECTOR_OTLP_ENABLED=true \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 6831:6831/udp \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 6832:6832/udp \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 5778:5778 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 16686:16686 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 4317:4317 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 4318:4318 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 14250:14250 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 14268:14268 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 14269:14269 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  -p 9411:9411 \</strong></pre><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">  jaegertracing/all-in-one:1.37</strong></pre></li>
				<li lang="en-GB">Now, we can start all our services locally by executing the <strong class="source-inline" lang="">go run *.go</strong> command inside each <span class="No-Break" lang=""><strong class="source-inline" lang="">cmd</strong></span><span class="No-Break" lang=""> directory.</span></li>
				<li lang="en-GB">Let’s make some requests to our movie service. In <a href="B18865_05.xhtml#_idTextAnchor076"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 5</em></span></a>, we mentioned the <strong class="source-inline" lang="">grpcurl</strong> tool. Let’s use it again to make a manual <span class="No-Break" lang="">gRPC query:</span><pre class="source-code" lang="en-GB">
<strong class="bold" lang="">grpcurl -plaintext -d '{"movie_id":"1"}' localhost:8083 MovieService/GetMovieDetails</strong></pre></li>
			</ol>
			<p lang="en-GB">If everything was correct, we should get our trace in Jaeger. Let’s check it in the Jaeger UI by going to <strong class="source-inline" lang="">http://localhost:16686/</strong>. You should see a similar page <span class="No-Break" lang="">as follows:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/Figure_11.5_B18865.jpg" alt="Figure 11.5 – Jaeger UI&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Jaeger UI</p>
			<ol>
				<li lang="en-GB" value="4">Select the <strong class="bold" lang="">movie</strong> service<a id="_idIndexMarker660"/> in the <strong class="bold" lang="">Service</strong> field <a id="_idIndexMarker661"/>and click <strong class="bold" lang="">Find Traces</strong>. You should see some trace results, as <span class="No-Break" lang="">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/Figure_11.6_B18865.jpg" alt="Figure 11.6 – Jaeger traces for the movie service&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Jaeger traces for the movie service</p>
			<ol>
				<li lang="en-GB" value="5">If you click on the trace, you will see its visualized view, as <span class="No-Break" lang="">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/Figure_11.7_B18865.jpg" alt="Figure 11.7 – Jaeger trace view for the GetMovieDetails endpoint call&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Jaeger trace view for the GetMovieDetails endpoint call</p>
			<p lang="en-GB">On the left panel, you <a id="_idIndexMarker662"/>can see the request as a tree of<a id="_idIndexMarker663"/> spans, where the root span represents the <strong class="source-inline" lang="">MovieService/GetMovieDetails</strong> operation, which includes the calls to the <strong class="source-inline" lang="">MetadataService/GetMetadata</strong> and <strong class="source-inline" lang="">RatingService/GetAggregatedRating</strong> endpoints. Congratulations, you have set up distributed tracing for your microservices using the OpenTracing SDK! All our gRPC calls are now traced automatically, without any need to add any extra service logic. This provides us with a convenient mechanism for collecting valuable data on <span class="No-Break" lang="">service communication.</span></p>
			<p lang="en-GB">As an extra step, let’s illustrate how to add tracing for our database operations. As you can see from the trace view in the preceding screenshot, we currently don’t have any database-related spans on our graph. This is because our database logic has not been instrumented yet. Let’s demonstrate how to do <span class="No-Break" lang="">this manually:</span></p>
			<ol>
				<li lang="en-GB" value="1">Open the <strong class="source-inline" lang="">metadata/internal/repository/memory/memory.go</strong> file and add <strong class="source-inline" lang="">go.opentelemetry.io/otel</strong> to <span class="No-Break" lang="">its imports.</span></li>
				<li lang="en-GB">In the same file, add the <span class="No-Break" lang="">following constant:</span><pre class="source-code" lang="en-GB">
const tracerID = "metadata-repository-memory"</pre></li>
				<li lang="en-GB">At the beginning of the <strong class="source-inline" lang="">Get</strong> function, add the <span class="No-Break" lang="">following code:</span><pre class="source-code" lang="en-GB">
_, span := otel.Tracer(tracerID).Start(ctx, "Repository/Get")</pre><pre class="source-code" lang="en-GB">
defer span.End()</pre></li>
				<li lang="en-GB">Add a similar code block at the beginning of the <span class="No-Break" lang=""><strong class="source-inline" lang="">Put</strong></span><span class="No-Break" lang=""> function:</span><pre class="source-code" lang="en-GB">
_, span := otel.Tracer(tracerID).Start(ctx, "Repository/Put")</pre><pre class="source-code" lang="en-GB">
defer span.End()</pre></li>
			</ol>
			<p lang="en-GB">We just manually <a id="_idIndexMarker664"/>instrumented our in-memory<a id="_idIndexMarker665"/> metadata repository for emitting the trace data on its primary operations, <strong class="source-inline" lang="">Get</strong> and <strong class="source-inline" lang="">Put</strong>. Now, each call to these functions should create a span in the captured trace, allowing us to see when and how long each operation is <span class="No-Break" lang="">being executed.</span></p>
			<p lang="en-GB">Let’s test our newly added code. Restart the metadata service and make a new <strong class="source-inline" lang="">grpcurl</strong> request to the movie service provided previously. If you check for new traces in Jaeger, you should see the new one, with an <span class="No-Break" lang="">additional span:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/Figure_11.8_B18865.jpg" alt="Figure 11.8 – Jaeger trace view with an additional repository span&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Jaeger trace view with an additional repository span</p>
			<p lang="en-GB">Notice the last span in the trace view, representing the <strong class="source-inline" lang="">Repository/Get</strong> operation. It is the result of our change. Now, we can see the database operations on our traces. You can go ahead and update the rating service repository by including similar logic – follow the preceding instructions, and you should be able to make it work in the same way that we just did for the <span class="No-Break" lang="">metadata service.</span></p>
			<p lang="en-GB">When should you <a id="_idIndexMarker666"/>manually add span data to your<a id="_idIndexMarker667"/> functions? I would suggest doing this for each operation involving network calls, I/O operations (such as writing and reading from files), database writes and reads, and any other calls that can take a substantial amount of time. I would personally say that any function that takes more than 50 ms to complete is a good candidate <span class="No-Break" lang="">for tracing.</span></p>
			<p lang="en-GB">At this point, we have provided a high-level overview of Go tracing techniques, and this marks an end to our journey into telemetry data. In the next few chapters, we will continue our explorations into other fields, such as dashboarding, system-level performance analysis, and some advanced <span class="No-Break" lang="">observability techniques.</span></p>
			<h1 id="_idParaDest-168" lang="en-GB"><a id="_idTextAnchor169"/>Summary</h1>
			<p lang="en-GB">In this chapter, we covered observability by describing various techniques for analyzing the real-time performance of Go microservices and covering the main types of service telemetry data, such as logs, metrics, and traces. You learned about some of the best practices for performing logging, metric collection, and distributed tracing. We demonstrated how you can instrument your Go services to collect the telemetry data, as well as how to set up the tooling for distributed tracing. We also provided some examples of tracing the requests spanning three of the services that we implemented earlier in <span class="No-Break" lang="">this book.</span></p>
			<p lang="en-GB">The knowledge that you gained in this chapter should help you debug various performance issues of your microservices, as well as enable monitoring of various types of telemetry data. In <a href="B18865_12.xhtml#_idTextAnchor171"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 12</em></span></a>, we will demonstrate how to use the collected telemetry data to set up service alerting for detecting service-related incidents as quickly <span class="No-Break" lang="">as possible.</span></p>
			<h1 id="_idParaDest-169" lang="en-GB"><a id="_idTextAnchor170"/>Further reading</h1>
			<p lang="en-GB">To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break" lang="">following resources:</span></p>
			<ul>
				<li lang="en-GB"><em class="italic" lang="">Monitoring Distributed </em><span class="No-Break" lang=""><em class="italic" lang="">Systems</em></span><span class="No-Break" lang="">: </span><a href="https://sre.google/sre-book/monitoring-distributed-systems/"><span class="No-Break" lang="">https://sre.google/sre-book/monitoring-distributed-systems/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">Effective </em><span class="No-Break" lang=""><em class="italic" lang="">Troubleshooting</em></span><span class="No-Break" lang="">: </span><a href="https://sre.google/sre-book/effective-troubleshooting/"><span class="No-Break" lang="">https://sre.google/sre-book/effective-troubleshooting/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">Mastering Distributed </em><span class="No-Break" lang=""><em class="italic" lang="">Tracing</em></span><span class="No-Break" lang="">: </span><a href="https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464"><span class="No-Break" lang="">https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464</span></a></li>
				<li lang="en-GB">Logging best <span class="No-Break" lang="">practices: </span><a href="https://devcenter.heroku.com/articles/writing-best-practices-for-application-logs"><span class="No-Break" lang="">https://devcenter.heroku.com/articles/writing-best-practices-for-application-logs</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">Ten commandments of </em><span class="No-Break" lang=""><em class="italic" lang="">logging</em></span><span class="No-Break" lang="">: </span><a href="https://www.dataset.com/blog/the-10-commandments-of-logging/"><span class="No-Break" lang="">https://www.dataset.com/blog/the-10-commandments-of-logging/</span></a></li>
				<li lang="en-GB">Microservice logging <span class="No-Break" lang="">tips: </span><a href="https://www.techtarget.com/searchapparchitecture/tip/5-essential-tips-for-logging-microservices"><span class="No-Break" lang="">https://www.techtarget.com/searchapparchitecture/tip/5-essential-tips-for-logging-microservices</span></a></li>
				<li lang="en-GB">OpenTelemetry <span class="No-Break" lang="">documentation: </span><a href="https://opentelemetry.io/docs/"><span class="No-Break" lang="">https://opentelemetry.io/docs/</span></a></li>
				<li lang="en-GB">Beginner’s Guide to <span class="No-Break" lang="">OpenTelemetry: </span><a href="https://logz.io/learn/opentelemetry-guide/"><span class="No-Break" lang="">https://logz.io/learn/opentelemetry-guide/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">The 3 Pillars of System </em><span class="No-Break" lang=""><em class="italic" lang="">Observability</em></span><span class="No-Break" lang="">: </span><a href="https://iamondemand.com/blog/the-3-pillars-of-system-observability-logs-metrics-and-tracing/"><span class="No-Break" lang="">https://iamondemand.com/blog/the-3-pillars-of-system-observability-logs-metrics-and-tracing/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">What is </em><span class="No-Break" lang=""><em class="italic" lang="">observability?</em></span><span class="No-Break" lang="">: </span><a href="https://www.dynatrace.com/news/blog/what-is-observability-2/"><span class="No-Break" lang="">https://www.dynatrace.com/news/blog/what-is-observability-2/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">What is </em><span class="No-Break" lang=""><em class="italic" lang="">Telemetry?</em></span><span class="No-Break" lang="">: </span><a href="https://www.sumologic.com/insight/what-is-telemetry/"><span class="No-Break" lang="">https://www.sumologic.com/insight/what-is-telemetry/</span></a></li>
			</ul>
		</div>
	</body></html>