- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collecting Service Telemetry Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored the topic of service reliability and described
    various techniques for making your services more resilient to different types
    of errors. You learned that reliability-related work consists of making constant
    improvements in incident detection, mitigation, and prevention techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to take a closer look at various types of service
    performance data, which is essential for setting up service health monitoring
    and debugging and automating service incident detection. You will learn how to
    collect service logs, metrics, and traces, and how to visualize and debug communication
    between your microservices using the distributed tracing technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting service logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting service metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting service traces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s proceed to the overview of all the techniques that we are going to
    describe in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, you will need Go 1.11+ or above. You will also need
    the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**grpcurl**: [https://github.com/fullstorydev/grpcurl](https://github.com/fullstorydev/grpcurl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaeger**: [https://www.jaegertracing.io/](https://www.jaegertracing.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the GitHub code for this chapter here: [https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter11](https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter11).'
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the introduction to this chapter, we mentioned that there are different
    types of service performance data, all of which are essential for service health
    monitoring and troubleshooting. These types of data are called **telemetry data**
    and include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logs**: Messages recorded by your services that provide insights into the
    operations they perform or errors they encounter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: Performance data produced by your services, such as the number
    of registered users, API request error rate, or percentage of free disk space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traces**: Data that shows how your services perform various operations, such
    as API requests, which other services they call, which internal operations they
    perform, and how long these operations take'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Telemetry data is *immutable*: it captures events that have already happened
    to the service and provides the results of various measurements, such as service
    API response latency. When different types of telemetry data are combined, they
    become a powerful source of information about service behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to describe how to collect service telemetry
    data to monitor the health of services. There are two types of service health
    and performance monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**White-box monitoring**: Monitoring services while having access to different
    types of internally produced data. For example, you can monitor a server’s CPU
    utilization by viewing it in the system monitoring application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Black-box monitoring**: Monitoring services using only externally available
    data and indicators. In this case, you don’t know or have access to data related
    to their structure or internal behavior. For example, if a service has a publicly
    available health check API, an external system can monitor its health by calling
    that API without having access to internal service data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both types of monitoring are powered by collecting and continuously analyzing
    service performance data. In general, the more types of data you collect from
    your application, the more opportunities you get for extracting various types
    of information about its health and behavior. Let’s list some of the ways you
    can use the information about your service performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend analysis**: Detect any trends in your service performance data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is your service health getting better or worse over time?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does your API success rate change?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many new users are you getting compared to the previous day/month/year?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic graph capturing**: Capture data on how your services communicate
    with each other and with any other components, such as databases, external APIs,
    and message brokers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Automatically detect anomalies in your service behavior,
    such as sudden drops in API requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event correlation**: Detect relationships between various types of events,
    such as unsuccessful deployments and service panics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While observability opens lots of opportunities, it comes with the following
    challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collecting large datasets**: Real-time performance data often takes lots
    of space to store, especially if you have lots of services or if your services
    produce lots of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Need for specific tooling**: To collect, process, and visualize different
    types of data, such as logs, metrics, and traces, you need some extra tools. These
    tools often come at a price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex setup**: Observability tooling and infrastructure are often difficult
    to configure. To access all data coming from multiple services, you need to set
    up the proper data collection, aggregation, data retention policies, and many
    more strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to describe how to work with each type of telemetry data that you
    can collect in your microservices. For each type of data, we will provide some
    usage examples and describe the common ways of setting up the tooling for working
    with it. First, let’s proceed to look at service log collection.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting service logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Logging** is a technique that involves collecting real-time application performance
    data in the form of a time-ordered set of messages called a **log**. Here is an
    example of a service log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Logs can help us understand what was happening in the application at a particular
    moment in time. As you can see in the preceding example, the service started at
    11 P.M. and began connecting to the database a second later, finally logging a
    timeout error 10 seconds later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logs can provide lots of valuable insights about the component that emitted
    them, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Order of operations**: Logs can help us understand the logical sequence of
    operations performed by a service by showing when each operation took place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failed operations**: One of the most useful applications of logs is the ability
    to see the list of errors recorded by a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Panics**: If a service experiences an unexpected shutdown due to panic, a
    log can provide the relevant information, helping troubleshoot the issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debugging information**: Developers can log various types of additional information,
    such as request parameters or headers, that can help when debugging various issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warnings**: Logs can indicate various system-level warnings, such as low
    disk space, that can be used as notification mechanisms for preventing various
    types of errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We used logs in the services that we created in [*Chapter 2*](B18865_02.xhtml#_idTextAnchor027)
    – our services have been logging some important status messages via the built-in
    log library. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The built-in log library provides functionality for logging arbitrary text
    messages and panics. The output of the preceding operation would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By default, the log library records all logs to the `stdout` stream associated
    with the current process. But it is possible to set the output destination by
    calling the `SetOutput` function. This way, you can write your logs to files or
    send them over the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of functions are provided by the log library that can be used if
    a service experiences an unexpected or non-recoverable error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fatal`: Functions with this prefix immediately stop the execution of the process
    after logging the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`panic`: After logging the message, they call the Go `panic` function, writing
    the output of the associated error. The following is an example output of calling
    a `panic` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While the built-in log library provides a simple way of logging arbitrary text
    messages, it lacks some useful functionality that makes it easier to collect and
    process the service logs. Among the missing features is the ability to log events
    in popular serialization formats, such as JSON, which would simplify how message
    data is parsed. Another issue is that it lacks `Error` and `Errorf` functions,
    which could be used for explicitly logging errors. Since the built-in logging
    library only provides a `Print` function, it’s unclear by default whether the
    logged message indicates an error, a warning, or neither.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, the biggest missing piece in the built-in log library is the ability to
    perform structured logging. **Structured logging** is a technique that involves
    collecting log messages in the form of serialized structures, such as JSON records.
    A distinct feature of such structures, compared to arbitrary text strings, is
    that they can contain additional metadata in the form of fields – key-value records.
    This allows the service to represent the message metadata as any supported type,
    such as a number, string, or serialized record.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet includes an example of a JSON-encoded log structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may have noticed, in addition to using JSON as the output format, there
    are two additional features of the preceding log format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`level` that specifies the type of a log message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service`, which is used to indicate the service that emitted the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output format described earlier allows us to decode the log messages much
    easier. It also helps us interpret their contents based on the log level and the
    additional message fields. Additional metadata can also be used for searching:
    for example, we can search for messages that have a particular `service` field
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s focus on log-level metadata from the previous example. First, let’s
    review some of the common log levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Info**: Informational messages that do not indicate any error. An example
    of such a message is a log record indicating that the service successfully connected
    to the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error**: Messages indicating errors, such as network timeouts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warning**: Messages indicating some potential issues, such as too many open
    files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fatal**: Messages indicating critical or non-recoverable errors, such as
    insufficient memory, that make executing the service further impossible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Debug` messages is usually disabled by default, as they often generate a large
    amount of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Log levels also help us interpret log messages. Consider the following unstructured
    message produced by the built-in log library, which does not include any level
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Can you tell whether it’s a regular informational message indicating the regular
    behavior (for example, the service intentionally terminated a connection after
    performing some work), a warning, or an error? If this is an error, is it critical
    or not? Without the log level providing additional context, it is difficult to
    interpret this message.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of explicitly using log levels is the ability to enable or
    disable that ability to log specific types of levels. For example, logging `Debug`
    messages can be disabled under normal service conditions and enabled during troubleshooting.
    `Debug` messages often include much more information than regular ones, requiring
    more disk space and making it harder to navigate the other types of logs. Different
    logging libraries let us enable or disable specific levels, such as `Debug` or
    even `Info`, leaving only logs indicating warnings, errors, fatal errors, and
    panics.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review some popular Go logging libraries and focus on choosing the one
    that we would use in our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the logging library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will describe some of the existing Go logging libraries
    and review their features. This section should help you choose the logging library
    that you will use in your microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s list the features that we would like to get from the logging library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured logging**: Supports logging structured messages that may include
    additional fields in a key-value format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast performance**: Writing log messages should not have a noticeable impact
    on service performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log level support**: Enforce inclusion of a log level in message metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an additional feature that would be nice to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Printf`-like format (for example, support an `Errorf` function to log a formatted
    error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s review some of the most popular Go logging libraries. When evaluating
    library performance, we will be using the logging library benchmark data: https://github.com/uber-go/zap#performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of popular Go logging libraries includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Built-in Go log** **package** (https://pkg.go.dev/log):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Officially supported by the Go development team, included in the Go SDK
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not support structured logging and does not have built-in support for log
    levels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**zap** (https://github.com/uber-go/zap):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fastest performance among all logging libraries reviewed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-rich and supports a fast minimalistic logger, as well as a slightly
    slower one with additional features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**zerolog** (https://github.com/rs/zerolog):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple and elegant API
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`go-kit` toolkit for microservice development'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slightly slower than `zerolog` and `zap`, but faster than the other logging
    libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**apex/log** (https://github.com/apex/log):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has built-in support for various log storages, such as Elasticsearch, Graylog,
    and AWS Kinesis*   **log15** (https://github.com/inconshreveable/log15):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-rich logging toolkit
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Much slower than the other log libraries reviewed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list provides some high-level details about some of the popular
    Go logging libraries to help you choose the right one for your services. All libraries,
    except the built-in `log` package, provide the features that we need, including
    structured logging and log levels. Now, the question is, how do we select the
    best one among them?
  prefs: []
  type: TYPE_NORMAL
- en: My personal opinion is that the `zap` library provides the most flexible and
    yet most performant solution to service logging problems. It allows us to use
    two separate loggers, called `Logger` and `SugaredLogger`. `Logger` can be used
    in high-performance applications, while `SugaredLogger` can be used when you need
    some extra features; we will review these features in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using logging features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start practicing and demonstrate how to use some features of the `zap`
    logging library that we picked in the previous section. First, let’s start with
    the basics and illustrate how to log a simple message that has the `Info` level
    and an additional metadata field called `serviceName`. The complete Go code for
    this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We initialize the `logger` variable by calling the `zap.NewProduction` function,
    which returns a production-configured logger. This logger omits debug messages,
    uses JSON as the output format, and includes stack traces in the logs. Then, we
    create a structured log message by including a `serviceName` field by using the
    `zap.String` function, which can be used to log string data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `zap` library offers support for other types of Go primitives, such as
    `int`, `long`, `bool`, and many more. Corresponding functions for creating log
    field names follow the same naming format, such as `Int`, `Long`, and `Bool`.
    Additionally, `zap` includes a set of functions for the other built-in Go types,
    such as `time.Duration`. The following code shows an example of a `time.Duration`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s illustrate how to log arbitrary objects, such as structures. In [*Chapter
    2*](B18865_02.xhtml#_idTextAnchor027), we defined the `Metadata` structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s assume that we want to log the entire structure for debugging purposes.
    One way of doing so is to use the `zap.Stringer` field. This field allows us to
    log any structure or interface with the `String()` function. We can define a `String`
    function for our `Metadata` structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can log the `Metadata` structure as a log field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s illustrate one more useful technique of using the `zap` library.
    If you want to include the same fields in multiple messages, you can re-initialize
    the logger by using the `With` function, as illustrated in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The results of both calls to the `Debug` function will now include both the
    `endpoint` and `ratingId` fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use this technique when you create new service components in your
    code. In the following example, we are creating a sub-logger inside the `New`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This way, the newly created instance of a `Handler` structure will be initialized
    with a logger that includes the `component` field with the `ratingController`
    value in each message.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered some of the primary service logging use cases, let’s
    discuss how to store logs in a microservice environment.
  prefs: []
  type: TYPE_NORMAL
- en: Storing microservice logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, logs of each service instance are written to the output stream of
    the process running it. This mechanism of log collection allows us to monitor
    service operations by continuously reading the data from the associated stream
    (`stdout` in most cases) on a host running a service instance. However, without
    any additional software, the log data would not be persisted, so you would not
    be able to read your previously recorded logs after a service restart or a sudden
    crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various software solutions allow us to store and query the log data in a multi-service
    environment. They help solve multiple other problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed log collection**: If you have multiple services running on different
    hosts, you must collect the service logs on each host independently and send them
    for further aggregation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralized log storage**: To be able to query the data that’s emitted by
    different services, you need to store it in a centralized way – all logs across
    all services should be accessible during the query execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data retention**: Logging data usually takes a lot of disk space, and it
    often becomes too expensive to store it indefinitely for all your services. To
    solve this problem, you need to establish the right data retention policies for
    your services that will allow you to configure how long you can store the data
    for each one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient indexing**: To be able to quickly query your logging data, the
    logs need to be indexed and stored efficiently. Modern indexing software can help
    you query terabytes of log data in under 10 milliseconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different tools help facilitate such log operations, such as Elasticsearch and
    Graylog. Let’s briefly review Elasticsearch to provide an example of an end-to-end
    log management solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch** is a popular open source search engine that was created in
    2010 and quickly gained popularity as a scalable system for indexing and querying
    different types of structured data. While the primary use case of Elasticsearch
    is a full-text search, it can be efficiently used for storing and querying various
    types of structured data, such as service logs. Elasticsearch is also a part of
    the toolkit called the **Elastic Stack**, also called **ELK**, which includes
    some other systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logstash**: A data processing pipeline that can collect, aggregate, and transform
    various types of data, such as service logs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**: A user interface for accessing the data in Elasticsearch, providing
    convenient visualization and querying features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The log collection pipeline in the Elastic Stack looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Logging pipeline in the Elastic Stack'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.1_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Logging pipeline in the Elastic Stack
  prefs: []
  type: TYPE_NORMAL
- en: In this flow chart, service logs are collected by Logstash and sent to Elasticsearch
    for indexing and storing. Then, users can access the logs and other data indexed
    in Elasticsearch using the Kibana interface.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of the Elastic Stack is that most of its tools are
    available for free and are open source. It is well-maintained and extremely popular
    in the developer community, making it easier to search for relevant documentation,
    get additional support, or find some additional tooling. It also has a set of
    libraries for all popular languages, allowing us to perform various types of queries
    and API calls to all components of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The Go library for using the Elasticsearch API is called `go-elasticsearch`
    and can be found on GitHub at [https://github.com/elastic/go-elasticsearch](https://github.com/elastic/go-elasticsearch).
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to cover the Elastic Stack in detail as it’s outside of the
    scope of this chapter, but you can get more familiar with the Elastic Stack by
    reading its official documentation (https://www.elastic.co/guide/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Having covered some high-level details regarding some popular logging software,
    let’s move on to the next topic: describing the best practices of logging.'
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have covered the most important aspects of logging and described
    how to choose a logging library, as well as how to establish the logging infrastructure
    for collecting and analyzing data. Let’s describe some of the best practices for
    logging service data:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid using interpolated strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardize your log messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodically review your log data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up appropriate log retention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the message source in logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now cover each practice in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid using interpolated strings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the top logging anti-patterns is the usage of **interpolated strings**
    – messages that embed metadata inside text fields. Let’s take the following snippet
    of code as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem with this code is that it merges two types of data into a single
    text message: an operation name (user registration) and a user identifier. Such
    messages make it harder to search and process log metadata: each time you need
    to extract `userID` from a log message, you would need to parse a string that
    contains it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s update our example by following the structured logging approach, where
    we log additional metadata as message fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The updated version makes a big difference when you want to query your data.
    Now, you can query all log events that have `User successfully registered` text
    messages and easily access all user identifiers associated with them. Avoiding
    interpolated messages helps keep your log data easy to query and parse, simplifying
    all operations with it.
  prefs: []
  type: TYPE_NORMAL
- en: Standardize your log messages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we covered the benefits of log centralization and the advantages
    of querying the data across multiple services. But I would like to emphasize how
    it is important to standardize the format of log messages in a microservice environment.
    Sometimes, it is useful to execute log queries that span multiple services, API
    endpoint handlers, or other components. For example, you may need to perform the
    following types of queries on your log data:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the distribution of timeout errors across all services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the daily count of errors for each API endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get distinct error messages across all database repositories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your services log the data using different field names, you will not be able
    to easily gather such data using a common query function. On the opposite side,
    establishing the common field names helps ensure the log messages follow the same
    naming convention, simplifying any queries you write.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure the logs are emitted in the same way across all services and all
    components, you may follow these tips:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a shared package that includes log field names as constants; take the
    following example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To avoid forgetting to include some important field inside a certain structure,
    function, or set of functions, re-initialize the logger by setting the field as
    early as possible; take the following example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, ensure that the root logger of your service is setting the service
    name so that all your service components will automatically collect this field
    by default:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The tips that we just provided should help you standardize the usage of common
    fields across all your service components, making it easier to query the logged
    data and aggregate it in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Periodically review your log data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you start collecting your service logs, it is important to periodically
    review them. Look out for the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make sure there is no PII data in logs**: **Personally identifiable information**
    (**PII**), such as full names and SSNs, falls under many regulations and generally
    must not be stored in logs. Make sure that no component, such as an API handler
    or a repository component, emits any such data, even for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Check that your service doesn’t emit extra debug data**: Sometimes, developers
    log some additional data, such as request fields, to debug various issues. Check
    that no service is continuously emitting too many debug messages during a prolonged
    period, polluting the logs and using too much disk space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up appropriate log retention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Log data often takes a lot of space to store. If it keeps growing in size without
    any additional actions being taken, you may end up using all your disk space and
    having to urgently clean up the old records. To prevent this, various log storage
    solutions allow you to configure **retention policies** for your data. For example,
    you can configure your log storage to keep the logs for some services for up to
    a few years, while limiting some other services to just a few days, depending
    on the requirements. Additionally, you can set some size constraints so that the
    logs of your services don’t exceed a predefined size threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure you set retention policies for all types of your logs, avoiding situations
    when you need to clean up unneeded log records manually.
  prefs: []
  type: TYPE_NORMAL
- en: Identify the message source in logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine that you are viewing your system logs and notice the following error
    event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Can you understand the problem described in this event? The log record includes
    the `Request timed out` error message and has the `error` level, but it does not
    provide any meaningful context to us. Without any additional context, we can’t
    easily understand the problem that caused the log event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Providing the context of any log message is crucial for making it easy to work
    with the logs. This is especially important in a microservice environment, where
    similar operations can be performed by multiple services or components. It should
    always be easy to understand each message and have some reference to the component
    it is coming from. In this section, we already mentioned the practice of including
    some additional information, such as the name of the component, in a log event.
    Such metadata would generally include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name of the component emitting the event (for example, endpoint name)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name of the file (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more detailed version of the preceding log message looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have discussed the main topics related to logging and can
    move on to the next section, which describes another type of telemetry data –
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting service metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to describe another type of service telemetry
    data: **metrics**. To understand what metrics are and how they are different from
    log data, let’s start with an example. Imagine that you have a set of services
    providing APIs to their users, and you want to know how many times per second
    each API endpoint is called. How would you do this?'
  prefs: []
  type: TYPE_NORMAL
- en: One possible way of solving this problem is using logs. We could create a log
    event for each request, and then we would be able to count the number of events
    for each endpoint, aggregating them by second, minute, or in any other possible
    way. Such a solution would work until we get too many requests per endpoint and
    can’t log each one independently anymore. Let’s assume there is a service that
    processes more than a million requests per second. If we used logs to measure
    its performance, we would need to produce more than a million log events every
    second, generating lots of data.
  prefs: []
  type: TYPE_NORMAL
- en: A more optimal solution to this problem would be to use some sort of value-based
    aggregation. Instead of storing the data representing each request separately,
    we could summarize the count of requests per second, minute, or hour, making the
    data more optimal for storing.
  prefs: []
  type: TYPE_NORMAL
- en: The problem that we just described is a perfect use case for using metrics –
    real-time quantitative measurements of system performance, such as request rate,
    latency, or cumulative counts. Like logs, metrics are time-based – each record
    includes a timestamp representing a unique instant of time in the past. However,
    unlike log events, metrics are primarily used for storing individual values. In
    our example, the value of an endpoint request rate metric would be the count of
    requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics are generally represented as **time series** – sets of objects, called
    **data points**, containing the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: Timestamp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value (most commonly, the value is numerical)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional set of **tags**, defined as key-value pairs, that contain any additional
    metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help you better understand the use cases of using metrics, let’s define
    some common metric types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Counters**: These are time series representing the value of a cumulative
    counter over time. An example would be the counter of service requests – each
    data point would include a timestamp and the count of requests at that particular
    moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gauges**: These are time series representing the changes of a single scalar
    value over time. An example of a gauge is a dataset that contains the amount of
    free disk space on a server at different moments: each data point contains a single
    numerical value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histograms**: These are time series representing the distribution of some
    value against a predefined set of value ranges, called **buckets**. An example
    of a histogram metric is a dataset, containing the number of users for different
    age groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s focus on each metric type to help you understand their differences and
    the common use cases for each one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Counter metrics are generally used for measuring two types of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative value over time (for example, the total number of errors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change of the cumulative value over time (for example, the number of newly registered
    users per hour)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second use case is technically a different representation of the first one
    – if you know how many users you had at each moment in time, you can see how this
    value changes. Because of this, counters are often used to measure the rates of
    various events, such as API requests, over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet provides an example of a `Counter` interface in
    a `tally` metrics library (we’ll review this library later in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike counters, gauges are used for storing unique values of measurements,
    such as the service’s available memory over time. Here is a gauge example from
    a `tally` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the other gauge use cases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of goroutines running by a service instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of active connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of open files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Histograms are slightly different from counters and gauges. They require us
    to define a set of ranges that will be used to store the subsets of recorded data.
    The following are some examples of using histogram metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency tracking**: You can track how long it takes to perform a certain
    service operation by creating a set of buckets representing various duration ranges.
    For example, your buckets could be 0–100 ms, 100–200 ms, 200–300 ms, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cohort tracking**: You can track statistical data, such as the number of
    records in each group of values. For example, you can track how many users of
    each age subscribed to your service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered some high-level basics of metrics, let’s provide an
    overview of storing metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Storing metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to logs, storing metrics in a microservice environment brings some
    common challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collection and aggregation**: Metrics need to be collected from all service
    instances and sent for further aggregation and storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: Collected data needs to be aggregated, so various types of
    metrics, such as counters, would contain the data coming from all service instances.
    For example, the counter measuring the total number of requests should summarize
    the data across all service instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review some of the popular tools that provide such features.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prometheus is a popular open source monitoring solution that provides mechanisms
    for collecting and querying service metrics, as well as setting up automated alerts
    for detecting various types of incidents. Prometheus gained popularity in the
    developer community due to its simple data model and being a very flexible model
    for data ingestion, which we are going to cover in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Did you know that Prometheus is written in Go? You can check its source code
    on its GitHub page: [https://github.com/prometheus/prometheus](https://github.com/prometheus/prometheus).'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus supports three types of metrics – counters, gauges, and histograms.
    It stores each metric as a time series, similarly to the model that we described
    at the beginning of the *Collecting service metrics* section. Each metric contains
    the value, additional tags, called **labels**, and a name that can be used for
    identifying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data gets into the Prometheus time series storage, it is available
    for querying via its query language, called **PromQL**. PromQL allows us to fetch
    time series data using various functions that allow us to easily filter or exclude
    certain name and label combinations. The following is an example of a PromQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the query fetches time series with the `http_requests_total`
    name, a label that contains the environment key and production value, and any
    value of a method label that is not equal to `GET`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an official Prometheus Go client on GitHub that provides various mechanisms
    to get the metrics data into Prometheus, as well as to execute the PromQL queries.
    You can access it here: [https://github.com/prometheus/client_golang](https://github.com/prometheus/client_golang).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The documentation for instrumenting Go applications for using Prometheus can
    be found at the following link: [https://prometheus.io/docs/guides/go-application](https://prometheus.io/docs/guides/go-application).'
  prefs: []
  type: TYPE_NORMAL
- en: Graphite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graphite is another popular monitoring tool that offers metric collection, aggregation,
    and querying functionality that is similar to Prometheus. Although it has been
    among the oldest service monitoring tools in the industry, it remains an extremely
    powerful instrument for working with service metric data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical Graphite installation consists of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Carbon**: A service that listens for time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whisper**: A time series database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphite-web**: A web interface and an API for accessing the metrics data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graphite offers a quick integration with a data visualization tool called **Grafana**,
    which we are going to cover in [*Chapter 13*](B18865_13.xhtml#_idTextAnchor181)
    of this book. You can read more details about Graphite on its website: [https://graphiteapp.org](https://graphiteapp.org).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to the next section, where we will describe the popular libraries
    for emitting service metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Go metrics libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some popular Go libraries for working with metrics that could help
    you ingest and query your time series metrics data. Let’s provide a brief overview
    of some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tally** (https://github.com/uber-go/tally):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A performant and minimalistic library for emitting service metrics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in support for data ingestion in Prometheus, StatsD, and M3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rcrowley/go-metrics** (https://github.com/rcrowley/go-metrics):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Go port of a popular Java metric library (https://github.com/dropwizard/metrics)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports data ingestion into StatsD and Graphite
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Has lots of integrations for exporting data to various observability systems,
    such as Datadog and Prometheus
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`go-kit` toolkit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports multiple metric storages, such as StatsD and Graphite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will leave the decision of picking the metrics library for your services
    to you, as each library provides some useful features that you can leverage when
    developing your microservices. I am going to use the tally library in the examples
    throughout this chapter as it provides a simple and minimalistic API that can
    help illustrate the common metrics use cases. In the next section, we will review
    some of the use cases of using the metrics in the Go microservice code.
  prefs: []
  type: TYPE_NORMAL
- en: Emitting service metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will provide some examples of emitting and collecting service
    metrics while covering some common scenarios, such as measuring API request rates,
    operation latencies, and emitting gauge values. We will use the tally library
    in our examples, but you can implement this logic using all other popular metric
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s provide an example of how to initialize the tally library so that
    you can start using it in your service code. In the following example, we are
    initializing it using the StatsD client (you can use any other tool to collect
    the metrics):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are creating a tally reporter that will submit the metrics
    to the data collector (StatsD in our use case) and create a **scope** – an interface
    for reporting the metrics data – that would automatically submit them for collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tally scopes are hierarchical: when we initialize the library, we create a
    **root scope**, which includes initial metadata in the form of key-value tags.
    All scopes that are created from it would include the parent metadata, preventing
    cases of missing tags during the metric’s emission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you get the scope, you can start reporting the metrics. The following
    example illustrates how to increment a counter metric by measuring the API request
    count, which would be automatically reported by tally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `Inc` operation increments the value of the counter by `1`, and the updated
    value of the metric gets collected by tally automatically in the background. This
    does not affect the performance of the function that performs the provided operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to add some additional tags to the metric, you can use the `Tagged`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following example illustrates how to update the gauge value. Let’s say
    we have a function that calculates the number of active users in the system and
    we want to report this value to the metrics storage. We can achieve this by using
    a gauge metric in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s provide an example of reporting a time duration. A common use case
    of this is reporting the latency of various operations. In the following example,
    we are reporting how long it takes to execute our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we are initializing the `operation_latency` timer and calling
    the `Start` function of it to start measuring operation latency. The `Start` function
    returns the instance of a `Stopwatch` interface, which includes the `Stop` function.
    This reports the time it takes since the stopwatch’s start time.
  prefs: []
  type: TYPE_NORMAL
- en: 'When reporting the latency metrics, the tally library uses the default buckets,
    unless you provide their exact values. For example, when reporting the metrics
    to Prometheus, tally is using the following bucket configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s provide an example of using a histogram with a set of predefined numerical
    buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we are initializing the histogram metric using a set of predefined
    buckets from 0 to 130 and recording the value that matches the user age in years
    using it. Each bucket of the histogram will then contain the sum of the values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have provided some basic examples of emitting service metrics, let’s
    look at the best practices for working with metrics data.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will describe some of the best practices related to metric
    data collection. The list is not exhaustive, but should still be useful for setting
    up metric collection logic in your services.
  prefs: []
  type: TYPE_NORMAL
- en: Keep tag cardinality in mind
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you emit your metrics and add additional tags to time series data, keep
    in mind that most time series databases are not designed to store **high-cardinality**
    data, which can contain lots of possible tag values. For example, the following
    types of data should not be generally included in service metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Object identifiers, such as movie or rating IDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly generated data, such as UUIDs (for example, request UUIDs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason for this is indexing is that each tag key-value combination must
    be indexed to make the time series searchable, and it becomes expensive to perform
    this when there are lots of distinct tag values.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can still use some low-cardinality data in metric tags. The following are
    some possible examples:'
  prefs: []
  type: TYPE_NORMAL
- en: City ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endpoint name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember this tip to avoid reducing the throughput of your metrics pipeline
    and to ensure your services don’t emit user identifiers and other types of high-cardinality
    metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Standardize metric and tag names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you have hundreds of services, and each service follows different
    metric naming conventions. For example, one service could be using `api_errors`
    as the name of the API error counter metric, while the other could be using the
    `api_request_errors` name. If you wanted to compare the metrics for such services,
    you would need to remember which naming convention each service was using. Such
    metric discovery will always take time, reducing your ability to analyze your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: A much better solution is to standardize the names of common metrics and tags
    across all your services. This way, you can easily search and compare various
    performance indicators, such as service client and server error rates, API throughput,
    and request latency. In [*Chapter 12*](B18865_12.xhtml#_idTextAnchor171), we will
    review some common performance indicators that you can use to monitor the health
    of your services.
  prefs: []
  type: TYPE_NORMAL
- en: Set the appropriate retention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most time series databases are capable of storing large datasets of metrics
    due to efficient aggregation. Unlike logs that require you to store each record
    independently, metrics can be aggregated into a smaller dataset. For example,
    if you store the counter data, you can store the sums of the values instead of
    storing each value separately. Even with these optimizations, time series data
    can take a lot of disk space to store. Large companies can store terabytes of
    metrics data, so it becomes important to manage its size and set up data retention
    policies, similar to logs and other types of telemetry data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metric storages, such as Prometheus, have a default retention time of 15 days,
    allowing you to change it in the settings. For example, to set the data retention
    time in Prometheus to 60 days, you can use the following flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Limiting the storage retention time helps keep the size of the time series datasets
    under control, making it easier to manage the storage capacity and plan the infrastructure
    spending on data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve discussed the metrics data, let’s move on to the next section,
    which covers a powerful technique: tracing.'
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered two common types of observability data – logs and metrics.
    Having logs and metrics data in place is often sufficient for service debugging
    and troubleshooting. However, there is another type of data that is useful for
    getting insights into microservice communication and data flows.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to discuss **distributed tracing** – a technique
    that involves recording and analyzing interactions between different services
    and service components. The main idea behind distributed tracing is to automatically
    record all such interactions and provide a convenient way to visualize them. Let’s
    look at the following example, which illustrates a distributed tracing use case
    known as call analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Tracing visualization example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.2_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – Tracing visualization example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see the execution of a single `GetMovieDetails` request for our
    movie service. The data provides some insights into the operation’s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Soon after the request starts, two parallel calls come from the movie service;
    one to the metadata service and the other to the rating service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The call to the metadata service takes 100 milliseconds to complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The call to the rating service takes 1,100 milliseconds to complete, spanning
    almost the entire request processing time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data that we just extracted provided us with lots of valuable information
    for analyzing the movie service’s performance. First, it helped us understand
    how an individual request was handled by the movie service, as well as which sub-operations
    it performed. We can also see the duration of each operation and find out which
    one was slowing down the entire request. By using this data, we could troubleshoot
    endpoint performance issues, finding which components make a significant impact
    on request processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we showed the interaction between just three services, but
    tracing tools allow us to analyze the behavior of systems that use tens and even
    hundreds of services simultaneously. The following are some other use cases that
    make tracing a powerful tool for production debugging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error analysis**: Tracing allows us to visualize the errors on complex call
    paths, such as chains of calls spanning lots of different services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call path analysis**: Sometimes, you may investigate issues in systems you
    are not very familiar with. Tracing data helps you visualize the call path of
    various operations, helping you understand the logic of the services without requiring
    you to analyze their code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operation performance breakdown**: Tracing allows us to see the duration
    of individual steps of a long-running operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s describe the tracing data model so that you can get familiar with its
    common terminology. The core element of tracing is a **span** – a record representing
    some logical operation, such as a service endpoint call. Each span has the following
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Operation name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional set of tags providing some additional metadata related to the execution
    of the associated operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional set of associated logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spans can be grouped into hierarchies to model relationships between different
    operations. For example, in *Figure 11**.2*, our `GetMovieDetails` span included
    two child spans, representing `GetMetadata` and `GetAggregatedRating` operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how we can collect and use tracing data in our Go applications.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various tools for distributed tracing in a microservice environment.
    Among the most popular ones is Jaeger, which we will review in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple observability libraries and tools available for developers,
    including libraries for trace collection. To standardize the data model and make
    data interchangeable, there is an OpenTelemetry project that contains the specification
    of the tracing data model. You can get familiar with the project on its website:
    [https://opentelemetry.io](https://opentelemetry.io).'
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger is an open source distributed tracing tool that provides mechanisms for
    collecting, aggregating, and visualizing the tracing data. It offers a simple
    but highly flexible setup and a great user interface for accessing trace data.
    This is the reason why it quickly became one of the most popular observability
    tools across the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger is compatible with the OpenTelemetry specification, so it can be used
    in combination with any clients implementing the tracing specification, such as
    the Go SDK (https://opentelemetry.io/docs/instrumentation/go/). The OpenTelemetry
    SDK is currently the recommended way of emitting trace data from applications,
    so we are going to use it in our examples throughout this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general data flow for services using Jaeger looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Jaeger data flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.3_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – Jaeger data flow
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, a service using a Jaeger-compatible library is emitting
    traces to the Jaeger backend, which is storing them in the span storage. The data
    is accessible for querying and visualization via the Jaeger UI.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Jaeger is another example of an observability tool written in Go. You can check
    out the Jaeger source code on its official GitHub page: [https://github.com/jaegertracing/jaeger](https://github.com/jaegertracing/jaeger).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information about the Jaeger project on its website: [https://www.jaegertracing.io](https://www.jaegertracing.io).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples of instrumenting Go services to emit the tracing data.
    We will use the OpenTelemetry SDK in our examples to make our code compatible
    with different tracing software.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting tracing data with the OpenTelemetry SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will show you how to emit tracing data in your service code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned at the beginning of the *Tracing* section, the core benefit
    of distributed tracing is the ability to automatically capture data that shows
    how services and other network components communicate with each other. Unlike
    metrics, which measure the performance of individual operations, traces help to
    collect information on how each request or operation is handled across the entire
    network of nodes that report this data. To report the traces, service instances
    need to be **instrumented** so that they perform two distinct roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Report the data on distributed operations**: For each **traceable operation**
    – an operation spanning multiple components, such as network requests or database
    queries – an instrumented service should report spans. The report should contain
    the operation name, start time, and end time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitate context propagation**: The service should explicitly propagate
    context throughout the execution (if you are confused by this, please read the
    following paragraphs – this is the main trick behind distributed tracing!).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We already defined a span earlier in this chapter, so let’s move to the second
    requirement for service instrumentation. What is context propagation and how do
    we perform it in the Go microservice code?
  prefs: []
  type: TYPE_NORMAL
- en: Context propagation is a technique that involves explicitly passing an object,
    called a **context**, into other functions in the form of an argument. The context
    may contain arbitrary metadata, so passing it to another function helps propagate
    it further. That is, each function down the stream can either add new metadata
    to the context or access the metadata that already exists in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate context propagation via a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Context propagation example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.4_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – Context propagation example
  prefs: []
  type: TYPE_NORMAL
- en: In the previous flow chart, there is an HTTP request coming from `ctx.reqId`
    for the request to pass the request identifier. `ctx.reqId` header further to
    **Service C** so that all services can record the identifier of the request, for
    which they perform the operations.
  prefs: []
  type: TYPE_NORMAL
- en: The example that we just provided illustrates context propagation between three
    services. This is achieved by including specific HTTP headers in requests, which
    provide additional metadata for request processing. There are multiple ways of
    propagating the data when executing various operations. We will start by looking
    at the regular Go function calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered Go context propagation in [*Chapter 2*](B18865_02.xhtml#_idTextAnchor027)
    and mentioned the `context` package, which provides a type called `context.Context`.
    Passing the context between two Go functions of a single service is as easy as
    calling another function with an additional argument, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, we pass the context that we receive in our function into another
    one, propagating it throughout the execution chain. We can attach additional metadata
    to the context by using the `WithValue` function, as shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this updated example, we are passing the modified context to the other function,
    which will include some additional tracing metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s connect this knowledge with the core concept of tracing – a span.
    A span represents an individual operation, such as a network request, that can
    be related to some other operations, such as the other network calls that are
    made during the request’s execution. In our `getMovieDetails` example, the original
    request would be represented as a `getMovieDetails` request handling. To establish
    the relationship between the child and parent spans, we need to pass the identifier
    of the parent span to its children. We can do this by propagating it through the
    context of each function call, as we illustrated earlier. To make this easier
    to understand, let’s summarize the steps for collecting the trace data for a Go
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: For the original function being traced, we generate a new parent span object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the function makes calls to any other functions that need to be included
    in the trace (for example, network calls or database requests), we pass the parent
    span data to them as a part of the Go `context` argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a function receives a context with some parent span metadata, we include
    the parent span ID in the span data associated with the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the functions in the chain should follow the same steps and, at the end
    of each execution, report the captured span data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s demonstrate how to use this technique in Go applications. We are
    going to use the OpenTelemetry Go SDK in our examples, and use Jaeger as the data
    source of the tracing data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the configuration changes. Inside each service directory,
    update the `cmd/config.go` file to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The configuration that we just added will help us set the Jaeger URL for submitting
    the trace data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to update the `configs/base.yaml` file for each service so
    that it includes the Jaeger API URL property. We can do this by adding the following
    code at the end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a shared function that can be used in each service to initialize
    the tracing data provider. This is going to submit our traces to Jaeger. In our
    root `pkg` directory, create a directory called `tracing` and add a `tracing.go`
    file with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are initializing the Jaeger client and using it to create the OpenTelemetry
    trace data provider. The provider will automatically submit the trace data that
    we will collect throughout the service execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to update the `main.go` file of each service. Add the `go.opentelemetry.io/otel`
    import to the `imports` block of the `main.go` file for each service, and add
    the following code block after the first `log.Printf` call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last two lines of the code set the global OpenTelemetry trace provider to
    our Jaeger-based version. These lines also enable context propagation, which will
    allow us to transfer the tracing data between the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable client-side context propagation, update the `internal/grpcutil/grpcutil.go`
    file to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we added an OpenTelemetry-based interceptor that injects the tracing data
    into each request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `main.go` file of each service, change the line containing the `grpc.NewServer()`
    call to the following one, to enable server-side context propagation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The change that we just made is similar to the previous step, just for server-side
    handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to make sure all the new libraries are included in our project
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, our services have been instrumented with tracing code and emit span
    data on each API request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test our newly added code by running our services and making some requests
    to them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to collect the tracing data, you will need to run Jaeger locally.
    You can do this by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can start all our services locally by executing the `go run *.go` command
    inside each `cmd` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s make some requests to our movie service. In [*Chapter 5*](B18865_05.xhtml#_idTextAnchor076),
    we mentioned the `grpcurl` tool. Let’s use it again to make a manual gRPC query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If everything was correct, we should get our trace in Jaeger. Let’s check it
    in the Jaeger UI by going to `http://localhost:16686/`. You should see a similar
    page as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Jaeger UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.5_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.5 – Jaeger UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the **movie** service in the **Service** field and click **Find Traces**.
    You should see some trace results, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Jaeger traces for the movie service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.6_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – Jaeger traces for the movie service
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the trace, you will see its visualized view, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Jaeger trace view for the GetMovieDetails endpoint call'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.7_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.7 – Jaeger trace view for the GetMovieDetails endpoint call
  prefs: []
  type: TYPE_NORMAL
- en: On the left panel, you can see the request as a tree of spans, where the root
    span represents the `MovieService/GetMovieDetails` operation, which includes the
    calls to the `MetadataService/GetMetadata` and `RatingService/GetAggregatedRating`
    endpoints. Congratulations, you have set up distributed tracing for your microservices
    using the OpenTracing SDK! All our gRPC calls are now traced automatically, without
    any need to add any extra service logic. This provides us with a convenient mechanism
    for collecting valuable data on service communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an extra step, let’s illustrate how to add tracing for our database operations.
    As you can see from the trace view in the preceding screenshot, we currently don’t
    have any database-related spans on our graph. This is because our database logic
    has not been instrumented yet. Let’s demonstrate how to do this manually:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `metadata/internal/repository/memory/memory.go` file and add `go.opentelemetry.io/otel`
    to its imports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the same file, add the following constant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the beginning of the `Get` function, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a similar code block at the beginning of the `Put` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We just manually instrumented our in-memory metadata repository for emitting
    the trace data on its primary operations, `Get` and `Put`. Now, each call to these
    functions should create a span in the captured trace, allowing us to see when
    and how long each operation is being executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test our newly added code. Restart the metadata service and make a new
    `grpcurl` request to the movie service provided previously. If you check for new
    traces in Jaeger, you should see the new one, with an additional span:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Jaeger trace view with an additional repository span'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.8_B18865.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – Jaeger trace view with an additional repository span
  prefs: []
  type: TYPE_NORMAL
- en: Notice the last span in the trace view, representing the `Repository/Get` operation.
    It is the result of our change. Now, we can see the database operations on our
    traces. You can go ahead and update the rating service repository by including
    similar logic – follow the preceding instructions, and you should be able to make
    it work in the same way that we just did for the metadata service.
  prefs: []
  type: TYPE_NORMAL
- en: When should you manually add span data to your functions? I would suggest doing
    this for each operation involving network calls, I/O operations (such as writing
    and reading from files), database writes and reads, and any other calls that can
    take a substantial amount of time. I would personally say that any function that
    takes more than 50 ms to complete is a good candidate for tracing.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have provided a high-level overview of Go tracing techniques,
    and this marks an end to our journey into telemetry data. In the next few chapters,
    we will continue our explorations into other fields, such as dashboarding, system-level
    performance analysis, and some advanced observability techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered observability by describing various techniques for
    analyzing the real-time performance of Go microservices and covering the main
    types of service telemetry data, such as logs, metrics, and traces. You learned
    about some of the best practices for performing logging, metric collection, and
    distributed tracing. We demonstrated how you can instrument your Go services to
    collect the telemetry data, as well as how to set up the tooling for distributed
    tracing. We also provided some examples of tracing the requests spanning three
    of the services that we implemented earlier in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge that you gained in this chapter should help you debug various
    performance issues of your microservices, as well as enable monitoring of various
    types of telemetry data. In [*Chapter 12*](B18865_12.xhtml#_idTextAnchor171),
    we will demonstrate how to use the collected telemetry data to set up service
    alerting for detecting service-related incidents as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Monitoring Distributed* *Systems*: [https://sre.google/sre-book/monitoring-distributed-systems/](https://sre.google/sre-book/monitoring-distributed-systems/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Effective* *Troubleshooting*: [https://sre.google/sre-book/effective-troubleshooting/](https://sre.google/sre-book/effective-troubleshooting/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Distributed* *Tracing*: [https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464](https://www.packtpub.com/product/mastering-distributed-tracing/9781788628464)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logging best practices: [https://devcenter.heroku.com/articles/writing-best-practices-for-application-logs](https://devcenter.heroku.com/articles/writing-best-practices-for-application-logs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ten commandments of* *logging*: [https://www.dataset.com/blog/the-10-commandments-of-logging/](https://www.dataset.com/blog/the-10-commandments-of-logging/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microservice logging tips: [https://www.techtarget.com/searchapparchitecture/tip/5-essential-tips-for-logging-microservices](https://www.techtarget.com/searchapparchitecture/tip/5-essential-tips-for-logging-microservices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenTelemetry documentation: [https://opentelemetry.io/docs/](https://opentelemetry.io/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beginner’s Guide to OpenTelemetry: [https://logz.io/learn/opentelemetry-guide/](https://logz.io/learn/opentelemetry-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The 3 Pillars of System* *Observability*: [https://iamondemand.com/blog/the-3-pillars-of-system-observability-logs-metrics-and-tracing/](https://iamondemand.com/blog/the-3-pillars-of-system-observability-logs-metrics-and-tracing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is* *observability?*: [https://www.dynatrace.com/news/blog/what-is-observability-2/](https://www.dynatrace.com/news/blog/what-is-observability-2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is* *Telemetry?*: [https://www.sumologic.com/insight/what-is-telemetry/](https://www.sumologic.com/insight/what-is-telemetry/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
