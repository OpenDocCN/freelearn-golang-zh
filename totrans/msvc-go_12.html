<html><head></head><body>
		<div id="_idContainer053">
			<h1 id="_idParaDest-170" class="chapter-number" lang="en-GB"><a id="_idTextAnchor171"/>12</h1>
			<h1 id="_idParaDest-171" lang="en-GB"><a id="_idTextAnchor172"/>Setting Up Service Alerting</h1>
			<p lang="en-GB">In the previous chapter, we described various types of service telemetry data, such as logs, metrics and traces, and illustrated how to collect them for troubleshooting service <span class="No-Break" lang="">performance issues.</span></p>
			<p lang="en-GB">In this chapter, we will illustrate how to use telemetry data to automatically detect incidents by setting up alerts for our microservices. You will learn which types of service metrics to collect, how to define the conditions for various incidents, and how to establish the complete alerting pipeline for your microservices using a popular monitoring and alerting <span class="No-Break" lang="">tool, Prometheus.</span></p>
			<p lang="en-GB">We will cover the <span class="No-Break" lang="">following topics:</span></p>
			<ul>
				<li lang="en-GB"><span class="No-Break" lang="">Alerting basics</span></li>
				<li lang="en-GB">Introduction <span class="No-Break" lang="">to Prometheus</span></li>
				<li lang="en-GB">Setting up Prometheus alerting for <span class="No-Break" lang="">our microservices</span></li>
				<li lang="en-GB">Alerting <span class="No-Break" lang="">best practices</span></li>
			</ul>
			<p lang="en-GB">Now, we are going to proceed to the overview of <span class="No-Break" lang="">alerting basics.</span></p>
			<h1 id="_idParaDest-172" lang="en-GB"><a id="_idTextAnchor173"/>Technical requirements</h1>
			<p lang="en-GB">To complete this chapter, you will need Go 1.11+ or above. You will also need the Docker tool, which you can download <span class="No-Break" lang="">at </span><span class="No-Break" lang="">https://www.docker.com/</span><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">You can find the code examples for this chapter on <span class="No-Break" lang="">GitHub: </span><a href="https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter12"><span class="No-Break" lang="">https://github.com/PacktPublishing/microservices-with-go/tree/main/Chapter12</span></a><span class="No-Break" lang="">.</span></p>
			<h1 id="_idParaDest-173" lang="en-GB"><a id="_idTextAnchor174"/>Alerting basics</h1>
			<p lang="en-GB">No microservice operates without incidents; even if you have <a id="_idIndexMarker668"/><a id="_idIndexMarker669"/>a stable, highly tested, and well-maintained service, it can still experience various types of issues, such as <span class="No-Break" lang="">the following:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Resource constraints</strong>: A host running the service may <a id="_idIndexMarker670"/><a id="_idIndexMarker671"/>experience high CPU utilization or insufficient RAM or <span class="No-Break" lang="">disk space.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Network congestion</strong>: The service may experience<a id="_idIndexMarker672"/><a id="_idIndexMarker673"/> a sudden increase in load or decreased performance in any of its dependencies. This could limit its ability to process incoming requests or operate at the expected <span class="No-Break" lang="">performance level.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Dependency failures</strong>: Other services or libraries that <a id="_idIndexMarker674"/><a id="_idIndexMarker675"/>your service is depending on may experience various issues, affecting your <span class="No-Break" lang="">service execution.</span></li>
			</ul>
			<p lang="en-GB">Such issues can be self-resolving. For example, a slower network throughput could be a transient issue caused by temporary maintenance or a network device being restarted. Many other types of issues, which we call incidents, require some actions from the engineers to <span class="No-Break" lang="">be mitigated.</span></p>
			<p lang="en-GB">To mitigate an incident, first, we need to detect it. Once the issue is known, we can notify the engineers or perform automated actions, such as an automated <a id="_idIndexMarker676"/><a id="_idIndexMarker677"/>deployment rollback or application restart. In this chapter, we will describe the <strong class="bold" lang="">alerting technique</strong> that combines incident detection and notification. This technique can be used to automate the incident response to various types of <span class="No-Break" lang="">microservice issues.</span></p>
			<p lang="en-GB">The key principles behind alerting are pretty simple and can be summarized by the <span class="No-Break" lang="">following statements:</span></p>
			<ul>
				<li lang="en-GB">To set up alerts, developers <a id="_idIndexMarker678"/><a id="_idIndexMarker679"/>define the <span class="No-Break" lang=""><strong class="bold" lang="">alerting conditions.</strong></span></li>
				<li lang="en-GB">Alerting conditions are based on the telemetry data (most commonly, metrics) and are defined in the form <span class="No-Break" lang="">of queries.</span></li>
				<li lang="en-GB">Each defined alerting condition is evaluated periodically, such as <span class="No-Break" lang="">every minute.</span></li>
				<li lang="en-GB">If the alerting condition is met, the associated actions are executed (for example, an email or an SMS is sent to <span class="No-Break" lang="">an engineer).</span></li>
			</ul>
			<p lang="en-GB">To illustrate how alerting works, imagine that one of your services is emitting a metric called <strong class="source-inline" lang="">active_user_count</strong> that reports the number of active users at a particular moment. Let’s assume that we would like to get notified if the number of active users suddenly drops to zero. Such a situation would likely indicate some incident with our service unless we have too few users (for simplicity, we will assume our system should always have some <span class="No-Break" lang="">active users).</span></p>
			<p lang="en-GB">Using pseudocode, we could define the alerting condition for our use case in the <span class="No-Break" lang="">following way:</span></p>
			<pre class="source-code" lang="en-GB">
active_user_count == 0</pre>
			<p lang="en-GB">Once the alerting condition has been met, the alerting software would check actions that should be triggered based on its configuration. Assuming that we have configured our alerts to trigger email notifications, it would send the emails and include any necessary <a id="_idIndexMarker680"/><a id="_idIndexMarker681"/>metadata. The metadata would include information such as which incident just occurred and, if provided, the steps to <span class="No-Break" lang="">mitigate it.</span></p>
			<p lang="en-GB">We will provide some examples of alerting configurations later in this chapter. For now, we will focus on some practical use cases, providing you with some ideas for setting up alerting for <span class="No-Break" lang="">your services.</span></p>
			<h2 id="_idParaDest-174" lang="en-GB"><a id="_idTextAnchor175"/>Alerting use cases</h2>
			<p lang="en-GB">There are many use cases for which you would need to <a id="_idIndexMarker682"/><a id="_idIndexMarker683"/>set up automated alerts. In this section, we will provide some common examples that can act as a reference point <span class="No-Break" lang="">for you.</span></p>
			<p lang="en-GB">In the <em class="italic" lang="">Google SRE</em> book we mentioned earlier in <a href="B18865_10.xhtml#_idTextAnchor139"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 10</em></span></a>, there was a definition of <strong class="bold" lang="">The Four Golden Signals</strong> of monitoring, which can be used to monitor various types of applications, from microservices to data processing pipelines. These signals provide a great basis for service alerting, so let’s review them and describe how you can use each one to increase your <span class="No-Break" lang="">service reliability:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Latency</strong>: Latency is a measure of <a id="_idIndexMarker684"/><a id="_idIndexMarker685"/>processing time, such as the duration of processing an API request, a Kafka message, or any other operation. It is the main indicator of system performance – when it gets too high, the system starts affecting its callers, creating network congestion. You should generally track the latency of your primary operations, such as API endpoints providing the <span class="No-Break" lang="">critical functionality.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Traffic</strong>: Traffic measures the load on <a id="_idIndexMarker686"/><a id="_idIndexMarker687"/>your system, such as the number of requests your microservices are getting at the current moment. An example of a traffic-based metric is an API request rate, measured as the number of requests per second. Measuring traffic is important for ensuring you have enough capacity to handle the requests to <span class="No-Break" lang="">your system.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Errors</strong>: Errors are often measured as<a id="_idIndexMarker688"/><a id="_idIndexMarker689"/> the <strong class="bold" lang="">error rate</strong> or the ratio between the failed and total operations. Measuring the error rate is critical for ensuring your services <span class="No-Break" lang="">remain operational.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Saturation</strong>: Saturation generally measures the<a id="_idIndexMarker690"/><a id="_idIndexMarker691"/> utilization of your resources, such as RAM or disk usage, CPU, or I/O load. You should keep track of saturation to ensure your services don’t fail unexpectedly due to <span class="No-Break" lang="">resource insufficiency.</span></li>
			</ul>
			<p lang="en-GB">These Four Golden Signals can help you establish monitoring and alerting for your services and critical operations, such as your primary API endpoints. Let’s provide some practical examples to help you understand some common alerting <span class="No-Break" lang="">use cases.</span></p>
			<p lang="en-GB">First, let’s start with the common signals for API alerting that can be measured either across all endpoints or on a <span class="No-Break" lang="">per-endpoint basis:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">API client error rate</strong>: The ratio between the requests<a id="_idIndexMarker692"/><a id="_idIndexMarker693"/> that fail due to client errors and <span class="No-Break" lang="">all requests</span></li>
				<li lang="en-GB"><strong class="bold" lang="">API server error rate</strong>: The ratio between the requests that fail due<a id="_idIndexMarker694"/><a id="_idIndexMarker695"/> to server errors and <span class="No-Break" lang="">all requests</span></li>
				<li lang="en-GB"><strong class="bold" lang="">API latency</strong>: The time it takes to<a id="_idIndexMarker696"/><a id="_idIndexMarker697"/> <span class="No-Break" lang="">process requests</span></li>
			</ul>
			<p lang="en-GB">Now, let’s provide some examples of signals for measuring <span class="No-Break" lang="">system saturation:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">CPU utilization</strong>: How much your CPUs are being used on a scale from 0% (unused/idle) to 100% (fully used, no <span class="No-Break" lang="">extra capacity).</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Memory utilization</strong>: Ratio between <a id="_idIndexMarker698"/><a id="_idIndexMarker699"/>the used and <span class="No-Break" lang="">total memory.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Disk utilization</strong>: Percentage of used <span class="No-Break" lang="">disk space.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Open file descriptors</strong>: File descriptors are often used to handle network requests, file writes and reads, and other I/O operations. There is usually a limit on the number of open file descriptors per process, so if your service reaches a critical limit (based on your OS settings), your service may fail to <span class="No-Break" lang="">serve requests.</span></li>
			</ul>
			<p lang="en-GB">Let’s also provide some examples of other <a id="_idIndexMarker700"/><a id="_idIndexMarker701"/>signals <span class="No-Break" lang="">to monitor:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Service panics</strong>: The general recommendation is not to tolerate any service panics, as they often signal application bugs or issues such as <span class="No-Break" lang="">out-of-memory errors.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Failed deployments</strong>: You can automate the detection of failed deployments and emit a metric indicating the failure, using it to create <span class="No-Break" lang="">automated alerts.</span></li>
			</ul>
			<p lang="en-GB">Now that we have covered some common alerting use cases, let’s proceed to the overview of Prometheus, which we will use to set up our <span class="No-Break" lang="">microservice alerts.</span></p>
			<h1 id="_idParaDest-175" lang="en-GB"><a id="_idTextAnchor176"/>Introduction to Prometheus</h1>
			<p lang="en-GB">In <a href="B18865_11.xhtml#_idTextAnchor152"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 11</em></span></a>, we mentioned a popular <a id="_idIndexMarker702"/><a id="_idIndexMarker703"/>open source alerting and monitoring tool called Prometheus that can collect service metrics and set up automated alerts based on the metric data. In this section, we will demonstrate how to use Prometheus to set up alerts for <span class="No-Break" lang="">our microservices.</span></p>
			<p lang="en-GB">Let’s summarize our learning about Prometheus from <a href="B18865_11.xhtml#_idTextAnchor152"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 11</em></span></a><span class="No-Break" lang="">:</span></p>
			<ul>
				<li lang="en-GB">Prometheus allows us to collect and store service metrics in the form of a <span class="No-Break" lang="">time series.</span></li>
				<li lang="en-GB">There are three types of metrics – counters, histograms, <span class="No-Break" lang="">and gauges.</span></li>
				<li lang="en-GB">To query metrics data, Prometheus offers a query language <span class="No-Break" lang="">called PromQL.</span></li>
				<li lang="en-GB">Service alerts can be configured using a tool <span class="No-Break" lang="">called Alertmanager.</span></li>
			</ul>
			<p lang="en-GB">Metrics can be imported from service<a id="_idIndexMarker704"/><a id="_idIndexMarker705"/> instances into Prometheus in two <span class="No-Break" lang="">different ways:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Scraping</strong>: Prometheus reads metrics<a id="_idIndexMarker706"/><a id="_idIndexMarker707"/> from <span class="No-Break" lang="">service instances.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Pushing</strong>: The service instance sends<a id="_idIndexMarker708"/><a id="_idIndexMarker709"/> metrics to Prometheus using a dedicated service, the <span class="No-Break" lang="">Prometheus Pushgateway.</span></li>
			</ul>
			<p lang="en-GB">Scraping is the recommended way of setting up metric data ingestion in Prometheus. Each service instance needs to expose an endpoint to provide<a id="_idIndexMarker710"/><a id="_idIndexMarker711"/> the metrics, and Prometheus takes care of pulling the data and storing it for further querying, as shown in the <span class="No-Break" lang="">following diagram:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/Figure_12.1_B18865.jpg" alt="Figure 12.1 – Prometheus scraping model&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Prometheus scraping model</p>
			<p lang="en-GB">Let’s provide an example of a service instance response to a scraping request by Prometheus. Let’s assume you add a separate HTTP API endpoint called <strong class="source-inline" lang="">/metrics</strong> and return the newest service instance metrics in the <span class="No-Break" lang="">following format:</span></p>
			<pre class="source-code" lang="en-GB">
active_user_count 755
api_requests_total_count 18900
api_requests_getuser_count 500</pre>
			<p lang="en-GB">In this example, the service instance reports three metrics in the form of key-value pairs, where the key defines a time series name and the value defines the value of the time series at the current moment. Once Prometheus calls the <strong class="source-inline" lang="">/metrics</strong> endpoint, the service instance should provide a new dataset containing only time series that have not been included in <span class="No-Break" lang="">previous responses.</span></p>
			<p lang="en-GB">Once Prometheus collects the metrics, they become available for querying using a Prometheus-specific language called PromQL. PromQL-based queries can be used to analyze the time series data through the Prometheus UI or to set up automated alerts using Alertmanager. For example, the following query returns all values of the <strong class="source-inline" lang="">active_user_count</strong> time series, as well as <span class="No-Break" lang="">their tags:</span></p>
			<pre class="source-code" lang="en-GB">
active_user_count</pre>
			<p lang="en-GB">You can use additional<a id="_idIndexMarker712"/> query filters, called <strong class="bold" lang="">matchers</strong>, to include only specific data points. For example, if multiple services emit the <strong class="source-inline" lang="">active_user_count</strong> metric, you can only request<a id="_idIndexMarker713"/> time series that have a particular <span class="No-Break" lang="">tag value:</span></p>
			<pre class="source-code" lang="en-GB">
active_user_count{service="rating-ui"}</pre>
			<p lang="en-GB">Alerting conditions are generally defined as expressions that return Boolean results. For example, to define the alerting condition when the active user count drops to zero, you would use the following PromQL query with the <strong class="source-inline" lang="">==</strong> <span class="No-Break" lang="">operator:</span></p>
			<pre class="source-code" lang="en-GB">
active_user_count == 0</pre>
			<p lang="en-GB">The PromQL language provides some other types of time series matchers, such as <strong class="source-inline" lang="">quantile</strong>, which can be used to perform various aggregations. The following query example can be used to check whether the median <strong class="source-inline" lang="">api_request_latency</strong> value <span class="No-Break" lang="">exceeds </span><span class="No-Break" lang=""><strong class="source-inline" lang="">1</strong></span><span class="No-Break" lang="">:</span></p>
			<pre class="source-code" lang="en-GB">
api_request_latency{quantile="0.5"} &gt; 1</pre>
			<p lang="en-GB">You can become familiar with <a id="_idIndexMarker714"/>the other aspects of the PromQL language by reading the official documentation on its website: <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">https://prometheus.io/docs/prometheus/latest/querying/basics/</a>. Now, let’s explore how to set up alerts using the Prometheus alerting <span class="No-Break" lang="">tool, Alertmanager.</span></p>
			<p lang="en-GB">Alertmanager is a separate component of Prometheus that allows us to configure alerts and notifications to detect various types of incidents. Alertmanager operates by reading the provided configuration and querying Prometheus time series data periodically. Let’s provide an example of <span class="No-Break" lang="">Alertmanager’s configuration:</span></p>
			<pre class="source-code" lang="en-GB">
groups:
- name: Availability alerts
  rules:
  - alert: Rating service down
    expr: service_availability{service="rating"} == 0
    for: 3m
    labels:
      severity: page
    annotations:
      title: Rating service availability down
      description: No available instance of the rating service.</pre>
			<p lang="en-GB">In our configuration example, we set an alert for when the value of the <strong class="source-inline" lang="">service_availability</strong> metric, which has a <strong class="source-inline" lang="">service="rating"</strong> tag, is equal to <strong class="source-inline" lang="">0</strong> for <strong class="source-inline" lang="">3</strong> minutes or more, triggering a PagerDuty incident to notify the on-call engineer about <span class="No-Break" lang="">the issue.</span></p>
			<p lang="en-GB">Some other features of Alertmanager include<a id="_idIndexMarker715"/> notification grouping, notification retries, and alert suppression. To illustrate how Prometheus and Alertmanager work in practice, let’s describe how to set them up for our example microservices from the <span class="No-Break" lang="">previous chapters.</span></p>
			<h1 id="_idParaDest-176" lang="en-GB"><a id="_idTextAnchor177"/>Setting up Prometheus alerting for our microservices</h1>
			<p lang="en-GB">In this section, we will illustrate how to <a id="_idIndexMarker716"/>set up service alerting using Prometheus and its alerting extension, Alertmanager, for the services <a id="_idIndexMarker717"/>we created in previous chapters. You will learn how to expose the service metrics for collection, how to set up Prometheus and Alertmanager to aggregate and store the metrics from multiple services, and how to define and process <span class="No-Break" lang="">service alerts.</span></p>
			<p lang="en-GB">Our high-level approach is <span class="No-Break" lang="">as follows:</span></p>
			<ol>
				<li lang="en-GB">Set up Prometheus metric reporting to <span class="No-Break" lang="">our services.</span></li>
				<li lang="en-GB">Install Prometheus and configure it to scrape the data from the three example services that we created in <span class="No-Break" lang="">previous chapters.</span></li>
				<li lang="en-GB">Configure service availability alerts <span class="No-Break" lang="">using Alertmanager.</span></li>
				<li lang="en-GB">Test our alerts by triggering an alerting condition and <span class="No-Break" lang="">running Alertmanager.</span></li>
			</ol>
			<p lang="en-GB">Let’s start by illustrating <a id="_idIndexMarker718"/>how to integrate our services with Prometheus. To do this, we need to add a metric<a id="_idIndexMarker719"/> collection to our services by exposing an endpoint that will provide the newest metrics <span class="No-Break" lang="">to Prometheus.</span></p>
			<p lang="en-GB">First, we need to add Prometheus configuration to our services. In each service directory, update the <strong class="source-inline" lang="">cmd/config.go</strong> file to <span class="No-Break" lang="">the following:</span></p>
			<pre class="source-code" lang="en-GB">
package main
type config struct {
    API        apiConfig        `yaml:"api"`
    Jaeger     jaegerConfig     `yaml:"jaeger"`
    Prometheus prometheusConfig `yaml:"prometheus"`
}
type apiConfig struct {
    Port int `yaml:"port"`
}
type jaegerConfig struct {
    URL string `yaml:"url"`
}
type prometheusConfig struct {
    MetricsPort int `yaml:"metricsPort"`
}</pre>
			<p lang="en-GB">Our new configuration allows us to specify the service port of the metric collection endpoint. Inside each <strong class="source-inline" lang="">configs/base.yaml</strong> file, add the <span class="No-Break" lang="">following block:</span></p>
			<pre class="source-code" lang="en-GB">
prometheus:
  metricsPort: 8091</pre>
			<p lang="en-GB">We are ready to update<a id="_idIndexMarker720"/> our services so that they can start <a id="_idIndexMarker721"/>reporting the metrics. Update the <strong class="source-inline" lang="">main.go</strong> file of each service by adding the <span class="No-Break" lang="">following imports:</span></p>
			<pre class="source-code" lang="en-GB">
    "github.com/uber-go/tally"
    "github.com/uber-go/tally/prometheus"</pre>
			<p lang="en-GB">In any part of the <strong class="source-inline" lang="">main</strong> function, add the <span class="No-Break" lang="">following code:</span></p>
			<pre class="source-code" lang="en-GB">
    reporter := prometheus.NewReporter(prometheus.Options{})
    _, closer := tally.NewRootScope(tally.ScopeOptions{
        Tags:           map[string]string{"service": "metadata"},
        CachedReporter: reporter,
    }, 10*time.Second)
    defer closer.Close()
    http.Handle("/metrics", reporter.HTTPHandler())
    go func() {
        if err := http.ListenAndServe(fmt.Sprintf(":%d", cfg.Prometheus.MetricsPort), nil); err != nil {
            logger.Fatal("Failed to start the metrics handler", zap.Error(err))
        }
    }()
    counter := scope.Tagged(map[string]string{
        "service": "metadata",
    }).Counter("service_started")
    counter.Inc(1)</pre>
			<p lang="en-GB">In the code we just added, we initialized the <strong class="source-inline" lang="">tally</strong> library to collect and report the metrics data, which we mentioned in <a href="B18865_11.xhtml#_idTextAnchor152"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 11</em></span></a> of this book. We used a built-in Prometheus reporter <a id="_idIndexMarker722"/>that implements metric data <a id="_idIndexMarker723"/>collection using the Prometheus time series format and exposed an HTTP endpoint to allow Prometheus to collect <span class="No-Break" lang="">our data.</span></p>
			<p lang="en-GB">Let’s test the newly added endpoint. Restart the metadata service and try accessing the new endpoint by opening <strong class="source-inline" lang="">http://localhost:8091/metrics</strong> in your browser. You should get a <span class="No-Break" lang="">similar response:</span></p>
			<pre class="source-code" lang="en-GB">
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
...</pre>
			<p lang="en-GB">The response of the metrics handler includes the Go runtime data, such as the number of goroutines at the current moment, the Go library version, and many other <span class="No-Break" lang="">useful metrics.</span></p>
			<p lang="en-GB">Now, we are ready to set up Prometheus alerting. Inside the <strong class="source-inline" lang="">src</strong> directory of our project, create a directory called <strong class="source-inline" lang="">configs</strong> and add a <strong class="source-inline" lang="">prometheus.yaml</strong> file with the <span class="No-Break" lang="">following contents:</span></p>
			<pre class="source-code" lang="en-GB">
global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s
alerting:
  alertmanagers:
  - follow_redirects: true
    enable_http2: true
    scheme: http
    timeout: 10s
    api_version: v2
    static_configs:
    - targets:
      - host.docker.internal:9093</pre>
			<p lang="en-GB">Additionally, add<a id="_idIndexMarker724"/> the following configuration to <span class="No-Break" lang="">the</span><span class="No-Break" lang=""><a id="_idIndexMarker725"/></span><span class="No-Break" lang=""> file:</span></p>
			<pre class="source-code" lang="en-GB">
rule_files:
- alerts.rules
scrape_configs:
- job_name: prometheus
  honor_timestamps: true
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: http
  follow_redirects: true
  enable_http2: true
  static_configs:
  - targets:
    - localhost:9090
  - targets:
    - host.docker.internal:8091
    labels:
      service: metadata
  - targets:
    - host.docker.internal:8092
    labels:
      service: rating
  - targets:
    - host.docker.internal:8093
    labels:
      service: movie</pre>
			<p lang="en-GB">Let’s describe the configuration that we just added. We set the scraping interval provided to <strong class="source-inline" lang="">15</strong> seconds and provided<a id="_idIndexMarker726"/> a set of targets to scrape the metrics data, which includes the address of each of our services. You may<a id="_idIndexMarker727"/> notice that we are using the <strong class="source-inline" lang="">host.docker.internal</strong> network address in each target definition — we will run Prometheus using Docker and the <strong class="source-inline" lang="">host.docker.internal</strong> address will allow it to access our newly added endpoints running outside <span class="No-Break" lang="">of Docker.</span></p>
			<p lang="en-GB">Note that we provided a static list of service addresses inside the <strong class="source-inline" lang="">static_configs</strong> block. We did this intentionally to illustrate the simplest scraping approach, which is when Prometheus knows the address of each service instance. In a dynamic environment, where service instances can be added or removed, you would need to use Prometheus with a service registry, such as Consul. Prometheus provides built-in support for scraping metrics from services registered with Consul: instead of <strong class="source-inline" lang="">static_configs</strong>, you could define the Consul <span class="No-Break" lang="">scraping configuration:</span></p>
			<pre class="source-code" lang="en-GB">
    consul_sd_configs:
    - server:  host.docker.internal:8500
      services:
        - &lt;SERVICE_NAME&gt;</pre>
			<p lang="en-GB">Next, we will demonstrate how to scrape a static list of service instances; you can try setting up Consul-based <a id="_idIndexMarker728"/>Prometheus scraping as an<a id="_idIndexMarker729"/> additional exercise after reading this chapter. Let’s add alerting rules for our services. Inside the newly added <strong class="source-inline" lang="">configs</strong> directory, create the <strong class="source-inline" lang="">alerts.rules</strong> file and add the following <span class="No-Break" lang="">to it:</span></p>
			<pre class="source-code" lang="en-GB">
groups:
- name: Service availability
  rules:
  - alert: Metadata service down
    expr: up{service="metadata"} == 0
    labels:
      severity: warning
    annotations:
      title: Metadata service is down
      description: Failed to scrape {{ $labels.service }}. Service possibly down.
  - alert: Rating service down
    expr: up{service="rating"} == 0
    labels:
      severity: warning
    annotations:
      title: Metadata service is down
      description: Failed to scrape {{ $labels.service }} service on {{ $labels.instance }}. Service possibly down.
  - alert: Movie service down
    expr: up{service="movie"} == 0
    labels:
      severity: warning
    annotations:
      title: Metadata service is down
      description: Failed to scrape {{ $labels.service }} service on {{ $labels.instance }}. Service possibly down.</pre>
			<p lang="en-GB">The file we just added <a id="_idIndexMarker730"/>includes the alert definitions for<a id="_idIndexMarker731"/> each of our services. Each alert definition includes the expression Prometheus would check to evaluate whether an associated alert should <span class="No-Break" lang="">be fired.</span></p>
			<p lang="en-GB">Now, we are ready to install and run Prometheus to test our alerting. Inside the <strong class="source-inline" lang="">src</strong> directory of our project, run the following command to run Prometheus using the newly <span class="No-Break" lang="">created configuration:</span></p>
			<pre class="source-code" lang="en-GB">
docker run \
    -p 9090:9090 \
    -v configs:/etc/prometheus \
    prom/prometheus</pre>
			<p lang="en-GB">If everything is successful, you should be able to access the Prometheus UI by opening <strong class="source-inline" lang="">http://localhost:9090/</strong>. On the initial screen, you will see the search input you can use to access the Prometheus metrics emitted by our services. Type <strong class="source-inline" lang="">up</strong> into the search input and click <strong class="bold" lang="">Execute</strong> to access <span class="No-Break" lang="">the metrics:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/Figure_12.2_B18865.jpg" alt="Figure 12.2 – Prometheus metrics search&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Prometheus metrics search</p>
			<p lang="en-GB">You can go to the <strong class="bold" lang="">Alerts</strong> tab to <a id="_idIndexMarker732"/>see the currently configured alerts that we defined in our <span class="No-Break" lang=""><strong class="source-inline" lang="">alerts.rules</strong></span><span class="No-Break" lang=""> file:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/Figure_12.3_B18865.jpg" alt="Figure 12.3 – Prometheus Alerts view&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Prometheus Alerts view</p>
			<p lang="en-GB">If all three services are<a id="_idIndexMarker733"/> running, all three associated alerts should be marked as <strong class="bold" lang="">inactive</strong>. We will get back to the <strong class="bold" lang="">Alerts</strong> page shortly; for now, let’s proceed and set up Alertmanager so that we can trigger some alerts for <span class="No-Break" lang="">our services.</span></p>
			<p lang="en-GB">Inside our <strong class="source-inline" lang="">configs</strong> directory, including the Prometheus configuration, add a file called <strong class="source-inline" lang="">alertmanager.yml</strong> with the <span class="No-Break" lang="">following contents:</span></p>
			<pre class="source-code" lang="en-GB">
global:
  resolve_timeout: 5m
route:
  repeat_interval: 1m
  receiver: 'email'
receivers:
- name: 'email'
  email_configs:
  - to: 'your_email@gmail.com'
    from: 'your_email@gmail.com'
    smarthost: smtp.gmail.com:587
    auth_username: 'your_email@gmail.com'
    auth_identity: 'your_email@gmail.com'
    auth_password: 'your_password'</pre>
			<p lang="en-GB">Update the email<a id="_idIndexMarker734"/> configuration in the file we just <a id="_idIndexMarker735"/>created so that Alertmanager can send some emails for <span class="No-Break" lang="">our alerts.</span></p>
			<p lang="en-GB">Now, run the following command to <span class="No-Break" lang="">start Alertmanager:</span></p>
			<pre class="source-code" lang="en-GB">
docker run -p 9093:9093 -v &lt;PATH_TO_CONFIGS_DIR&gt;:/etc/alertmanager prom/alertmanager --config.file=/etc/alertmanager/alertmanager.yml</pre>
			<p lang="en-GB">Don’t forget to replace the <strong class="source-inline" lang="">&lt;PATH_TO_CONFIGS_DIR&gt;</strong> placeholder with the full local path to the <strong class="source-inline" lang="">configs</strong> directory containing the newly added <span class="No-Break" lang=""><strong class="source-inline" lang="">alertmanager.yml</strong></span><span class="No-Break" lang=""> file.</span></p>
			<p lang="en-GB">Now, let’s simulate the alerting condition by manually stopping the rating and movie services. Once you do this, open the <strong class="bold" lang="">Alerts</strong> page in the Prometheus UI; you should see that both alerts <span class="No-Break" lang="">are </span><span class="No-Break" lang=""><strong class="bold" lang="">firing</strong></span><span class="No-Break" lang="">:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/Figure_12.4_B18865.jpg" alt="Figure 12.4 – Firing Prometheus alerts&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Firing Prometheus alerts</p>
			<p lang="en-GB">You can access the <a id="_idIndexMarker736"/>Alertmanager UI by <a id="_idIndexMarker737"/>going <span class="No-Break" lang="">to </span><span class="No-Break" lang=""><strong class="source-inline" lang="">http://localhost:9093</strong></span><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">If alerts are fired in Prometheus, you should also see them in the <span class="No-Break" lang="">Alertmanager UI:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/Figure_12.5_B18865.jpg" alt="Figure 12.5 – The Alertmanager UI&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – The Alertmanager UI</p>
			<p lang="en-GB">If you configured Alertmanager correctly, you should get an email to the address you provided in the configuration. If you haven’t received an email, check the Docker logs of Alertmanager – users with two-factor email authentication may receive additional instructions for <span class="No-Break" lang="">enabling notifications.</span></p>
			<p lang="en-GB">If everything worked well – congratulations, you have set up service alerting! We intentionally haven’t <a id="_idIndexMarker738"/>covered many of Alertmanager’s features – it includes many configurable settings that are outside the scope of this <a id="_idIndexMarker739"/>chapter. If you <a id="_idIndexMarker740"/>are interested in learning more about it, check the official documentation <span class="No-Break" lang="">at </span><a href="https://prometheus.io/docs"><span class="No-Break" lang="">https://prometheus.io/docs</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">Now, let’s proceed to the next section, where we will provide some best practices for setting up service alerting that should help you increase your <span class="No-Break" lang="">service reliability.</span></p>
			<h1 id="_idParaDest-177" lang="en-GB"><a id="_idTextAnchor178"/>Alerting best practices</h1>
			<p lang="en-GB">The knowledge you<a id="_idIndexMarker741"/> will gain by reading this section should be useful for establishing the new alerting process for your services. It will also help you improve existing alerts if you are working with some established <span class="No-Break" lang="">alerting processes.</span></p>
			<p lang="en-GB">Among the most valuable best practices, I would highlight the <span class="No-Break" lang="">following ones:</span></p>
			<ul>
				<li lang="en-GB"><strong class="bold" lang="">Keep your alerts immediately actionable</strong>: Alerting is a powerful technique for ensuring any issues or incidents get acknowledged and addressed. However, you should not overuse it for the types of issues that do not require immediate attention. Some types of alerts, such as alerts indicating high saturation, are not necessarily actionable. For example, a sudden increase in CPU load may not indicate any immediately actionable issue, unless it remains high for some prolonged period (for example, the CPU load not going below 85% for more than 10 minutes), and may just be a transient symptom of high service usage. When creating alerts, think about whether an engineer needs to perform any manual action as a result of a notification, and reduce any possible noise as much as possible (for example, specify how long the metric should be breaching the threshold before an alert gets fired by providing the <strong class="source-inline" lang="">for</strong> value in the <span class="No-Break" lang="">rule configuration).</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Include the runbook references</strong>: For each alert, ensure you have a runbook in place that provides clear instructions to the on-call engineers receiving it. Having an accurate and up-to-date runbook for each alert helps reduce the incident mitigation time and share relevant knowledge among <span class="No-Break" lang="">all engineers.</span></li>
				<li lang="en-GB"><strong class="bold" lang="">Ensure the alerting configuration is reviewed periodically</strong>: The best solution for ensuring the alerting configuration is accurate is to make it easy to access and review. One of the easiest solutions is to make the alerting configuration a part <a id="_idIndexMarker742"/>of your code base so that all alert configurations are easily reviewable. Perform periodic checks of your alerts to ensure all important scenarios are covered, as well as to ensure no alerts <span class="No-Break" lang="">are outdated.</span></li>
			</ul>
			<p lang="en-GB">This list contains just a handful of best practices to improve your service alerting. If you are interested in the topic, I strongly suggest that you read the relevant chapters of the <em class="italic" lang="">Google SRE</em> book, including the <em class="italic" lang="">Monitoring Distributed Systems</em> chapter <span class="No-Break" lang="">from </span><a href="https://sre.google/sre-book/monitoring-distributed-systems/"><span class="No-Break" lang="">https://sre.google/sre-book/monitoring-distributed-systems/</span></a><span class="No-Break" lang="">.</span></p>
			<p lang="en-GB">This summarizes a brief overview of service alerting. Now, let’s summarize <span class="No-Break" lang="">this chapter.</span></p>
			<h1 id="_idParaDest-178" lang="en-GB"><a id="_idTextAnchor179"/>Summary</h1>
			<p lang="en-GB">In this chapter, we covered one of the most important aspects of service reliability work – alerting. You learned how to set up the service metric collection using the Prometheus tool and the <strong class="source-inline" lang="">tally</strong> library, set up service alerts using the Alertmanager tool, and connect all these components to create an end-to-end service <span class="No-Break" lang="">alerting pipeline.</span></p>
			<p lang="en-GB">The material in this chapter summarizes our learning from the reliability and service telemetry topics from <a href="B18865_10.xhtml#_idTextAnchor139"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 10</em></span></a> and <a href="B18865_11.xhtml#_idTextAnchor152"><span class="No-Break" lang=""><em class="italic" lang="">Chapter 11</em></span></a>. By collecting the telemetry data and establishing the notification mechanisms using the alerting tools, we can quickly detect various service issues and get notified each time we need to <span class="No-Break" lang="">mitigate them.</span></p>
			<p lang="en-GB">In the next chapter, we will continue covering some advanced aspects of Go development, including system profiling <span class="No-Break" lang="">and dashboarding.</span></p>
			<h1 id="_idParaDest-179" lang="en-GB"><a id="_idTextAnchor180"/>Further reading</h1>
			<p lang="en-GB">To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break" lang="">following resources:</span></p>
			<ul>
				<li lang="en-GB"><em class="italic" lang="">Practical Alerting from Time-Series </em><span class="No-Break" lang=""><em class="italic" lang="">Data</em></span><span class="No-Break" lang="">: </span><a href="https://sre.google/sre-book/practical-alerting/&#13;"><span class="No-Break" lang="">https://sre.google/sre-book/practical-alerting/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">Monitoring Distributed </em><span class="No-Break" lang=""><em class="italic" lang="">Systems</em></span><span class="No-Break" lang="">: </span><a href="https://sre.google/sre-book/monitoring-distributed-systems/&#13;"><span class="No-Break" lang="">https://sre.google/sre-book/monitoring-distributed-systems/</span></a></li>
				<li lang="en-GB">Prometheus <span class="No-Break" lang="">documentation: </span><a href="https://prometheus.io/docs/introduction/overview/&#13;"><span class="No-Break" lang="">https://prometheus.io/docs/introduction/overview/</span></a></li>
				<li lang="en-GB"><em class="italic" lang="">Eliminating </em><span class="No-Break" lang=""><em class="italic" lang="">Toil</em></span><span class="No-Break" lang="">: </span><a href="https://sre.google/workbook/eliminating-toil/&#13;"><span class="No-Break" lang="">https://sre.google/workbook/eliminating-toil/</span></a></li>
			</ul>
		</div>
	</body></html>