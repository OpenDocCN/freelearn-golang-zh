- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refreshing Concurrency and Parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will explore goroutines at the core of Go’s concurrency. You will
    learn how they function, distinguish between concurrency and parallelism, manage
    currently running goroutines, handle data race issues, use channels for communication,
    and use `Channel` states and signaling to maximize their potential. Mastering
    these concepts is essential to write efficient and error-free Go code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding goroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing data races
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sense of channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The guarantee of delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State and signaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find this chapter’s source code at [https://github.com/PacktPublishing/System-Programming-Essentials-with-Go/tree/main/ch2](https://github.com/PacktPublishing/System-Programming-Essentials-with-Go/tree/main/ch2).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding goroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Goroutines are functions created and scheduled to be run independently by the
    Go scheduler. The Go scheduler is responsible for the management and execution
    of goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, we have a complex algorithm to make goroutines work. Fortunately,
    in Golang, we can achieve this highly complex operation with simplicity using
    the `go` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are accustomed to a language that has the `async`/`await` feature, you
    probably are used to deciding your function beforehand. It will be used concurrently
    to change the function signature to sign that the function can be paused/resumed.
    Calling this function also needs a special notation. When using goroutines, there
    is no need to change the function signature.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippets, we have a main function calling sequentially the
    `say` function, passing as an argument `"hello"` and `"``world"`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `say` function receives a string as a parameter and iterates five times.
    For each iteration, we make the function sleep for 500 milliseconds and print
    the `s` parameter immediately after:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute the program, it should print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we introduce the `go` keyword right before the first call to the `say`
    function to introduce concurrency in our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output should alternate between `hello` and `world`.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can achieve the same result if we create a goroutine for the second function
    call, right?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the results of the program now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Wait! Something is wrong here. What did we do wrong? The main function and the
    goroutine seem out of sync.
  prefs: []
  type: TYPE_NORMAL
- en: We didn’t do anything wrong. That is the expected behavior. When you take a
    closer look at the first program, the goroutine is fired, and the second call
    of `say` executes in the context of the main function sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the program should wait for the function to terminate to reach
    the end of the `main` function. For the second program, we have the opposite behavior.
    The first call is a normal function call, so it prints five times as expected,
    but when the second goroutine is fired, there is no following instruction on the
    main function, so the program terminates.
  prefs: []
  type: TYPE_NORMAL
- en: Although the behavior is correct from the perspective of how the program works,
    this is not our intention. We need a way to synchronize the `wait` for all the
    goroutines in this group of executions before giving the `main` function a chance
    to terminate. In situations such as this, we can leverage Go’s construct, the
    `sync` package, called `WaitGroup`.
  prefs: []
  type: TYPE_NORMAL
- en: WaitGroup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`WaitGroup`, as the name suggests, is a Go standard library mechanism that
    allows us to wait for a group of goroutines until they finish explicitly.'
  prefs: []
  type: TYPE_NORMAL
- en: No particular factory function exists to create them, since their zero-value
    is already a valid usable state. Since `WaitGroup` has been created, we need to
    control how many goroutines we are waiting for. We can use the `Add()` method
    to inform the group.
  prefs: []
  type: TYPE_NORMAL
- en: How can we inform the group that we have completed one of the routines? It couldn’t
    be more intuitive. We can achieve this using the `Done()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we introduce the wait group to make our program output
    the messages as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We create the `WaitGroup` ( `wg := sync.WaitGroup{}`) and declare that two goroutines
    participate in this group (`wg.Add(2)`).
  prefs: []
  type: TYPE_NORMAL
- en: In the last line of the program, we explicitly hold the execution with the `Wait()`
    method to avoid the program termination.
  prefs: []
  type: TYPE_NORMAL
- en: To make our function interact with `Waitgroup`, we need to send a reference
    to this group. Once we have its reference, the function can defer, calling `Done()`,
    to ensure that we signal correctly for our group every time the function is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the new `say` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We don’t need to rely on `time.Sleep()`, so this version doesn’t have it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can control our group of goroutines. Let’s deal with one central worrisome
    issue in concurrent programming – state.
  prefs: []
  type: TYPE_NORMAL
- en: Changing shared state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a scenario where two diligent workers are tasked with packing items
    into boxes in a busy warehouse. Each worker fills a fixed number of things into
    packets, and we must keep track of the total number of items packed.
  prefs: []
  type: TYPE_NORMAL
- en: This seemingly straightforward task, analogous to concurrent programming, can
    quickly become a nightmare when not handled properly. With proper synchronization,
    the workers may avoid intentionally interfering with each other’s work, leading
    to incorrect results and unpredictable behavior. It’s a classic example of a data
    race, a common challenge in concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: The following code will walk you through an analogy where two warehouse workers
    face a data race issue while packing items into boxes. We’ll first present the
    code without proper synchronization, demonstrating the data race problem. Then,
    we’ll modify the code to address the issue, ensuring that the workers collaborate
    smoothly and accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s step into the bustling warehouse and witness firsthand the challenges
    of concurrency and the importance of synchronization in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `main` function starts by calling the `PackItems` function with an initial
    `totalItems` value of 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `PackItems` function, there are two constants defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`workers`: The number of worker goroutines (set to 2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`itemsPerWorker`: The number of items each worker should pack into boxes (set
    to 1,000)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WaitGroup` named `wg` is created to wait for all worker goroutines to finish
    before returning the final `totalItems` value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A loop runs `workers` times, where each iteration starts a new goroutine to
    simulate a worker packing items into boxes. Inside the goroutine, the following
    steps are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: A worker ID is passed to the goroutine as an argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `defer wg.Done()` statement ensures that the wait group is decremented when
    the goroutine exits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An `itemsPacked` variable is initialized with the current value of `totalItems`
    to keep track of the items packed by this worker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A loop runs `itemsPerWorker` times, simulating the process of packing items
    into boxes. However, there’s no actual packing happening;the loop’s just incrementing
    the `itemsPacked` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the last step in the inner loop, `totalItems` receive the altered value of
    the `itemsPacked` variable, which contains the number of items packed by the worker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`totalItems` variable by adding the `itemsPacked` value to it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since multiple goroutines attempt to modify `totalItems` concurrently without
    proper synchronization, a data race occurs, leading to unpredictable and incorrect
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterministic results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider this alternative `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The program constantly runs the `PackItems` function until the expected result
    of 2,000 is not achieved. Once this occurs, the program will display the incorrect
    value returned by the function and the number of attempts it took to reach that
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the non-deterministic nature of the Go scheduler, the result would
    be right *most of the time*. This code would need a lot of runs to reveal its
    synchronization flaw.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a single execution, I needed more than 16,000 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Your turn!
  prefs: []
  type: TYPE_NORMAL
- en: Experiment running the code on your machine. How many iterations did your code
    need to fail?
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using your personal computer, there are likely many tasks being performed,
    but your machine probably has a lot of unused resources. However, it’s important
    to consider the amount of noise on shared nodes in a cluster if you’re running
    programs in cloud environments with containers. By “noise,” I mean the work done
    on the host machine while running your program. It may be just as idle as your
    local experiment. Still, it’s likely being used to its full potential in a cost-effective
    scenario where every core and memory is utilized.
  prefs: []
  type: TYPE_NORMAL
- en: This scenario of a constant contest for resources makes our schedule much more
    inclined to choose another workload instead of just continuing to run our goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we call the `runtime.Gosched` function to emulate
    noise. The idea is to give a hint to the Go scheduler, saying, “*Hey! Maybe it
    is a good moment to* *pause me*”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the main function again, we can see that the erroneous results occur
    much faster than before. In my execution, for example, I need just four iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, the code is still buggy. How can we anticipate that? At this
    point, you should have guessed that Go tools have the answer, and you’re right
    again. We can manage data races on our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Managing data races
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When multiple goroutines access shared data or resources concurrently, a “race
    condition” can occur. As we can attest, this type of concurrency bug can lead
    to unpredictable and undesirable behavior. The Go test tool has a built-in feature
    called **Go race detection** that can detect and identify race conditions in your
    Go code.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s create a `main_test.go` file with a simple test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s use the race detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result in the console will be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output can be quite intimidating at first glance, but the most revealing
    information initially is the message `WARNING:` `DATA RACE`.
  prefs: []
  type: TYPE_NORMAL
- en: To fix the synchronization issue in this code, we should use synchronization
    mechanisms to protect access to the `totalItems` variable. Without proper synchronization,
    concurrent writes to shared data can lead to race conditions and unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: We have used `WaitGroup` from the `sync` package. Let’s explore more synchronization
    mechanisms to ensure the program’s correctness.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s heartbreaking that the term “atomic” in Go doesn’t involve physically manipulating
    atoms, like in physics or chemistry. It would be fascinating to have that capability
    in programming; instead, atomic operations in Go are focused on synchronizing
    and managing concurrency among goroutines using the sync/atomic package.
  prefs: []
  type: TYPE_NORMAL
- en: Go offers atomic operations to load, store, add, and `int32`, `int64`, `uint32`,
    `uint64`, `uintptr`, `float32`, and `float64`. Atomic operations can’t be directly
    performed on arbitrary data structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s change our program using the atomic package. First, we should import
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of updating `totalItems` directly, we will leverage the `AddInt32`
    function to guarantee the synchronization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If we check for data races again, no problem will be reported.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic structures are great when we need to synchronize a single operation,
    but when we want to synchronize a block of code, other tools are a better fit,
    such as mutexes.
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ah, mutexes! They’re like the bouncers at a party for goroutines. Imagine a
    bunch of these little Go creatures trying to dance around with shared data. It’s
    all fun and games until chaos breaks loose, and you have a goroutine traffic jam
    with data spills all over the place!
  prefs: []
  type: TYPE_NORMAL
- en: Do not worry, as mutexes swoop in like the dance-floor supervisors, ensuring
    that only one groovy goroutine can bust a move in the critical section at a time.
    They’re like the rhythm keepers of concurrency, ensuring that everyone takes turns
    and nobody steps on each other’s toes.
  prefs: []
  type: TYPE_NORMAL
- en: You can create a mutex by declaring a variable of type `sync.Mutex`. A mutex
    allows us to protect a critical section of code, using the `Lock()` and `Unlock()`
    methods. When a goroutine calls `Lock()`, it acquires the mutex lock, and any
    other goroutines attempting to call `Lock()` will be blocked until the lock is
    released with `Unlock()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for our program using mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we lock a block of code handling to change our shared state,
    and when we’re done, we unlock the mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the mutexes ensure the correctness handling shared state, you could consider
    two options:'
  prefs: []
  type: TYPE_NORMAL
- en: You could use lock and unlock for every critical line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could simply lock in the beginning of the function and defer the unlock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, you could! Sadly, there is a catch in both approaches. We introduce latency
    indiscriminately. To make my point, let’s benchmark the second approach versus
    the original use of mutex.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a second version of the function using multiple calls to lock/unlock,
    called `MultiplePackItems`, where everything remains the same except the function
    name and the inner loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the inner loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the performance of both options running a benchmark test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The version with multiple locks is approximately **~64%** slower than the first
    one in terms of the time taken per operation.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover in detail benchmarks and other techniques of performance measurement
    in [*Chapter 6*](B21662_06.xhtml#_idTextAnchor145), *Analyzing Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: These examples show goroutines performing their tasks independently, without
    collaborating with each other. However, in many cases, our tasks require exchanging
    information or signals to make decisions, such as starting or stopping a procedure.
  prefs: []
  type: TYPE_NORMAL
- en: When exchanging information is crucial, we can use a flagship tool in Go called
    a channel.
  prefs: []
  type: TYPE_NORMAL
- en: Making sense of channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the channel carnival!
  prefs: []
  type: TYPE_NORMAL
- en: Imagine Go channels as magical, clown-sized pipes that allow circus performers
    (goroutines) to pass around juggling balls (data) while making sure nobody drops
    the ball – quite literally!
  prefs: []
  type: TYPE_NORMAL
- en: How to use channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use channels, we need to use a built-in function called `make()`, informing
    what type of data we’re interested in passing using this channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want a channel of `string`, we should declare the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can inform a capacity. Channels with capacity are called buffered channels.
    We won’t bother going into detail about capacity for now. We create an unbuffered
    channel when we don’t inform the capacity.
  prefs: []
  type: TYPE_NORMAL
- en: An unbuffered channel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An unbuffered channel is a way to communicate between multiple goroutines, and
    it needs to respect a simple rule – the goroutine that wants to send in the channel
    and the one that wants to receive should be **ready** at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Think of this as a “trust fall” exercise. The sender and receiver must trust
    each other fully, ensuring the safety of the data, just like acrobats trust their
    partners to catch them mid-air.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract? Let’s explore this concept with examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s send information to a channel with no receiver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute, the console will print something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let’s break down this output.
  prefs: []
  type: TYPE_NORMAL
- en: '`all goroutines are sleep – deadlock!` is the main error message. It tells
    us that all goroutines in our program are in a `sleep` state, which implies that
    they are waiting for some event or resource to become available. However, because
    all of them are waiting and cannot make any progress, your program has encountered
    a deadlock situation.'
  prefs: []
  type: TYPE_NORMAL
- en: '`goroutine 1 [chan send]:` is the part of the message that provides additional
    information about the specific goroutine that has encountered the deadlock. In
    this case, it’s `goroutine 1`, and it was involved in a channel send operation
    (`chan send`).'
  prefs: []
  type: TYPE_NORMAL
- en: This deadlock occurs because the execution is paused, waiting for another goroutine
    to receive the information, but there’s none.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlocks
  prefs: []
  type: TYPE_NORMAL
- en: A deadlock is a condition where two or more processes or goroutines are unable
    to proceed because they are all waiting for something that will never happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can try the opposite; in the next example, we want to receive from
    a channel with no sender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output in the console is very similar, except that now, the error is about
    receiving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, following the rule is as simple as sending and receiving simultaneously.
    So, declaring both will be sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s a good idea, but unfortunately, it doesn’t work, as we can see in the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If we’re following the rule, why is it not working?
  prefs: []
  type: TYPE_NORMAL
- en: Well, we’re not exactly following the rule. The rule states that the goroutine
    that wants to send in the channel and the one that wants to receive should be
    *ready* at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to take note of is the final part – *ready at the* *same
    time*.
  prefs: []
  type: TYPE_NORMAL
- en: Since the code runs sequentially, line by line, when we try to send `c <- "message"`,
    the program waits for the receiver to receive the message. We need to make these
    two parties send and receive the message simultaneously. We can use our concurrent
    programming knowledge to make this happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add goroutines to the mix, using the circus analogy. We’ll introduce
    a function, `throwBalls`, that will expect the color of the balls to be thrown
    (`color`) and the channel (`balls`) where it should receive these throws:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have three major steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We create an unbuffered string channel named `balls`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A goroutine is launched inline using the `throwBalls` function to send “red”
    into the channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main function receives and prints the value received from the channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output for this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We did it! We successfully passed information between goroutines using channels!
  prefs: []
  type: TYPE_NORMAL
- en: 'But what happens when we send one more ball? Let’s try it with a green ball:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The output shows just one ball being received. What happened?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Red or green?
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re launching more than one goroutine, the scheduler will elect arbitrarily
    what should execute first. Therefore, you can see green or red randomly running
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fix the issue by putting in one more `print` statement received from
    the channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it works, it’s not the most elegant solution. We could have trouble
    with deadlocks again if we have more receivers than senders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The last print will await forever, causing another deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to make code work with any number of balls, we should stop adding
    more and more lines and replace them all with the `range` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating over a channel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The mechanism used to iterate over the values sent through a channel is the
    `range` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s change the code to iterate over the channel values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We can happily check the console to see the balls received elegantly, but wait
    – all the goroutines are asleep! Deadlock again?
  prefs: []
  type: TYPE_NORMAL
- en: This error occurs when we iterate over channels and the range expects a channel
    to be closed to stop the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Closing a channel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To close a channel, we need to call the built-in `close` function, passing
    the channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, we can now guarantee that the channel is closed. Let’s change the code
    by adding the `close` call between the senders and `range`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that if the range stops when the channel is closed, with
    this code, the range will never run once the channel has closed.
  prefs: []
  type: TYPE_NORMAL
- en: We need to orchestrate this group of tasks, and yes, you’re right – we’re using
    `WaitGroup` to save us again. This time, we don’t want to taint the `throwBalls`
    signature to receive our `WaitGroup`, so we’ll create inline anonymous functions
    to keep our functions unaware of the concurrency. Additionally, we want to close
    the channel when we have the guarantee that all the tasks are done. We infer this
    with the `Wait()` method from our `WaitGroup`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Phew! This time, the output is correctly shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: What a ride, huh? But wait! We still need to explore the buffered channels!
  prefs: []
  type: TYPE_NORMAL
- en: Buffered channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s analogy time!
  prefs: []
  type: TYPE_NORMAL
- en: These are the channels where clowns come into play! Imagine a clown car with
    a limited number of seats (capacity). Clowns (senders) can hop in and out of the
    car, dropping juggling balls (data) into it.
  prefs: []
  type: TYPE_NORMAL
- en: We want to create a program with buffered channels that simulate a circus car
    ride, where clowns try to get into a clown car (limited to three clowns at a time)
    with balloons. The driver controls the car and manages the clowns’ rides while
    the clowns attempt to get in. If the car is full, they wait and print a message.
    After all the clowns are done, the program waits for the car driver to finish
    and then prints that the circus car ride is over.
  prefs: []
  type: TYPE_NORMAL
- en: If a clown tries to stuff too many juggling balls into the car, it’s as hilarious
    as a car overflowing with clowns and juggling balls, creating a comical spectacle!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create the program structure to receive our senders and receivers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the driver’s goroutine (receiver):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We add the clown logic (sender) just below the rider’s block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code, we can see all the trouble that the clowns are making:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: select
  prefs: []
  type: TYPE_NORMAL
- en: The `select` statement allows us to wait on multiple communication channels
    and select the first one that becomes ready, effectively allowing us to perform
    non-blocking operations on channels.
  prefs: []
  type: TYPE_NORMAL
- en: When working with channels, it’s easy to get caught up in comparing message
    queues and channels, but there may be better ways to understand them. The channel
    internals are ring buffers, and this information can be confusing and unhelpful
    when choosing the program design. By prioritizing an understanding of signaling
    and the guaranteed delivery of messages, you’d be better equipped to work efficiently
    with channels.
  prefs: []
  type: TYPE_NORMAL
- en: The guarantee of delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difference between buffered and unbuffered channels is the guarantee
    of delivery.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, the unbuffered channels always guarantee delivery, since
    they only send a message when the receiver is ready. Conversely, the buffered
    channels can’t ensure message delivery because they can “buffer” an arbitrary
    number of messages before the synchronization step becomes mandatory. Therefore,
    the reader could fail to read a message from the channel buffer.
  prefs: []
  type: TYPE_NORMAL
- en: The most considerable side effect of choosing between them is how much latency
    you can afford to introduce to your program.
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency in the context of concurrent programming refers to the time it takes
    for a piece of data to travel from a sender (goroutine) to a receiver (goroutine)
    through a channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Go channels, latency is influenced by several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Buffering**: Buffering can reduce latency when the sender and receiver are
    not perfectly synchronized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blocking**: Unbuffered channels block the sender and receiver until they
    are ready to communicate, leading to potentially higher latency. Buffered channels
    allow the sender to continue without immediate synchronization, potentially reducing
    latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goroutine scheduling**: The latency in channel communication also depends
    on how the Go runtime schedules goroutines. Factors such as the number of available
    CPU cores and the scheduling algorithm influence how quickly goroutines can be
    executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a channel type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a rule of thumb, we consider an unbuffered channel a strong choice for the
    following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Guaranteed delivery**: Provide a guarantee that the value being sent is received
    by another goroutine. This is especially useful in scenarios where you need to
    ensure data integrity and that no data is lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-to-one communication**: Unbuffered channels are best suited for one-to-one
    communication between goroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: Unbuffered channels can be used to implement load-balancing
    patterns, ensuring that work is distributed evenly among worker goroutines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conversely, buffered channels offer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous communication**: Buffered channels allow for asynchronous communication
    between goroutines. When sending data on a buffered channel, the sender won’t
    block until the data is received, if there is space in the channel’s buffer. This
    can improve throughput in certain scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing contention**: In scenarios where you have multiple senders and receivers,
    using a buffered channel can reduce contention. For example, in a producer-consumer
    pattern, you can use a buffered channel to allow producers to keep producing without
    waiting for consumers to catch up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preventing deadlocks**: Buffered channels can help prevent goroutine deadlocks
    by allowing a certain level of buffering, which can be useful when you have unpredictable
    variations in a workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch processing**: Buffered channels can be used for batch processing or
    pipelining where data is produced at one rate and consumed at another rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered the key aspects of latency and how it impacts channel
    communication in concurrent programming, let’s shift our focus to another critical
    aspect – state and signaling. Understanding the semantics of state and signaling
    is essential to avoid common pitfalls and make informed design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: State and signaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring the semantics of state and signaling puts you ahead of the curve in
    avoiding more straightforward bugs or making good design choices.
  prefs: []
  type: TYPE_NORMAL
- en: State
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Go eased the adoption of concurrency with channels, there are some
    characteristics and pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: We should remember that channels have three states – nil, open (empty, not empty),
    and closed. These states strongly relate to what we can and cannot do with channels,
    whether from the sender’s or receiver’s perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a channel when you want to read from:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading to a `write-only` channel results in a compilation error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the channel is `nil`, reading from it indefinitely blocks your goroutine
    until it is initialized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading will be blocked in an `open` and `empty` channel until data is available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an `open` and `not empty` channel, reading will return data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the channel is `closed`, reading it will return the default value for its
    type and `false` to indicate closure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Writing also has its nuances:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing to a `read-only` channel results in a compilation error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to a `nil` channel block until it’s initialized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to an `open` and `full` channel blocks until there’s space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an `open` and `not full` channel, writing is successful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing on a `closed` channel leads to a panic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Closing a channel depends on its state:'
  prefs: []
  type: TYPE_NORMAL
- en: Closing an `open channel with data` allows reads until drained, and then returns
    the default value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closing an `open empty channel` immediately closes it, and reads also return
    the default value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attempting to close an `already closed channel` results in a `panic`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closing a read-only channel results in a compilation error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Signaling between goroutines is an everyday use case for channels. You can use
    channels to coordinate and synchronize the execution of different goroutines by
    sending signals or messages between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of how to use a Go channel to signal between two goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we create a channel called `signalChannel` to signal between
    the two goroutines. `Goroutine 1` waits for a signal on the channel using `<-signalChannel`,
    and `Goroutine 2` sends a signal using `signalChannel <-` `true`.
  prefs: []
  type: TYPE_NORMAL
- en: The `sync.WaitGroup` ensures that we wait for both goroutines to finish before
    printing `"Both goroutines` `have finished."`.
  prefs: []
  type: TYPE_NORMAL
- en: When you run this program, you’ll see that `Goroutine 1` waits for the signal
    from `Goroutine 2` and then proceeds with its task.
  prefs: []
  type: TYPE_NORMAL
- en: Go channels are a flexible way to synchronize and coordinate complex interactions
    between goroutines. They can be used to implement concurrency patterns producer-consumer
    or fan-out/fan-in.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your synchronization mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Are channels always the answer? Definitely not! We can use mutexes or channels
    to solve the same problem. How do we choose? Prefer pragmatism. When mutexes make
    your solution easy to read and maintain, don’t think twice and go with mutexes!
  prefs: []
  type: TYPE_NORMAL
- en: If you have trouble choosing between them, here is an opinionated guideline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use channels when you need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the ownership of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute units of work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate results in an asynchronous way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use mutexes when you’re handling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Caches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, let’s wrap things up and recap what we’ve covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the functioning of goroutines, their simplicity,
    and the importance of synchronization using `WaitGroup`. We also became aware
    of the difficulties in managing shared state, using a warehouse analogy to explain
    data races. Additionally, we were introduced to Go’s race detection tool to identify
    race conditions, the significance of communication channels, and their potential
    pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our concurrency knowledge is refreshed, let’s explore in the next chapter
    interactions with an operational system using system calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Interaction with the OS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will delve into system-level programming concepts using Go.
    You will explore inter-process communication (IPC) mechanisms, system event handling,
    file operations, and Unix sockets. This section provides practical examples and
    detailed explanations to equip you with the knowledge and skills to build robust
    and efficient system-level applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B21662_03.xhtml#_idTextAnchor089), *Understanding System Calls*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21662_04.xhtml#_idTextAnchor110), *File and Directories operations*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21662_05.xhtml#_idTextAnchor126), *Working with System Events*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21662_06.xhtml#_idTextAnchor145), *Understanding Pipes in Inter-Process
    Communication*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21662_07.xhtml#_idTextAnchor160), *Unix Sockets*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
