<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer018">
			<h1 id="_idParaDest-142" class="chapter-number"><a id="_idTextAnchor179"/>8</h1>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor180"/>Memory Management</h1>
			<p>In this chapter, we’ll dive into the world of memory management in Go, focusing on the mechanisms and strategies underpinning garbage collection. As we navigate the garbage collection concepts, including its evolution within Go, the distinctions between stack and heap memory allocations, and the advanced techniques employed to manage memory efficiently, you will understand the inner workings of Go’s memory <span class="No-Break">management system.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Garbage collection</span></li>
				<li><span class="No-Break">Memory arenas</span></li>
			</ul>
			<p>By the end of the chapter, you should be able to optimize your code to reduce memory usage, minimize garbage collection overhead, and ultimately improve the scalability and responsiveness of <span class="No-Break">your applications.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor181"/>Technical requirements</h1>
			<p>All the code shown in this chapter can be found in the <strong class="source-inline">ch8</strong> directory of our <span class="No-Break">GitHub repository.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor182"/>Garbage collection</h1>
			<p>Before garbage-collected languages, we<a id="_idIndexMarker387"/> needed to handle memory management ourselves. Despite the focused attention that this discipline craves, the main problems we ran in circles to avoid were memory leaks, dangling pointers, and <span class="No-Break">double frees.</span></p>
			<p>The garbage collector in Go has some jobs to avoid common mistakes and accidents: it tracks allocations on the heap, frees unneeded allocations, and keeps the allocations in use. These jobs are commonly referred to in academia as memory inference, or “What memory should I free?”. The two main strategies for dealing with memory inference are tracing and <span class="No-Break">reference counting..</span></p>
			<p>Go uses a tracing garbage collector (GC for short), which means the GC will trace objects reachable by a chain of references from “root” objects, consider the rest as “garbage,” and collect them. Go’s garbage collector has a long journey of optimization and learning. You can find the whole path to today’s state of the art in this blog post from Go’s dev <span class="No-Break">team: </span><a href="https://go.dev/blog/ismmkeynote"><span class="No-Break">https://go.dev/blog/ismmkeynote</span></a><span class="No-Break">.</span></p>
			<p>In this very blog post, the <a id="_idIndexMarker388"/>Go team reports enormous gains. For instance, one garbage collection cycle dropped from 300 ms (Go 1.0) to, shockingly, 0.5 ms in the <span class="No-Break">latest version.</span></p>
			<p>You must have heard this at least once in the tech community: “Garbage collection in Go is automatic, so you can forget about memory management.” Yeah, and I’ve got some prime real estate on the moon to sell you. Believing this is like thinking your house cleans itself because you’ve got a Roomba. In Go, understanding garbage collection is not just a nice-to-have; it’s your ticket to writing efficient, high-performance code. So, buckle up, we’re diving into a world where “automatic” doesn’t <span class="No-Break">mean “magical.”</span></p>
			<p>Imagine, if you will, a software development team that never reviews code because, hey, they have a linter. That’s similar to how some approach Go’s garbage collector. It’s like entrusting your entire code base quality to a program that checks for extra whitespaces. Sure, the GC in Go is a neat little janitor, tirelessly tidying up your memory mess. But misunderstanding its <em class="italic">modus operandi</em> is like thinking your linter will refactor your spaghetti code into a <span class="No-Break">Michelin-star-worthy dish.</span></p>
			<p>To pave the terrain to more advanced knowledge regarding GC, first, we need to understand two areas of memory: stack <span class="No-Break">and heap.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor183"/>Stack and heap allocation</h2>
			<p>Stack allocation<a id="_idIndexMarker389"/> in Go is used for variables whose lifetimes are predictable and tied to the function calls that create them. These are your local variables, function parameters, and return values. The stack is remarkably efficient because of<a id="_idIndexMarker390"/> its <strong class="bold">Last In, First Out</strong> (<strong class="bold">LIFO</strong>) nature. Allocating and deallocating memory here is just a matter of moving the stack pointer up or down. This simplicity makes stack allocation fast, but it’s not without its limitations. The size of the stack is<a id="_idIndexMarker391"/> relatively small, and trying to put too much stuff on the stack can lead to the dreaded <span class="No-Break">stack overflow.</span></p>
			<p>Contrastingly, heap allocation<a id="_idIndexMarker392"/> is for variables whose lifetimes are less predictable and not strictly tied to where they were created. These are typically variables that must live beyond the scope of the function they were created in. The heap is a more flexible, dynamic space, and variables here can be accessed globally. However, this flexibility comes at a cost. Allocating memory on the heap is slower due to the need for more complex bookkeeping, and the responsibility of managing this memory falls to the garbage collector, which <span class="No-Break">adds overhead.</span></p>
			<p>Go’s compiler performs a neat trick called “escape analysis” (more on this topic in <a href="B21662_09.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Analyzing Performance</em>) to decide whether a variable should live on the stack or the heap. If the compiler determines that the lifetime of a variable doesn’t escape the function it’s in, to the stack it goes. But if the variable’s reference is passed around or returned from the function, then it “escapes” to <span class="No-Break">the heap.</span></p>
			<p>This automatic decision-making process is a boon for developers, as it optimizes memory usage and performance without manual intervention. The distinction between stack and heap allocation has significant performance implications. Stack-allocated memory tends to lead to better performance due to its straightforward allocation and <span class="No-Break">deallocation mechanism.</span></p>
			<p>Heap-allocated memory, while necessary for more complex and dynamic data, incurs a performance cost due to the overhead of garbage collection. As a Go developer, being mindful of how your variables are allocated can help in writing more efficient code. While Go abstracts much of the memory management complexity, having a good understanding of how heap and stack allocations work can greatly impact the performance of <span class="No-Break">your applications.</span></p>
			<p>As a rule of thumb, keep your variables in the scope as narrow as possible, and be cautious with pointers and references that might cause unnecessary <span class="No-Break">heap allocations.</span></p>
			<p>OK, let’s get technical. Go’s garbage collection is based on a concurrent, tri-color mark-and-sweep algorithm. Now, before your eyes glaze over like a donut, let’s break <span class="No-Break">that down.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor184"/>The GC algorithm</h2>
			<p><em class="italic">Concurrent</em> means it runs alongside<a id="_idIndexMarker393"/> your program, not halting everything to clean up. This is crucial for performance, especially in real-time systems where pausing for housekeeping is as welcome as a screen freeze on <span class="No-Break">launch day.</span></p>
			<p>The <em class="italic">tri-color</em> bit is about how the GC views objects. Think of it as a traffic light for memory: green for “in use,” red for “ready to delete,” and yellow for “maybe, <span class="No-Break">maybe not.”</span></p>
			<p>The last part, <em class="italic">mark and sweep</em>, is the definition of the two main phases of the process. The quick version of the story is: during the “mark” phase, the GC scans your objects, flipping their colors based on accessibility. In the “sweep” phase, it takes out the trash – the red objects. This two-step process helps in managing memory efficiently without stepping on the toes of your running program. Once we have the big picture, we can explore the details of these two phases <span class="No-Break">with ease.</span></p>
			<h3>Marking phase</h3>
			<p>The “mark” phase is split<a id="_idIndexMarker394"/> into two parts. In the initial part, the GC pauses the program briefly (less than 0.3 milliseconds) – think of it as a quick inhale before diving underwater. During this pause, known as the <strong class="bold">stop-the-world</strong> (<strong class="bold">STW</strong>) phase, the GC identifies the<a id="_idIndexMarker395"/> root set. These roots are essentially variables directly accessible from the stack, globals, and other special locations. In other words, it is the moment when the GC will start its search to identify what’s in use and <span class="No-Break">what’s not.</span></p>
			<p>After identifying the root set, the GC proceeds to the actual marking, <em class="italic">which happens concurrently with the program’s execution</em>. This is where the “tri-color” metaphor shines. Objects are initially marked “white,” meaning their fate is undecided. As the GC encounters these objects from the roots, it marks them “gray,” indicating they need to be explored further, and eventually turns them “black” once fully processed, signifying they are in use. This color-coded system ensures that the GC comprehensively assesses each <span class="No-Break">object’s accessibility.</span></p>
			<p>There are more crucial details to expand on in this process. Since we want to create highly performant systems, we need to master our GC knowledge instead of keeping <span class="No-Break">things theoretical.</span></p>
			<p>During the marking phase, the Go runtime deliberately allocates about <strong class="bold">25%</strong> of the available CPU resources. This allocation is a calculated decision, ensuring that the GC is efficient enough to keep memory usage in check while not overwhelming the system. It’s a balancing act, similar to a juggler who ensures each ball gets enough airtime but doesn’t hog the spotlight. This 25% allocation is crucial for keeping the GC’s work steady <span class="No-Break">and unobtrusive.</span></p>
			<p>In addition to the <a id="_idIndexMarker396"/>standard CPU allocation, there’s a provision for an extra <strong class="bold">5%</strong> of CPU to be used via mark assists. These mark assists are triggered when the program makes memory allocations during the GC cycle. If the GC is lagging behind, allocating goroutines lends a hand (or in this case, some CPU cycles) to assist in the marking process. This additional 5% can be viewed as a reserve force, called into action when the situation demands it, ensuring that the GC keeps pace with the memory <span class="No-Break">allocation rate.</span></p>
			<h3>Sweep</h3>
			<p>Moving to the <a id="_idIndexMarker397"/>sweep phase, this is where deallocations come into play. After the marking phase has identified which objects are no longer needed (those still marked as “white”), the sweep phase begins the process of deallocating this memory. This phase is crucial because it’s where the actual memory reclamation occurs, freeing up space for future allocations. The efficiency of this phase directly impacts the application’s memory footprint and overall performance. But it’s not all rainbows and butterflies. The GC can still lead to performance issues, such as latency spikes, especially when dealing with large heaps or memory-hungry applications. Understanding how to optimize your code to play nice with the GC is an art. It involves deep dives into pointer management, avoiding memory leaks, and sometimes just knowing when to say, “Hey, GC, you can take a break; I’ve <span class="No-Break">got this.”</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor185"/>GOGC</h2>
			<p>The <strong class="source-inline">GOGC</strong> environment variable<a id="_idIndexMarker398"/> in Go is the tuning knob of the garbage collector. It’s like the thermostat of your home’s heating system, controlling how hot or cold you want in the room. In Go’s context, <strong class="source-inline">GOGC</strong> dictates the aggressiveness of the garbage collection process. It determines how much newly allocated memory is allowed before the garbage collector triggers another cycle. Understanding and adjusting this variable can significantly impact your Go application’s memory usage and performance. The default value is <strong class="source-inline">100</strong>, which means that the GC tries to leave at least 100% of the initial heap memory available after a new GC cycle. Adjusting the <strong class="source-inline">GOGC</strong> value<a id="_idIndexMarker399"/> allows you to tailor the garbage collection to the specific needs of <span class="No-Break">your application.</span></p>
			<h3>Go env</h3>
			<p><strong class="source-inline">GOGC</strong> is an <a id="_idIndexMarker400"/>environment variable that affects the GC, but it is not a configuration option specific to the Go toolchain <span class="No-Break">or compiler.</span></p>
			<p>Setting <strong class="source-inline">GOGC</strong> to a lower value, say <strong class="source-inline">50</strong>, means the GC will run more frequently, keeping the heap size smaller but using more CPU time. On the flip side, setting it higher, such as <strong class="source-inline">200</strong>, means the GC will run less frequently, allowing more memory allocation but potentially leading to an undesired increased <span class="No-Break">memory usage.</span></p>
			<p>The <strong class="source-inline">GOGC</strong> variable can take <em class="italic">any integer value greater than 0</em>. Setting it to a very low value can lead to a performance hit due to the GC running too often, like a cleaner who’s constantly tidying up to the point of being a nuisance. Conversely, setting it too high can cause your application to use more memory than necessary, which might not be ideal in memory-constrained environments. It’s important to find the sweet spot specific to your application’s memory and <span class="No-Break">performance characteristics.</span></p>
			<p>There are also special values for <strong class="source-inline">GOGC</strong>. Setting it to <strong class="source-inline">off</strong> disables automatic garbage collection entirely. This might be useful in scenarios where the short-lived nature of the program doesn’t warrant the overhead of GC. However, with great power comes great responsibility; disabling GC can lead to unchecked memory growth. It’s a bit like turning off your house’s automatic thermostat – it can be beneficial in the right circumstances but requires much more attention to <span class="No-Break">prevent problems.</span></p>
			<p>In practice, tuning <strong class="source-inline">GOGC</strong> is a matter of understanding your application’s memory profile and performance requirements. It requires careful experimentation and monitoring. Adjusting this variable can yield significant performance improvements, especially in systems with large heaps or <span class="No-Break">real-time constraints.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor186"/>GC pacer</h2>
			<p>The GC pacer<a id="_idIndexMarker401"/> in Go can be likened to a conductor of an orchestra, ensuring every section comes in at the right time to create a harmonious symphony. Its job is to regulate the timing of garbage collection cycles, balancing the need to reclaim memory with the need to keep the program running efficiently. The pacer’s decisions are based on the current heap size, the allocation rate, and the goal of maintaining <span class="No-Break">program performance.</span></p>
			<p>The primary role of the pacer is to determine when to start a new garbage collection cycle. It does this by monitoring the rate of memory allocation and the size of the live heap (hinted by GOGC) – the memory in use that can’t be reclaimed. The pacer’s strategy is to trigger a GC cycle before the program allocates too much memory, which could lead to increased latency or memory pressure. It’s a preventive measure, similar to changing the oil in your car before it turns into a <span class="No-Break">bigger problem.</span></p>
			<p>One of the key features of the GC pacer is its adaptive nature. It continuously adjusts its thresholds based on the application’s behavior. If an application allocates memory rapidly, the pacer responds by triggering GC cycles more frequently to keep up. Conversely, if the application’s allocation rate slows down, the pacer will allow more memory to be allocated before initiating a GC cycle. This adaptiveness ensures that the pacer’s behavior aligns with the application’s <span class="No-Break">current needs.</span></p>
			<p>The pacer works in tandem with the <strong class="source-inline">GOGC</strong> environment variable. <strong class="source-inline">GOGC</strong> sets the percentage growth of the heap allowed before a GC cycle is triggered. The pacer uses this value as a guideline to determine <span class="No-Break">its thresholds.</span></p>
			<p>The effectiveness of the GC pacer has a direct impact on application performance. A well-tuned pacer ensures that garbage collection happens smoothly, without causing significant pauses or latency spikes. However, if the pacer’s thresholds are not well aligned with the application’s behavior, it could lead to either excessive GC cycles, which can degrade performance, or delayed collections, which can increase memory usage. It’s like finding the right speed for cruise control – too fast or too slow can lead to an <span class="No-Break">uncomfortable ride.</span></p>
			<p>The GC pacer <a id="_idIndexMarker402"/>in Go is a critical component that ensures the efficiency of the garbage collection process. It’s not just about writing code; it’s about understanding the environment in which your code runs, and the GC pacer is a significant part of <span class="No-Break">that environment.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor187"/>GODEBUG</h2>
			<p>The <strong class="source-inline">GODEBUG</strong> environment variable<a id="_idIndexMarker403"/> in Go is a powerful tool for developers, offering insights into the inner workings of the Go runtime. Specifically, the <strong class="source-inline">GODEBUG=gctrace=1</strong> setting is often used to gain detailed information about garbage collection processes. Let’s explore this <span class="No-Break">in depth.</span></p>
			<p><strong class="source-inline">GODEBUG</strong> in Go is like a diagnostic toolkit for your car. Just as you might plug in a diagnostic tool to understand what’s happening under the hood of your car, <strong class="source-inline">GODEBUG</strong> provides insights into the Go runtime. Among its various capabilities, one of the most used is <strong class="source-inline">gctrace</strong>. When set to <strong class="source-inline">1</strong> (<strong class="source-inline">GODEBUG=gctrace=1</strong>), it enables the tracing of GC activities, offering a window into how and when garbage collection occurs in your <span class="No-Break">Go application.</span></p>
			<p>Enabling <strong class="source-inline">gctrace</strong> to <strong class="source-inline">1</strong> outputs detailed information for each GC cycle, including the time it starts, its duration, the heap size before and after collection, and the amount of memory reclaimed. This data is invaluable for understanding the GC’s impact on your application’s performance. It’s like getting a play-by-play commentary on how the GC is managing memory, which can be critical for <span class="No-Break">performance tuning.</span></p>
			<p>The output from <strong class="source-inline">gctrace=1</strong> can be quite dense and may seem intimidating at first. It includes several metrics, such as STW times, which indicate how long your application pauses during GC. Other details include the number of goroutines running, heap sizes, and the number of <a id="_idIndexMarker404"/>GC cycles. Reading this output is like decoding a treasure map; once you understand the symbols and numbers, it reveals valuable information about where your application’s performance can be improved. Take this output as <span class="No-Break">an example:</span></p>
			<pre class="console">
gc 1 @0.019s 2%: 0.015+2.5+0.003 ms clock, 0.061+0.5/2.0/3.0+0.012 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P</pre>			<p>Let’s break down <span class="No-Break">this output:</span></p>
			<ul>
				<li><strong class="source-inline">gc 1</strong>: This indicates the sequence number of the garbage <span class="No-Break">collection cycle</span></li>
				<li><strong class="source-inline">@0.019s</strong>: The time (in seconds) since the program started when this GC <span class="No-Break">cycle began</span></li>
				<li><strong class="source-inline">2%</strong>: Percentage of the total program runtime spent <span class="No-Break">on GC</span></li>
				<li><strong class="source-inline">0.015+2.5+0.003 ms clock</strong>: Breakdown of the GC <span class="No-Break">cycle time</span><ul><li><strong class="source-inline">0.015 ms</strong>: STW sweep termination <span class="No-Break">phase time</span></li><li><strong class="source-inline">2.5 ms</strong>: Concurrent mark and scan <span class="No-Break">phase time</span></li><li><strong class="source-inline">0.003 ms</strong>: STW mark termination <span class="No-Break">phase time</span></li></ul></li>
				<li><strong class="source-inline">0.061+0.5/2.0/3.0+0.012 ms cpu</strong>: CPU time for the <span class="No-Break">GC cycle</span><ul><li><strong class="source-inline">0.061 ms</strong>: CPU time for STW <span class="No-Break">sweep termination</span></li><li><strong class="source-inline">0.5/2.0/3.0</strong>: CPU time for concurrent phases (mark/scan, <span class="No-Break">assist, background)</span></li><li><strong class="source-inline">0.012 ms</strong>: CPU time for STW <span class="No-Break">mark termination</span></li></ul></li>
				<li><strong class="source-inline">4-&gt;4-&gt;1 MB</strong>: Heap size at the start, midpoint, and end of <span class="No-Break">the GC</span></li>
				<li><strong class="source-inline">5 MB goal</strong>: Next GC cycle’s target <span class="No-Break">heap size</span></li>
				<li><strong class="source-inline">4 P</strong>: Number of <span class="No-Break">processors used</span></li>
			</ul>
			<p>We can observe the <a id="_idIndexMarker405"/>following with <span class="No-Break">this data:</span></p>
			<ul>
				<li><strong class="bold">Frequent high percentage</strong>: If the percentage of time spent on GC is high and frequent, it could signal <span class="No-Break">performance issues</span></li>
				<li><strong class="bold">STW times</strong>: Longer STW times can indicate that optimizations are needed to reduce <span class="No-Break">GC pauses</span></li>
				<li><strong class="bold">Heap size trends</strong>: Growing heap size without similar decreases after GC cycles might point to <span class="No-Break">memory leaks</span></li>
				<li><strong class="bold">CPU time</strong>: Higher CPU times might suggest that the GC is working harder than expected, potentially due to inefficient <span class="No-Break">memory usage</span></li>
			</ul>
			<p>Setting <strong class="source-inline">GODEBUG=gctrace=1</strong> is particularly useful in scenarios where you suspect memory leaks, or when you’re trying to optimize memory usage and GC overhead. For instance, if you observe longer STW times, it might indicate that your application is spending too much time on garbage collection, leading to performance bottlenecks. Similarly, if the heap size grows continuously, it might be a sign of a memory leak. This level of insight is crucial for making informed decisions about code optimizations and memory management. However, like any powerful tool, it should be used with understanding and care. By leveraging <strong class="source-inline">gctrace</strong>, developers can significantly enhance the efficiency and performance of their <span class="No-Break">Go applications.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor188"/>Memory ballast</h1>
			<p>Memory ballast<a id="_idIndexMarker406"/> in Go, at its core, is like putting a heavy suitcase in the trunk of a car to prevent it from being too light and skidding on ice. In Go’s context, a memory ballast is a large allocation of memory that is never used but serves to influence the behavior of the <span class="No-Break">garbage collector.</span></p>
			<p>Traditionally, Go’s GC would trigger based on the heap size doubling from the size at the end of the last collection (<strong class="source-inline">GOGC=100</strong>). In applications with large heap sizes, this could lead to long periods between GC cycles, followed by large, <span class="No-Break">disruptive collections.</span></p>
			<p>Developers used memory ballast as a buffer, artificially increasing the heap size to prompt more frequent, but smaller and less disruptive, GC cycles. It was a manual tuning method to optimize performance, particularly in high-throughput, low-latency systems. This technique was developed by the streaming company Twitch in 2019 in their post <em class="italic">How I learnt to stop worrying and love the </em><span class="No-Break"><em class="italic">heap</em></span><span class="No-Break"> (</span><a href="https://blog.twitch.tv/en/2019/04/10/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap/"><span class="No-Break">https://blog.twitch.tv/en/2019/04/10/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap/</span></a><span class="No-Break">).</span></p>
			<p>Twitch has a service called Visage that acts as the API frontend and is the central gateway for all externally originating API traffic. It’s built with Go and runs on AWS EC2. They faced challenges with handling large traffic spikes, notably during “refresh storms” when a popular broadcaster’s stream drops and restarts, causing viewers to refresh their pages repeatedly. The Visage application was triggering a high number of garbage collection cycles per second, which was consuming a significant portion of CPU cycles and increasing API latency during peak loads. The heap size of the application was relatively small, and during traffic spikes, the number of GC cycles would increase, further <span class="No-Break">degrading performance.</span></p>
			<p>When they introduced a memory ballast, it increased the base size of the heap, delaying GC triggers and reducing the number of GC cycles over time. This was achieved by allocating a very large byte array, which doesn’t get swept as garbage since it’s still referenced by the application. This array was created as the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
ballast := make([]byte, 10&lt;&lt;30)</pre>			<p>Very simple yet powerful, right? The results for them were <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The introduction of the memory ballast led to a ~99% reduction in <span class="No-Break">GC cycles</span><ul><li>CPU utilization of the API frontend servers reduced by ~30%, and the overall 99<span class="superscript">th</span>-percentile API latency during peak load reduced <span class="No-Break">by ~45%</span></li></ul></li>
				<li>The ballast effectively allowed the heap to grow larger before triggering GC, which improved per-host throughput and provided more reliable per-request handling <span class="No-Break">under load</span></li>
				<li>The ballast allocation resides mostly in virtual memory, making it a <span class="No-Break">cost-effective solution</span></li>
			</ul>
			<p>The memory <a id="_idIndexMarker407"/>ballast technique, while powerful in certain contexts like Twitch’s, is not universally applicable and should be avoided or used with caution in <span class="No-Break">several scenarios:</span></p>
			<ul>
				<li><strong class="bold">Memory-sensitive applications</strong>: In environments where memory is a scarce resource, allocating a large chunk of memory as a ballast might not be feasible. This is especially true for applications running on hardware with limited memory or in dense containerized environments where memory overhead is a <span class="No-Break">critical factor.</span></li>
				<li><strong class="bold">Applications with dynamic memory usage</strong>: If an application’s memory usage is highly dynamic and unpredictable, setting a fixed-size memory ballast could lead to inefficient <span class="No-Break">memory utilization.</span></li>
				<li><strong class="bold">Low-latency systems</strong>: While the memory ballast can reduce the frequency of garbage collection and thus improve throughput, it might not always benefit low-latency systems where the predictability of GC pauses is more critical. The ballast technique primarily optimizes throughput at the potential cost of increased latency due to larger heap sizes before <span class="No-Break">GC triggers.</span></li>
				<li><strong class="bold">Small heap footprint applications</strong>: Applications that naturally maintain a small heap footprint might not benefit from a memory ballast. In such cases, the overhead of managing a large, unused memory allocation might outweigh the benefits of reduced <span class="No-Break">GC frequency.</span></li>
				<li><strong class="bold">When GC tuning is sufficient</strong>: Sometimes, tuning the garbage collector’s parameters (e.g., the <strong class="source-inline">GOGC</strong> environment variable) can achieve the desired performance improvements without the need for a memory ballast. This approach should be considered first, as it’s a less invasive way to optimize <span class="No-Break">GC behavior.</span></li>
				<li><strong class="bold">When it masks underlying performance issues</strong>: Using a memory ballast to improve performance might mask underlying inefficiencies in the application code or architecture. It’s important to address these fundamental issues directly rather than relying on a memory ballast as <span class="No-Break">a workaround.</span></li>
			</ul>
			<p>The<a id="_idIndexMarker408"/> memory ballast is an excellent choice for managing these critical scenarios, but it’s only relevant up to Go version 1.19. Starting from version 1.20 and onward, there’s a standardized method to set the application’s “soft” memory limits using the <strong class="source-inline">GOMEMLIMIT</strong> <span class="No-Break">environment variable.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor189"/>GOMEMLIMIT</h1>
			<p>With <strong class="source-inline">GOMEMLIMIT</strong>, you set a soft cap <a id="_idIndexMarker409"/>on the memory usage of the Go runtime, encompassing the heap and other runtime-managed memory. This cap is like telling your application, “Here’s your memory budget; spend <span class="No-Break">it wisely.”</span></p>
			<p>Since Go 1.20, the strategic focus has shifted from manual tweaks such as memory ballast to leveraging built-in runtime features for memory management. <strong class="source-inline">GOMEMLIMIT</strong> offers a more straightforward and manageable approach to limiting <span class="No-Break">memory usage.</span></p>
			<p>The <strong class="source-inline">GOMEMLIMIT</strong> variable is used to set a soft memory limit for the runtime. This limit encompasses the Go heap and all other memory managed by the runtime, but it doesn’t include external memory sources such as mappings of the binary, memory managed in other languages, or memory held by the operating system on behalf of the Go program. <strong class="source-inline">GOMEMLIMIT</strong> is a numeric value measured in bytes, with the option to add a unit suffix for clarity. The supported suffixes include B, KiB, MiB, GiB, and TiB, following the IEC 80000-13 standard. These suffixes denote quantities of bytes based on powers of 2; for instance, KiB means 2^10 bytes, MiB means 2^20 bytes, and so on. By default, GOMEMLIMIT is set to <strong class="source-inline">math.MaxInt64</strong>, effectively disabling the memory limit. However, you can change this limit during runtime using <strong class="source-inline">runtime/debug.SetMemoryLimit</strong>.The key aspect to understand about <strong class="source-inline">GOMEMLIMIT</strong> is its “soft cap” nature. Unlike a hard limit, which would act as a strict ceiling on memory usage, a soft cap is more flexible. <strong class="source-inline">GOMEMLIMIT</strong> influences the garbage collector’s behavior, prompting it to run more aggressively as memory usage approaches the set limit. However, this doesn’t equate to an absolute prevention of going over the limit. It’s like a speed warning sign on a road; it suggests a safe speed but can’t physically<a id="_idIndexMarker410"/> slow down <span class="No-Break">your car.</span></p>
			<p class="callout-heading">Why not both?</p>
			<p class="callout">Using memory ballast alongside <strong class="source-inline">GOMEMLIMIT</strong> can be redundant, like wearing two watches to tell the same time. Ballast is used to artificially inflate the heap size to alter GC behavior, but with <strong class="source-inline">GOMEMLIMIT</strong>, you’re already defining the heap’s <span class="No-Break">upper limit.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor190"/>Memory arenas</h1>
			<p>The Go 1.20 release introduced an <a id="_idIndexMarker411"/>experimental arena package that offers memory arenas. These arenas can enhance performance by decreasing the number of allocations and deallocations that need to be done <span class="No-Break">during runtime.</span></p>
			<p>Memory arenas are a useful tool for allocating objects from a contiguous region of memory and freeing them all at once with minimal memory management or garbage collection overhead. They are especially helpful in functions that require the allocation of many objects, processing them for a significant amount of time, and then freeing all the objects at the end. It’s important to note that memory arenas are an experimental feature, available in Go 1.20 only when the <strong class="source-inline">GOEXPERIMENT=arenas</strong> environment variable <span class="No-Break">is set.</span></p>
			<p class="callout-heading">Warning</p>
			<p class="callout">The Go team does not provide support or guarantee compatibility for the API and implementation of memory arenas, and it may not exist in <span class="No-Break">future releases.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor191"/>Using memory arenas</h2>
			<p>Once we’ve <a id="_idIndexMarker412"/>set the <strong class="source-inline">GOEXPERIMENT=arenas</strong> environment variable, we can import the <span class="No-Break"><strong class="source-inline">arena</strong></span><span class="No-Break"> package:</span></p>
			<pre class="source-code">
import "arena"</pre>			<p>To create a new arena, we can use the <strong class="source-inline">NewArena()</strong> function, which returns the new <span class="No-Break">arena reference:</span></p>
			<pre class="source-code">
mem := arena.NewArena()</pre>			<p>Once we have<a id="_idIndexMarker413"/> an arena to use, we can ask for new references for our types. In the next snippet, we are creating a new reference in our arena for a <strong class="source-inline">Person</strong> <span class="No-Break">struct type:</span></p>
			<pre class="source-code">
mem := arena.NewArena()
p := arena.New[Person](mem)</pre>			<p>This is an important distinction from the normal flow of allocation. We aren’t creating new references and putting them in the arena. We ask the arena for <span class="No-Break">new references.</span></p>
			<p>There are some new APIs introduced in the arena package, such as <strong class="source-inline">MakeSlice</strong>, that ask for a predetermined capacity slice for an arena. If we want to ask for a new arena-bounded slice, we can use code <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
mem := arena.NewArena()
slice := arena.MakeSlice[string](mem, 100, 100)</pre>			<p>We can repeat this process and manipulate the objects normally, but when we’re done with our arena, we can <span class="No-Break">call </span><span class="No-Break"><strong class="source-inline">Free()</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
mem := arena.NewArena()
p := arena.New[Person](mem)
... other set of arena related operations
mem.Free()</pre>			<p>Remember, freeing the arena will deallocate all objects at once instead of scattered deallocations made during the sweep phase in the normal flow of <span class="No-Break">Go’s GC.</span></p>
			<p>Sometimes we <a id="_idIndexMarker414"/>want to send some objects created in the arena to heap (garbage collected) before freeing all objects in the arena. This can be achieved by using the <span class="No-Break"><strong class="source-inline">Clone</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
mem := arena.NewArena()
p1 := arena.New[Person](mem) // arena-allocated
p2 := arena.Clone(p1) // heap-allocated</pre>			<p>In this snippet, <strong class="source-inline">p1</strong> is arena-allocated while <strong class="source-inline">p2</strong> <span class="No-Break">is heap-allocated.</span></p>
			<h3>New solutions, old problems</h3>
			<p>Since we <a id="_idIndexMarker415"/>need to actively free our arena. This new step in our development can be error-prone. The most common problem is using arena objects after the arena is freed. To make things easier, the Go toolchain has a flag during the program execution to activate<a id="_idIndexMarker416"/> the <strong class="bold">address </strong><span class="No-Break"><strong class="bold">sanitizer</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">asan</strong></span><span class="No-Break">).</span></p>
			<p>Consider <span class="No-Break">this program:</span></p>
			<pre class="source-code">
type T struct {
    Num int
}
func main() {
    mem := arena.NewArena()
    o := arena.New[T](mem)
    mem.Free()
    o.Num = 123 // &lt;- this is a problem
}</pre>			<p>So, we can execute the program using the <span class="No-Break">address sanitizer:</span></p>
			<pre class="console">
go run -asan main.go</pre>			<p>The output will show the problem <span class="No-Break">as intended:</span></p>
			<pre class="console">
accessed data from freed user arena 0x40c0007ff7f8</pre>			<h3>Opportunities</h3>
			<p>There are several<a id="_idIndexMarker417"/> areas of Go development that could be positively affected by memory arenas. The most prevalent example is gRPC. Every time the program handles an RPC request, many objects are allocated during the process of encoding and decoding the messages. As we saw before, it tends to put more pressure on the GC. This strategy is somewhat the proof of how it affects the performance since the C++ implementation of gRPC already uses the concept of memory<a id="_idIndexMarker418"/> arenas (<a href="https://protobuf.dev/reference/cpp/arenas/">https://protobuf.dev/reference/cpp/arenas/</a>). Another example of arenas (as a concept) being used for performance gains is in the JSON serialization process. The fastjson<a id="_idIndexMarker419"/> project (<a href="https://github.com/valyala/fastjson">https://github.com/valyala/fastjson</a>) uses memory arenas to deal with marshaling and is allegedly 15x faster than the Go <span class="No-Break">standard library.</span></p>
			<h3>Guidelines</h3>
			<p>There are some<a id="_idIndexMarker420"/> questions that you can ask yourself before introducing arenas in <span class="No-Break">your project.</span></p>
			<p class="list-inset">            <em class="italic">Do I have the data about the issue </em><span class="No-Break"><em class="italic">I suspect?</em></span></p>
			<p>If you don’t have the data, <span class="No-Break">you’re guessing:</span></p>
			<p class="list-inset">            <em class="italic">Do I have several allocations or just </em><span class="No-Break"><em class="italic">a few?</em></span></p>
			<p>If you don’t have many allocations, your program will use more memory by introducing arenas. You can use the rule of thumb that an arena is 8 MB <span class="No-Break">in size.</span></p>
			<p class="list-inset">            <em class="italic">Does it have the same </em><span class="No-Break"><em class="italic">small structure?</em></span></p>
			<p>Maybe you’re looking for the wrong tool. Consider <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">sync.Pool</strong></span><span class="No-Break">.</span></p>
			<p class="list-inset">            <em class="italic">Is it the hot path of </em><span class="No-Break"><em class="italic">my program?</em></span></p>
			<p>It is probably a premature optimization. Experiment with several combinations of GC and <strong class="source-inline">GOMEMLIMIT</strong> before considering <span class="No-Break">memory arenas.</span></p>
			<p>It’s time to wrap up our knowledge of <span class="No-Break">memory management.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor192"/>Summary</h1>
			<p>We’ve explored GC, the difference between stack and heap allocation, and ways to optimize memory usage for better performance. Also, we’ve uncovered the evolution of Go’s garbage collector and its methods, including advanced topics such as its algorithm (tri-color/concurrent/mark <span class="No-Break">and sweep).</span></p>
			<p>We’ve also discussed practical approaches including using environmental variables such as <strong class="source-inline">GOGC</strong> to fine-tune garbage collection and employing techniques such as memory ballast and <strong class="source-inline">GOMEMLIMIT</strong> to help the GC manage the <span class="No-Break">program memory.</span></p>
			<p>During this chapter, you probably ask yourself: How much performance are we gaining, tinkering, and tweaking the GC and runtime parameters, and combining <span class="No-Break">these techniques?</span></p>
			<p>The answer is simple: <em class="italic">Performance is not a guessing game. We should </em><span class="No-Break"><em class="italic">measure it.</em></span></p>
			<p>In the next chapter (on analyzing performance), we’ll explore how to profile our application in terms of memory, CPU, allocations, and <span class="No-Break">much more.</span></p>
		</div>
	</div>
</div>
</body></html>