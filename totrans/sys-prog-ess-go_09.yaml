- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will embark on a deep dive into the intricacies of performance
    analysis within the Go programming language, focusing on critical concepts such
    as escape analysis, stack and pointers, and the nuanced interplay between stack
    and heap memory allocations. By exploring these fundamental aspects, this chapter
    aims to equip you with the knowledge and skills necessary to optimize Go applications
    for maximum efficiency and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these concepts is crucial for improving the performance of Go
    applications and gaining insight into system programming principles. This knowledge
    is invaluable in the real world, where efficient memory management and performance
    optimization can significantly impact the scalability, reliability, and overall
    success of software projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Escape analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid foundation in analyzing and
    optimizing the performance of Go applications, preparing them for more advanced
    topics in system programming and application development.
  prefs: []
  type: TYPE_NORMAL
- en: Escape analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Escape analysis is a compiler optimization technique that’s used to determine
    whether a variable can be safely allocated on the stack or if it must “escape”
    to the heap. The primary goal of escape analysis is to improve memory usage and
    performance by allocating variables on the stack whenever possible since stack
    allocations are faster and more CPU cache-friendly than heap allocations.
  prefs: []
  type: TYPE_NORMAL
- en: Stack and pointers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ah, stacks and pointers in Go – the bread and butter of any self-respecting
    system programmer and yet, somehow, the source of an unending stream of confusion
    for many. Let’s be clear: if you think managing stacks and pointers is as easy
    as pie, you’re probably not baking it right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a software development world where pointers are like those high-maintenance
    friends who need constant updates on where you are and what you’re doing. Except,
    in this world, failing to keep them in the loop doesn’t just hurt feelings; it
    crashes programs. This is the delightful quagmire of stacks and pointers in Go:
    a never-ending party where everyone needs to know exactly where to stand, or the
    whole thing comes tumbling down.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get down to brass tacks. The stack, in the context of Go, is a beautifully
    simple yet profoundly complex beast. It’s where all your local variables hang
    out, living their short, ephemeral lives before gracefully bowing out when their
    function calls end. It’s efficient, it’s tidy, and it’s mercilessly unforgiving
    if you don’t play by its rules.
  prefs: []
  type: TYPE_NORMAL
- en: Pointers, on the other hand, are the stack’s extroverted cousins. They don’t
    live on the stack; they thrive on pointing to values, wherever those values might
    reside. Whether it’s on the stack, the heap, or the twilight zone of memory management,
    pointers are your ticket to manipulating data directly, bypassing the pleasantries
    of value copying and embracing the raw power of memory access.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the interplay between the stack and pointers is crucial for any
    Go programmer. It’s about knowing when to let your variables live a carefree life
    on the stack and when to introduce a pointer into the mix, to point at something
    potentially far more enduring. It’s a dance of memory management, performance
    optimization, and avoiding the dreaded segmentation fault.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this simple Go code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `a` lives on the stack, a happy local variable. `b` is a pointer to `a`,
    allowing us to manipulate the value of `a` directly through `b`. It’s a small
    window into the power of pointers and the stack, showing how they interact in
    a controlled environment.
  prefs: []
  type: TYPE_NORMAL
- en: Reflecting on my early days of wrestling with Go, I recall a project that was
    plagued with memory management issues. It felt like being lost in a forest, with
    pointers as my only compass. The breakthrough came when I realized that pointers
    and the stack were not just tools but the very fabric of Go’s memory management.
    It was like understanding that to navigate the forest, I didn’t just need to know
    where the trees were; I needed to understand how the forest grew. This moment
    of clarity came when I likened pointers to bookmarks in a novel, marking where
    the important parts of the story were, allowing me to jump back and forth without
    losing my place.
  prefs: []
  type: TYPE_NORMAL
- en: Think of the stack as a stack of dishes. When you’re cleaning up after dinner,
    you start piling dishes one on top of the other. The last dish you put on the
    stack is the first one you wash. The stack in Go works similarly with your function
    calls and local variables. When a function is called, Go throws everything it
    needs (such as variables) onto the stack. Once the function is done, Go clears
    those off, making room for the next function’s stuff. It’s a tidy way to handle
    memory that’s super-fast because it’s all automatic. You don’t need to tell Go
    to clean up; it just does.
  prefs: []
  type: TYPE_NORMAL
- en: Now, onto pointers. If the stack is about organization, pointers are about connections.
    A pointer in Go is like having the address of a friend’s house. You don’t have
    a house, but you know where to find it. In Go, pointers hold the memory address
    of a variable. This means you can directly change the value of a variable somewhere
    else in your program without needing to pass around the variable itself. It’s
    like texting your friend to turn on their porch light instead of walking over
    to do it yourself. Pointers are powerful because they let you manipulate data
    efficiently. However, with great power comes great responsibility. Misusing pointers
    can lead to bugs that are hard to track down.
  prefs: []
  type: TYPE_NORMAL
- en: In system programming, you’re often working closer to the hardware, where efficiency
    and control over memory are critical. Understanding how the stack works helps
    you write efficient functions that don’t waste memory. Pointers give you the control
    you need to interact with memory locations directly, which is essential for tasks
    such as handling resources or working with low-level system structures.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts are fundamental in Go because they are designed to be simple
    yet powerful. It manages memory automatically in many cases, but knowing how and
    why it does this gives you the edge in writing high-performance applications.
    Whether you’re managing resources, optimizing performance, or just trying to debug
    your program, a solid grasp of stacks and pointers will make your life much easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as we dive deeper into the mechanics of Go, remember: understanding stacks
    and pointers is not just about memorizing definitions. It’s about getting to know
    the very fabric of system programming in Go, enabling you to write cleaner, faster,
    and more efficient code.'
  prefs: []
  type: TYPE_NORMAL
- en: Pointers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pointers are your Swiss Army knife. They’re not just a feature; they’re a fundamental
    concept that can make or break your code’s efficiency and simplicity. Let’s demystify
    pointers and learn how to wield them with precision.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a pointer is a variable that holds the address of another variable.
    Instead of carrying around the value itself, it points to where the value lives
    in memory. Imagine that you’re at a huge music festival. A pointer is not the
    stage where the band is playing; it’s the map that shows you where the stage is.
    In Go, this concept allows you to directly interact with the memory location of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To declare a pointer in Go, you use an asterisk (`*`) before the type. This
    tells Go, “This variable is going to hold a memory address, not a direct value.”
    Here’s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This line declares a pointer, `p`, that will point to an integer. But right
    now, `p` doesn’t point to anything. It’s like having a map with no marked locations.
    To point it at an actual integer, you must use the address-of operator (`&`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, `p` holds the address of `x`. You’ve marked your stage on the festival
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dereferencing is how you access the value at the memory address the pointer
    is holding. You can do this with the same asterisk (`*`) you used to declare a
    pointer, but in a different context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This line doesn’t print the memory address stored in `p`; it prints the value
    of `x` that `p` points to, thanks to dereferencing. You’ve gone from looking at
    the map to standing in front of the stage, enjoying the music.
  prefs: []
  type: TYPE_NORMAL
- en: With pointers, you can manipulate data without copying it around, saving time
    and memory – a critical advantage when resources are tight, or speed is paramount.
    They also allow you to interact with hardware, perform low-level system calls,
    or handle data structures in the most efficient way possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some best practices concerning pointers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep it simple**: Only use pointers when necessary. Go’s garbage collector
    works wonders with memory management, but pointers, when used wisely, can enhance
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nil` before dereferencing to avoid runtime panics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pointer passing**: When passing large structs to functions, use pointers
    to avoid copying the entire structure. It’s faster and more memory-efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pointers are a gateway to mastering Go, especially for system programming, where
    direct memory access and manipulation are often required. By understanding and
    applying pointers effectively, you unlock a deeper level of control over your
    programs, paving the way for writing more efficient, powerful, and sophisticated
    system-level applications.
  prefs: []
  type: TYPE_NORMAL
- en: Stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stack plays a crucial role and acts as the backbone of memory management.
    It’s where the magic happens for managing function calls and local variables.
    Let’s dive into the stack and why it’s a big deal in system programming.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the stack as a stack of trays in a cafeteria. Each tray represents a
    function call with its own set of dishes (local variables). When a new function
    is called, a tray is added to the top. When the function returns, the tray is
    removed, leaving no mess behind. This last-in, first-out mechanism ensures that
    the most recent function call is always on top, ready to be cleaned up as soon
    as it’s done.
  prefs: []
  type: TYPE_NORMAL
- en: Go leverages the stack to manage the life cycle of function calls and their
    local variables. When a function is called, Go automatically allocates space on
    the stack for its local variables. This space is efficiently managed by Go, freeing
    up the memory once the function call is complete. This automatic handling is a
    boon for system programmers as it simplifies memory management and enhances performance.
  prefs: []
  type: TYPE_NORMAL
- en: Each function call creates what’s known as a “stack frame” on the stack. This
    frame contains all the necessary information for the function, including its local
    variables, arguments, and the return address. The stack frame is critical for
    the function’s execution, providing a self-contained block of memory that’s efficiently
    managed by the Go runtime.
  prefs: []
  type: TYPE_NORMAL
- en: While the stack is efficient, it’s not limitless. Each Go program has a fixed
    stack size, which means you need to be mindful of how much memory your function
    calls and local variables are using. Deep recursion or large local variables can
    lead to a stack overflow, crashing your program. However, Go’s runtime tries to
    mitigate this by using a dynamically resizing stack, which grows and shrinks as
    needed, within limits.
  prefs: []
  type: TYPE_NORMAL
- en: Heap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think back to our cafeteria analogy. The stack, with its trays, is great for
    quick meals where items are neatly contained on a single tray. But what about
    a buffet-style situation or an elaborate dinner party? You’d need a much larger,
    more flexible space to lay everything out. This is where the heap comes in.
  prefs: []
  type: TYPE_NORMAL
- en: The heap is a less structured area of memory. It’s like a giant storage room
    where Go can store data of varying sizes as needed. When you need to hold a big
    array that expands and contracts over time or create complex objects with lots
    of interconnected pieces, the heap is your go-to place.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of this flexibility is a slight loss in speed. The system needs to
    keep track of what’s on the heap, where free space is available, and when memory
    is no longer in use. This bookkeeping makes things a tad slower than the stack’s
    streamlined operation.
  prefs: []
  type: TYPE_NORMAL
- en: The stack and the heap – partners in memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Go, the stack and the heap work together seamlessly. Imagine the following
    scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: You write a function that creates a large data structure, let’s say a linked
    list. The function itself gets a tidy spot on the stack (its stack frame).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linked list itself, with its nodes and data, gets space on the heap, where
    it can grow and shrink as needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside your function’s stack frame, there’s a pointer referencing the start
    of your linked list on the heap. This way, the function can find and manipulate
    the data structure living in the flexible heap space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heap, while powerful, requires careful attention from system programmers.
    If you constantly allocate and deallocate varying-sized chunks of memory from
    the heap, it can become fragmented over time, making it harder to find large,
    contiguous spaces. It’s commonly referenced as memory fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some best practices concerning allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimize large local variables**: Consider using the heap for large data
    structures to avoid consuming too much stack space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Be cautious with recursion**: Ensure recursive functions have a clear termination
    condition to prevent stack overflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understand stack versus heap allocation**: Use the stack for short-lived
    variables and the heap for variables that need to outlive the function call'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can make sure where our variables live using escape analysis.
  prefs: []
  type: TYPE_NORMAL
- en: How can we analyze?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Escape analysis in Go is the dark art that even seasoned developers pretend
    to understand while secretly googling it during code reviews. It’s like claiming
    you enjoy free jazz; it sounds sophisticated until someone asks you to explain
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re at a party, and someone decides to explain quantum mechanics,
    but every explanation somehow loops back to their sourdough starter. That’s the
    equivalent of trying to wrap your head around escape analysis without getting
    your hands dirty in the code. It’s complex and slightly pretentious, and everyone
    nods along without really getting it.
  prefs: []
  type: TYPE_NORMAL
- en: Escape analysis, at its core, is the compiler’s way of deciding where variables
    live in your Go programs. It’s like a strict landlord deciding whether your variable
    is trustworthy enough to rent space on the stack or if it’s too sketchy and needs
    to be kicked out of the heap. The goal here is efficiency and speed. Variables
    on the stack are like friends crashing on your couch for the night; they’re easy
    to manage and leave quickly. Variables on the heap are more like signing a lease;
    more commitment is required, and the process is slower.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler performs this analysis during the compilation phase, scrutinizing
    your code to predict how variables are used and whether they escape the function
    they’re created in. If a variable is passed back to the caller, it’s considered
    to have “escaped.” This decision impacts performance significantly. Stack allocation
    is faster and more CPU cache-friendly than heap allocation, which is slower and
    requires garbage collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this, let’s dive into a simple code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, `a` is an integer that, in a simpler world, would happily live
    on the stack. However, because we take its address and assign it to `b`, the compiler
    fears `a` might escape the confines of `main()`. Thus, it might decide to allocate
    `a` on the heap to be safe, even though, in this case, it doesn’t escape.
  prefs: []
  type: TYPE_NORMAL
- en: Recalling the hurdles of my early days of learning Go, I recall a project where
    optimizing a critical path led me down the rabbit hole of escape analysis. After
    hours of profiling and tweaking, the breakthrough came when I realized a variable,
    innocuously passed by reference to several functions, was the culprit of my heap
    allocation woes. By adjusting the code to keep this variable on the stack, the
    performance gains were akin to switching from a tricycle to a sports car on an
    open highway.
  prefs: []
  type: TYPE_NORMAL
- en: In Go, a goroutine’s stack memory is strictly its own; *no goroutine can have
    a pointer to another goroutine’s stack*. This isolation ensures that the runtime
    does not need to manage complex pointer references across goroutines, simplifying
    memory management and avoiding potential latency issues from stack resizing.
  prefs: []
  type: TYPE_NORMAL
- en: When a value is passed outside its function’s stack frame, it may need to be
    allocated on the heap to ensure its persistence beyond the function call. This
    determination is made by the compiler through escape analysis. The compiler analyzes
    function calls and variable references to decide whether a variable’s lifetime
    extends beyond its current stack frame, necessitating heap allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example, which illustrates escape analysis in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `createPerson` function creates a `person` struct and returns
    a pointer to it. Due to the `return &p` statement, the `person` struct “escapes”
    to the heap because its reference is passed back to the caller, extending its
    lifetime beyond the `createPerson` function’s stack frame.
  prefs: []
  type: TYPE_NORMAL
- en: To see how the Go compiler performs escape analysis, you can compile your Go
    program with the `-gcflags "-m -``m"` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ch9/escape-analysis` directory, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This command prints detailed information about the compiler’s decisions on variable
    allocations. Understanding these reports can help you write more efficient Go
    code by minimizing unnecessary heap allocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive a little deeper into this sequence brought by escape analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inlining and `go:noinline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`createPerson` function. This is sometimes necessary for complex functions
    or if inlining introduces unwanted side effects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complexity cost and budget:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`80`, in this case). Exceeding this budget means the compiler decides the function
    is too complex to benefit from inlining.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Informational:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is informational. The compiler is successfully inlining a call to the `fmt.Println`
    function. It’s good practice to keep `fmt.Println` usage simple, ensuring it doesn’t
    impede inlining.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Escape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Escape analysis**: Go analyzes whether variables “escape” their current function’s
    scope. If a variable escapes, it must be allocated on the heap (for a longer lifetime)
    instead of the stack.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a variable, `p`, whose address is being returned on line 18\. Since
    this address can be used outside the current function, `p` must live on the heap.
  prefs: []
  type: TYPE_NORMAL
- en: Escape analysis is a powerful feature of the Go compiler that helps manage memory
    efficiently by determining the most appropriate location for variable allocation.
    By understanding how and why variables escape to the heap, you can write more
    efficient Go programs that make better use of system resources.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue to develop in Go, keep escape analysis in mind, especially when
    working with pointers and function returns. Remember, the goal is to allow the
    compiler to optimize memory usage, improving the performance of your Go applications.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can check where our allocations go, how do we determine if the performance
    has improved? A good start is to benchmark our code.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking your code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarking in Go is the sacred ritual where developers often embark on a quest
    for performance enlightenment, only to find themselves lost in a maze of micro-optimizations.
    It’s like preparing for a marathon by obsessively timing how fast you can tie
    your shoelaces, completely missing the point of the broader training regimen.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, if you will, a seasoned software developer likened to a master chef,
    meticulously selecting each ingredient for the perfect dish. In this culinary
    quest, the chef knows that the choice between Himalayan pink salt and sea salt
    isn’t just about taste – it’s about the subtle nuances that can elevate a dish
    from good to sublime. Similarly, in software development, the choice between different
    algorithms or data structures isn’t just about speed or memory usage on paper;
    it’s about understanding the intricate dance of cache misses, branch prediction,
    and execution pipelines. It’s an art form where brushstrokes matter as much as
    the canvas.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get into the meat of the matter – benchmarks in Go. At its core,
    benchmarking is a systematic method of measuring and comparing the performance
    of software. It’s not just about running a piece of code and seeing how fast it
    goes; it’s about creating a controlled environment where you can understand the
    impact of changes in code, algorithms, or system architecture. The goal is to
    provide actionable insights that guide optimization efforts, ensuring that they’re
    not just shots in the dark.
  prefs: []
  type: TYPE_NORMAL
- en: Go, with its rich standard library and tooling, offers a robust framework for
    benchmarking. The `testing` package is a jewel in the crown, allowing developers
    to write benchmark tests that are as straightforward as their unit tests. These
    benchmarks can then be executed with the `go test` command, providing detailed
    performance metrics that can be used to identify bottlenecks or validate efficiency
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that `Fib` is a function that calculates the nth Fibonacci number.
    To create a benchmark, you must write a function in a `_test.go` file that starts
    with `Benchmark` and takes a `*testing.B` parameter. The `go test` command is
    used to run these benchmark functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet illustrates the essence of Go’s benchmarking approach: concise,
    readable, and focused on measuring the performance of a specific piece of code
    under repeatable conditions. The `b.N` loop allows the benchmarking framework
    to adjust the number of iterations dynamically, ensuring that the measurements
    are both accurate and reliable.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing your first benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For your first benchmark, you’ll create a function called `Sum` that adds two
    integers. The benchmark function, `BenchmarkSum`, measures how long it takes to
    execute `Sum(1, 2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `*testing.B` parameter provides control and reporting facilities for the
    benchmark. The most important field in `*testing.B` is `N`, which represents the
    number of iterations the benchmark function should execute the code under test.
    The Go testing framework automatically determines the best value of `N` to get
    a reliable measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run benchmarks, use the `go test` command with the `-bench` flag, specifying
    a regular expression as an argument to match the benchmark functions you want
    to run. For example, to run all benchmarks, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of a benchmark run provides several pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BenchmarkSum-8`: The name of the benchmark function, with -8 indicating the
    value of `GOMAXPROCS`, which shows the benchmark was run with parallelism set
    to 8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1000000000`: The number of iterations determined by the testing framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.277 ns/op`: The average time taken per operation (in this case, nanoseconds
    per operation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go allows you to define sub-benchmarks within a benchmark function, enabling
    you to test different scenarios or inputs systematically. Here’s how you can use
    sub-benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a` and `b`. These structs are used to provide different inputs to the `Sum`
    function, allowing us to benchmark its performance across different scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`for` loop. For each case, it calls `b.Run()` to execute a sub-benchmark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b.Run()` function takes two parameters: the name of the sub-benchmark (derived
    from the test case) and a function that contains the actual benchmark code. This
    allows the Go testing framework to treat each set of inputs as a separate benchmark,
    providing individual performance metrics for each.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b.N` times, calling the `Sum` function with the test case’s inputs. This measures
    the performance of `Sum` under the specific conditions defined by the inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we run the tests with the benchmark flag again, the result should be something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That’s great! Now, we can explore how much memory our program parts are using
    so that we can get a better understanding of their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To measure memory allocations, you can use the `-benchmem` flag when running
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This flag adds two more columns to the output: `allocs/op`, which specifies
    the number of memory allocations per operation, and `B/op`, which specifies the
    number of bytes allocated per operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s some example output when using `-benchmem`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`16 B/op`: This indicates that each operation (in this case, each call to `Sum`)
    allocates 16 bytes of memory. This metric helps identify how changes to your code
    affect its memory footprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2 allocs/op`: This shows the number of memory allocations that occur per operation.
    In this example, each call to `Sum` results in two memory allocations. Reducing
    the number of allocations can often improve performance, especially in tight loops
    or performance-critical sections of code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are doing very well at this point, but how can we identify if our code changes
    were effective? In this case, we should rely on comparing the benchmark results.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To compare benchmarks, we’ll use a Go tool called `benchstat`, which provides
    a statistical analysis of benchmark results. It is particularly useful for comparing
    benchmark outputs from different test runs, making it easier to understand the
    performance changes between different versions of your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to install `benchstat`. Assuming you have Go installed on your
    system, you can install `benchstat` using the `go install` command. Since Go 1.16,
    it’s recommended to use this command with a version suffix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This command downloads and installs the `benchstat` binary in your Go binary
    directory (usually `$GOPATH/bin` or `$HOME/go/bin`). Ensure this directory is
    in your system’s `PATH` so that you can run `benchstat` from any terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to run benchmarks and save their outputs to files. You can run
    your benchmarks using the `go test -bench` command, redirecting the output to
    a file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the first benchmark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make changes to your code and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the benchmark results saved in `old.txt` and `new.txt`, you can use `benchstat`
    to compare these results and analyze the performance differences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Interprete the output of `benchstat`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our new tool provides a tabulated output with several columns. Here’s an example
    of what the output might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name of the benchmark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`old time/op`: The average time per operation for the first set of benchmarks
    (from `old.txt`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`new time/op`: The average time per operation for the second set of benchmarks
    (from `new.txt`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delta`: The percentage change in time per operation from the old to the new
    benchmarks. A negative delta indicates an improvement (faster code), while a positive
    delta indicates a regression (slower code).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p`: The p-value from a statistical test (usually a t-test) compares the old
    and new benchmarks. A low p-value (typically <0.05) suggests that the observed
    performance difference is statistically significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: The number of samples used to compute the average time per operation for
    both the old and new benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical terms
  prefs: []
  type: TYPE_NORMAL
- en: The `±` symbol, when followed by a percentage, indicates the margin of error
    around the average time per operation. It gives you an idea of the variability
    of your benchmark results.
  prefs: []
  type: TYPE_NORMAL
- en: The `benchstat` binary is a powerful tool for analyzing the performance of your
    Go code, offering a clear, statistical comparison of benchmark results. Remember,
    while `benchstat` can highlight significant changes, it’s also important to consider
    the context of your benchmarks and the real-world implications of any performance
    differences.
  prefs: []
  type: TYPE_NORMAL
- en: Extra arguments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When running benchmarks in Go, you have the flexibility to control not only
    how long and how many times the benchmarks are run but also which specific benchmarks
    to execute. This is particularly useful when you’re working on optimizing or debugging
    a specific part of your code and you only want to run benchmarks related to that
    code. The `-benchtime=`, `-count`, and `-bench=` flags can be combined effectively
    to run benchmarks selectively and with precise control over their execution parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Using the -bench= flag to filter benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `-bench=` flag allows you to specify a **regular expression** (**regex**)
    that matches the names of the benchmarks you want to run. Only benchmarks whose
    names match the regex will be executed. This is incredibly useful for selectively
    running benchmarks without having to run your entire suite.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say you have several benchmarks in your package: `BenchmarkSum`,
    `BenchmarkMultiply`, and `BenchmarkDivide`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you only want to run `BenchmarkMultiply`, you can use the `-bench=` flag
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This command tells the Go test runner to only execute benchmarks whose names
    match `BenchmarkMultiply`. The matching is case-sensitive and based on Go’s regular
    expression syntax, giving you a lot of flexibility in specifying which benchmarks
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: Combining all of them
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can combine `-bench=` with `-benchtime=` and `-count` to finely control
    the execution of specific benchmarks. For instance, if you want to run `BenchmarkMultiply`
    for a longer duration and repeat the benchmark multiple times to get a more reliable
    measurement, you could use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This command will run the `BenchmarkMultiply` benchmark for at least 3 seconds
    each time and repeat the whole benchmark five times. This approach is beneficial
    when you’re trying to measure the impact of performance optimizations or ensure
    that changes haven’t introduced performance regressions.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for filtering benchmarks
  prefs: []
  type: TYPE_NORMAL
- en: There are three main tips for filtering benchmarks. The first is often called
    `-bench=.` will run all benchmarks in the package, while `-bench=Benchmark` will
    run any benchmark that starts with `Benchmark`.
  prefs: []
  type: TYPE_NORMAL
- en: The second is `-bench=` flag. For example, if you have sub-benchmarks named
    `BenchmarkMultiply/small` and `BenchmarkMultiply/large`, you can run just the
    “large” sub-benchmarks with `-bench=BenchmarkMultiply/large`.
  prefs: []
  type: TYPE_NORMAL
- en: The last one is making sure you avoid `-bench=Multiply` would match `BenchmarkMultiply`
    but could also match `BenchmarkComplexMultiply` if such a benchmark exists. Use
    more specific patterns to narrow down the benchmarks you want to run.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to filter benchmarks with `-bench=`, control the benchmark time
    with `-benchtime=`, and specify the number of runs with `-count` provides a powerful
    set of tools for Go developers looking to optimize their code. By running only
    the benchmarks you’re interested in, for the duration and number of times that
    provide meaningful data, you can focus your optimization efforts more effectively
    and understand the performance characteristics of your code with greater clarity.
  prefs: []
  type: TYPE_NORMAL
- en: Common pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of common pitfalls during benchmarking. Let’s explore the most
    common ones.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 1 – benchmarking the** **wrong thing**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most fundamental mistakes is to benchmark the wrong aspect of your
    code. For instance, when benchmarking a function that sorts a slice, if the slice
    is sorted only once and then reused across benchmark iterations without re-initialization,
    subsequent iterations will operate on already sorted data, skewing the results.
    This mistake highlights the importance of setting up the benchmark’s state correctly
    for each iteration to ensure that you’re measuring the intended operation.
  prefs: []
  type: TYPE_NORMAL
- en: '`b.ResetTimer()` and properly initialize the state within the benchmark loop,
    ensuring each iteration benchmarks the operation under the same conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 2 –** **compiler optimizations**'
  prefs: []
  type: TYPE_NORMAL
- en: The Go compiler, like many others, optimizes code, which can lead to misleading
    benchmark results. For example, if the result of a function call is not used,
    the compiler might optimize away the call entirely. Similarly, constant propagation
    can lead to the compiler replacing a function call with a pre-computed result.
  prefs: []
  type: TYPE_NORMAL
- en: '`runtime.KeepAlive` to ensure the compiler treats the result as needed at runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 3 –** **warmup**'
  prefs: []
  type: TYPE_NORMAL
- en: Modern CPUs and systems have various levels of caches and optimizations that
    “warm up” over time. Starting measurements too early before the system reaches
    a steady state can lead to inaccurate results that do not reflect typical performance.
  prefs: []
  type: TYPE_NORMAL
- en: '`b.ResetTimer()` in Go benchmarks to start timing after the initial setup or
    warmup phase.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 4 –** **environment**'
  prefs: []
  type: TYPE_NORMAL
- en: Running benchmarks in environments that differ significantly from production
    can lead to results that are not representative of real-world performance. Differences
    in hardware, operating system, network conditions, and even the load under which
    the benchmark is run can all affect the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: As much as possible, run benchmarks under conditions that closely
    mimic production environments. This includes using similar hardware, running the
    same version of the Go runtime, and simulating realistic load and usage patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 5 – ignoring garbage collection and other** **runtime costs**'
  prefs: []
  type: TYPE_NORMAL
- en: Go’s runtime, including garbage collection, can significantly impact performance.
    Benchmarks that do not take these costs into account may not accurately reflect
    the performance users will experience.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: Be mindful of the impact of garbage collection and other runtime
    behaviors on your benchmarks. Use runtime metrics and profiling to understand
    how these factors affect your benchmarks. Consider running longer benchmarks to
    capture the impact of garbage collection cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall 6 – using** **b.N incorrectly**'
  prefs: []
  type: TYPE_NORMAL
- en: The misuse of the `b.N` argument in Go benchmarks can lead to inaccurate results
    and misinterpretations. There are at least two common scenarios where `b.N` is
    misused, each with its pitfalls. Let’s explore them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, developers might attempt to misuse `b.N` within a recursive
    function in a benchmark. This can lead to unexpected behavior and inaccurate measurements.
    Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `b.N` is misused as an argument to the `recursiveFibonacci` recursive
    function. This misuse can lead to unexpected behavior and incorrect benchmark
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, developers might misuse `b.N` when their benchmark code involves complex
    setup or initialization that should not be repeated for each iteration. Here’s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this scenario, `b.N` is misused to repeatedly execute the setup code within
    the benchmark loop. This can skew benchmark results if the setup is intended to
    be performed only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, developers might misuse `b.N` within benchmarks that involve conditional
    logic based on the iteration count. Let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `b.N` is misused to conditionally execute different code paths
    based on the iteration count. This can lead to inconsistent benchmark results
    and make it challenging to interpret performance measurements.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, benchmarking in Go – or any language, for that matter – is less
    about the raw pursuit of speed and more about the art of making informed decisions.
    It’s like navigating a ship through treacherous waters; without a compass (benchmarks)
    and a skilled navigator (the developer), you’re just drifting, hoping to reach
    your destination.
  prefs: []
  type: TYPE_NORMAL
- en: The real skill lies not in how fast you can go, but in knowing where to make
    the turns.
  prefs: []
  type: TYPE_NORMAL
- en: CPU profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CPU profiling is the process of analyzing how much CPU time is consumed by
    different sections of your Go program. This analysis helps you identify the following
    aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bottlenecks**: Code areas using excessive CPU time, slowing down your application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inefficiencies**: Functions or code blocks that can be optimized to use less
    CPU resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hotspots**: The most frequently executed parts of your program, the prime
    focus for optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To exercise profiling, we’ll create a **file change monitor**. The program will
    monitor a specified directory for file changes. To make the scope concise, our
    program will detect file creation, deletion, and modification. Also, upon detecting
    changes, it sends alerts (printed to the console).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code can be found in this book’s GitHub repository. For now, we
    are exploring the core features and the corresponding code sections so that we
    have a clearer understanding of how it operates:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the file metadata structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This struct defines the simplified file metadata the program will track, including
    the file’s name, modification time, and size. This is crucial for comparing the
    current state of the filesystem to a previous state to detect changes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scan the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `scanDirectory` function uses `filepath.WalkDir` to traverse the directory
    and subdirectories, collecting metadata for each file and storing it in a map.
    This map serves as a snapshot of the directory’s state at the time of scanning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compare directory states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `compareAndEmitEvents` function iterates through the new and old state maps
    to find differences, which indicate file creations, deletions, or modifications.
    For each detected change, it calls `sendAlert` using a goroutine, which allows
    these alerts to be processed asynchronously.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Emit alerts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function is responsible for handling the alerts. In the current implementation,
    it simply prints the alert to the console. Running this in a separate goroutine
    for each alert ensures that the directory scanning and comparison process is not
    blocked by the alerting mechanism.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Main monitoring loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `main()` function, the directory is initially scanned to establish a
    baseline state. The program then enters a loop, rescanning the directory at specified
    intervals, comparing the new scan results to the previous state, and updating
    the state for the next iteration. This loop continues indefinitely until the program
    is stopped.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Goroutine usage for alerts: The asynchronous execution of `sendAlert` via go
    `sendAlert(...)` inside `compareAndEmitEvents` ensures that the program remains
    responsive and that the monitoring interval is consistent, even if the alerting
    process has latency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Error handling: Error handling is demonstrated in both the scanning and main
    loop portions of the code, ensuring that the program can gracefully handle issues
    that are encountered during directory scanning. However, detailed error handling
    (especially for real-world applications) would involve more comprehensive checks
    and responses to various error conditions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To enable CPU profiling, we need to change our program. First, add the following
    import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This imports the `pprof` package from the Go runtime, which provides functions
    for collecting and writing profiling data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what each line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`os.Create("cpuprofile.out")`: This line creates a file named `cpuprofile.out`
    where the CPU profile data will be written. This file is created in the current
    working directory of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defer f.Close()`: This line ensures that the file is closed when the function
    returns. This is important to guarantee that all data is flushed to disk and the
    file is closed properly. Here, `defer` is used to schedule the close operation
    to run after the function completes, which includes normal completion or if an
    error causes an early return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pprof.StartCPUProfile(f)`: This line starts the CPU profiling process. It
    takes `io.Writer` as an argument (in this case, the file we created earlier) and
    begins recording CPU profile data. All the CPU that’s used by your application
    from this point until `pprof.StopCPUProfile()` is called will be recorded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defer pprof.StopCPUProfile()`: This line schedules when CPU profiling should
    stop – that is, when the function returns. This ensures that profiling is concluded
    properly, and all collected data is written to the specified file before the application
    exits or moves on to subsequent operations. The use of `defer` is critical here
    to ensure that profiling is stopped even if an error occurs, or a return is triggered
    earlier in your code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can build the program by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the program, ensuring it monitors an active directory (where you’ll
    simulate file changes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'While the program runs, introduce changes in the monitored directory: create
    files, delete files, and modify content within those files. This creates a realistic
    workload for profiling.'
  prefs: []
  type: TYPE_NORMAL
- en: After running your program with CPU profiling enabled, you can analyze the `cpuprofile.out`
    file using Go’s `pprof` tool to view the profiling results and identify hotspots
    in your code. This step is crucial for performance tuning and ensuring your application
    runs efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main options on how to analyze the `cpuprofile.out` file: textually
    and via a flame graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyze the profile textually, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This result lists functions sorted in descending order of CPU time consumed.
  prefs: []
  type: TYPE_NORMAL
- en: From this, we can interpret that we can focus on the top few entries. These
    are the primary candidates for optimization. Also, examine call stacks. They show
    how those expensive functions are reached within your program’s logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyze the profile using a flame graph, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This method provides a visual way to pinpoint hotspots. Wider bars represent
    functions that use more CPU time.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should keep the following points in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Width of bars**: This represents the proportion of CPU time spent within
    a function. Wider bars mean more time consumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchy**: This shows the call stacks. Functions that call other functions
    are stacked on top.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-down**: Start analyzing from the top of the graph, following the paths
    where the bars are widest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start to change the program to see the results in the profile, let’s
    learn how to memory profile this program to make the trade-offs between memory
    and CPU clear after making future improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory profiling helps you analyze how your Go program allocates and uses memory.
    It’s critical in systems programming. where you frequently deal with constrained
    resources and performance-sensitive operations. Here are some key questions it
    helps answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory leaks**: Are you unintentionally holding on to memory that’s no longer
    needed?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allocation hotspots**: Which functions or code blocks are responsible for
    most allocations?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory usage patterns**: How does memory use change over time, especially
    under different load conditions?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object sizes**: How can you understand the memory footprint of key data structures?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s learn how to set up memory profiling for our monitoring program based
    on the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s understand what’s happening here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`os.Create("memprofile.out")`: This line creates a file named `memprofile.out`
    in the current working directory. This file is designated to store the memory
    profile data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defer f.Close()`: This line schedules the `Close` method on `f` to be called
    once the surrounding function (main) returns. This is to ensure the file is closed
    properly and all data written to it is flushed to disk, regardless of how the
    function exits (normally or due to an error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runtime.GC()`: This line is optional and triggers garbage collection before
    writing the heap profile. Its purpose is to clean up unused memory and provide
    a more accurate view of what memory is actively in use by your program at the
    time of profiling. It helps in identifying memory that is truly needed by your
    program as opposed to memory that is ready to be collected but hasn’t been yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pprof.WriteHeapProfile(f)`: This line writes the memory profile data to the
    previously created file. This profile includes information about memory allocation
    by your program, which can be analyzed to understand memory usage patterns and
    identify potential issues, such as memory leaks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can build and run the program again, but this time, after simulating the
    workload, we’ll have a new file: `memprofile.out`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can analyze this file textually by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Focus on functions that are allocating large amounts of memory or holding on
    to it for extended periods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the web-based view by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that we now have a flame graph variant. Like CPU flame graphs, instead
    of bar width representing time, it represents memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s recommended to start from the top and identify areas with heavy memory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our program, we have key areas to watch:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scanDirectory`: How much memory is allocated to build `map[string]FileInfo`?
    This grows with directory size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compareAndEmitEvents`: Is memory usage heavily affected by the frequency of
    file changes, or is the memory footprint of the comparison logic itself a concern?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileInfo`: If you deal with very large files or long file paths, the size
    of your `FileInfo` struct might matter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling memory over time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get a better picture of potential memory leaks or growth, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify your code so that you can write heap profiles at intervals within the
    monitoring loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare profiles to see if objects remain allocated unexpectedly, implying a
    potential leak-like scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing to explore the trade-offs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To explore the results of our profiling techniques, let’s introduce a simple
    caching feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should capture this before introducing any caching. After that, we can design
    our caching mechanism. Let’s consider the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eviction policy**: How do you remove old data when the cache reaches a size
    limit?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profile with caching**: Analyze the new memory profile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scanDirectory` decrease?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New bottlenecks**: Did the cache itself become a significant memory consumer?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple caching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s our implementation of the simple caching mechanism, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Global cache declaration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A global variable called `cachedDirectoryState` is declared to store the cached
    state of the directory. This map holds `FileInfo` structures indexed by their
    file paths. Declaring it globally allows the cache to persist across multiple
    calls to the `scanDirectory` function, enabling reuse of previously gathered data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Cache check in `scanDirectory`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before performing the filesystem walk, the function checks if there is an existing
    cache (`cachedDirectoryState`). If the cache is not `nil`, meaning it has been
    populated from a previous scan, it copies the cached `FileInfo` entries into the
    results map. This step ensures that the function starts with data from the last
    scan, potentially reducing the amount of work needed if many files remain unchanged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Cache update after scanning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As the directory is walked and each file is processed, the `results` map is
    updated with the latest `FileInfo` for each path. Unlike the initial cache check,
    this update occurs inside the `filepath.WalkDir` call, ensuring that the most
    current information is captured. After processing each file, the entire `cachedDirectoryState`
    is replaced with the current results. This means the cache is always reflective
    of the most recent state of the directory, as determined by the last scan.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Caveat
  prefs: []
  type: TYPE_NORMAL
- en: This caching strategy might introduce stale data issues if files are changed,
    added, or removed between scans, and the program relies on the cache without revalidating
    it. To mitigate this, you might consider strategies for invalidating or updating
    the cache based on certain triggers or after a predefined interval.
  prefs: []
  type: TYPE_NORMAL
- en: A production-ready cache likely would need a size limit and an eviction strategy
    (such as **least recently** **used** (**LRU**).
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time for you to repeat the memory and CPU analyses to identify how
    the program’s behavior changed. Ensure you provide another name for the profile
    results so that you don’t override them!
  prefs: []
  type: TYPE_NORMAL
- en: From a CPU perspective, have you noticed the top CPU-consuming functions change
    their order? Also, did specific functions see significant increases or decreases
    in CPU time percentage?
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you should see reduced CPU time within `scanDirectory`.
  prefs: []
  type: TYPE_NORMAL
- en: From a memory perspective, have you noticed the top-allocating functions change?
    Did specific functions increase or decrease their allocation volume significantly?
  prefs: []
  type: TYPE_NORMAL
- en: Expect increased memory usage due to the cache itself. Analyze whether this
    trade-off is acceptable for the performance gains. The core idea of profiling
    your programs is to ideally change only one aspect of your code or workload at
    a time for a clearer comparison.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve evaluated our application through CPU and memory profile data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have explored the core aspects of performance analysis
    within Go, providing an understanding of how Go’s memory management mechanisms
    work and how they can be optimized for better application performance. Key concepts
    such as escape analysis, the roles of stack and pointers, and the distinctions
    between stack and heap memory allocations were thoroughly examined.
  prefs: []
  type: TYPE_NORMAL
- en: As we turn the page from the intricacies of memory management and performance
    optimization, the next chapter invites us into the expansive world of networking
    in Go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Connected Apps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will explore other topics in the Go programming development
    ecosystem, focusing on networking, telemetry, and application distribution. This
    section will equip you with in-depth knowledge and practical skills to enhance
    your Go applications’ observability, connectivity, and distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21662_10.xhtml#_idTextAnchor211), *Networking*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B21662_11.xhtml#_idTextAnchor224), *Telemetry*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B21662_12.xhtml#_idTextAnchor240), *Distributing Apps*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
