<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer023">
			<h1 id="_idParaDest-187" class="chapter-number"><a id="_idTextAnchor224"/>11</h1>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor225"/>Telemetry</h1>
			<p>In this chapter, we explore the practical world of <strong class="bold">telemetry</strong>, where<a id="_idIndexMarker573"/> the elegance of Go’s programming model meets the crucial need for application observability. We are equipping you with the tools of logging, tracing, and metrics to shed light on the inner workings of your Go applications, empowering you to ensure they run efficiently <span class="No-Break">and reliably.</span></p>
			<p>This chapter is your guide to enhancing the art and science of application telemetry. From the comprehensive practice of structured logging, which brings order and clarity to application logs, to the detailed insights offered by tracing and the thorough analysis enabled by metrics, this chapter covers <span class="No-Break">it all.</span></p>
			<p>The chapter will cover the following <span class="No-Break">key topics:</span></p>
			<ul>
				<li><span class="No-Break">Logs</span></li>
				<li><span class="No-Break">Traces</span></li>
				<li><span class="No-Break">Metrics</span></li>
				<li>The <strong class="bold">OpenTelemetry</strong> (<span class="No-Break"><strong class="bold">OTel</strong></span><span class="No-Break">) project</span></li>
			</ul>
			<p>By the end of this chapter, you will have acquired the skills to observe, understand, and actively improve the performance and reliability of your apps, fostering a sense of engagement and motivation in <span class="No-Break">your work.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor226"/>Technical requirements</h1>
			<p>Make sure you have Docker installed on your machine. You can download it from the official Docker <span class="No-Break">website (</span><a href="https://www.docker.com/get-started"><span class="No-Break">https://www.docker.com/get-started</span></a><span class="No-Break">).</span></p>
			<p>All the code shown in this chapter can be found in the <strong class="source-inline">ch11</strong> directory of our <span class="No-Break">Git repository.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor227"/>Logs</h1>
			<p>Logging, the <a id="_idIndexMarker574"/>unsung hero of system programming, is often as overlooked as the “terms and conditions” checkbox on software updates. Most developers<a id="_idIndexMarker575"/> treat logging the same way teenagers treat a clean room: a nice idea in theory but somehow never a priority until things start to smell funny. The common misconception here? That logging is just an afterthought, a mere diary for your code to occasionally scribble in. Spoiler alert: <span class="No-Break">it’s not!</span></p>
			<p>Imagine, if you will, a software development version of an archeological dig. Each log entry is a carefully unearthed artifact, offering clues to the civilization (code base) that once thrived. Now, picture some developers at this dig, using a bulldozer (poor logging practices) to uncover these delicate treasures. The result? A lot of broken pottery and bewildered faces. This, my friends, is what happens when logging into Go is not given the respect and precision <span class="No-Break">it demands.</span></p>
			<p>Logging in Go, especially in the context of system programming, is an essential tool for understanding the behavior of applications. It provides visibility into the system, enabling developers to track down bugs, monitor performance, and understand traffic patterns. Go, being the pragmatic language it is, offers built-in support for logging via the standard library’s log package, but the plot thickens when system-level programming comes <span class="No-Break">into play.</span></p>
			<p>For system programming, where performance and resource optimization are paramount, the standard <strong class="source-inline">log</strong> package might not always cut it. This is where structured logging comes into the spotlight. Structured logging, as opposed to plain text logging, organizes log entries into a structured format, typically JSON. This format makes logs easier to query, analyze, and understand, especially when you’re sifting through mountains of data trying to find the proverbial needle in <span class="No-Break">a haystack.</span></p>
			<p>Let’s not just talk the talk; let’s walk the walk with a code snippet illustrating structured logging <span class="No-Break">in Go:</span></p>
			<pre class="source-code">
package main
import (
     "os"
     "log/slog"
)
func main() {
     handler := slog.NewJSONHandler(os.Stdout)
     logger := slog.New(handler)
     logger.Info("A group of walrus emerges from the ocean", slog.Attr("animal", "walrus"), slog.Attr("size", 10))
}</pre>			<p>This <a id="_idIndexMarker576"/>code utilizes the experimental <strong class="source-inline">slog</strong> package introduced in Go 1.21. It resides within the <strong class="source-inline">log</strong> sub-package (<strong class="source-inline">log/slog</strong>). It offers the convenience of no external dependencies being required, simplifying <span class="No-Break">project management.</span></p>
			<p>Let’s explore the snippet’s <span class="No-Break">key points:</span></p>
			<ul>
				<li><strong class="source-inline">handler := slog.NewJSONHandler(os.Stdout)</strong>: This line creates a <strong class="source-inline">slog.Handler</strong> responsible for formatting and potentially routing log entries. Here, <strong class="source-inline">slog.NewJSONHandler</strong> generates a JSON formatter and <strong class="source-inline">os.Stdout</strong> specifies the standard output as <span class="No-Break">the destination.</span></li>
				<li><strong class="source-inline">logger := slog.New(handler)</strong>: This line creates a <strong class="source-inline">slog.Logger</strong> instance. The newly created JSON handler is used to configure the logger’s output format <span class="No-Break">and destination.</span></li>
				<li><strong class="bold">Structured logging </strong><span class="No-Break"><strong class="bold">with attributes</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">logger.Info("A group of walrus emerges from the ocean", slog.Attr("animal", "walrus"), slog.Attr("size", 10))</strong>: This logs <a id="_idIndexMarker577"/>an informational message using the <span class="No-Break"><strong class="source-inline">Info</strong></span><span class="No-Break"> method</span></li></ul></li>
				<li><strong class="source-inline">slog.Attr("animal", "walrus"), slog.Attr("size", 10)</strong>: These leverage <strong class="source-inline">slog.Attr</strong> to create key-value pairs (attributes) that enhance the log message with structured data. This makes logs easier to parse and analyze by tools or <span class="No-Break">downstream applications.</span></li>
			</ul>
			<p>Logging in <a id="_idIndexMarker578"/>Go is not just about keeping a record; it’s about making sense of your application’s story, one log entry at a time. Remember – like any good story, the devil is in the details (or in this case, <span class="No-Break">the data).</span></p>
			<p>Logging, in the realm of software development, serves as the cornerstone for understanding, diagnosing, and tracking the behavior of applications. It is akin to the breadcrumb trail left by Hansel and Gretel in the famous fairy tale, offering guidance back through the complex woods of your code base to understand what happened, when, <span class="No-Break">and why.</span></p>
			<p>At its core, logging involves recording events and data during the execution of a program. These events could range from general information about the application’s operation to errors and system-specific messages that provide insight into its health and performance. The significance of logging can be likened to the role of a flight recorder or “black box” in aviation; it captures crucial information that can be analyzed post-factum to understand events leading up to an incident or to optimize <span class="No-Break">future performance.</span></p>
			<p>Effective logging <a id="_idIndexMarker579"/>practices empower developers through <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Debugging and troubleshooting</strong>: Logs are one of the main places to look when something goes wrong. They can help pinpoint where an error occurred and under what circumstances, reducing the time it takes to <span class="No-Break">resolve issues.</span></li>
				<li><strong class="bold">Security auditing</strong>: Logging access and transaction data can help detect unauthorized access attempts, data breaches, and other security threats, facilitating <span class="No-Break">swift action.</span></li>
				<li><strong class="bold">Compliance and record keeping</strong>: In many industries, keeping detailed logs is a regulatory requirement for compliance purposes, serving as proof of proper data handling and <span class="No-Break">other practices.</span></li>
				<li><strong class="bold">Understanding user behavior</strong>: Logging can provide insights into how users interact with your application, which features are most popular, and where users may <span class="No-Break">encounter difficulties.</span></li>
			</ul>
			<p>Despite its critical role, logging is not without challenges. It requires a careful balance to ensure that the right amount of information is captured – too little and you may miss important clues; too <a id="_idIndexMarker580"/>much, and you’re sifting through a haystack looking for needles. The art and science of logging lie in determining what to log, how to log it, and how to make sense of the data collected, all while minimizing performance impacts on <span class="No-Break">the application.</span></p>
			<p>When we look for performance today, uber/zap is one of the fastest logging libraries out there. Let’s explore the main differences between using slog <span class="No-Break">versus zap.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor228"/>Zap versus slog</h2>
			<p>When <a id="_idIndexMarker581"/>deciding between slog and zap, consider your application’s <span class="No-Break">specific needs.</span></p>
			<p>For applications where performance is paramount, and you need fine-grained control over logging, zap offers proven speed <span class="No-Break">and configurability.</span></p>
			<p>If you’re looking for a modern, efficient logging solution that integrates well with Go’s context package and emphasizes simplicity and flexibility, slog may be the <span class="No-Break">right choice.</span></p>
			<p>Here is the zap version of the <span class="No-Break">same example:</span></p>
			<pre class="source-code">
package main
import (
    "os"
    "go.uber.org/zap"
    "go.uber.org/zap/zapcore"
)
func main() {
    encoderConfig := zapcore.EncoderConfig{
        MessageKey: "message",
        LevelKey:    "level",
        EncodeLevel: zapcore.CapitalLevelEncoder,
        TimeKey:    "time",
        EncodeTime: zapcore.ISO8601TimeEncoder,
        CallerKey:    "caller",
        EncodeCaller: zapcore.ShortCallerEncoder,
    }
    consoleEncoder := zapcore.NewConsoleEncoder(encoderConfig)
    consoleSink := zapcore.AddSync(os.Stdout)
    core := zapcore.NewCore(consoleEncoder, consoleSink, zap.InfoLevel)
    logger := zap.New(core)
    sugar := logger.Sugar()
    sugar.Infow("A group of walrus emerges from the ocean",
        "animal", "walrus",
        "size", 10,
    )
}</pre>			<p>This <a id="_idIndexMarker582"/>example is intentionally more exaggerated to depict how configurable<a id="_idIndexMarker583"/> the zap library is. Let me explain what is going on here, step <span class="No-Break">by step:</span></p>
			<ol>
				<li><strong class="bold">Imports</strong>: We import <strong class="source-inline">go.uber.org/zap</strong> for core zap functionality and <strong class="source-inline">go.uber.org/zap/zapcore</strong> for <span class="No-Break">low-level configuration.</span></li>
				<li><strong class="bold">Encoder configuration</strong>: Zap uses encoders to format log entries. We set up a production-ready <strong class="source-inline">encoderConfig</strong> configuration for JSON output with <span class="No-Break">human-readable keys.</span></li>
				<li><strong class="bold">Console logging</strong>: We create a console encoder (<strong class="source-inline">consoleEncoder</strong>) and an output destination (<strong class="source-inline">consoleSink</strong>) that writes to the <span class="No-Break">standard output.</span></li>
				<li><strong class="bold">Core creation</strong>: The <strong class="source-inline">zapcore.NewCore</strong> function constructs the core of our logger, which combines the encoder, the sink, and the configured log <span class="No-Break">level (</span><span class="No-Break"><strong class="source-inline">zap.InfoLevel</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Logger creation</strong>: Using <strong class="source-inline">zap.New</strong>, we build a zap logger based <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">core</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Sugared logger</strong>: Zap’s sugared logger provides convenient methods such as <strong class="source-inline">Infow</strong> for logging. It makes it easier to add structured data to log messages (and runs slower than the <span class="No-Break">non-sugared version).</span></li>
			</ol>
			<p>However, both<a id="_idIndexMarker584"/> slog and zap enhance Go’s logging capabilities, extending<a id="_idIndexMarker585"/> beyond the standard library to offer structured, efficient, and flexible logging solutions. The choice between them depends on your application’s specific requirements, including performance considerations, the need for structured logging, and the level of <span class="No-Break">customization required.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor229"/>Logging for debugging or monitoring?</h2>
			<p>Debugging logs are<a id="_idIndexMarker586"/> primarily used during the development<a id="_idIndexMarker587"/> phase or when diagnosing issues in a system. Their main aim is to provide developers with detailed, contextual information about the application’s behavior at a specific moment in time, particularly when errors or unexpected <span class="No-Break">behaviors occur.</span></p>
			<p>Here are the characteristics <a id="_idIndexMarker588"/>of <span class="No-Break">debugging logs:</span></p>
			<ul>
				<li><strong class="bold">Granularity</strong>: Debugging logs are often highly detailed, including verbose information about the state of the application, variable values, execution paths, and <span class="No-Break">error messages.</span></li>
				<li><strong class="bold">Temporary</strong>: These logs may be generated in a development environment or temporarily enabled in production to track down specific issues. They are not typically kept running permanently in a living environment due to their <span class="No-Break">verbose nature.</span></li>
				<li><strong class="bold">Developer focused</strong>: The audience for debugging logs is usually the developers who are familiar with the application’s code base. The information is technical <a id="_idIndexMarker589"/>and requires a deep understanding <a id="_idIndexMarker590"/>of the <span class="No-Break">application’s internals.</span></li>
			</ul>
			<p>The most common examples of these logs are stack traces and key variables at <span class="No-Break">certain checkpoints.</span></p>
			<p>When we’re<a id="_idIndexMarker591"/> logging for monitoring, logs are designed for the ongoing <a id="_idIndexMarker592"/>observation of an application in production. They help in understanding the application’s health and usage patterns over time, facilitating proactive maintenance <span class="No-Break">and optimization.</span></p>
			<p>Here are the <a id="_idIndexMarker593"/>characteristics of <span class="No-Break">monitoring logs:</span></p>
			<ul>
				<li><strong class="bold">Aggregation friendly</strong>: Monitoring logs are structured to be easily aggregated and analyzed by monitoring tools. They often follow a consistent format, making it simpler to extract metrics <span class="No-Break">and trends.</span></li>
				<li><strong class="bold">Persistent</strong>: These logs are continuously generated and collected as part of the application’s normal operation in production environments. They are less detailed than debugging logs to balance informativeness with <span class="No-Break">performance overhead.</span></li>
				<li><strong class="bold">Operational insight</strong>: The focus is on information relevant to the operation of the application, user activity, and error rates. The audience includes not only developers but also system administrators and <span class="No-Break">operations teams.</span></li>
			</ul>
			<p>For instance, we can see this kind of logging strategy on HTTP request logs including method, URL, and <span class="No-Break">status code.</span></p>
			<p>The main difference between these two methods is the objective, detail level, audience, and <span class="No-Break">life span.</span></p>
			<p>In essence, logging for debugging and monitoring serve complementary but distinct roles in the life cycle of an application. Effective logging strategies recognize these differences, implementing tailored approaches to meet the unique needs of debugging <span class="No-Break">and monitoring.</span></p>
			<p>When it comes to logging, the format you choose can significantly impact the readability, processing speed, and overall usefulness of your log data. Two popular formats are JSON logs and structured text logs. Choosing between them requires understanding their <a id="_idIndexMarker594"/>differences, advantages, and the specific needs of your<a id="_idIndexMarker595"/> application or environment. Let’s outline a framework to help us to make an <span class="No-Break">informed decision.</span></p>
			<p>First, we should consider the log <span class="No-Break">consumption tools:</span></p>
			<ul>
				<li><strong class="bold">JSON logs</strong>: If <a id="_idIndexMarker596"/>you’re using modern log management<a id="_idIndexMarker597"/> systems or tools designed to ingest and query JSON data (such as <strong class="bold">Elasticsearch, Logstash, Kibana</strong> (<strong class="bold">ELK</strong>), or Splunk), JSON logs can be highly advantageous. These <a id="_idIndexMarker598"/>tools can natively parse JSON, allowing for more efficient querying, filtering, <span class="No-Break">and analysis.</span></li>
				<li><strong class="bold">Structured text logs</strong>: If<a id="_idIndexMarker599"/> your log <a id="_idIndexMarker600"/>consumption mainly involves reading logs directly for debugging purposes or using tools that don’t natively parse JSON, structured text logs might be preferable. Structured text logs can be easier to read for humans, especially when tailing logs in <span class="No-Break">a console.</span></li>
			</ul>
			<p>Also, we evaluate log <span class="No-Break">data complexity:</span></p>
			<ul>
				<li><strong class="bold">JSON logs</strong>: JSON is well suited for logging complex and nested data structures. If your application logs contain a wide variety of data types or structured data that benefits from hierarchical organization, JSON logs can encapsulate this complexity <span class="No-Break">more effectively.</span></li>
				<li><strong class="bold">Structured text logs</strong>: For simpler logging requirements where logs are primarily flat messages with a few key-value pairs, structured text logs can be sufficient and more straightforward to <span class="No-Break">work with.</span></li>
			</ul>
			<p>After this evaluation, we can assess performance <span class="No-Break">and overhead:</span></p>
			<ul>
				<li><strong class="bold">JSON logs</strong>: Writing logs in JSON format can introduce additional computational overhead due to serialization costs. For high-throughput applications where performance is critical, assess whether your system can handle this overhead without <span class="No-Break">significant impact.</span></li>
				<li><strong class="bold">Structured text logs</strong>: Generally, generating structured text logs is less CPU-intensive than JSON serialization. If performance is a paramount concern and your log data is relatively simple, structured text logging may be the more <span class="No-Break">efficient choice.</span></li>
			</ul>
			<p>Then, we can <a id="_idIndexMarker601"/>create <a id="_idIndexMarker602"/>a plan for log analysis <span class="No-Break">and troubleshooting.</span></p>
			<ul>
				<li><strong class="bold">JSON logs</strong>: For scenarios where logs are extensively analyzed to gain insights into application behavior, and user actions, or for troubleshooting complex issues, JSON logs provide a more structured and “queryable” format. They facilitate deeper analysis and can be automatically processed by <span class="No-Break">many tools.</span></li>
				<li><strong class="bold">Structured text logs</strong>: If your log analysis needs are straightforward or you primarily use logs for real-time troubleshooting without complex querying, structured text logs <span class="No-Break">might suffice.</span></li>
			</ul>
			<p>Lastly, we can assess the development and <span class="No-Break">maintenance context:</span></p>
			<ul>
				<li><strong class="bold">JSON logs</strong>: Consider whether your development team is comfortable with JSON format and parsing, as well as whether your logging framework and infrastructure support JSON <span class="No-Break">logging effectively</span></li>
				<li><strong class="bold">Structured text logs</strong>: Structured text logs might be preferred for teams looking for simplicity and ease of use, especially if they are not using advanced log <span class="No-Break">processing tools</span></li>
			</ul>
			<p>The general guideline is <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Log consumption tools</strong>: Choose JSON for advanced processing tools; choose structured text for simplicity or <span class="No-Break">direct consumption.</span></li>
				<li><strong class="bold">Data complexity</strong>: Use JSON for complex, nested data; structured text for <span class="No-Break">simpler data.</span></li>
				<li><strong class="bold">Performance considerations</strong>: Opt for structured text when performance is critical; use JSON with performance impact <span class="No-Break">in mind.</span></li>
				<li><strong class="bold">Analysis and troubleshooting</strong>: Select JSON for in-depth analysis needs; structured text for <span class="No-Break">basic troubleshooting.</span></li>
				<li><strong class="bold">Team and infrastructure</strong>: Consider team familiarity and <span class="No-Break">infrastructure capabilities.</span></li>
			</ul>
			<p>Ultimately, the <a id="_idIndexMarker603"/>choice between JSON and structured text logs depends on balancing <a id="_idIndexMarker604"/>the specific needs of your application, the capabilities of your log processing infrastructure, and your team’s preferences and skills. It’s not uncommon for systems to employ both types in different contexts or layers of the application to optimize for both human readability and <span class="No-Break">machine processing.</span></p>
			<p>Understanding what to log and what not to log is crucial for maintaining efficient, secure, and useful <span class="No-Break">logging practices.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor230"/>What to log?</h2>
			<p>Proper<a id="_idIndexMarker605"/> logging can help you debug issues, monitor system performance, and understand user behavior. However, excessive, or inappropriate, logging can lead to performance degradation, storage issues, and <span class="No-Break">security vulnerabilities.</span></p>
			<p>Here’s a guide to help navigate <span class="No-Break">these decisions:</span></p>
			<ul>
				<li><strong class="bold">Errors</strong>: Capture any errors that occur. Include stack traces to <span class="No-Break">facilitate debugging.</span></li>
				<li><strong class="bold">System state changes</strong>: Log significant state changes within your application, such as system startup or shutdown, configuration changes, and status changes of <span class="No-Break">critical components.</span></li>
				<li><strong class="bold">User actions</strong>: Log key user actions, especially those that modify data or trigger significant processes in your application. This helps in understanding user behavior and <span class="No-Break">diagnosing issues.</span></li>
				<li><strong class="bold">When you don’t have a metrics server </strong><span class="No-Break"><strong class="bold">in place</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Performance metrics</strong>: Log performance-related metrics such as response times, throughput, and resource utilization. This information is crucial for monitoring the health and performance of <span class="No-Break">your system.</span></li><li><strong class="bold">Security events</strong>: Log security-related events, such as login attempts, access control violations, and other suspicious activities. These logs are vital for security monitoring and <span class="No-Break">incident response.</span></li><li><strong class="bold">API calls</strong>: When<a id="_idIndexMarker606"/> your application interacts with external services through APIs, logging these calls can be helpful for tracking dependencies and <span class="No-Break">troubleshooting issues.</span></li></ul></li>
				<li><strong class="bold">When you don’t have an audit system to send the </strong><span class="No-Break"><strong class="bold">system events</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Critical business transactions</strong>: Log important business transactions to provide an audit trail that can be used for compliance, reporting, and business <span class="No-Break">intelligence purposes.</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor231"/>What not to log?</h2>
			<p>There is a <a id="_idIndexMarker607"/>series of information that’s not suitable for logging, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Sensitive information</strong>: Avoid logging sensitive information such as passwords, <strong class="bold">personal identification information</strong> (<strong class="bold">PII</strong>), credit card numbers, and security<a id="_idIndexMarker608"/> tokens. Exposure to such information can lead to security breaches and <span class="No-Break">compliance violations.</span></li>
				<li><strong class="bold">Verbose or debug information in production</strong>: While verbose or debug-level logs can be incredibly useful during development, they can overwhelm production systems. Use appropriate log levels and consider dynamic log <span class="No-Break">level adjustment.</span></li>
				<li><strong class="bold">Redundant or irrelevant information</strong>: Logging the same information multiple times or capturing irrelevant details can clutter your logs and consume <span class="No-Break">unnecessary storage.</span></li>
				<li><strong class="bold">Large binary data</strong>: Avoid logging large binary objects, such as files or images. These can significantly increase the size of your log files and <span class="No-Break">degrade performance.</span></li>
				<li><strong class="bold">User input without sanitization</strong>: Logging <a id="_idIndexMarker609"/>raw user input can introduce security risks, such as injection attacks. Always sanitize input before <span class="No-Break">logging it.</span></li>
			</ul>
			<p>The best practices <a id="_idIndexMarker610"/>can be <span class="No-Break">summarized here:</span></p>
			<ul>
				<li><strong class="bold">Use structured logging</strong>: Structured logs make it easier to search and analyze data. Use a consistent format such as JSON across <span class="No-Break">your logs</span></li>
				<li><strong class="bold">Implement log rotation and retention policies</strong>: Automatically rotate logs and define retention policies to manage disk space and comply with data <span class="No-Break">retention requirements</span></li>
				<li><strong class="bold">Secure log data</strong>: Ensure that logs are stored securely, access is controlled, and transmission of log data <span class="No-Break">is encrypted</span></li>
				<li><strong class="bold">Monitor log files for anomalies</strong>: Regularly review log files for unusual activity or errors that could indicate operational or <span class="No-Break">security issues</span></li>
			</ul>
			<p>By following these guidelines, you can ensure that your logging practices contribute positively to the maintenance, performance, and security of <span class="No-Break">your applications.</span></p>
			<p>Remember, the goal is to capture enough information to be useful without compromising system performance <span class="No-Break">or security.</span></p>
			<p>Often, we need more information regarding the program execution and our series of records (logs) are not enough. This is where we rely <span class="No-Break">on traces.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor232"/>Traces</h1>
			<p>So, you’ve <a id="_idIndexMarker611"/>heard that tracing in Golang is as straightforward as pie, have you? Let’s not kid ourselves; in the realm of system programming, tracing is more like<a id="_idIndexMarker612"/> baking a soufflé in a microwave – sure, you might end up with something edible, but it’s hardly going to win you any <span class="No-Break">Michelin stars.</span></p>
			<p>Here’s an analogy that might tickle your fancy: Imagine you’re a detective in a software development murder mystery. The victim? System performance. The suspects? A motley crew of goroutines, each more suspicious than the last. Your only hope of cracking the case lies in the intricate art of trace analysis. But beware, this is no child’s play. You’ll need all your wit, wisdom, and a hefty dose of sarcasm to navigate through the quagmire of stack traces and <span class="No-Break">execution threads.</span></p>
			<p>Tracing in<a id="_idIndexMarker613"/> Golang, for those unacquainted with the finer points of system programming, is the Sherlock Holmes debugging tool. It allows developers to observe the behavior of their programs during execution, offering invaluable insights into performance bottlenecks and sneaky bugs that would otherwise remain as elusive as a well-behaved cat in a room full of <span class="No-Break">rocking chairs.</span></p>
			<p>At its core, Golang’s tracing framework leverages the <strong class="source-inline">runtime/trace</strong> package to let you peer into the running soul of your application. By collecting a wide range of events related to goroutines, heap allocation, garbage collection, and more, it sets the stage for a deep dive into the inner workings of <span class="No-Break">your code.</span></p>
			<p>The power of trace analysis comes alive with tools such as <strong class="source-inline">go tool trace</strong>, which parses trace files generated by your Go application and serves them up in a web interface that’s as revealing as it is mesmerizing. Here, you can visualize the execution of goroutines, track down latency issues, and get to the bottom of those performance mysteries that keep you up <span class="No-Break">at night.</span></p>
			<p>Let’s take a practical look with a<a id="_idIndexMarker614"/> simple code example. Imagine you’ve wrapped your critical section with trace calls <span class="No-Break">like so:</span></p>
			<pre class="source-code">
package main
import (
     "os"
     "runtime/trace"
)
func main() {
     trace.Start(os.Stderr)
     defer trace.Stop()
     // Your code here. Let's pretend it's something impressive.
}</pre>			<p>When you run this program, it outputs a kind of ugly <span class="No-Break">output, right?</span></p>
			<p>This snippet kick-starts <a id="_idIndexMarker615"/>the tracing process, directing the output to stderr, where you can later analyze it to your heart’s content. Remember, this is just the tip of <span class="No-Break">the iceberg.</span></p>
			<p>Let’s step back and learn how to add the trace in our programs and check the <span class="No-Break">output properly.</span></p>
			<p>As you can see, to start tracing, you need to import the <strong class="source-inline">runtime/trace</strong> package. This package provides the functionality to start and <span class="No-Break">stop tracing:</span></p>
			<pre class="source-code">
import (
    "os"
    "runtime/trace"
)</pre>			<p>We need to call <strong class="source-inline">trace.Start</strong> at the point in your code where you want to begin tracing. Similarly, you should call <strong class="source-inline">trace.Stop</strong> when you want to end the tracing, usually after a specific operation you’re interested <span class="No-Break">in measuring:</span></p>
			<pre class="source-code">
func main() {
    f, err := os.Create("trace.out")
    if err != nil {
        panic(err)
    }
    defer f.Close()
    err = trace.Start(f)
    if err != nil {
        panic(err)
    }
    defer trace.Stop()
    // Your program logic here
}</pre>			<p>Run your Go program as <a id="_idIndexMarker616"/>usual. The program will execute and generate a trace file named <strong class="source-inline">trace.out</strong> (or whatever you named your file) in the <span class="No-Break">current directory:</span></p>
			<pre class="console">
go run your_program.go</pre>			<p>After your program has run, you can analyze the trace file using <strong class="source-inline">go tool trace</strong>. This command will start a web server that hosts a web-based user interface for analyzing <span class="No-Break">the trace:</span></p>
			<pre class="console">
go tool trace trace.out</pre>			<p>When you run this command, it will print a URL to your console. Open this URL in your web browser to view the trace viewer. The viewer provides various views to analyze different aspects of your program’s execution, such as the goroutine analysis, heap analysis, and other aspects we explored in <a href="B21662_09.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <span class="No-Break"><em class="italic">Analyzing Performance</em></span><span class="No-Break">.</span></p>
			<p>For programs with HTTP servers, the approach is slightly different. Let’s add tracing capabilities to this <span class="No-Break">simple program:</span></p>
			<pre class="source-code">
package main
import (
    "fmt"
    "net/http"
)
func main() {
    http.HandleFunc("/", handler)
    fmt.Println("Server is listening on :8080")
    http.ListenAndServe(":8080", nil)
}
func handler(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, "Hello, Tracing!")
}</pre>			<p>To trace the <a id="_idIndexMarker617"/>HTTP endpoint, we’ll need to wrap your handler with a function that starts and stops tracing around the handler’s execution. You can use the <strong class="source-inline">runtime/trace</strong> package for tracing and <strong class="source-inline">net/http/httptrace</strong> for more detailed <span class="No-Break">HTTP tracing.</span></p>
			<p>First, let’s modify our main package to include the <strong class="source-inline">runtime/trace</strong> package, as shown in the previous snippet. Then, create a trace wrapper for your <span class="No-Break">HTTP handler:</span></p>
			<pre class="source-code">
import (
    "net/http"
    "runtime/trace"
)
func TraceHandler(inner http.HandlerFunc) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        ctx, task := trace.NewTask(r.Context(), r.URL.Path)
        defer task.End()
        trace.Log(ctx, "HTTP Method", r.Method)
        trace.Log(ctx, "URL", r.URL.String())
        inner(w, r.WithContext(ctx))
    }
}</pre>			<p>Then, wrap <a id="_idIndexMarker618"/>your HTTP handlers <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">TraceHandler</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
func main() {
    http.HandleFunc("/", TraceHandler(handler))
    fmt.Println("Server is listening on :8080")
    http.ListenAndServe(":8080", nil)
}</pre>			<p>Follow the same steps as in the previous program to start and stop tracing, and then run your application. Make some requests to your server to ensure there’s activity <span class="No-Break">to trace.</span></p>
			<p>After stopping the trace and generating the trace file, use the <strong class="source-inline">go tool trace</strong> command to analyze the trace data. Pay special attention to the sections related to network I/O and HTTP requests to understand the performance of <span class="No-Break">your endpoint.</span></p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor233"/>Effective tracing</h2>
			<p>Instead <a id="_idIndexMarker619"/>of tracing your entire program, focus on the parts where performance is critical. This approach reduces the size of the trace file and makes <span class="No-Break">analysis easier.</span></p>
			<p>Spend some time<a id="_idIndexMarker620"/> exploring the different views available in the trace viewer. Each view provides insights into specific aspects of your program’s execution. Also, when analyzing the trace, look for unusual patterns or anomalies, such as goroutines that are blocked for a long time or excessive garbage <span class="No-Break">collection pauses.</span></p>
			<p>Ensure that the context containing the trace is passed to any downstream calls made during the request handling. This allows for a more comprehensive trace that includes the entire request <span class="No-Break">life cycle.</span></p>
			<p>When possible, use middleware for tracing. For more complex applications, consider implementing tracing as middleware in your HTTP server. This approach allows for more flexibility and reusability across different parts of <span class="No-Break">your application.</span></p>
			<p>Reflecting on my own trials and tribulations with Golang’s tracing, I recall a project that was as bogged down as a luxury sedan in a mud wrestling pit. After hours of poring over trace outputs, I stumbled upon a revelation that was as profound as discovering your car keys in the refrigerator. It <a id="_idIndexMarker621"/>dawned on me that tracing, much like a skilled sommelier, could discern the subtle nuances between a fine performance and a disastrous bottleneck. In the end, the solution was as simple as rearranging some database calls, yet it underscored the nuanced sophistication of Golang’s <span class="No-Break">tracing capabilities.</span></p>
			<p>Tracing is <a id="_idIndexMarker622"/>primarily used for performance analysis and debugging. It’s particularly useful for identifying concurrency issues, understanding system behavior under load, and pinpointing sources of latency in distributed systems. It offers a more granular view of program execution compared to logging. While logging records discrete events or states, tracing in Go can provide a continuous, detailed account of program execution, including <span class="No-Break">system-level events.</span></p>
			<p class="callout-heading">Logging versus tracing</p>
			<p class="callout">Also, there <a id="_idIndexMarker623"/>are performance considerations in both cases. Logging and tracing<a id="_idIndexMarker624"/> can impact the performance of a Go application, but the impact is generally more significant with tracing, especially when using execution tracing in a production environment. Developers need to balance the level of detail captured against the <span class="No-Break">performance overhead.</span></p>
			<p>To wrap it up, think of tracing in Golang like dissecting a complex piece of machinery. Without the right tools and knowledge, you’re just a monkey with a wrench. But arm yourself with Golang’s tracing package, and you transform into a master mechanic, tuning your application to purr like a kitten on a warm lap. Remember, the devil is in the details, and sometimes, those details are hidden deep within the traces of <span class="No-Break">your code.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor234"/>Distributed tracing</h2>
			<p>Distributed tracing<a id="_idIndexMarker625"/> involves monitoring the complete journey of a request as it travels across various interconnected services in a distributed system. Imagine a complex e-commerce application with separate services for product search, shopping cart, payment processing, and order fulfillment. A single user request might trigger interactions with all <span class="No-Break">these services.</span></p>
			<p>How does it work? You might ask yourself. There are four key concepts: unique identifier, propagation, spans, and collection <span class="No-Break">and analysis.</span></p>
			<p>A <a id="_idIndexMarker626"/>unique identifier (trace ID) is assigned to the initial request. This ID becomes the thread that ties together all subsequent logs and events related to that <span class="No-Break">specific request.</span></p>
			<p>The trace ID is then propagated across all services involved in handling the request. This can be done through headers in HTTP requests, messages in queues, or any mechanism suitable for the communication protocol <span class="No-Break">between services.</span></p>
			<p>Each service creates a “span” that captures information about its role in handling the request. This span might include details such as timestamps, service names, function calls, and any <span class="No-Break">errors encountered.</span></p>
			<p>The spans are collected by a central tracing system, which then stitches them together based on the trace ID. This provides a holistic view of the entire request flow, encompassing all the <span class="No-Break">services involved.</span></p>
			<p>The main <a id="_idIndexMarker627"/>benefits of distributed tracing are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Enhanced observability</strong>: Distributed tracing sheds light on how requests move through your system, revealing potential bottlenecks <span class="No-Break">and inefficiencies</span></li>
				<li><strong class="bold">Root cause analysis</strong>: When errors occur, tracing helps pinpoint the exact service or component responsible, even if the error manifests itself much later in the <span class="No-Break">request flow</span></li>
				<li><strong class="bold">Performance optimization</strong>: By analyzing trace data, you can identify slow services or communication issues between services, enabling performance <span class="No-Break">optimization efforts</span></li>
				<li><strong class="bold">Debugging microservices</strong>: Debugging complex interactions between microservices becomes significantly easier with the context provided by <span class="No-Break">distributed tracing</span></li>
			</ul>
			<p>Several open-source<a id="_idIndexMarker628"/> and commercial tools are available for implementing distributed tracing. Some popular options include Zipkin, Jaeger, Honeycomb, <span class="No-Break">and Datadog.</span></p>
			<p>But what<a id="_idIndexMarker629"/> about the freedom to switch between observability tools and backend providers without requiring large changes to your application’s code? Later in this chapter, we’ll see there is a gap the OTel project is trying <span class="No-Break">to fill.</span></p>
			<p>Let’s continue to expand our knowledge with the next pillar of <span class="No-Break">telemetry: metrics.</span></p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor235"/>Metrics</h1>
			<p>Nothing<a id="_idIndexMarker630"/> screams “I’ve made it as a programmer” quite like obsessing over performance data in a language that was designed to be as exciting as watching<a id="_idIndexMarker631"/> paint dry on a rainy day. But here we are, poised to dive into the thrilling world of Go metrics, armed with the enthusiasm of a sloth on tranquilizers. It’s a delightful journey through a labyrinth of numbers and charts, where the Minotaur you’re facing is your own code, mysteriously gobbling up resources in ways that make quantum physics seem straightforward <span class="No-Break">by comparison.</span></p>
			<p>Now, for those brave souls still with me and not deterred by the ominous shadows of impending doom, let’s get serious for a moment. Metrics in the context of Go are essential tools for understanding the behavior and performance of your applications. They provide insights into various aspects of your system, such as memory usage, CPU load, and goroutine counts. Go, with its minimalist charm and concurrency model, offers a plethora of opportunities for system programmers to shoot themselves in the foot, performance wise. Thankfully, it also provides band-aids in the form of built-in and third-party libraries designed to collect, report, and analyze these metrics. The Go runtime, for example, exposes a wealth of performance data through the <strong class="source-inline">runtime</strong> and <strong class="source-inline">net/http/pprof</strong> packages, allowing programmers to monitor their applications in <span class="No-Break">real time.</span></p>
			<p>One of the more popular third-party<a id="_idIndexMarker632"/> libraries is Prometheus, with its Go client library offering a rich set of tools to define and collect metrics. It integrates seamlessly into Go applications, providing a robust solution for monitoring not just system-level metrics but also application-specific metrics that can help in diagnosing performance bottlenecks and understanding <span class="No-Break">user behavior.</span></p>
			<p>To give you a taste, let’s consider a simple<a id="_idIndexMarker633"/> example using Prometheus to collect HTTP request count in a Go <span class="No-Break">web service:</span></p>
			<pre class="source-code">
package main
import (
     "fmt"
     "net/http"
     "time"
     "github.com/prometheus/client_golang/prometheus"
     "github.com/prometheus/client_golang/prometheus/promhttp"
)
var (
     requestsProcessed = prometheus.NewCounterVec(
          prometheus.CounterOpts{
               Name: "http_requests_processed",
               Help: "Total number of processed HTTP requests.",
          },
          []string{"status_code"},
     )
)
func init() {
     // Register metrics with Prometheus
     prometheus.MustRegister(requestsProcessed)
}
func main() {
     http.Handle("/metrics", promhttp.Handler())
     http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
          time.Sleep(50 * time.Millisecond)
          code := http.StatusOK
          if time.Now().Unix()%2 == 0 {
               code = http.StatusInternalServerError
          }          requestsProcessed.WithLabelValues(fmt.Sprintf("%d", code)).Inc()
          w.WriteHeader(code)
          fmt.Fprintf(w, "Request processed.")
     })
     fmt.Println("Starting server on port 8080...")
     http.ListenAndServe(":8080", nil)
}</pre>			<p>This snippet uses the <strong class="source-inline">prometheus/client_golang</strong> library to interact with Prometheus. A <a id="_idIndexMarker634"/>counter metric <strong class="source-inline">http_requests_processed</strong> is used to track the number of HTTP requests, labeled by status code. The <strong class="source-inline">/metrics</strong> endpoint exposes metrics for Prometheus to scrape. Inside the HTTP handler, the counter metric is incremented with appropriate status <span class="No-Break">code labels.</span></p>
			<p class="callout-heading">Simplicity</p>
			<p class="callout">This is a basic example. Real-world applications would involve richer metrics <span class="No-Break">and instrumentation.</span></p>
			<p>Let’s run our Prometheus server by following <span class="No-Break">these steps.</span></p>
			<ol>
				<li>Create a Prometheus <span class="No-Break">configuration file:</span><ul><li>Create a new file and name <span class="No-Break">it </span><span class="No-Break"><strong class="source-inline">prometheus.yml</strong></span></li><li>Paste the following basic configuration into <span class="No-Break">the file:</span><pre class="source-code">
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']</pre></li></ul></li>				<li>Pull the Prometheus <span class="No-Break">Docker image:</span><ul><li>Open your terminal and run the following command to download the latest Prometheus <span class="No-Break">Docker image:</span><pre class="source-code">
docker pull prom/prometheus</pre></li></ul></li>				<li>Run the <span class="No-Break">Prometheus container:</span><ul><li>Use the following command to run Prometheus, mapping <strong class="source-inline">prometheus.yml</strong> to <span class="No-Break">the container:</span><pre class="source-code">
docker run -p 9090:9090 -v &lt;path_to_your_prometheus.yml&gt;:/etc/prometheus/prometheus.yml prom/prometheus</pre></li><li>Replace <strong class="source-inline">&lt;path_to_your_prometheus.yml&gt;</strong> with the actual path to your <span class="No-Break">configuration file.</span></li></ul></li>				<li>Access<a id="_idIndexMarker635"/> the Prometheus <span class="No-Break">web interface:</span><ul><li>Open your web browser and go <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">http://localhost:9090</strong></span><span class="No-Break">.</span></li><li>You should now see the Prometheus <span class="No-Break">user interface.</span></li></ul></li>
				<li><span class="No-Break">Explore Prometheus:</span><ul><li>In the expression browser (the <strong class="bold">Graph</strong> tab), type in a basic query such as <strong class="source-inline">up</strong> and click <strong class="bold">Execute</strong>. This should show you whether Prometheus itself <span class="No-Break">is running.</span></li><li>Explore other built-in metrics, experiment with the query language, and get a feel <span class="No-Break">for Prometheus.</span></li></ul></li>
			</ol>
			<p>Now, we can execute our code and see the metrics. First, we need to save the code and <span class="No-Break">build it:</span></p>
			<pre class="source-code">
go build app.go &amp;&amp; ./app</pre>			<p>Explore <span class="No-Break">the metrics:</span></p>
			<ul>
				<li><strong class="bold">Access the metrics endpoint</strong>: Open<a id="_idIndexMarker636"/> your browser and visit <strong class="source-inline">http://localhost:8080/metrics</strong>. You should see the raw Prometheus <span class="No-Break">metrics output.</span></li>
				<li><strong class="bold">Query the metrics</strong>: In the <a id="_idIndexMarker637"/>Prometheus UI (<strong class="source-inline">http://localhost:9090</strong>), try queries such as <span class="No-Break">the following:</span><ul><li><strong class="source-inline">http_requests_processed</strong>: See the total number of requests, broken down by <span class="No-Break">status codes</span></li><li><strong class="source-inline">rate(http_requests_processed[1m])</strong>: View the request rate over the <span class="No-Break">last minute</span></li></ul></li>
			</ul>
			<p>We now can see our metrics, but what metrics can we use, and what metric should we use? Let’s <span class="No-Break">explore this!</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor236"/>What metric should we use?</h2>
			<p>Choosing the right type of metric to monitor in your application is akin to selecting the appropriate tool for a job—use a hammer for nails, not for screws. In the world of monitoring and observability, the primary metric types—<strong class="bold">Counter</strong>, <strong class="bold">Gauge</strong>, <strong class="bold">Histogram</strong>, and <strong class="bold">Summary</strong>—each serve distinct purposes. Understanding these purposes is crucial to effectively measure and analyze your application’s behavior <span class="No-Break">and performance.</span></p>
			<h3>Counters</h3>
			<p>A Counter<a id="_idIndexMarker638"/> is a simple metric that only goes up (increments) over time and <a id="_idIndexMarker639"/>resets to zero on restarts. It’s perfect for tracking occurrences of events. Use a Counter when you want to count things, such as requests served, tasks completed, or errors that occurred. For example, counting the number of times a user performs a specific action on <span class="No-Break">your site.</span></p>
			<p>Here are some <a id="_idIndexMarker640"/><span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="bold">Event counting</strong>: Perfect for counting occurrences of specific events. For instance, you could use a counter to track the number of user signups, tasks completed, or <span class="No-Break">errors encountered.</span></li>
				<li><strong class="bold">Rate measurement</strong>: Although the counter itself only goes up, you can measure the rate of increase over time, making it suitable for understanding how frequently an event is happening, such as requests <span class="No-Break">per second.</span></li>
			</ul>
			<h3>Gauges</h3>
			<p>A Gauge is a<a id="_idIndexMarker641"/> metric that represents a single numerical value that can <a id="_idIndexMarker642"/>arbitrarily go up and down. It’s like a thermometer that measures the <span class="No-Break">current temperature.</span></p>
			<p>Use a Gauge for values that fluctuate over time, such as current memory usage, number of concurrent sessions, or the temperature of a machine. Gauges are great for monitoring resources where the current state at a specific point in time is more relevant than the rate <span class="No-Break">of change.</span></p>
			<p>Here are some<a id="_idIndexMarker643"/> <span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="bold">Resource levels</strong>: Gauges are well suited for measuring quantities that can increase and decrease, such as the current memory usage, disk space remaining, or the number of <span class="No-Break">active users</span></li>
				<li><strong class="bold">Sensor readings</strong>: Any real-time measurement that fluctuates over time, such as temperature sensors, CPU load, or <span class="No-Break">queue lengths</span></li>
			</ul>
			<h3>Histograms</h3>
			<p>A<a id="_idIndexMarker644"/> Histogram<a id="_idIndexMarker645"/> samples observations (typically things such as request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all <span class="No-Break">observed values.</span></p>
			<p>Use a Histogram when you need to understand the distribution of a metric, not just its average. Histograms are ideal for tracking the latency of requests or the size of responses in your application because they allow you to see not just the average but also how the values are spread out, such as the 95th <span class="No-Break">percentile latency.</span></p>
			<p>Here are some<a id="_idIndexMarker646"/> <span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="bold">Distribution measurement</strong>: Histograms excel when you need to capture the distribution of metric values over time. This is crucial for understanding not just averages but the variability and outliers in <span class="No-Break">your data.</span></li>
				<li><strong class="bold">Performance analysis</strong>: Ideal for measuring request latencies or response sizes. Histograms help identify long-tail delays that might not affect the average much but significantly impact <span class="No-Break">user experience.</span></li>
			</ul>
			<h3>Summaries</h3>
			<p>Like Histograms, Summaries<a id="_idIndexMarker647"/> also sample observations. However, they<a id="_idIndexMarker648"/> calculate sliding window quantiles (e.g., the 50th, 90th, and 99th percentiles) instead of providing buckets. Summaries can be more computation intensive than Histograms because they compute these quantiles on <span class="No-Break">the fly.</span></p>
			<p>Use a Summary when you need precise quantiles over a sliding time window, especially for metrics where long-term accuracy is less critical than recent trends. They’re particularly useful for tracking request durations and response sizes when you need to know the exact <span class="No-Break">distribution dynamically.</span></p>
			<p>Here are <a id="_idIndexMarker649"/>some <span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="bold">Dynamic quantiles</strong>: When you need accurate quantiles in a sliding time window, summaries are the best choice. They provide a more detailed view of metric distributions, adjusting as new data <span class="No-Break">comes in.</span></li>
				<li><strong class="bold">Recent trends analysis</strong>: Suitable for scenarios where recent performance is more relevant than long-term averages, allowing you to respond to changes in <span class="No-Break">patterns quickly.</span></li>
			</ul>
			<h3>Choosing the right metric</h3>
			<p>The decision<a id="_idIndexMarker650"/> boils down to the nature of what you’re measuring and how you intend to use <span class="No-Break">the data:</span></p>
			<ul>
				<li>Counting occurrences? Go with <span class="No-Break">a Counter</span></li>
				<li>Measuring values that increase and decrease? A Gauge is <span class="No-Break">your friend</span></li>
				<li>Need to understand distributions? Histograms <span class="No-Break">shine here</span></li>
				<li>Require dynamic quantiles over recent data? Summaries are <span class="No-Break">the answer</span></li>
			</ul>
			<p>Remember, the goal is not just to collect metrics but to derive actionable insights from them. Therefore, choosing the right type of metric is crucial for effective monitoring and analysis. It ensures you’re not just collecting data for the sake of it but are gathering information that can genuinely inform decisions about your application’s performance <span class="No-Break">and design.</span></p>
			<p>To learn <a id="_idIndexMarker651"/>more about metrics and how to query them, please look at the Prometheus <span class="No-Break">documentation (</span><a href="https://prometheus.io/docs/concepts/metric_types/"><span class="No-Break">https://prometheus.io/docs/concepts/metric_types/</span></a><span class="No-Break">).</span></p>
			<p>In conclusion, metrics in Golang is like embarking on a grand adventure in a submarine. You’re under the surface, in the deep dark sea of code, navigating through the murky waters of performance. Your metrics are your sonar, pinging against potential issues and guiding you through the abyss to the promised land of efficient, scalable software. Remember, in the vast ocean of system programming, it’s not the size of the ship that matters, but the power of your metrics that charts the course <span class="No-Break">to success.</span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor237"/>The OTel project</h1>
			<p>OTel is an <a id="_idIndexMarker652"/>open-source, vendor-neutral project under the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>). It <a id="_idIndexMarker653"/>provides a set of standards, APIs, and SDKs for instrumenting, generating, collecting, and exporting <span class="No-Break">telemetry data.</span></p>
			<p>This data includes traces (the flow of requests through systems), metrics (measurements about system behavior), and logs (structured event records). Also, it aims to standardize how applications are instrumented, making it easier to adopt observability tools without <span class="No-Break">vendor lock-in.</span></p>
			<p>When we look from the maturity perspective, Golang is one of the primary supported languages within OTel. Basically, it provides a comprehensive SDK with libraries for <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Tracing</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">go.opentelemetry.io/otel/trace</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Metrics</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">go.opentelemetry.io/otel/metric</strong></span></li>
				<li><strong class="bold">Context </strong><span class="No-Break"><strong class="bold">propagation</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">go.opentelemetry.io/otel/propagation</strong></span></li>
			</ul>
			<p>OTel’s Go SDK integrates seamlessly with popular libraries and frameworks, making adding instrumentation to your existing Golang <span class="No-Break">applications easy.</span></p>
			<p>Also, the SDK supports various exporters, enabling you to send your telemetry data to different analysis backends. An exhaustive list of vendors can be found on the OTel <span class="No-Break">website (</span><a href="https://opentelemetry.io/ecosystem/vendors/"><span class="No-Break">https://opentelemetry.io/ecosystem/vendors/</span></a><span class="No-Break">).</span></p>
			<p>The major benefits of adopting <a id="_idIndexMarker654"/>OTel for Go projects are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Vendor neutrality</strong>: You have the freedom to switch between observability tools and backend providers without requiring large changes to your <span class="No-Break">application’s code</span></li>
				<li><strong class="bold">Streamlined instrumentation</strong>: OTel makes instrumenting your Golang services easier and <span class="No-Break">less tedious</span></li>
				<li><strong class="bold">Unified data format</strong>: It provides standardized data formats, ensuring your trace and metric data can be understood by multiple platforms <span class="No-Break">and tools</span></li>
				<li><strong class="bold">Strong community</strong>: The Golang SDK is backed by an active community, offering support, and contributing to <span class="No-Break">continuous improvement</span></li>
			</ul>
			<p>As OTel gains even wider adoption, it’s likely to become the de facto standard for observability in Golang applications. This standardization benefits the entire ecosystem by promoting vendor neutrality, portability, and easier adoption of <span class="No-Break">best practices.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor238"/>OTel</h1>
			<p>So, you think adding OTel <a id="_idIndexMarker655"/>to your program is like snapping some fancy Lego bricks together, huh? A bit of configuration magic, a sprinkle of auto-instrumentation, and voila – instant observability! Well, let’s just say you’re in for a surprise, <span class="No-Break">my friend.</span></p>
			<p>Now, before you toss your keyboard in frustration, let’s break down what OTel is. Think of it as a universal toolbox for collecting telemetry data from your application. OTel, in turn, is like your application’s internal monologue – traces of its execution, performance metrics, logs, and other whispers of its inner workings. OTel lets you shine that light into the darkest corners of your code base, revealing where things slow down, where errors sprout, and how your users interact with <span class="No-Break">your creation.</span></p>
			<p class="callout-heading">Logs are not ready yet</p>
			<p class="callout">The Logs SDK<a id="_idIndexMarker656"/> for Go is in development and we can follow the status on the official status page for the SDK: <a href="https://opentelemetry.io/status/">https://opentelemetry.io/status/</a>. Therefore, the following examples will use the uber/zap library <span class="No-Break">for logging.</span></p>
			<p>OTel itself is a set of specifications, APIs, and SDKs. It doesn’t magically make your app observable. You’ll need to strategically place sensors (think of them as fancy probes) throughout your code. This is where the “fun” of manual instrumentation comes in, along with deciding what data to collect in the <span class="No-Break">first place.</span></p>
			<p>Let’s create a<a id="_idIndexMarker657"/> program that uses Otel from scratch. The following are <span class="No-Break">the steps:</span></p>
			<ol>
				<li><strong class="bold">Create your Go project</strong>: Create a new directory for your project and initialize a <span class="No-Break">Go module:</span><pre class="source-code">
mkdir telemetry-example
cd telemetry-example
go mod init telemetry-example</pre></li>				<li><strong class="bold">Install dependencies</strong>: Install the necessary packages for OTel and <span class="No-Break">zap logging:</span><pre class="source-code">
go get go.uber.org/zap
go get go.opentelemetry.io/otel
go get go.opentelemetry.io/otel/exporters/otlp/otlptrace
go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp
go get go.opentelemetry.io/otel/sdk/resource
go get go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
go get go.opentelemetry.io/otel/semconv/v1.7.0</pre></li>				<li><strong class="bold">Initialize zap Logger</strong>: Create a new <strong class="source-inline">main.go</strong> file in your project directory. First, let’s set up zap for <span class="No-Break">advanced logging:</span><pre class="source-code">
package main
import (
    "go.uber.org/zap"
)
func main() {
    logger, _ := zap.NewProduction()
    defer logger.Sync() // Flushes buffer, if any
    sugar := logger.Sugar()
    sugar.Infow("This is an example log message", "location", "main", "type", "exampleLog")
}</pre><p class="list-inset">This code<a id="_idIndexMarker658"/> snippet initializes a production-grade logger with zap, which provides structured <span class="No-Break">logging capabilities.</span></p></li>				<li><strong class="bold">Configure OTel tracing</strong>: Next, add OTel tracing to your application, sending data to the <span class="No-Break">OTel Collector:</span><pre class="source-code">
import (
    "context"
    "net/http"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.7.0"
    "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
)
func main() {
    // Previous Zap logger setup...
    ctx := context.Background()
    traceExporter, err := otlptrace.New(ctx, otlptracehttp.NewClient())
    if err != nil {
        sugar.Fatal("failed to create trace exporter: ", err)
    }
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(traceExporter),
        sdktrace.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceNameKey.String("ExampleService"),
        )),
    )
    otel.SetTracerProvider(tp)
}</pre><p class="list-inset">This section adds tracing, configured <a id="_idIndexMarker659"/>to export trace data via the <strong class="bold">OTel </strong><span class="No-Break"><strong class="bold">Protocol</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">OTLP</strong></span><span class="No-Break">).</span></p></li>				<li><strong class="bold">Add a sample HTTP handler</strong>: For <a id="_idIndexMarker660"/>demonstration, add a simple HTTP handler that emits traces and logs for <span class="No-Break">each request:</span><pre class="source-code">
func exampleHandler(w http.ResponseWriter, r *http.Request) {
    _, span := otel.Tracer("example-tracer").Start(r.Context(), "handleRequest")
    defer span.End()
    zap.L().Info("Handling request")
    w.Write([]byte("Hello, World!"))
}
func main() {
    // Previous setup...
    http.Handle("/", otelhttp.NewHandler(http.HandlerFunc(exampleHandler), "Example"))
    sugar.Fatal(http.ListenAndServe(":8080", nil))
}</pre></li>				<li> <strong class="bold">Run your application</strong>: Before running your application, run <strong class="source-inline">docker-compose</strong> with the file located in the <strong class="source-inline">ch11/otel/</strong> directory in <span class="No-Break">your terminal:</span><pre class="source-code">
docker-compose up</pre><p class="list-inset">The collector should be set up to receive traces on the default OTLP port and route them to your <span class="No-Break">tracing backend.</span></p><p class="list-inset">Run <span class="No-Break">your application:</span></p><pre class="source-code">go run main.go</pre></li>				<li><strong class="bold">Access the application</strong> (e.g., <strong class="source-inline">http://localhost:8080/</strong>) from your browser or <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="source-code">
curl http://localhost:8080/</pre><p class="list-inset"><em class="italic">Voilà</em>! We made an application leveraging OTel’s <span class="No-Break">lock-in-free characteristics!</span></p></li>			</ol>
			<p>Back in my day, we <a id="_idIndexMarker661"/>used to debug systems with print statements and the occasional panicked curse. OTel is a far more civilized approach. Think of it like building your own intricate network of informants within your code. They’ll report back every detail, letting you pinpoint problems not just faster, but sometimes even before they <span class="No-Break">wreak havoc.</span></p>
			<p>Isn’t that better than a good ol’ debugging brawl? Now, it’s time to <span class="No-Break">wrap up.</span></p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor239"/>Summary</h1>
			<p>As we conclude this chapter on telemetry in Go, we’ve journeyed through the essential practices and tools that illuminate the inner mechanics of Go applications, enhancing their observability. This exploration began with an in-depth look at logging, where we learned to transcend essential log messages, adopting structured logging for its clarity and ease of analysis. We then navigated the complex yet crucial world of tracing, uncovering the intricate execution paths of our applications to identify and resolve performance bottlenecks. Also, we ventured into metrics, where quantitative data measurement enabled us to monitor and tune our applications for optimal performance. Lastly, we combined all the knowledge in a vendor-free solution backed <span class="No-Break">by OTel.</span></p>
			<p>In the next chapter, we’ll start to look at how to distribute <span class="No-Break">our apps.</span></p>
		</div>
	</div>
</div>
</body></html>