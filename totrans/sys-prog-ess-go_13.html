<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer027">
			<h1 id="_idParaDest-214" class="chapter-number"><a id="_idTextAnchor251"/>13</h1>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor252"/>Capstone Project – Distributed Cache</h1>
			<p>The grand finale is where we take everything we’ve learned and apply it to a real-world challenge. You might be thinking, “Surely, building a distributed cache can’t be that complex.” Spoiler alert: it’s not just about slapping together some in-memory storage and calling it a day. This is where theory meets practice, and trust me, it’s a <span class="No-Break">wild ride.</span></p>
			<p>Our distributed cache will be designed to handle frequent read and write operations with minimal latency. It will distribute data across multiple nodes to ensure scalability and fault tolerance. We’ll implement key features such as data sharding, replication, and <span class="No-Break">eviction policies.</span></p>
			<p>This chapter will cover the following <span class="No-Break">key topics:</span></p>
			<ul>
				<li>Setting up <span class="No-Break">the project</span></li>
				<li>Implementing <span class="No-Break">data sharding</span></li>
				<li><span class="No-Break">Adding replication</span></li>
				<li><span class="No-Break">Eviction policies</span></li>
			</ul>
			<p>By the end of this capstone project, you’ll have built a fully functional distributed cache from scratch. You’ll understand the intricacies of data distribution and performance optimization. More importantly, you’ll have the confidence to tackle similar challenges in your <span class="No-Break">own projects.</span></p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor253"/>Technical requirements</h1>
			<p>All the code shown in this chapter can be found in the <strong class="source-inline">ch13</strong> directory of this book’s <span class="No-Break">GitHub repository.</span></p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor254"/>Understanding distributed caching</h1>
			<p>So, do you think<a id="_idIndexMarker710"/> distributed caching is just a fancy term for storing stuff in memory across a few servers? Bless your heart. If only life were that simple. Let me guess, you’re the type who thinks that simply slapping “distributed” in front of anything makes it automatically better, faster, and cooler. Well, strap in because we’re about to dive into the rabbit hole of distributed caching, where nothing is as straightforward as <span class="No-Break">it seems.</span></p>
			<p>Imagine you’re at a software developer’s party (because we all know how wild those get), and someone casually mentions, “Hey, why don’t we just cache everything?” This is like saying, “Why don’t we just solve world hunger by ordering more pizza?” Sure, the idea is nice, but the devil is in the details. Distributed caching is not about stuffing more data into memory. It’s about smartly managing data spread across multiple nodes while ensuring that it doesn’t turn into a synchronized swimming event gone <span class="No-Break">horribly wrong.</span></p>
			<p>First, let’s tackle the basics. A distributed cache<a id="_idIndexMarker711"/> is a data storage layer that lies between your application and your primary data store. It’s designed to store frequently accessed data in a way that reduces latency and improves read throughput. Think of it as the app’s version of having a mini fridge next to your desk. You don’t need to walk to the kitchen every time you need a drink. Instead, you have quick access to your favorite beverage, right at <span class="No-Break">your fingertips.</span></p>
			<p>But, as with all things in life and software, there’s a catch. Ensuring that the data in this mini fridge is always fresh, cold, and available to everyone in your office simultaneously is no small feat. Distributed caches must maintain consistency across multiple nodes, handle failures gracefully, and efficiently manage data eviction. They must ensure that data isn’t stale and that updates propagate correctly, all while keeping latency to <span class="No-Break">a minimum.</span></p>
			<p>Then comes the architecture. One popular approach is sharding, where data is divided into smaller chunks and distributed across different <a id="_idIndexMarker712"/>nodes. This helps in balancing the load and ensures that no single node becomes a bottleneck. Another essential feature is<a id="_idIndexMarker713"/> replication. It’s not enough to have the data spread out; you also need copies of it to handle node failures. However, balancing consistency, availability, and partition tolerance (the CAP theorem) is where things <span class="No-Break">get tricky.</span></p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor255"/>System requirements</h1>
			<p>Each feature we’ll cover is<a id="_idIndexMarker714"/> crucial to building a robust and high-performing distributed cache system. By understanding and implementing these features, you will gain a comprehensive understanding of the intricacies involved in <span class="No-Break">distributed caching.</span></p>
			<p>At the heart of a distributed cache is its in-memory storage capability. In-memory storage allows for fast data access, significantly reducing the latency compared to disk-based storage. This feature is particularly important for applications that require high-speed data retrieval. Let’s explore our <span class="No-Break">project requirements.</span></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor256"/>Requirements</h2>
			<p>Welcome to <a id="_idIndexMarker715"/>the delightful world of requirements! Now, before you roll your eyes and groan about another tedious checklist, let’s set the record straight. Requirements aren’t figments of some overly ambitious product manager’s imagination. They’re intentional choices that shape the very essence of what you’re building. Think of them as the DNA of your project. Without them, you’re just blindly writing code and praying it works out. Spoiler alert: <span class="No-Break"><strong class="bold">it won’t</strong></span><span class="No-Break">.</span></p>
			<p>Requirements are your guiding light, your North Star. They keep you focused, ensure you’re building the right thing, and save you from the dreaded scope creep. In the context of our distributed cache project, they’re critical. So, let’s dive in and joyfully embrace the necessities that will make our distributed cache not just functional, <span class="No-Break">but outstanding.</span></p>
			<h3>Performance</h3>
			<p>We want our cache to be<a id="_idIndexMarker716"/> lightning-fast. This means millisecond response times for data retrieval and minimal latency for data updates. Achieving this requires thoughtful design choices around in-memory storage and efficient <span class="No-Break">data structures.</span></p>
			<p>Here are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li>Fast data access <span class="No-Break">and retrieval</span></li>
				<li>Minimal latency for <span class="No-Break">data updates</span></li>
				<li>Efficient data<a id="_idIndexMarker717"/> structures <span class="No-Break">and algorithms</span></li>
			</ul>
			<h3>Scalability</h3>
			<p>Our cache should scale horizontally, meaning <a id="_idIndexMarker718"/>we can add more nodes to handle increased load. This involves implementing sharding and ensuring that our architecture can grow seamlessly without <span class="No-Break">significant rework.</span></p>
			<p>The following are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li><span class="No-Break">Horizontal scalability</span></li>
				<li>Implementing <span class="No-Break">data sharding</span></li>
				<li>Seamless addition of <span class="No-Break">new nodes</span></li>
			</ul>
			<h3>Fault tolerance</h3>
			<p>Data should remain<a id="_idIndexMarker719"/> available even if some nodes fail. This requires implementing replication and ensuring that our system can handle node failures gracefully without data loss or <span class="No-Break">significant downtime.</span></p>
			<p>Here are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li>High availability despite <span class="No-Break">node failures</span></li>
				<li>Data replication across <span class="No-Break">multiple nodes</span></li>
				<li>Graceful handling of <span class="No-Break">node failures</span></li>
			</ul>
			<h3>Data expiry and eviction</h3>
			<p>Our cache should efficiently manage <a id="_idIndexMarker720"/>memory by expiring old data and evicting less frequently accessed data. Implementing <strong class="bold">time to live</strong> (<strong class="bold">TTL</strong>) and <strong class="bold">least recently used</strong> (<strong class="bold">LRU</strong>) eviction policies will <a id="_idIndexMarker721"/>help us <a id="_idIndexMarker722"/>manage limited memory <span class="No-Break">resources effectively.</span></p>
			<p>The following are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li>Efficient <span class="No-Break">memory management</span></li>
				<li>Implementing TTL and LRU <span class="No-Break">eviction policies</span></li>
				<li>Keeping<a id="_idIndexMarker723"/> the cache fresh <span class="No-Break">and relevant</span></li>
			</ul>
			<h3>Monitoring and metrics</h3>
			<p>To ensure our cache performs <a id="_idIndexMarker724"/>optimally, we need robust monitoring and metrics. This involves logging cache operations, tracking performance metrics (such as hit/miss ratios), and setting up alerts for <span class="No-Break">potential issues.</span></p>
			<p>Here are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li>Robust monitoring of <span class="No-Break">cache operations</span></li>
				<li>Performance metrics (<span class="No-Break">hit/miss ratios)</span></li>
				<li>Alerts for <span class="No-Break">potential issues</span></li>
			</ul>
			<h3>Security</h3>
			<p>Security is<a id="_idIndexMarker725"/> non-negotiable. We need to ensure that our cache is secure from unauthorized access and potential attacks. This involves implementing authentication, encryption, and secure <span class="No-Break">communication channels.</span></p>
			<p>The following are some key points <span class="No-Break">to consider:</span></p>
			<ul>
				<li>Securing the cache from <span class="No-Break">unauthorized access</span></li>
				<li>Implementing authentication <span class="No-Break">and encryption</span></li>
				<li>Ensuring secure <span class="No-Break">communication channels</span></li>
				<li>Speed – in-memory storage provides rapid access <span class="No-Break">to data</span></li>
				<li>Volatility – data stored in memory is volatile and can be lost if the <span class="No-Break">node fails</span></li>
			</ul>
			<p>Now that we’ve embraced our requirements, it’s time to dive into the core of the project: the design decisions. Imagine that you’re a master chef who’s been handed a list of ingredients and asked to create a five-star dish. The ingredients are your requirements, but how you <a id="_idIndexMarker726"/>combine them, what cooking techniques you use, and the presentation – well, that’s all up to your <span class="No-Break">design decisions.</span></p>
			<p>Designing a distributed cache is no different. Each requirement we’ve outlined necessitates thoughtful consideration and careful selection of strategies and technologies. The trade-offs we make will determine how well our cache performs, scales, handles faults, maintains consistency, and <span class="No-Break">so on.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor257"/>Design and trade-offs</h1>
			<p>Alright, brace<a id="_idIndexMarker727"/> yourselves, because we’re diving into the deep end of design decisions. Think of it as being handed a pristine Go environment and being asked to build a distributed cache. Simple, right? Sure, if by “simple” you mean navigating a minefield of trade-offs that could blow up your system if you take one <span class="No-Break">wrong step.</span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor258"/>Creating the project</h2>
			<p>Although the <a id="_idIndexMarker728"/>fully tested and functional version of our cache is available in this book’s GitHub repository, let’s reproduce all the steps to make our <span class="No-Break">cache system:</span></p>
			<ol>
				<li> Creating the <span class="No-Break">project directory:</span><pre class="source-code">
mkdir spewg-cache
cd spewg-cache</pre></li>				<li> Initialize the <span class="No-Break"><strong class="source-inline">go</strong></span><span class="No-Break"> module:</span><pre class="source-code">
go mod init spewg-cache</pre></li>				<li>Create the <span class="No-Break"><strong class="source-inline">cache.go</strong></span><span class="No-Break"> file:</span><pre class="source-code">
package main
type CacheItem struct {
    Value string
}
type Cache struct {
    items map[string]CacheItem
}
func NewCache() *Cache {
    return &amp;Cache{
       items: make(map[string]CacheItem),
    }
}
func (c *Cache) Set(key, value string) {
    c.items[key] = CacheItem{
       Value: value,
    }
}
func (c *Cache) Get(key string) (string, bool) {
    item, found := c.items[key]
    if !found {
       return "", false
    }
    return item.Value, true
}</pre></li>			</ol>
			<p>This code <a id="_idIndexMarker729"/>defines a simple cache data structure for storing and retrieving string values using string keys. Think of it as a temporary storage space where you can put values and quickly get them back later by remembering their <span class="No-Break">associated keys.</span></p>
			<p class="callout-heading">How can we know if this code works?</p>
			<p class="callout">Luckily, I read your mind and heard you shouting <span class="No-Break">silently: </span><span class="No-Break"><strong class="bold">Tests!</strong></span></p>
			<p class="callout">From time to time, look in the test files to learn how we’re testing our <span class="No-Break">project components.</span></p>
			<p>We have a simple cache in memory, but concurrent access is not secure. Let’s solve this issue by choosing a way to handle <span class="No-Break">thread safety.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor259"/>Thread safety</h2>
			<p>Ensuring concurrency <a id="_idIndexMarker730"/>safety is crucial to prevent data races and inconsistencies when multiple goroutines access the cache<a id="_idIndexMarker731"/> simultaneously. Here are some options you <span class="No-Break">can consider:</span></p>
			<ul>
				<li>The standard library’s <span class="No-Break"><strong class="source-inline">sync</strong></span><span class="No-Break"> package:</span><ul><li><strong class="source-inline">sync.Mutex</strong>: The <a id="_idIndexMarker732"/>simplest way to achieve concurrency safety is to use a mutex to lock the entire cache during read or write operations. This ensures that only one goroutine can access the cache at a time. However, it can lead to contention and reduced performance under <span class="No-Break">heavy load.</span></li><li><strong class="source-inline">sync.RWMutex</strong>: A<a id="_idIndexMarker733"/> read-write mutex allows multiple readers to access the cache concurrently, but only one writer at a time. This can improve performance when reading is more frequent <span class="No-Break">than writing.</span></li></ul></li>
				<li>Concurrent <span class="No-Break">map implementations:</span><ul><li><strong class="source-inline">sync.Map</strong>: Go provides a<a id="_idIndexMarker734"/> built-in concurrent map implementation that handles synchronization internally. It’s optimized for frequent reads and infrequent writes, making it a such as choice for many <span class="No-Break">caching scenarios.</span></li><li><strong class="bold">Third-party libraries</strong>: Libraries<a id="_idIndexMarker735"/> like <strong class="source-inline">hashicorp/golang-lru</strong>(<a href="https://github.com/hashicorp/golang-lru">https://github.com/hashicorp/golang-lru</a>), <strong class="source-inline">patrickmn/go-cache</strong>(<a href="https://github.com/patrickmn/go-cache">https://github.com/patrickmn/go-cache</a>), and <strong class="source-inline">dgraph-io/ristretto</strong> (<a href="https://github.com/dgraph-io/ristretto">https://github.com/dgraph-io/ristretto</a>) offer <a id="_idIndexMarker736"/>concurrent-safe cache implementations<a id="_idIndexMarker737"/> with additional features such as eviction policies <span class="No-Break">and expiration.</span></li></ul></li>
				<li>Lock-free <span class="No-Break">data structures:</span><ul><li><strong class="bold">Atomic operations</strong>: For specific <a id="_idIndexMarker738"/>use cases, you might employ atomic operations to perform certain updates without explicit locking. However, this requires careful design and is generally more complex to <span class="No-Break">implement correctly.</span></li></ul></li>
				<li><span class="No-Break">Channel-based synchronization:</span><ul><li><strong class="bold">Serializing access</strong>: You can create a dedicated goroutine that handles all cache operations. Other<a id="_idIndexMarker739"/> goroutines communicate with this goroutine through channels, effectively serializing access to <span class="No-Break">the cache.</span></li><li><strong class="bold">Sharded cache</strong>: Divide the <a id="_idIndexMarker740"/>cache into multiple shards, each protected by its own mutex or concurrent map. This can <a id="_idIndexMarker741"/>reduce contention by distributing the load across<a id="_idIndexMarker742"/> <span class="No-Break">multiple locks.</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor260"/>Choosing the right approach</h2>
			<p>The best <a id="_idIndexMarker743"/>approach for concurrency safety depends on your <span class="No-Break">specific requirements:</span></p>
			<ul>
				<li><strong class="bold">Read/write ratio</strong>: If reads are significantly more frequent than writes, <strong class="source-inline">sync.RWMutex</strong> or <strong class="source-inline">sync.Map</strong> might be a <span class="No-Break">suitable choice</span></li>
				<li><strong class="bold">Performance</strong>: If maximum performance is critical, consider lock-free data structures or a <span class="No-Break">sharded cache</span></li>
				<li><strong class="bold">Simplicity</strong>: If ease of implementation is a priority, <strong class="source-inline">sync.Mutex</strong> or a channel-based approach might <span class="No-Break">be simpler</span></li>
			</ul>
			<p>For now, let’s make things simpler. A <strong class="source-inline">sync.RWMutex</strong> will strike the right balance between simplicity <span class="No-Break">and performance.</span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor261"/>Adding thread safety</h2>
			<p>We must <a id="_idIndexMarker744"/>update <strong class="source-inline">cache.go</strong> to add <a id="_idIndexMarker745"/>thread safety <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">sync.RWMutex</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import "sync"
type Cache struct {
    mu    sync.RWMutex
    items map[string]CacheItem
}
func (c *Cache) Set(key, value string) {
    c.mu.Lock()
    defer c.mu.Unlock()
    c.items[key] = CacheItem{
        Value: value,
    }
}
func (c *Cache) Get(key string) (string, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    item, found := c.items[key]
    if !found {
        return "", false
    }
    return item.Value, true
}</pre>			<p>Now we’re talking! Our cache is now thread-safe. What about the interface to the external world? Let’s explore <span class="No-Break">the possibilities.</span></p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor262"/>The interface</h1>
			<p>When<a id="_idIndexMarker746"/> designing a distributed cache, one<a id="_idIndexMarker747"/> of the key decisions you’ll face is choosing the appropriate program interface for communication between clients and the cache servers. The main options available are the <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>), <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>), and other <a id="_idIndexMarker748"/>specialized protocols. Each has its own set of advantages and trade-offs, and understanding these will help us make an informed decision. For our project, we’ll settle on HTTP as the interface of choice, but let’s <span class="No-Break">explore why.</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor263"/>TCP</h2>
			<p>As we saw in previous chapters, TCP<a id="_idIndexMarker749"/> is a cornerstone of modern networking, but like any technology, it comes with its own set of trade-offs. On the one hand, TCP shines in its efficiency. Operating at a low level, it minimizes overhead, making it a lean and mean communication machine. This efficiency is often coupled with superior performance compared to higher-level protocols, especially in terms of latency and throughput, making it a preferred choice for applications where speed is critical. Moreover, TCP empowers developers with granular control over connection management, data flow regulation, and error handling, allowing for tailored solutions to specific <span class="No-Break">networking challenges.</span></p>
			<p>However, this power and efficiency come at a price. The inner workings of TCP are intricate, requiring a deep dive into the world of network programming. Implementing a TCP-based interface often means manually grappling with connection establishment, data packet assembly, and error mitigation strategies, demanding both expertise and time. Even with the technical know-how, developing a robust TCP interface can be a lengthy process, potentially delaying project timelines. Another hurdle lies in the lack of standardization for application-level protocols built upon TCP. While TCP itself adheres to well-defined standards, the protocols that are layered on top often vary widely, leading to compatibility headaches and hindering seamless communication between <span class="No-Break">different systems.</span></p>
			<p>In essence, TCP<a id="_idIndexMarker750"/> is a powerful tool with the potential for high performance and customization, but it requires a significant investment in terms of development effort <span class="No-Break">and expertise.</span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor264"/>HTTP</h2>
			<p>With its clear-cut request/response model, HTTP<a id="_idIndexMarker751"/> is relatively easy to grasp and implement, even for developers new to networking. This ease of use is further bolstered by its status as a widely embraced standard, ensuring seamless compatibility across diverse platforms and clients. Additionally, the vast ecosystem surrounding HTTP, brimming with tools, libraries, and frameworks, accelerates development and deployment cycles. And let’s not forget its stateless nature, which simplifies scaling and fault tolerance, making it easier to handle increased traffic and <span class="No-Break">unexpected failures.</span></p>
			<p>However, like any technology, HTTP isn’t without its drawbacks. Its simplicity comes with a trade-off in the form of overhead. The inclusion of headers and reliance on text-based formatting introduce additional data, potentially impacting performance in bandwidth-constrained environments. Furthermore, while statelessness offers scaling advantages, it can also lead to increased latency compared to persistent TCP connections. Each request necessitates establishing a fresh connection, a process that can add up over time unless newer protocols such as HTTP/2 or keep-alive mechanisms <span class="No-Break">are employed.</span></p>
			<p>In essence, HTTP <a id="_idIndexMarker752"/>provides a straightforward, standardized, and widely supported foundation for web communication. Its simplicity and vast ecosystem make it a popular choice for many applications. However, developers must be mindful of the potential overhead and latency implications, especially in scenarios where performance <span class="No-Break">is paramount.</span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor265"/>Others</h2>
			<p>gRPC<a id="_idIndexMarker753"/> emerges as a high-performance contender in the world of network communication. It harnesses the power of HTTP/2 and <strong class="bold">Protocol Buffers</strong> (<strong class="bold">Protobuf</strong>) to deliver efficient, low-latency<a id="_idIndexMarker754"/> interactions. The use of<a id="_idIndexMarker755"/> Protobuf introduces strong typing and well-defined service contracts, leading to more robust and maintainable code. However, this power comes with a touch of complexity. Setting up gRPC requires support for both HTTP/2 and Protobuf, which may not be universally available, and the learning curve can be steeper compared to <span class="No-Break">simpler protocols.</span></p>
			<p>Alternatively, WebSockets<a id="_idIndexMarker756"/> offer a different kind of advantage: full-duplex communication. Through a single, persistent connection, WebSockets enable real-time, bidirectional data flow between client and server. This makes them ideal for applications such as chat, gaming, or live dashboards where instant updates are crucial. However, this flexibility comes with challenges. Implementing and managing WebSocket connections can be more intricate than traditional request/response models. The requirement for long-lived connections can also complicate scaling and introduce potential points of failure that need to be <span class="No-Break">handled carefully.</span></p>
			<p>In essence, gRPC and WebSockets each excel in different domains. gRPC shines in scenarios where efficiency and well-structured communication are paramount, while WebSockets unlock the potential for seamless real-time interactions. The choice between them often boils down to the specific requirements of the application and the trade-offs developers are willing <span class="No-Break">to make.</span></p>
			<h3>Decision – why HTTP for our project?</h3>
			<p>Given the requirements and the nature of our distributed cache project, HTTP stands out as the most suitable choice for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">Simplicity and ease of use</strong>: HTTP’s <a id="_idIndexMarker757"/>well-defined request/response model makes it easy to<a id="_idIndexMarker758"/> implement and understand. This simplicity is especially beneficial for a project intended to make us learn the <span class="No-Break">core concepts.</span></li>
				<li><strong class="bold">Standardization and compatibility</strong>: HTTP is a widely adopted standard with broad compatibility across different platforms, programming languages, and clients. This ensures that our cache can be easily integrated with various applications <span class="No-Break">and tools.</span></li>
				<li><strong class="bold">Rich ecosystem</strong>: The rich ecosystem of libraries, tools, and frameworks available for HTTP can significantly speed up development. We can leverage existing solutions for tasks such as request parsing, routing, and <span class="No-Break">connection management.</span></li>
				<li><strong class="bold">Statelessness</strong>: HTTP’s stateless nature simplifies scaling and fault tolerance. Each request is independent, making it easier to distribute load across multiple nodes and recover <span class="No-Break">from failures.</span></li>
				<li><strong class="bold">Development speed</strong>: Using HTTP allows us to focus on implementing the core functionality of the distributed cache rather than getting bogged down with low-level networking details. This is crucial for getting things up and running, where the goal is to convey key concepts without unnecessary complexity. We can add another <a id="_idIndexMarker759"/>protocol <a id="_idIndexMarker760"/>once the project <span class="No-Break">is ready.</span></li>
			</ul>
			<h3>Introducing the HTTP server</h3>
			<p>Create <strong class="source-inline">server.go</strong> that will <a id="_idIndexMarker761"/>include <span class="No-Break">HTTP handlers:</span></p>
			<pre class="source-code">
import (
    "encoding/json"
    "net/http"
)
type CacheServer struct {
    cache *Cache
}
func NewCacheServer() *CacheServer {
    return &amp;CacheServer{
        cache: NewCache(),
    }
}
func (cs *CacheServer) SetHandler(w http.ResponseWriter, r *http.Request) {
    var req struct {
        Key   string `json:"key"`
        Value string `json:"value"`
    }
    if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        return
    }
    cs.cache.Set(req.Key, req.Value)
    w.WriteHeader(http.StatusOK)
}
func (cs *CacheServer) GetHandler(w http.ResponseWriter, r *http.Request) {
    key := r.URL.Query().Get("key")
    value, found := cs.cache.Get(key)
    if !found {
        http.NotFound(w, r)
        return
    }
    json.NewEncoder(w).Encode(map[string]string{"value": value})
}</pre>			<p>To initialize our<a id="_idIndexMarker762"/> server, we should create <strong class="source-inline">main.go</strong> <span class="No-Break">like so:</span></p>
			<pre class="source-code">
package main
import (
    "fmt"
    "net/http"
)
func main() {
    cs := NewCacheServer()
    http.HandleFunc("/set", cs.SetHandler)
    http.HandleFunc("/get", cs.GetHandler)
    err := http.ListenAndServe(":8080", nil)
    if err != nil {
       fmt.Println(err)
       return
    }
}</pre>			<p>Now, we can run our cache server for the very first time! In the terminal, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
go run main.go server.go</pre>			<p>The server should now be running <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">http://localhost:8080</strong></span><span class="No-Break">.</span></p>
			<p>You can use <strong class="source-inline">curl</strong> (or a tool such as Postman) to interact with <span class="No-Break">your server.</span></p>
			<pre class="console">
curl -X POST -H "Content-Type: application/json" -d '{"key":"foo", "value":"bar"}' -i http://localhost:8080/set</pre>			<p>This should return a <strong class="source-inline">200 </strong><span class="No-Break"><strong class="source-inline">OK</strong></span><span class="No-Break"> status.</span></p>
			<p>To get the value, we can do something <span class="No-Break">very similar:</span></p>
			<pre class="console">
curl –i "http://localhost:8080/get?key=foo"</pre>			<p>This should return <strong class="source-inline">{"value":"bar"}</strong> if the <span class="No-Break">key exists.</span></p>
			<p>Choosing <a id="_idIndexMarker763"/>HTTP as the interface for our distributed cache project strikes a balance between simplicity, standardization, and ease of integration. While TCP offers performance benefits, the complexity it introduces outweighs its advantages for our educational purposes. By using HTTP, we can leverage a widely understood and supported protocol, making our distributed cache accessible, scalable, and easy to implement. With this decision made, we can now focus on building out the core functionality and features of our <span class="No-Break">distributed cache.</span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor266"/>Eviction policies</h2>
			<p>We can’t just keep everything in <a id="_idIndexMarker764"/>memory indefinitely, right? Eventually, we’ll run out of space. This is where eviction policies come into play. An eviction policy determines which items are removed from the cache when the cache reaches its maximum capacity. Let’s explore some common eviction policies, discuss their trade-offs, and determine the best fit for our distributed <span class="No-Break">cache project.</span></p>
			<h3>LRU</h3>
			<p>LRU <a id="_idIndexMarker765"/>evicts the least recently accessed item first. It assumes that items that haven’t been accessed recently are less likely to be accessed in <span class="No-Break">the future.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li><strong class="bold">Predictable</strong>: Simple <a id="_idIndexMarker766"/>to implement <span class="No-Break">and understand</span></li>
				<li><strong class="bold">Effective</strong>: Works well for many access patterns where recently used items are more likely to <span class="No-Break">be reused</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li><strong class="bold">Memory overhead</strong>: Requires <a id="_idIndexMarker767"/>maintaining a list or other structure to track access order, which can add some <span class="No-Break">memory overhead</span></li>
				<li><strong class="bold">Complexity</strong>: Slightly more complex to implement than FIFO or <span class="No-Break">random eviction</span></li>
			</ul>
			<h3>TTL</h3>
			<p>TTL assigns an<a id="_idIndexMarker768"/> expiration time to each cache item. When the item’s time is up, it’s evicted from <span class="No-Break">the cache.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li><strong class="bold">Simplicity</strong>: Easy to<a id="_idIndexMarker769"/> understand <span class="No-Break">and implement</span></li>
				<li><strong class="bold">Freshness</strong>: Ensures that data in the cache is fresh <span class="No-Break">and relevant</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li><strong class="bold">Predictability</strong>: Less <a id="_idIndexMarker770"/>predictable than LRU as items are evicted based on time rather <span class="No-Break">than usage</span></li>
				<li><strong class="bold">Resource management</strong>: This may require additional resources to periodically check and remove <span class="No-Break">expired items</span></li>
			</ul>
			<h3>First-in, first-out (FIFO)</h3>
			<p>FIFO evicts the <a id="_idIndexMarker771"/>oldest item in the cache based on the time it <span class="No-Break">was added.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li><strong class="bold">Simplicity</strong>: Very<a id="_idIndexMarker772"/> easy <span class="No-Break">to implement</span></li>
				<li><strong class="bold">Predictability</strong>: Predictable <span class="No-Break">eviction pattern</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li><strong class="bold">Inefficiency</strong>: Doesn’t <a id="_idIndexMarker773"/>consider how recently an item was accessed, potentially evicting frequently <span class="No-Break">used items</span></li>
			</ul>
			<h4>Choosing the right eviction policy</h4>
			<p>For our distributed <a id="_idIndexMarker774"/>cache project, we’ll need to balance performance, memory management, and simplicity. Given these considerations, LRU and TTL are both <span class="No-Break">strong candidates.</span></p>
			<p>LRU is ideal for scenarios where the most recently accessed data is likely to be accessed again soon. It helps keep frequently accessed items in memory, which can improve cache hit rates. TTL ensures that data is fresh and relevant by evicting items after a certain period. This is particularly useful when cached data can become <span class="No-Break">stale quickly.</span></p>
			<p>For our project, we’ll implement both LRU and TTL policies. This combination allows us to handle different use cases effectively: LRU for performance optimization based on access patterns, and TTL for ensuring <span class="No-Break">data freshness.</span></p>
			<p>Let’s incrementally add TTL and LRU eviction policies to <span class="No-Break">our implementation.</span></p>
			<h3>Adding TTL</h3>
			<p>There are two main approaches<a id="_idIndexMarker775"/> to adding TTL to our cache: using a goroutine with <strong class="source-inline">Ticker</strong> and evicting <span class="No-Break">during </span><span class="No-Break"><strong class="source-inline">Get</strong></span><span class="No-Break">.</span></p>
			<h4>Goroutine with Ticker</h4>
			<p>In this <a id="_idIndexMarker776"/>approach, we can use a separate goroutine to run <strong class="source-inline">time.Ticker</strong>. The ticker periodically triggers the <strong class="source-inline">evictExpiredItems</strong> function to check for and remove expired entries. Let’s analyze <span class="No-Break">the trade-offs:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Pros</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Proactive eviction</strong>: Expired <a id="_idIndexMarker777"/>items are removed periodically, even if they aren’t accessed. This ensures a cleaner cache and predictable <span class="No-Break">memory usage.</span></li><li><strong class="bold">Potentially lower latency on Get</strong>: The <strong class="source-inline">Get</strong> method doesn’t need to perform eviction checks, potentially making it slightly faster in cases where many items <span class="No-Break">have expired.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Cons</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Additional goroutine</strong>: This introduces <a id="_idIndexMarker778"/>the overhead of managing a separate goroutine <span class="No-Break">and ticker</span></li><li><strong class="bold">Unnecessary checks</strong>: If items expire infrequently or the cache is small, the periodic checks might be <span class="No-Break">unnecessary overhead</span></li></ul></li>
			</ul>
			<h4>Eviction during Get</h4>
			<p>In this approach, we don’t need <a id="_idIndexMarker779"/>a separate goroutine or ticker. The expiration checks are performed only when an item is accessed using the <strong class="source-inline">Get</strong> method. If the item has expired, it’s evicted before the “not found” response is returned. Let’s analyze <span class="No-Break">the trade-offs:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Pros</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Simpler implementation</strong>: There’s no <a id="_idIndexMarker780"/>need to manage an extra goroutine, which leads to less <span class="No-Break">complex code</span></li><li><strong class="bold">Reduced overhead</strong>: Avoids the potential overhead of a continuously <span class="No-Break">running goroutine</span></li><li><strong class="bold">On-demand eviction</strong>: Resources are only used for eviction <span class="No-Break">when necessary</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Cons</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Delayed eviction</strong>: Items <a id="_idIndexMarker781"/>might remain in the cache past their TTL if they <span class="No-Break">aren’t accessed.</span></li><li><strong class="bold">Potential latency on Get</strong>: If many items have expired at the same time, the eviction<a id="_idIndexMarker782"/> process during <strong class="source-inline">Get</strong> might add <span class="No-Break">some latency</span></li></ul></li>
			</ul>
			<p class="callout-heading">Which approach is better?</p>
			<p class="callout">The “better” approach depends on your specific use case <span class="No-Break">and priorities.</span></p>
			<p class="callout">You should choose the first approach in the <span class="No-Break">following instances:</span></p>
			<p class="callout">- You need strict control over when items are evicted and want to ensure a clean cache regardless of <span class="No-Break">access patterns</span></p>
			<p class="callout">- You have a large cache with frequent expirations, and the overhead of the goroutine <span class="No-Break">is acceptable</span></p>
			<p class="callout">- Minimizing latency on <strong class="source-inline">Get</strong> operations is critical, even if it means slightly higher <span class="No-Break">overall overhead</span></p>
			<p class="callout">On the other hand, you should choose the second approach in the <span class="No-Break">following instances:</span></p>
			<p class="callout">- You want a simpler implementation with <span class="No-Break">minimal overhead</span></p>
			<p class="callout">- You are comfortable with some delay in eviction if items are <span class="No-Break">eventually removed</span></p>
			<p class="callout">- Your cache is relatively small, and the potential latency on <strong class="source-inline">Get</strong> due to eviction <span class="No-Break">is acceptable</span></p>
			<p class="callout">You could potentially combine both approaches. Use the second approach for most cases but periodically run a separate eviction process (the first approach) as a background task to clean up any remaining <span class="No-Break">expired items.</span></p>
			<p>All things considered, let’s proceed with the goroutine version so that we can focus on the <strong class="source-inline">Get</strong> <span class="No-Break">method’s latency.</span></p>
			<p>We’ll modify the <strong class="source-inline">CacheItem</strong> struct<a id="_idIndexMarker783"/> so that it includes an expiry time and add logic to the <strong class="source-inline">Set</strong> and <strong class="source-inline">Get</strong> methods so that they can <span class="No-Break">handle TTL:</span></p>
			<pre class="source-code">
package main
import (
     "sync"
     "time"
)
type CacheItem struct {
     Value      string
     ExpiryTime time.Time
}
type Cache struct {
     mu    sync.RWMutex
     items map[string]CacheItem
}
func NewCache() *Cache {
     return &amp;Cache{
          items: make(map[string]CacheItem),
     }
}
func (c *Cache) Set(key, value string, ttl time.Duration) {
     c.mu.Lock()
     defer c.mu.Unlock()
     c.items[key] = CacheItem{
          Value:      value,
          ExpiryTime: time.Now().Add(ttl),
     }
}
func (c *Cache) Get(key string) (string, bool) {
     c.mu.RLock()
     defer c.mu.RUnlock()
     item, found := c.items[key]
     if !found || time.Now().After(item.ExpiryTime) {
          // If the item is not found or has expired, return false
          return "", false
     }
     return item.Value, true
}</pre>			<p>Next, we’ll add a<a id="_idIndexMarker784"/> background goroutine to periodically evict <span class="No-Break">expired items:</span></p>
			<pre class="source-code">
func (c *Cache) startEvictionTicker(d time.Duration) {
     ticker := time.NewTicker(d)
     go func() {
          for range ticker.C {
               c.evictExpiredItems()
          }
     }()
}
func (c *Cache) evictExpiredItems() {
     c.mu.Lock()
     defer c.mu.Unlock()
     now := time.Now()
     for key, item := range c.items {
          if now.After(item.ExpiryTime) {
               delete(c.items, key)
          }
     }
}</pre>			<p>Also, during the cache’s initialization (<strong class="source-inline">main.go</strong>), we need to start <span class="No-Break">this goroutine:</span></p>
			<pre class="source-code">
cache := NewCache()
cache.startEvictionTicker(1 * time.Minute)</pre>			<h3>Adding LRU</h3>
			<p>We’ll enhance our cache <a id="_idIndexMarker785"/>implementation by adding an LRU eviction policy. LRU ensures that the least recently accessed items are evicted first when the cache reaches its maximum capacity. We will use a doubly linked list to keep track of the access order of <span class="No-Break">cache items.</span></p>
			<p>First, we need to modify our <strong class="source-inline">Cache</strong> struct so that it includes a doubly-linked list (<strong class="source-inline">list.List</strong>) for eviction and a <strong class="source-inline">map</strong> struct to keep track of the list elements. Additionally, we’ll<a id="_idIndexMarker786"/> define a <strong class="source-inline">capacity</strong> struct to limit the number of items in <span class="No-Break">the cache:</span></p>
			<pre class="source-code">
package main
import (
     "container/list"
     "sync"
     "time"
)
type CacheItem struct {
     Value      string
     ExpiryTime time.Time
}
type Cache struct {
     mu       sync.RWMutex
     items    map[string]*list.Element // Map of keys to list elements
     eviction *list.List               // Doubly-linked list for eviction
     capacity int                      // Maximum number of items in the cache
}
type entry struct {
     key   string
     value CacheItem
}
func NewCache(capacity int) *Cache {
     return &amp;Cache{
          items:    make(map[string]*list.Element),
          eviction: list.New(),
          capacity: capacity,
     }
}</pre>			<p>Next, we’ll modify the <strong class="source-inline">Set</strong> method so that it manages the doubly linked list and enforces the <a id="_idIndexMarker787"/><span class="No-Break">cache capacity:</span></p>
			<pre class="source-code">
func (c *Cache) Set(key, value string, ttl time.Duration) {
     c.mu.Lock()
     defer c.mu.Unlock()
     // Remove the old value if it exists
     if elem, found := c.items[key]; found {
          c.eviction.Remove(elem)
          delete(c.items, key)
     }
     // Evict the least recently used item if the cache is at capacity
     if c.eviction.Len() &gt;= c.capacity {
          c.evictLRU()
     }
     item := CacheItem{
          Value:      value,
          ExpiryTime: time.Now().Add(ttl),
     }
     elem := c.eviction.PushFront(&amp;entry{key, item})
     c.items[key] = elem
}</pre>			<p>Here, we should pay<a id="_idIndexMarker788"/> attention to the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Check if the key exists</strong>: If the key already exists in the cache, remove the old value from the doubly linked list and <span class="No-Break">the map</span></li>
				<li><strong class="bold">Evict if necessary</strong>: If the cache is at capacity, call <strong class="source-inline">evictLRU</strong> to remove the least recently <span class="No-Break">used item</span></li>
				<li><strong class="bold">Add a new item</strong>: Add the new item to the front of the list and update <span class="No-Break">the map</span></li>
			</ul>
			<p>Now, we need to update the <strong class="source-inline">Get</strong> method so that it can move accessed items to the front of the <span class="No-Break">eviction list:</span></p>
			<pre class="source-code">
func (c *Cache) Get(key string) (string, bool) {
     c.mu.Lock()
     defer c.mu.Unlock()
     elem, found := c.items[key]
     if !found || time.Now().After(elem.Value.(*entry).value.ExpiryTime) {
          // If the item is not found or has expired, return false
          if found {
               c.eviction.Remove(elem)
               delete(c.items, key)
          }
          return "", false
     }
     // Move the accessed element to the front of the eviction list
     c.eviction.MoveToFront(elem)
     return elem.Value.(*entry).value.Value, true
}</pre>			<p>In the preceding <a id="_idIndexMarker789"/>code, if the item is found but has expired, it’s removed from the list and the map. Also, when the item is valid, the code moves it to the front of the list to mark it as <span class="No-Break">recently accessed.</span></p>
			<p>We should also implement the <strong class="source-inline">evictLRU</strong> method to handle the least recently used item <span class="No-Break">being evicted:</span></p>
			<pre class="source-code">
func (c *Cache) evictLRU() {
     elem := c.eviction.Back()
     if elem != nil {
          c.eviction.Remove(elem)
          kv := elem.Value.(*entry)
          delete(c.items, kv.key)
     }
}</pre>			<p>The function removes the item at the back of the list (LRU) and deletes it from <span class="No-Break">the map.</span></p>
			<p>The following<a id="_idIndexMarker790"/> code ensures the background eviction routine removes expired <span class="No-Break">items periodically:</span></p>
			<pre class="source-code">
func (c *Cache) startEvictionTicker(d time.Duration) {
     ticker := time.NewTicker(d)
     go func() {
          for range ticker.C {
               c.evictExpiredItems()
          }
     }()
}
func (c *Cache) evictExpiredItems() {
     c.mu.Lock()
     defer c.mu.Unlock()
     now := time.Now()
     for key, elem := range c.items {
          if now.After(elem.Value.(*entry).value.ExpiryTime) {
               c.eviction.Remove(elem)
               delete(c.items, key)
          }
     }
}</pre>			<p>In this snippet, the <strong class="source-inline">startEvictionTicker</strong> function starts a goroutine that periodically checks and removes expired items from <span class="No-Break">the cache.</span></p>
			<p>Lastly, update <a id="_idIndexMarker791"/>the <strong class="source-inline">main</strong> function so that it creates a cache with a specified capacity and test the TTL and <span class="No-Break">LRU features:</span></p>
			<pre class="source-code">
func main() {
     cache := NewCache(5) // Setting capacity to 5 for LRU
     cache.startEvictionTicker(1 * time.Minute)
}</pre>			<p>With that, we’ve incrementally added TTL and LRU eviction features to our cache implementation! This enhancement ensures that our cache effectively manages memory by keeping frequently accessed items and evicting stale or less-used data. This combination of TTL and LRU makes our cache robust, efficient, and well-suited for various <span class="No-Break">use cases.</span></p>
			<p>Eviction policies are a critical aspect of any cache system, directly impacting its performance and efficiency. By understanding the trade-offs and strengths of LRU, TTL, and other policies, we can make informed decisions that align with our project’s goals. Implementing both <a id="_idIndexMarker792"/>LRU and TTL in our distributed cache ensures we balance performance and data freshness, providing a robust and versatile <span class="No-Break">caching solution.</span></p>
			<p>Now that we’ve tackled the vital task of managing our cache’s memory through effective eviction policies such as LRU and TTL, it’s time to address another critical aspect: replicating <span class="No-Break">our cache.</span></p>
			<h3>Replication</h3>
			<p>To replicate data<a id="_idIndexMarker793"/> across multiple instances of your cache server, you have several options. Here are some <span class="No-Break">common approaches:</span></p>
			<ul>
				<li><strong class="bold">Primary replica replication</strong>: In this setup, one instance is designated as the primary, and the <a id="_idIndexMarker794"/>others are replicas. The <a id="_idIndexMarker795"/>primary handles all writes and propagates changes to <span class="No-Break">the replica.</span></li>
				<li><strong class="bold">Peer-to-peer (P2P) replication</strong>: In P2P replication, all nodes can both send and receive updates. This <a id="_idIndexMarker796"/>approach is <a id="_idIndexMarker797"/>more complex but avoids a single point <span class="No-Break">of failure.</span></li>
				<li><strong class="bold">Publish-subscribe (Pub/Sub) model</strong>: This approach uses a message broker to broadcast updates to all<a id="_idIndexMarker798"/> <span class="No-Break">cache instances.</span></li>
				<li><strong class="bold">Distributed consensus protocols</strong>: Protocols such as Raft and Paxos ensure strong consistency <a id="_idIndexMarker799"/>across replicas. This approach is more complex and often implemented using specialized libraries (for example, etcd <span class="No-Break">and Consul).</span></li>
			</ul>
			<p>Choosing the right replication strategy depends on various factors, such as scalability, fault tolerance, ease of implementation, and the specific requirements of the application. Here’s why we’ll be going for P2P replication over the other <span class="No-Break">three approaches:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Scalability</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">P2P</strong>: In a P2P <a id="_idIndexMarker800"/>architecture, each node can communicate <a id="_idIndexMarker801"/>with any other node, distributing the load evenly across the network. This allows the system to scale horizontally more efficiently as there is no single point <span class="No-Break">of contention.</span></li></ul></li>
			</ul>
			<p><strong class="bold">Primary replica</strong>: Scalability is limited because the master node can become a bottleneck. All write operations are handled by the master, which can lead to performance issues as the number of <span class="No-Break">clients increases.</span></p>
			<p><strong class="bold">Pub/Sub</strong>: While scalable, the message broker can become a bottleneck or single point of failure if not managed properly. Scalability depends on the broker’s performance <span class="No-Break">and architecture.</span></p>
			<p><strong class="bold">Distributed consensus protocols</strong>: These can be scalable, but achieving consensus among many nodes can introduce latency and complexity. They are often more suitable for smaller clusters or where strong consistency <span class="No-Break">is crucial.</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Fault tolerance</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">P2P</strong>: In a P2P network, there is no single point of failure. If one node fails, the remaining nodes can continue to operate and communicate with each other, making the system more robust <span class="No-Break">and resilient.</span></li></ul></li>
			</ul>
			<p><strong class="bold">Primary replica</strong>: The primary node is a single point of failure. If the primary goes down, the entire system’s write capability is affected until a new primary is elected or the old one <span class="No-Break">is restored.</span></p>
			<p><strong class="bold">Pub/Sub</strong>: The message broker can be a single point of failure. While you can have multiple brokers and failover mechanisms, this adds complexity and more <span class="No-Break">moving parts.</span></p>
			<p><strong class="bold">Distributed consensus protocols</strong>: These are designed to handle node failures, but they come with increased complexity. Achieving consensus in the presence of failures can be challenging and may <span class="No-Break">affect performance.</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Consistency</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">P2P</strong>: While eventual consistency is more common in P2P systems, you can implement mechanisms to ensure stronger consistency if needed. This approach provides flexibility in balancing consistency <span class="No-Break">and availability.</span></li></ul></li>
			</ul>
			<p><strong class="bold">Primary replica</strong>: It typically provides strong consistency since all writes go through the master. However, reading consistency might be delayed <span class="No-Break">on replicas.</span></p>
			<p><strong class="bold">Pub/Sub</strong>: It provides eventual consistency as updates are propagated to <span class="No-Break">subscribers asynchronously.</span></p>
			<p><strong class="bold">Distributed consensus protocols</strong>: These provide strong consistency but at the cost of higher latency <span class="No-Break">and complexity.</span></p>
			<ul>
				<li><strong class="bold">Ease of implementation </strong><span class="No-Break"><strong class="bold">and management</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">P2P</strong>: While more <a id="_idIndexMarker802"/>complex than primary replica<a id="_idIndexMarker803"/> replication, P2P systems can be easier to manage at scale because they don’t require a central coordination point. Each node is equal, simplifying <span class="No-Break">the architecture.</span></li><li><strong class="bold">Primary replica</strong>: This is easier to implement initially but can become complex to manage at scale, especially with failover and load <span class="No-Break">balancing mechanisms.</span></li></ul></li>
				<li><strong class="bold">Pub/Sub</strong>: This is relatively easy to implement using existing message brokers, but managing the broker infrastructure and ensuring high availability can <span class="No-Break">add complexity.</span></li>
				<li><strong class="bold">Distributed consensus protocols</strong>: These are generally complex to implement and manage as they require a deep understanding of consensus algorithms and their <span class="No-Break">operational overhead.</span></li>
				<li><span class="No-Break"><strong class="bold">Flexibility</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">P2P</strong>: This offers high flexibility in terms of topology and can adapt to changes in the network easily. Nodes can join or leave the network without <span class="No-Break">significant disruption.</span></li><li><strong class="bold">Master-slave</strong>: This is less flexible due to the centralized nature of the master node. Adding or removing nodes requires reconfiguration and can affect the <span class="No-Break">system’s availability.</span></li><li><strong class="bold">Pub/Sub</strong>: This is flexible in terms of adding new subscribers, but the broker infrastructure can become complex <span class="No-Break">to manage.</span></li><li><strong class="bold">Distributed consensus protocols</strong>: These are flexible in terms of fault tolerance and consistency but require careful planning and management to handle node changes and <span class="No-Break">network partitions.</span></li></ul></li>
			</ul>
			<p>P2P replication is <a id="_idIndexMarker804"/>a compelling choice for our cache project. It avoids<a id="_idIndexMarker805"/> the single point of failure associated with the primary replica and Pub/Sub models and is generally more straightforward to scale and manage than distributed consensus protocols. While it may not provide the strong consistency guarantees of consensus protocols, it offers a balanced approach that can be tailored to meet various <span class="No-Break">consistency requirements.</span></p>
			<p>Don’t get me wrong! P2P isn’t perfect, but it is a reasonable approach to get things going. It also has <em class="italic">hard</em> problems to solve, such as eventual consistency, conflict resolution, replication overhead, bandwidth consumption, <span class="No-Break">and more.</span></p>
			<h3>Implementing P2P replication</h3>
			<p>First, we need to modify the <a id="_idIndexMarker806"/>cache server so that it’s aware of <span class="No-Break">the peers:</span></p>
			<pre class="source-code">
type CacheServer struct {
     cache *Cache
     peers []string
     mu    sync.Mutex
}
func NewCacheServer(peers []string) *CacheServer {
     return &amp;CacheServer{
          cache: NewCache(10),
          peers: peers,
     }
}</pre>			<p>We also need<a id="_idIndexMarker807"/> to create a function to replicate the data to <span class="No-Break">the peers:</span></p>
			<pre class="source-code">
func (cs *CacheServer) replicateSet(key, value string) {
    cs.mu.Lock()
    defer cs.mu.Unlock()
    req := struct {
       Key   string `json:"key"`
       Value string `json:"value"`
    }{
       Key:   key,
       Value: value,
    }
    data, _ := json.Marshal(req)
    for _, peer := range cs.peers {
       go func(peer string) {
          client := &amp;http.Client{}
          req, err := http.NewRequest("POST", peer+"/set", bytes.NewReader(data))
          if err != nil {
             log.Printf("Failed to create replication request: %v", err)
             return
          }
          req.Header.Set("Content-Type", "application/json")
          req.Header.Set(replicationHeader, "true")
          _, err = client.Do(req)
          if err != nil {
             log.Printf("Failed to replicate to peer %s: %v", peer, err)
          }
          log.Println("replication successful to", peer)
       }(peer)
    }
}</pre>			<p>The core idea here is to iterate over all the peers (<strong class="source-inline">cs.peers</strong>) in the cache server’s configuration and for <span class="No-Break">each peer:</span></p>
			<p>For each peer, the<a id="_idIndexMarker808"/> <span class="No-Break">following happens:</span></p>
			<ul>
				<li>A new goroutine (<strong class="source-inline">go func(...)</strong>) is launched. This allows replication to happen concurrently for each peer, <span class="No-Break">improving performance.</span></li>
				<li>An HTTP POST request is constructed to send the JSON data to the peer’s <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">set</strong></span><span class="No-Break"> endpoint.</span></li>
				<li>A custom header called <strong class="source-inline">replicationHeader</strong> is added to the request. This likely helps the receiving peer distinguish replication requests from regular <span class="No-Break">client requests.</span></li>
				<li>The HTTP request is sent <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">client.Do(req)</strong></span><span class="No-Break">.</span></li>
				<li>If there are any errors during request creation or sending, <span class="No-Break">they’re logged.</span></li>
			</ul>
			<p>We can now<a id="_idIndexMarker809"/> use the replication during <span class="No-Break">our </span><span class="No-Break"><strong class="source-inline">SetHandler</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
func (cs *CacheServer) SetHandler(w http.ResponseWriter, r *http.Request) {
    var req struct {
       Key   string `json:"key"`
       Value string `json:"value"`
    }
    if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {
       http.Error(w, err.Error(), http.StatusBadRequest)
       return
    }
    cs.cache.Set(req.Key, req.Value, 1*time.Hour)
    if r.Header.Get(replicationHeader) == "" {
       go cs.replicateSet(req.Key, req.Value)
    }
    w.WriteHeader(http.StatusOK)
}</pre>			<p>This new conditional block serves as a check to determine whether an incoming request to the cache server (<strong class="source-inline">r</strong>) is a regular client request or a replication request from another cache server. Based on this determination, it decides whether to trigger further replication to <span class="No-Break">other peers.</span></p>
			<p>To glue <a id="_idIndexMarker810"/>everything together, let’s change the main function so that it receives the peers and bootstraps the code <span class="No-Break">with them:</span></p>
			<pre class="source-code">
var port string
var peers string
func main() {
    flag.StringVar(&amp;port, "port", ":8080", "HTTP server port")
    flag.StringVar(&amp;peers, "peers", "", "Comma-separated list of peer addresses")
    flag.Parse()
    peerList := strings.Split(peers, ",")
    cs := spewg.NewCacheServer(peerList)
    http.HandleFunc("/set", cs.SetHandler)
    http.HandleFunc("/get", cs.GetHandler)
    err := http.ListenAndServe(port, nil)
    if err != nil {
       fmt.Println(err)
       return
    }
}</pre>			<p>With that, the implementation is complete! Let’s run two instances of our cache and see if our data is <span class="No-Break">being replicated.</span></p>
			<p>Let’s run the <span class="No-Break">first instance:</span></p>
			<pre class="console">
go run main.go -port=:8080 -peers=http://localhost:8081</pre>			<p>Now, let’s run the <span class="No-Break">second instance:</span></p>
			<pre class="console">
go run main.go -port=:8081 -peers=http://localhost:8080</pre>			<p>We can now use <strong class="source-inline">curl</strong> or any HTTP client to test the <strong class="source-inline">Set</strong> and <strong class="source-inline">Get</strong> operations across <span class="No-Break">the cluster.</span></p>
			<p>Set a <span class="No-Break">key-value pair:</span></p>
			<pre class="console">
curl -X POST -d '{"key":"foo","value":"bar"}' -H "Content-Type: application/json" http://localhost:8080/set</pre>			<p>Get the key-value pair from a <span class="No-Break">different instance:</span></p>
			<pre class="console">
curl http://localhost:8081/get?key=foo</pre>			<p>You should<a id="_idIndexMarker811"/> see a value of <strong class="source-inline">bar</strong> if the replication is <span class="No-Break">working correctly.</span></p>
			<p>Check the logs of each instance to see the replication process in action. You should see log entries being applied across all instances! If you’re feeling adventurous, run multiple instances of the cache and see the dance of replication in front of your <span class="No-Break">very eyes.</span></p>
			<p>We can add features and optimize our cache infinitely, but infinite seems too much for our project. The last piece of our puzzle will be sharding <span class="No-Break">our data.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor267"/>Sharding</h2>
			<p>Sharding<a id="_idIndexMarker812"/> is a fundamental technique that’s used to partition data across multiple nodes, ensuring scalability and performance. Sharding offers several key benefits that make it an attractive option for <span class="No-Break">distributed caches:</span></p>
			<ul>
				<li><strong class="bold">Horizontal scaling</strong>: Sharding allows you to scale horizontally by adding more nodes (shards) to your system. This enables the cache to handle larger datasets and higher request volumes without <span class="No-Break">degrading performance.</span></li>
				<li><strong class="bold">Load distribution</strong>: By distributing data across multiple shards, sharding helps balance the load, preventing any single node from becoming <span class="No-Break">a bottleneck.</span></li>
				<li><strong class="bold">Parallel processing</strong>: Multiple shards can process requests in parallel, leading to faster query and <span class="No-Break">update operations.</span></li>
				<li><strong class="bold">Isolation of failures</strong>: If one shard fails, the others can continue to operate, ensuring that the system remains available even in the presence <span class="No-Break">of failures.</span></li>
				<li><strong class="bold">Simplified management</strong>: Each shard can be managed independently, allowing for easier maintenance and upgrades without affecting the <span class="No-Break">entire system.</span></li>
			</ul>
			<h3>Approaches to implementing sharding</h3>
			<p>There are several approaches to implementing sharding, each with its advantages and trade-offs. The most common approaches include range-based sharding, hash-based sharding, and <span class="No-Break">consistent hashing.</span></p>
			<h4>Range-based sharding</h4>
			<p>In range-based sharding, data is <a id="_idIndexMarker813"/>divided into contiguous ranges based on the shard<a id="_idIndexMarker814"/> key (for example, numerical or alphabetical ranges). Each shard is responsible for a specific range <span class="No-Break">of keys.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li>Simple to <a id="_idIndexMarker815"/>implement <span class="No-Break">and understand</span></li>
				<li>Efficient <span class="No-Break">range queries</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li>Uneven <a id="_idIndexMarker816"/>distribution of data if the key distribution <span class="No-Break">is skewed</span></li>
				<li>Hotspots can form if certain ranges are accessed <span class="No-Break">more frequently</span></li>
			</ul>
			<h4>Hash-based sharding</h4>
			<p>In hash-based sharding, a hash<a id="_idIndexMarker817"/> function is applied to the shard key to determine<a id="_idIndexMarker818"/> the shard. This approach ensures a more uniform distribution of data <span class="No-Break">across shards.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li>Even <a id="_idIndexMarker819"/>distribution <span class="No-Break">of data</span></li>
				<li>Avoids hotspots caused by skewed <span class="No-Break">key distributions</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li>Range <a id="_idIndexMarker820"/>queries are inefficient as they may span <span class="No-Break">multiple shards</span></li>
				<li>Re-sharding (adding/removing nodes) can <span class="No-Break">be complex</span></li>
			</ul>
			<h4>Consistent hashing</h4>
			<p>Consistent hashing is a <a id="_idIndexMarker821"/>specialized form of hash-based sharding that minimizes the impact of re-sharding. Nodes and keys are hashed to a circular space, and each node is responsible for the keys in <span class="No-Break">its range.</span></p>
			<p><span class="No-Break">Advantages:</span></p>
			<ul>
				<li>Minimizes <a id="_idIndexMarker822"/>data movement <span class="No-Break">during re-sharding</span></li>
				<li>Provides good load balancing and <span class="No-Break">fault tolerance</span></li>
			</ul>
			<p><span class="No-Break">Disadvantages:</span></p>
			<ul>
				<li>More complex to<a id="_idIndexMarker823"/> implement compared to simple <span class="No-Break">hash-based sharding</span></li>
				<li>Requires careful tuning <span class="No-Break">and management</span></li>
			</ul>
			<p>Let’s go with consistent hashing. This approach will help us achieve a balanced distribution of data and handle <span class="No-Break">re-sharding efficiently.</span></p>
			<h4>Implementing consistent hashing</h4>
			<p>The first thing we <a id="_idIndexMarker824"/>need to do is create our hash ring. But wait! What is a hash ring? Keep calm and bear <span class="No-Break">with me!</span></p>
			<p>Imagine a circular ring on which each point represents a possible output of a hash function. This is our “<span class="No-Break">hash ring.”</span></p>
			<p>Each cache server (or node) in our system is assigned a random position on the ring that’s usually determined by hashing the server’s unique identifier (such as its address). These positions represent the node’s “ownership range” on the ring. Every piece of data (a cache entry) is hashed. The resulting hash value is also mapped to a point on <span class="No-Break">the ring.</span></p>
			<p>A data key is assigned to the first node it encounters while moving clockwise on the ring from <span class="No-Break">its position.</span></p>
			<h4>Visualizing the hash ring</h4>
			<p>In the following example, we <a id="_idIndexMarker825"/>can see <span class="No-Break">the following:</span></p>
			<ul>
				<li>Key 1 is assigned to <span class="No-Break">Node A</span></li>
				<li>Key 2 is assigned to <span class="No-Break">Node B</span></li>
				<li>Key 3 is assigned to <span class="No-Break">Node C</span></li>
			</ul>
			<p>Let’s take a <span class="No-Break">closer look:</span></p>
			<pre class="console">
      Node B
     /
    /      Key 2
   /
  /
 Node A --------- Key 1
  \
   \     Key 3
    \
     \
      Node C</pre>			<p>The following file, <strong class="source-inline">hashring.go</strong>, is the <a id="_idIndexMarker826"/>foundation for managing the consistent <span class="No-Break">hash ring:</span></p>
			<pre class="source-code">
package spewg
// ... (imports) ...
type Node struct {
    ID   string // Unique identifier
    Addr string // Network address
}
type HashRing struct {
    nodes  []Node         // List of nodes
    hashes []uint32       // Hashes for nodes (for efficient searching)
    lock   sync.RWMutex   // Concurrency protection
}
func NewHashRing() *HashRing { ... }
func (h *HashRing) AddNode(node Node) { ... }
func (h *HashRing) RemoveNode(nodeID string) { ... }
func (h *HashRing) GetNode(key string) Node { ... }
func (h *HashRing) hash(key string) uint32 { ... }</pre>			<p>Upon exploring the file<a id="_idIndexMarker827"/> in the repository, we can see <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">nodes</strong>: A slice to store node structs (the ID and address of <span class="No-Break">each server).</span></li>
				<li><strong class="source-inline">hashes</strong>: A slice of uint32 values to store the hashes of each node. This allows for efficient searching to find the <span class="No-Break">responsible node.</span></li>
				<li><strong class="source-inline">lock</strong>: A mutex to ensure safe, concurrent access to <span class="No-Break">the ring.</span></li>
				<li><strong class="source-inline">hash()</strong>: This function uses SHA-1 to hash node IDs and <span class="No-Break">data keys.</span></li>
				<li><strong class="source-inline">AddNode</strong>: This calculates a node’s hash, inserts it into the hashes slice, and sorts the slice to <span class="No-Break">maintain order.</span></li>
				<li><strong class="source-inline">GetNode</strong>: Given a key, it performs a binary search on the sorted hashes to find the first hash that’s equal to or greater than the key’s hash. The corresponding node in the <strong class="source-inline">nodes</strong> slice is <span class="No-Break">the owner.</span></li>
			</ul>
			<p>We also need to<a id="_idIndexMarker828"/> update <strong class="source-inline">server.go</strong> so that it can interact with the <span class="No-Break">hash ring:</span></p>
			<pre class="source-code">
type CacheServer struct {
     cache    *Cache
     peers    []string
     hashRing *HashRing
     selfID   string
     mu       sync.Mutex
}
func NewCacheServer(peers []string, selfID string) *CacheServer {
     cs := &amp;CacheServer{
          cache:    NewCache(10),
          peers:    peers,
          hashRing: NewHashRing(),
          selfID:   selfID,
     }
     for _, peer := range peers {
          cs.hashRing.AddNode(Node{ID: peer, Addr: peer})
     }
     cs.hashRing.AddNode(Node{ID: selfID, Addr: "self"})
     return cs
}</pre>			<p>Now, we need to <a id="_idIndexMarker829"/>modify <strong class="source-inline">SetHandler</strong> so that it handles replication and <span class="No-Break">request forwarding:</span></p>
			<pre class="source-code">
const replicationHeader = "X-Replication-Request"
func (cs *CacheServer) SetHandler(w http.ResponseWriter, r *http.Request) {
     var req struct {
          Key   string `json:"key"`
          Value string `json:"value"`
     }
     if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {
          http.Error(w, err.Error(), http.StatusBadRequest)
          return
     }
     targetNode := cs.hashRing.GetNode(req.Key)
     if targetNode.Addr == "self" {
          cs.cache.Set(req.Key, req.Value, 1*time.Hour)
          if r.Header.Get(replicationHeader) == "" {
               go cs.replicateSet(req.Key, req.Value)
          }
          w.WriteHeader(http.StatusOK)
     } else {
          cs.forwardRequest(w, targetNode, r)
     }
}</pre>			<p>We also need to<a id="_idIndexMarker830"/> add the <strong class="source-inline">replicateSet</strong> method to replicate the <strong class="source-inline">set</strong> request to <span class="No-Break">other peers:</span></p>
			<pre class="source-code">
func (cs *CacheServer) replicateSet(key, value string) {
     cs.mu.Lock()
     defer cs.mu.Unlock()
     req := struct {
          Key   string `json:"key"`
          Value string `json:"value"`
     }{
          Key:   key,
          Value: value,
     }
     data, _ := json.Marshal(req)
     for _, peer := range cs.peers {
          if peer != cs.selfID {
               go func(peer string) {
                    client := &amp;http.Client{}
                    req, err := http.NewRequest("POST", peer+"/set", bytes.NewReader(data))
                    if err != nil {
                         log.Printf("Failed to create replication request: %v", err)
                         return
                    }
                    req.Header.Set("Content-Type", "application/json")
                    req.Header.Set(replicationHeader, "true")
                    _, err = client.Do(req)
                    if err != nil {
                         log.Printf("Failed to replicate to peer %s: %v", peer, err)
                    }
               }(peer)
          }
     }
}</pre>			<p>Once we’ve done this, we<a id="_idIndexMarker831"/> can change <strong class="source-inline">GetHandler</strong> so that it forwards requests to the <span class="No-Break">appropriate node:</span></p>
			<pre class="source-code">
func (cs *CacheServer) GetHandler(w http.ResponseWriter, r *http.Request) {
     key := r.URL.Query().Get("key")
     targetNode := cs.hashRing.GetNode(key)
     if targetNode.Addr == "self" {
          value, found := cs.cache.Get(key)
          if !found {
               http.NotFound(w, r)
               return
          }
          err := json.NewEncoder(w).Encode(map[string]string{"value": value})
          if err != nil {
               w.WriteHeader(http.StatusInternalServerError)
               return
          }
     } else {
          originalSender := r.Header.Get("X-Forwarded-For")
          if originalSender == cs.selfID {
               http.Error(w, "Loop detected", http.StatusBadRequest)
               return
          }
          r.Header.Set("X-Forwarded-For", cs.selfID)
          cs.forwardRequest(w, targetNode, r)
     }
}</pre>			<p>Both methods<a id="_idIndexMarker832"/> are using <strong class="source-inline">forwardRequest</strong>. Let’s create it <span class="No-Break">as well:</span></p>
			<pre class="source-code">
func (cs *CacheServer) forwardRequest(w http.ResponseWriter, targetNode Node, r *http.Request) {
     client := &amp;http.Client{}
     var req *http.Request
     var err error
     if r.Method == http.MethodGet {
          url := fmt.Sprintf("%s%s?%s", targetNode.Addr, r.URL.Path, r.URL.RawQuery)
          req, err = http.NewRequest(r.Method, url, nil)
     } else if r.Method == http.MethodPost {
          body, err := io.ReadAll(r.Body)
          if err != nil {
               http.Error(w, "Failed to read request body", http.StatusInternalServerError)
               return
          }
          url := fmt.Sprintf("%s%s", targetNode.Addr, r.URL.Path)
          req, err = http.NewRequest(r.Method, url, bytes.NewReader(body))
          if err != nil {
               http.Error(w, "Failed to create forward request", http.StatusInternalServerError)
               return
          }
          req.Header.Set("Content-Type", r.Header.Get("Content-Type"))
     }
     if err != nil {
          log.Printf("Failed to create forward request: %v", err)
          http.Error(w, "Failed to create forward request", http.StatusInternalServerError)
          return
     }
     req.Header = r.Header
     resp, err := client.Do(req)
     if err != nil {
          log.Printf("Failed to forward request to node %s: %v", targetNode.Addr, err)
          http.Error(w, "Failed to forward request", http.StatusInternalServerError)
          return
     }
     defer resp.Body.Close()
     w.WriteHeader(resp.StatusCode)
     _, err = io.Copy(w, resp.Body)
     if err != nil {
          log.Printf("Failed to write response from node %s: %v", targetNode.Addr, err)
     }
}</pre>			<p>The last step is to <a id="_idIndexMarker833"/>update <strong class="source-inline">main.go</strong> so that it considers <span class="No-Break">the nodes:</span></p>
			<pre class="source-code">
var port string
var peers string
func main() {
    flag.StringVar(&amp;port, "port", ":8080", "HTTP server port")
    flag.StringVar(&amp;peers, "peers", "", "Comma-separated list of peer addresses")
    flag.Parse()
    nodeID := fmt.Sprintf("%s%d", "node", rand.Intn(100))
    peerList := strings.Split(peers, ",")
    cs := spewg.NewCacheServer(peerList, nodeID)
    http.HandleFunc("/set", cs.SetHandler)
    http.HandleFunc("/get", cs.GetHandler)
    http.ListenAndServe(port, nil)
}</pre>			<p>Let’s test our <span class="No-Break">consistent hashing!</span></p>
			<p>Run the <span class="No-Break">first instance:</span></p>
			<pre class="console">
go run main.go -port=:8083 -peers=http://localhost:8080</pre>			<p>Run the <span class="No-Break">second instance:</span></p>
			<pre class="console">
go run main.go -port=:8080 -peers=http://localhost:8083</pre>			<p>The first set of tests will be the basic <strong class="source-inline">SET</strong> and <strong class="source-inline">GET</strong> commands. Let’s set a key-value pair on Node <span class="No-Break">A (localhost:8080):</span></p>
			<pre class="console">
curl -X POST -H "Content-Type: application/json" -d '{"key": "mykey", "value": "myvalue"}' localhost:8080/set</pre>			<p>Now, we can get the value from the <span class="No-Break">correct node:</span></p>
			<pre class="console">
curl localhost:8080/get?key=mykey
# OR
curl localhost:8083/get?key=mykey</pre>			<p>Depending on how <strong class="source-inline">mykey</strong> hashes, the value should be returned from either port <strong class="source-inline">8080</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">8083</strong></span><span class="No-Break">.</span></p>
			<p>To test the<a id="_idIndexMarker834"/> hashing and key distribution, we can set <span class="No-Break">multiple keys:</span></p>
			<pre class="console">
curl -X POST -H "Content-Type: application/json" -d '{"key": "key1", "value": "value1"}' localhost:8080/set
curl -X POST -H "Content-Type: application/json" -d '{"key": "key2", "value": "value2"}' localhost:8080/set
curl -X POST -H "Content-Type: application/json" -d '{"key": "key3", "value": "value3"}' localhost:8080/set</pre>			<p>Then, we can get the values and observe <span class="No-Break">the distribution:</span></p>
			<pre class="console">
curl localhost:8080/get?key=key1
curl localhost:8083/get?key=key2
curl localhost:8080/get?key=key3</pre>			<p class="callout-heading">Note</p>
			<p class="callout">Some keys might be on one server, while others might be on the second server, depending on how their hashes map onto <span class="No-Break">the ring.</span></p>
			<p>The key takeaways from this implementation are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The hash ring provides a way to consistently map keys to nodes, even as the <span class="No-Break">system scales</span></li>
				<li>Consistent hashing minimizes disruptions caused by adding or <span class="No-Break">removing nodes</span></li>
				<li>The implementation in the patch focuses on simplicity, using SHA-1 for hashing and a sorted slice for efficient <span class="No-Break">node lookup</span></li>
			</ul>
			<p>Congratulations! You’ve embarked on an exhilarating journey through the world of distributed caching, constructing a system that’s not just functional but primed for new optimization. Now, it’s time to unleash the full potential of your creation by digging deeper into the realms of optimization, metrics, and profiling. Think of this as fine-tuning your high-performance engine, ensuring it purrs with efficiency <span class="No-Break">and speed.</span></p>
			<p>Where you can go from here? Let’s sum <span class="No-Break">this up:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Optimization techniques</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Cache replacement algorithms</strong>: Experiment <a id="_idIndexMarker835"/>with alternative cache replacement<a id="_idIndexMarker836"/> algorithms such as <strong class="bold">Low Inter-Reference Recency Set</strong> (<strong class="bold">LIRS</strong>) or <strong class="bold">Adaptive Replacement Cache</strong> (<strong class="bold">ARC</strong>). These <a id="_idIndexMarker837"/>algorithms can offer improved hit rates and better adaptability to varying workloads compared to <span class="No-Break">traditional LRU.</span></li><li><strong class="bold">Tuning eviction policies</strong>: Fine-tune <a id="_idIndexMarker838"/>your TTL values and LRU thresholds based on your specific data characteristics and access patterns. This prevents the premature eviction of valuable data and ensures that the cache remains responsive to <span class="No-Break">changing demands.</span></li><li><strong class="bold">Compression</strong>: Implement data compression<a id="_idIndexMarker839"/> techniques to reduce the memory footprint of cached items. This allows you to store more data in the cache and potentially improve hit rates, especially for compressible <span class="No-Break">data types.</span></li><li><strong class="bold">Connection pooling</strong>: Optimize <a id="_idIndexMarker840"/>network communication by implementing connection pooling between your cache clients and servers. This reduces the overhead for establishing new connections for each request, leading to faster <span class="No-Break">response times.</span></li></ul></li>
				<li><strong class="bold">Metrics </strong><span class="No-Break"><strong class="bold">and monitoring</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Key metrics</strong>: Continuously<a id="_idIndexMarker841"/> monitor essential metrics such as cache hit rate, miss rate, eviction rate, latency, throughput, and memory usage. These metrics provide valuable insights into the cache’s performance and help identify potential bottlenecks or areas <span class="No-Break">for improvement.</span></li><li><strong class="bold">Visualization</strong>: Utilize visualization<a id="_idIndexMarker842"/> tools such as Grafana to create dashboards that display these metrics in real time. This allows you to easily track trends, spot anomalies, and make data-driven decisions about <span class="No-Break">cache optimization.</span></li><li><strong class="bold">Alerting</strong>: Set up <a id="_idIndexMarker843"/>alerts based on predefined thresholds for critical metrics. For example, you could receive an alert if the cache hit rate drops below a certain percentage or if latency exceeds a specified limit. This enables you to proactively address issues before they <span class="No-Break">impact users.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Profiling</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">CPU profiling</strong>: Identify <a id="_idIndexMarker844"/>CPU-intensive functions or operations within your cache code. This <a id="_idIndexMarker845"/>helps you pinpoint areas where optimizations can yield the most significant <span class="No-Break">performance gains.</span></li><li><strong class="bold">Memory profiling</strong>: Analyze <a id="_idIndexMarker846"/>memory usage patterns to detect<a id="_idIndexMarker847"/> memory leaks or inefficient memory allocation. Optimizing memory usage can improve the cache’s overall performance <span class="No-Break">and stability.</span></li></ul></li>
			</ul>
			<p>With dedication and a data-driven approach, you’ll unlock the full potential of your distributed cache and ensure it remains an asset in your future <span class="No-Break">software architectures.</span></p>
			<p>Oof! What a ride, huh? We explored a lot of design decisions and implementation during this chapter. Let’s wrap up what we <span class="No-Break">have done.</span></p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor268"/>Summary</h1>
			<p>In this chapter, we built a distributed cache from scratch. We started with a simple in-memory cache and gradually added features such as thread safety, HTTP interface, eviction policies (LRU and TTL), replication, and consistent hashing for sharding. Each step was a building block that contributed to the robustness, scalability, and performance of <span class="No-Break">our cache.</span></p>
			<p>While our cache is functional, it’s just the beginning. There are countless avenues for further exploration and optimization. The world of distributed caching is vast and ever-evolving, and this chapter has equipped you with the essential knowledge and practical skills to navigate it confidently. Remember, building a distributed cache is not just about the code; it’s about understanding the underlying principles, making informed design decisions, and continuously iterating to meet the evolving demands of <span class="No-Break">your applications.</span></p>
			<p>Now that we’ve navigated the treacherous waters of design decisions and trade-offs, we’ve laid a solid foundation for our distributed cache. We’ve combined the right strategies, technologies, and a dash of cynicism to create a robust, scalable, and efficient system. But designing a system is only half the battle; the other half is writing code that doesn’t make future developers (including ourselves) weep tears <span class="No-Break">of frustration.</span></p>
			<p>In the next chapter, <em class="italic">Effective Code Practices</em>, we’ll cover essential techniques to elevate your Go coding game. You’ll learn how to maximize performance by efficiently reusing system resources, eliminate redundant task execution for streamlined processes, master memory management to keep your system lean and fast, and sidestep common issues that can degrade performance. Prepare for a deep dive into Go’s best practices, where precision, clarity, and a touch of sarcasm are the keys <span class="No-Break">to success.</span></p>
		</div>
	</div>
</div>
</body></html>