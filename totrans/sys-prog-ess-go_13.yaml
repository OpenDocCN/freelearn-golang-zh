- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capstone Project – Distributed Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The grand finale is where we take everything we’ve learned and apply it to
    a real-world challenge. You might be thinking, “Surely, building a distributed
    cache can’t be that complex.” Spoiler alert: it’s not just about slapping together
    some in-memory storage and calling it a day. This is where theory meets practice,
    and trust me, it’s a wild ride.'
  prefs: []
  type: TYPE_NORMAL
- en: Our distributed cache will be designed to handle frequent read and write operations
    with minimal latency. It will distribute data across multiple nodes to ensure
    scalability and fault tolerance. We’ll implement key features such as data sharding,
    replication, and eviction policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eviction policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this capstone project, you’ll have built a fully functional distributed
    cache from scratch. You’ll understand the intricacies of data distribution and
    performance optimization. More importantly, you’ll have the confidence to tackle
    similar challenges in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code shown in this chapter can be found in the `ch13` directory of this
    book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding distributed caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, do you think distributed caching is just a fancy term for storing stuff
    in memory across a few servers? Bless your heart. If only life were that simple.
    Let me guess, you’re the type who thinks that simply slapping “distributed” in
    front of anything makes it automatically better, faster, and cooler. Well, strap
    in because we’re about to dive into the rabbit hole of distributed caching, where
    nothing is as straightforward as it seems.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re at a software developer’s party (because we all know how wild
    those get), and someone casually mentions, “Hey, why don’t we just cache everything?”
    This is like saying, “Why don’t we just solve world hunger by ordering more pizza?”
    Sure, the idea is nice, but the devil is in the details. Distributed caching is
    not about stuffing more data into memory. It’s about smartly managing data spread
    across multiple nodes while ensuring that it doesn’t turn into a synchronized
    swimming event gone horribly wrong.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s tackle the basics. A distributed cache is a data storage layer
    that lies between your application and your primary data store. It’s designed
    to store frequently accessed data in a way that reduces latency and improves read
    throughput. Think of it as the app’s version of having a mini fridge next to your
    desk. You don’t need to walk to the kitchen every time you need a drink. Instead,
    you have quick access to your favorite beverage, right at your fingertips.
  prefs: []
  type: TYPE_NORMAL
- en: But, as with all things in life and software, there’s a catch. Ensuring that
    the data in this mini fridge is always fresh, cold, and available to everyone
    in your office simultaneously is no small feat. Distributed caches must maintain
    consistency across multiple nodes, handle failures gracefully, and efficiently
    manage data eviction. They must ensure that data isn’t stale and that updates
    propagate correctly, all while keeping latency to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Then comes the architecture. One popular approach is sharding, where data is
    divided into smaller chunks and distributed across different nodes. This helps
    in balancing the load and ensures that no single node becomes a bottleneck. Another
    essential feature is replication. It’s not enough to have the data spread out;
    you also need copies of it to handle node failures. However, balancing consistency,
    availability, and partition tolerance (the CAP theorem) is where things get tricky.
  prefs: []
  type: TYPE_NORMAL
- en: System requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each feature we’ll cover is crucial to building a robust and high-performing
    distributed cache system. By understanding and implementing these features, you
    will gain a comprehensive understanding of the intricacies involved in distributed
    caching.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of a distributed cache is its in-memory storage capability. In-memory
    storage allows for fast data access, significantly reducing the latency compared
    to disk-based storage. This feature is particularly important for applications
    that require high-speed data retrieval. Let’s explore our project requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Welcome to the delightful world of requirements! Now, before you roll your
    eyes and groan about another tedious checklist, let’s set the record straight.
    Requirements aren’t figments of some overly ambitious product manager’s imagination.
    They’re intentional choices that shape the very essence of what you’re building.
    Think of them as the DNA of your project. Without them, you’re just blindly writing
    code and praying it works out. Spoiler alert: **it won’t**.'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements are your guiding light, your North Star. They keep you focused,
    ensure you’re building the right thing, and save you from the dreaded scope creep.
    In the context of our distributed cache project, they’re critical. So, let’s dive
    in and joyfully embrace the necessities that will make our distributed cache not
    just functional, but outstanding.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We want our cache to be lightning-fast. This means millisecond response times
    for data retrieval and minimal latency for data updates. Achieving this requires
    thoughtful design choices around in-memory storage and efficient data structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast data access and retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimal latency for data updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient data structures and algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our cache should scale horizontally, meaning we can add more nodes to handle
    increased load. This involves implementing sharding and ensuring that our architecture
    can grow seamlessly without significant rework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamless addition of new nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data should remain available even if some nodes fail. This requires implementing
    replication and ensuring that our system can handle node failures gracefully without
    data loss or significant downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: High availability despite node failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data replication across multiple nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graceful handling of node failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data expiry and eviction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our cache should efficiently manage memory by expiring old data and evicting
    less frequently accessed data. Implementing **time to live** (**TTL**) and **least
    recently used** (**LRU**) eviction policies will help us manage limited memory
    resources effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing TTL and LRU eviction policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the cache fresh and relevant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure our cache performs optimally, we need robust monitoring and metrics.
    This involves logging cache operations, tracking performance metrics (such as
    hit/miss ratios), and setting up alerts for potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Robust monitoring of cache operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance metrics (hit/miss ratios)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts for potential issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Security is non-negotiable. We need to ensure that our cache is secure from
    unauthorized access and potential attacks. This involves implementing authentication,
    encryption, and secure communication channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some key points to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Securing the cache from unauthorized access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing authentication and encryption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring secure communication channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed – in-memory storage provides rapid access to data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatility – data stored in memory is volatile and can be lost if the node fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we’ve embraced our requirements, it’s time to dive into the core of
    the project: the design decisions. Imagine that you’re a master chef who’s been
    handed a list of ingredients and asked to create a five-star dish. The ingredients
    are your requirements, but how you combine them, what cooking techniques you use,
    and the presentation – well, that’s all up to your design decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a distributed cache is no different. Each requirement we’ve outlined
    necessitates thoughtful consideration and careful selection of strategies and
    technologies. The trade-offs we make will determine how well our cache performs,
    scales, handles faults, maintains consistency, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Design and trade-offs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, brace yourselves, because we’re diving into the deep end of design
    decisions. Think of it as being handed a pristine Go environment and being asked
    to build a distributed cache. Simple, right? Sure, if by “simple” you mean navigating
    a minefield of trade-offs that could blow up your system if you take one wrong
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although the fully tested and functional version of our cache is available
    in this book’s GitHub repository, let’s reproduce all the steps to make our cache
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating the project directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `go` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `cache.go` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code defines a simple cache data structure for storing and retrieving string
    values using string keys. Think of it as a temporary storage space where you can
    put values and quickly get them back later by remembering their associated keys.
  prefs: []
  type: TYPE_NORMAL
- en: How can we know if this code works?
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, I read your mind and heard you shouting silently: **Tests!**'
  prefs: []
  type: TYPE_NORMAL
- en: From time to time, look in the test files to learn how we’re testing our project
    components.
  prefs: []
  type: TYPE_NORMAL
- en: We have a simple cache in memory, but concurrent access is not secure. Let’s
    solve this issue by choosing a way to handle thread safety.
  prefs: []
  type: TYPE_NORMAL
- en: Thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensuring concurrency safety is crucial to prevent data races and inconsistencies
    when multiple goroutines access the cache simultaneously. Here are some options
    you can consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard library’s `sync` package:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Mutex`: The simplest way to achieve concurrency safety is to use a mutex
    to lock the entire cache during read or write operations. This ensures that only
    one goroutine can access the cache at a time. However, it can lead to contention
    and reduced performance under heavy load.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.RWMutex`: A read-write mutex allows multiple readers to access the cache
    concurrently, but only one writer at a time. This can improve performance when
    reading is more frequent than writing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concurrent map implementations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Map`: Go provides a built-in concurrent map implementation that handles
    synchronization internally. It’s optimized for frequent reads and infrequent writes,
    making it a such as choice for many caching scenarios.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hashicorp/golang-lru`([https://github.com/hashicorp/golang-lru](https://github.com/hashicorp/golang-lru)),
    `patrickmn/go-cache`([https://github.com/patrickmn/go-cache](https://github.com/patrickmn/go-cache)),
    and `dgraph-io/ristretto` ([https://github.com/dgraph-io/ristretto](https://github.com/dgraph-io/ristretto))
    offer concurrent-safe cache implementations with additional features such as eviction
    policies and expiration.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lock-free data structures:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atomic operations**: For specific use cases, you might employ atomic operations
    to perform certain updates without explicit locking. However, this requires careful
    design and is generally more complex to implement correctly.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Channel-based synchronization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serializing access**: You can create a dedicated goroutine that handles all
    cache operations. Other goroutines communicate with this goroutine through channels,
    effectively serializing access to the cache.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sharded cache**: Divide the cache into multiple shards, each protected by
    its own mutex or concurrent map. This can reduce contention by distributing the
    load across multiple locks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The best approach for concurrency safety depends on your specific requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sync.RWMutex` or `sync.Map` might be a suitable choice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: If maximum performance is critical, consider lock-free data
    structures or a sharded cache'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Mutex` or a channel-based approach might be simpler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For now, let’s make things simpler. A `sync.RWMutex` will strike the right balance
    between simplicity and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Adding thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must update `cache.go` to add thread safety using `sync.RWMutex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we’re talking! Our cache is now thread-safe. What about the interface to
    the external world? Let’s explore the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: The interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing a distributed cache, one of the key decisions you’ll face is
    choosing the appropriate program interface for communication between clients and
    the cache servers. The main options available are the **Transmission Control Protocol**
    (**TCP**), **Hypertext Transfer Protocol** (**HTTP**), and other specialized protocols.
    Each has its own set of advantages and trade-offs, and understanding these will
    help us make an informed decision. For our project, we’ll settle on HTTP as the
    interface of choice, but let’s explore why.
  prefs: []
  type: TYPE_NORMAL
- en: TCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in previous chapters, TCP is a cornerstone of modern networking, but
    like any technology, it comes with its own set of trade-offs. On the one hand,
    TCP shines in its efficiency. Operating at a low level, it minimizes overhead,
    making it a lean and mean communication machine. This efficiency is often coupled
    with superior performance compared to higher-level protocols, especially in terms
    of latency and throughput, making it a preferred choice for applications where
    speed is critical. Moreover, TCP empowers developers with granular control over
    connection management, data flow regulation, and error handling, allowing for
    tailored solutions to specific networking challenges.
  prefs: []
  type: TYPE_NORMAL
- en: However, this power and efficiency come at a price. The inner workings of TCP
    are intricate, requiring a deep dive into the world of network programming. Implementing
    a TCP-based interface often means manually grappling with connection establishment,
    data packet assembly, and error mitigation strategies, demanding both expertise
    and time. Even with the technical know-how, developing a robust TCP interface
    can be a lengthy process, potentially delaying project timelines. Another hurdle
    lies in the lack of standardization for application-level protocols built upon
    TCP. While TCP itself adheres to well-defined standards, the protocols that are
    layered on top often vary widely, leading to compatibility headaches and hindering
    seamless communication between different systems.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, TCP is a powerful tool with the potential for high performance and
    customization, but it requires a significant investment in terms of development
    effort and expertise.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With its clear-cut request/response model, HTTP is relatively easy to grasp
    and implement, even for developers new to networking. This ease of use is further
    bolstered by its status as a widely embraced standard, ensuring seamless compatibility
    across diverse platforms and clients. Additionally, the vast ecosystem surrounding
    HTTP, brimming with tools, libraries, and frameworks, accelerates development
    and deployment cycles. And let’s not forget its stateless nature, which simplifies
    scaling and fault tolerance, making it easier to handle increased traffic and
    unexpected failures.
  prefs: []
  type: TYPE_NORMAL
- en: However, like any technology, HTTP isn’t without its drawbacks. Its simplicity
    comes with a trade-off in the form of overhead. The inclusion of headers and reliance
    on text-based formatting introduce additional data, potentially impacting performance
    in bandwidth-constrained environments. Furthermore, while statelessness offers
    scaling advantages, it can also lead to increased latency compared to persistent
    TCP connections. Each request necessitates establishing a fresh connection, a
    process that can add up over time unless newer protocols such as HTTP/2 or keep-alive
    mechanisms are employed.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, HTTP provides a straightforward, standardized, and widely supported
    foundation for web communication. Its simplicity and vast ecosystem make it a
    popular choice for many applications. However, developers must be mindful of the
    potential overhead and latency implications, especially in scenarios where performance
    is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Others
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: gRPC emerges as a high-performance contender in the world of network communication.
    It harnesses the power of HTTP/2 and **Protocol Buffers** (**Protobuf**) to deliver
    efficient, low-latency interactions. The use of Protobuf introduces strong typing
    and well-defined service contracts, leading to more robust and maintainable code.
    However, this power comes with a touch of complexity. Setting up gRPC requires
    support for both HTTP/2 and Protobuf, which may not be universally available,
    and the learning curve can be steeper compared to simpler protocols.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, WebSockets offer a different kind of advantage: full-duplex
    communication. Through a single, persistent connection, WebSockets enable real-time,
    bidirectional data flow between client and server. This makes them ideal for applications
    such as chat, gaming, or live dashboards where instant updates are crucial. However,
    this flexibility comes with challenges. Implementing and managing WebSocket connections
    can be more intricate than traditional request/response models. The requirement
    for long-lived connections can also complicate scaling and introduce potential
    points of failure that need to be handled carefully.'
  prefs: []
  type: TYPE_NORMAL
- en: In essence, gRPC and WebSockets each excel in different domains. gRPC shines
    in scenarios where efficiency and well-structured communication are paramount,
    while WebSockets unlock the potential for seamless real-time interactions. The
    choice between them often boils down to the specific requirements of the application
    and the trade-offs developers are willing to make.
  prefs: []
  type: TYPE_NORMAL
- en: Decision – why HTTP for our project?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the requirements and the nature of our distributed cache project, HTTP
    stands out as the most suitable choice for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity and ease of use**: HTTP’s well-defined request/response model
    makes it easy to implement and understand. This simplicity is especially beneficial
    for a project intended to make us learn the core concepts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization and compatibility**: HTTP is a widely adopted standard with
    broad compatibility across different platforms, programming languages, and clients.
    This ensures that our cache can be easily integrated with various applications
    and tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rich ecosystem**: The rich ecosystem of libraries, tools, and frameworks
    available for HTTP can significantly speed up development. We can leverage existing
    solutions for tasks such as request parsing, routing, and connection management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statelessness**: HTTP’s stateless nature simplifies scaling and fault tolerance.
    Each request is independent, making it easier to distribute load across multiple
    nodes and recover from failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development speed**: Using HTTP allows us to focus on implementing the core
    functionality of the distributed cache rather than getting bogged down with low-level
    networking details. This is crucial for getting things up and running, where the
    goal is to convey key concepts without unnecessary complexity. We can add another
    protocol once the project is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the HTTP server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create `server.go` that will include HTTP handlers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To initialize our server, we should create `main.go` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run our cache server for the very first time! In the terminal,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The server should now be running on `http://localhost:8080`.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `curl` (or a tool such as Postman) to interact with your server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This should return a `200` `OK` status.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the value, we can do something very similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This should return `{"value":"bar"}` if the key exists.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing HTTP as the interface for our distributed cache project strikes a balance
    between simplicity, standardization, and ease of integration. While TCP offers
    performance benefits, the complexity it introduces outweighs its advantages for
    our educational purposes. By using HTTP, we can leverage a widely understood and
    supported protocol, making our distributed cache accessible, scalable, and easy
    to implement. With this decision made, we can now focus on building out the core
    functionality and features of our distributed cache.
  prefs: []
  type: TYPE_NORMAL
- en: Eviction policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can’t just keep everything in memory indefinitely, right? Eventually, we’ll
    run out of space. This is where eviction policies come into play. An eviction
    policy determines which items are removed from the cache when the cache reaches
    its maximum capacity. Let’s explore some common eviction policies, discuss their
    trade-offs, and determine the best fit for our distributed cache project.
  prefs: []
  type: TYPE_NORMAL
- en: LRU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LRU evicts the least recently accessed item first. It assumes that items that
    haven’t been accessed recently are less likely to be accessed in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictable**: Simple to implement and understand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Effective**: Works well for many access patterns where recently used items
    are more likely to be reused'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory overhead**: Requires maintaining a list or other structure to track
    access order, which can add some memory overhead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity**: Slightly more complex to implement than FIFO or random eviction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TTL assigns an expiration time to each cache item. When the item’s time is up,
    it’s evicted from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: Easy to understand and implement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Freshness**: Ensures that data in the cache is fresh and relevant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictability**: Less predictable than LRU as items are evicted based on
    time rather than usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource management**: This may require additional resources to periodically
    check and remove expired items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First-in, first-out (FIFO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FIFO evicts the oldest item in the cache based on the time it was added.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: Very easy to implement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictability**: Predictable eviction pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inefficiency**: Doesn’t consider how recently an item was accessed, potentially
    evicting frequently used items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right eviction policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For our distributed cache project, we’ll need to balance performance, memory
    management, and simplicity. Given these considerations, LRU and TTL are both strong
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: LRU is ideal for scenarios where the most recently accessed data is likely to
    be accessed again soon. It helps keep frequently accessed items in memory, which
    can improve cache hit rates. TTL ensures that data is fresh and relevant by evicting
    items after a certain period. This is particularly useful when cached data can
    become stale quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our project, we’ll implement both LRU and TTL policies. This combination
    allows us to handle different use cases effectively: LRU for performance optimization
    based on access patterns, and TTL for ensuring data freshness.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s incrementally add TTL and LRU eviction policies to our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Adding TTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two main approaches to adding TTL to our cache: using a goroutine
    with `Ticker` and evicting during `Get`.'
  prefs: []
  type: TYPE_NORMAL
- en: Goroutine with Ticker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this approach, we can use a separate goroutine to run `time.Ticker`. The
    ticker periodically triggers the `evictExpiredItems` function to check for and
    remove expired entries. Let’s analyze the trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Get` method doesn’t need to perform eviction checks, potentially making it
    slightly faster in cases where many items have expired.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional goroutine**: This introduces the overhead of managing a separate
    goroutine and ticker'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unnecessary checks**: If items expire infrequently or the cache is small,
    the periodic checks might be unnecessary overhead'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Eviction during Get
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this approach, we don’t need a separate goroutine or ticker. The expiration
    checks are performed only when an item is accessed using the `Get` method. If
    the item has expired, it’s evicted before the “not found” response is returned.
    Let’s analyze the trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simpler implementation**: There’s no need to manage an extra goroutine, which
    leads to less complex code'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced overhead**: Avoids the potential overhead of a continuously running
    goroutine'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-demand eviction**: Resources are only used for eviction when necessary'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Get` might add some latency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which approach is better?
  prefs: []
  type: TYPE_NORMAL
- en: The “better” approach depends on your specific use case and priorities.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should choose the first approach in the following instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '- You need strict control over when items are evicted and want to ensure a
    clean cache regardless of access patterns'
  prefs: []
  type: TYPE_NORMAL
- en: '- You have a large cache with frequent expirations, and the overhead of the
    goroutine is acceptable'
  prefs: []
  type: TYPE_NORMAL
- en: '- Minimizing latency on `Get` operations is critical, even if it means slightly
    higher overall overhead'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, you should choose the second approach in the following instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '- You want a simpler implementation with minimal overhead'
  prefs: []
  type: TYPE_NORMAL
- en: '- You are comfortable with some delay in eviction if items are eventually removed'
  prefs: []
  type: TYPE_NORMAL
- en: '- Your cache is relatively small, and the potential latency on `Get` due to
    eviction is acceptable'
  prefs: []
  type: TYPE_NORMAL
- en: You could potentially combine both approaches. Use the second approach for most
    cases but periodically run a separate eviction process (the first approach) as
    a background task to clean up any remaining expired items.
  prefs: []
  type: TYPE_NORMAL
- en: All things considered, let’s proceed with the goroutine version so that we can
    focus on the `Get` method’s latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll modify the `CacheItem` struct so that it includes an expiry time and
    add logic to the `Set` and `Get` methods so that they can handle TTL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll add a background goroutine to periodically evict expired items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, during the cache’s initialization (`main.go`), we need to start this
    goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Adding LRU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll enhance our cache implementation by adding an LRU eviction policy. LRU
    ensures that the least recently accessed items are evicted first when the cache
    reaches its maximum capacity. We will use a doubly linked list to keep track of
    the access order of cache items.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to modify our `Cache` struct so that it includes a doubly-linked
    list (`list.List`) for eviction and a `map` struct to keep track of the list elements.
    Additionally, we’ll define a `capacity` struct to limit the number of items in
    the cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll modify the `Set` method so that it manages the doubly linked list
    and enforces the cache capacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we should pay attention to the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Check if the key exists**: If the key already exists in the cache, remove
    the old value from the doubly linked list and the map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evictLRU` to remove the least recently used item'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add a new item**: Add the new item to the front of the list and update the
    map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we need to update the `Get` method so that it can move accessed items
    to the front of the eviction list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, if the item is found but has expired, it’s removed from
    the list and the map. Also, when the item is valid, the code moves it to the front
    of the list to mark it as recently accessed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also implement the `evictLRU` method to handle the least recently
    used item being evicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The function removes the item at the back of the list (LRU) and deletes it from
    the map.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code ensures the background eviction routine removes expired
    items periodically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, the `startEvictionTicker` function starts a goroutine that
    periodically checks and removes expired items from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, update the `main` function so that it creates a cache with a specified
    capacity and test the TTL and LRU features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With that, we’ve incrementally added TTL and LRU eviction features to our cache
    implementation! This enhancement ensures that our cache effectively manages memory
    by keeping frequently accessed items and evicting stale or less-used data. This
    combination of TTL and LRU makes our cache robust, efficient, and well-suited
    for various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Eviction policies are a critical aspect of any cache system, directly impacting
    its performance and efficiency. By understanding the trade-offs and strengths
    of LRU, TTL, and other policies, we can make informed decisions that align with
    our project’s goals. Implementing both LRU and TTL in our distributed cache ensures
    we balance performance and data freshness, providing a robust and versatile caching
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve tackled the vital task of managing our cache’s memory through
    effective eviction policies such as LRU and TTL, it’s time to address another
    critical aspect: replicating our cache.'
  prefs: []
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To replicate data across multiple instances of your cache server, you have
    several options. Here are some common approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Primary replica replication**: In this setup, one instance is designated
    as the primary, and the others are replicas. The primary handles all writes and
    propagates changes to the replica.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Peer-to-peer (P2P) replication**: In P2P replication, all nodes can both
    send and receive updates. This approach is more complex but avoids a single point
    of failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publish-subscribe (Pub/Sub) model**: This approach uses a message broker
    to broadcast updates to all cache instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: Protocols such as Raft and Paxos ensure
    strong consistency across replicas. This approach is more complex and often implemented
    using specialized libraries (for example, etcd and Consul).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choosing the right replication strategy depends on various factors, such as
    scalability, fault tolerance, ease of implementation, and the specific requirements
    of the application. Here’s why we’ll be going for P2P replication over the other
    three approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P2P**: In a P2P architecture, each node can communicate with any other node,
    distributing the load evenly across the network. This allows the system to scale
    horizontally more efficiently as there is no single point of contention.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary replica**: Scalability is limited because the master node can become
    a bottleneck. All write operations are handled by the master, which can lead to
    performance issues as the number of clients increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pub/Sub**: While scalable, the message broker can become a bottleneck or
    single point of failure if not managed properly. Scalability depends on the broker’s
    performance and architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: These can be scalable, but achieving consensus
    among many nodes can introduce latency and complexity. They are often more suitable
    for smaller clusters or where strong consistency is crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fault tolerance**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P2P**: In a P2P network, there is no single point of failure. If one node
    fails, the remaining nodes can continue to operate and communicate with each other,
    making the system more robust and resilient.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary replica**: The primary node is a single point of failure. If the
    primary goes down, the entire system’s write capability is affected until a new
    primary is elected or the old one is restored.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pub/Sub**: The message broker can be a single point of failure. While you
    can have multiple brokers and failover mechanisms, this adds complexity and more
    moving parts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: These are designed to handle node failures,
    but they come with increased complexity. Achieving consensus in the presence of
    failures can be challenging and may affect performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P2P**: While eventual consistency is more common in P2P systems, you can
    implement mechanisms to ensure stronger consistency if needed. This approach provides
    flexibility in balancing consistency and availability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary replica**: It typically provides strong consistency since all writes
    go through the master. However, reading consistency might be delayed on replicas.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pub/Sub**: It provides eventual consistency as updates are propagated to
    subscribers asynchronously.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: These provide strong consistency but at
    the cost of higher latency and complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of implementation** **and management**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P2P**: While more complex than primary replica replication, P2P systems can
    be easier to manage at scale because they don’t require a central coordination
    point. Each node is equal, simplifying the architecture.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary replica**: This is easier to implement initially but can become complex
    to manage at scale, especially with failover and load balancing mechanisms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pub/Sub**: This is relatively easy to implement using existing message brokers,
    but managing the broker infrastructure and ensuring high availability can add
    complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: These are generally complex to implement
    and manage as they require a deep understanding of consensus algorithms and their
    operational overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P2P**: This offers high flexibility in terms of topology and can adapt to
    changes in the network easily. Nodes can join or leave the network without significant
    disruption.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master-slave**: This is less flexible due to the centralized nature of the
    master node. Adding or removing nodes requires reconfiguration and can affect
    the system’s availability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pub/Sub**: This is flexible in terms of adding new subscribers, but the broker
    infrastructure can become complex to manage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed consensus protocols**: These are flexible in terms of fault tolerance
    and consistency but require careful planning and management to handle node changes
    and network partitions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: P2P replication is a compelling choice for our cache project. It avoids the
    single point of failure associated with the primary replica and Pub/Sub models
    and is generally more straightforward to scale and manage than distributed consensus
    protocols. While it may not provide the strong consistency guarantees of consensus
    protocols, it offers a balanced approach that can be tailored to meet various
    consistency requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t get me wrong! P2P isn’t perfect, but it is a reasonable approach to get
    things going. It also has *hard* problems to solve, such as eventual consistency,
    conflict resolution, replication overhead, bandwidth consumption, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing P2P replication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to modify the cache server so that it’s aware of the peers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to create a function to replicate the data to the peers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The core idea here is to iterate over all the peers (`cs.peers`) in the cache
    server’s configuration and for each peer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each peer, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: A new goroutine (`go func(...)`) is launched. This allows replication to happen
    concurrently for each peer, improving performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HTTP POST request is constructed to send the JSON data to the peer’s `/``set`
    endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom header called `replicationHeader` is added to the request. This likely
    helps the receiving peer distinguish replication requests from regular client
    requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP request is sent using `client.Do(req)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are any errors during request creation or sending, they’re logged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now use the replication during our `SetHandler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This new conditional block serves as a check to determine whether an incoming
    request to the cache server (`r`) is a regular client request or a replication
    request from another cache server. Based on this determination, it decides whether
    to trigger further replication to other peers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To glue everything together, let’s change the main function so that it receives
    the peers and bootstraps the code with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With that, the implementation is complete! Let’s run two instances of our cache
    and see if our data is being replicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the first instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s run the second instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can now use `curl` or any HTTP client to test the `Set` and `Get` operations
    across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set a key-value pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the key-value pair from a different instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You should see a value of `bar` if the replication is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Check the logs of each instance to see the replication process in action. You
    should see log entries being applied across all instances! If you’re feeling adventurous,
    run multiple instances of the cache and see the dance of replication in front
    of your very eyes.
  prefs: []
  type: TYPE_NORMAL
- en: We can add features and optimize our cache infinitely, but infinite seems too
    much for our project. The last piece of our puzzle will be sharding our data.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sharding is a fundamental technique that’s used to partition data across multiple
    nodes, ensuring scalability and performance. Sharding offers several key benefits
    that make it an attractive option for distributed caches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: Sharding allows you to scale horizontally by adding
    more nodes (shards) to your system. This enables the cache to handle larger datasets
    and higher request volumes without degrading performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load distribution**: By distributing data across multiple shards, sharding
    helps balance the load, preventing any single node from becoming a bottleneck.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel processing**: Multiple shards can process requests in parallel,
    leading to faster query and update operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation of failures**: If one shard fails, the others can continue to operate,
    ensuring that the system remains available even in the presence of failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified management**: Each shard can be managed independently, allowing
    for easier maintenance and upgrades without affecting the entire system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to implementing sharding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several approaches to implementing sharding, each with its advantages
    and trade-offs. The most common approaches include range-based sharding, hash-based
    sharding, and consistent hashing.
  prefs: []
  type: TYPE_NORMAL
- en: Range-based sharding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In range-based sharding, data is divided into contiguous ranges based on the
    shard key (for example, numerical or alphabetical ranges). Each shard is responsible
    for a specific range of keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple to implement and understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient range queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Uneven distribution of data if the key distribution is skewed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hotspots can form if certain ranges are accessed more frequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash-based sharding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In hash-based sharding, a hash function is applied to the shard key to determine
    the shard. This approach ensures a more uniform distribution of data across shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Even distribution of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoids hotspots caused by skewed key distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Range queries are inefficient as they may span multiple shards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-sharding (adding/removing nodes) can be complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent hashing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consistent hashing is a specialized form of hash-based sharding that minimizes
    the impact of re-sharding. Nodes and keys are hashed to a circular space, and
    each node is responsible for the keys in its range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizes data movement during re-sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides good load balancing and fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: More complex to implement compared to simple hash-based sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires careful tuning and management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go with consistent hashing. This approach will help us achieve a balanced
    distribution of data and handle re-sharding efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing consistent hashing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first thing we need to do is create our hash ring. But wait! What is a hash
    ring? Keep calm and bear with me!
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a circular ring on which each point represents a possible output of
    a hash function. This is our “hash ring.”
  prefs: []
  type: TYPE_NORMAL
- en: Each cache server (or node) in our system is assigned a random position on the
    ring that’s usually determined by hashing the server’s unique identifier (such
    as its address). These positions represent the node’s “ownership range” on the
    ring. Every piece of data (a cache entry) is hashed. The resulting hash value
    is also mapped to a point on the ring.
  prefs: []
  type: TYPE_NORMAL
- en: A data key is assigned to the first node it encounters while moving clockwise
    on the ring from its position.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the hash ring
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following example, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Key 1 is assigned to Node A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key 2 is assigned to Node B
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key 3 is assigned to Node C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following file, `hashring.go`, is the foundation for managing the consistent
    hash ring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon exploring the file in the repository, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nodes`: A slice to store node structs (the ID and address of each server).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hashes`: A slice of uint32 values to store the hashes of each node. This allows
    for efficient searching to find the responsible node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lock`: A mutex to ensure safe, concurrent access to the ring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hash()`: This function uses SHA-1 to hash node IDs and data keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AddNode`: This calculates a node’s hash, inserts it into the hashes slice,
    and sorts the slice to maintain order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GetNode`: Given a key, it performs a binary search on the sorted hashes to
    find the first hash that’s equal to or greater than the key’s hash. The corresponding
    node in the `nodes` slice is the owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need to update `server.go` so that it can interact with the hash ring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to modify `SetHandler` so that it handles replication and request
    forwarding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to add the `replicateSet` method to replicate the `set` request
    to other peers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve done this, we can change `GetHandler` so that it forwards requests
    to the appropriate node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Both methods are using `forwardRequest`. Let’s create it as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to update `main.go` so that it considers the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let’s test our consistent hashing!
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the first instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the second instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The first set of tests will be the basic `SET` and `GET` commands. Let’s set
    a key-value pair on Node A (localhost:8080):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can get the value from the correct node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Depending on how `mykey` hashes, the value should be returned from either port
    `8080` or `8083`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the hashing and key distribution, we can set multiple keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can get the values and observe the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some keys might be on one server, while others might be on the second server,
    depending on how their hashes map onto the ring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaways from this implementation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The hash ring provides a way to consistently map keys to nodes, even as the
    system scales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistent hashing minimizes disruptions caused by adding or removing nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementation in the patch focuses on simplicity, using SHA-1 for hashing
    and a sorted slice for efficient node lookup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations! You’ve embarked on an exhilarating journey through the world
    of distributed caching, constructing a system that’s not just functional but primed
    for new optimization. Now, it’s time to unleash the full potential of your creation
    by digging deeper into the realms of optimization, metrics, and profiling. Think
    of this as fine-tuning your high-performance engine, ensuring it purrs with efficiency
    and speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where you can go from here? Let’s sum this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization techniques**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache replacement algorithms**: Experiment with alternative cache replacement
    algorithms such as **Low Inter-Reference Recency Set** (**LIRS**) or **Adaptive
    Replacement Cache** (**ARC**). These algorithms can offer improved hit rates and
    better adaptability to varying workloads compared to traditional LRU.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tuning eviction policies**: Fine-tune your TTL values and LRU thresholds
    based on your specific data characteristics and access patterns. This prevents
    the premature eviction of valuable data and ensures that the cache remains responsive
    to changing demands.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression**: Implement data compression techniques to reduce the memory
    footprint of cached items. This allows you to store more data in the cache and
    potentially improve hit rates, especially for compressible data types.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connection pooling**: Optimize network communication by implementing connection
    pooling between your cache clients and servers. This reduces the overhead for
    establishing new connections for each request, leading to faster response times.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics** **and monitoring**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key metrics**: Continuously monitor essential metrics such as cache hit rate,
    miss rate, eviction rate, latency, throughput, and memory usage. These metrics
    provide valuable insights into the cache’s performance and help identify potential
    bottlenecks or areas for improvement.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: Utilize visualization tools such as Grafana to create dashboards
    that display these metrics in real time. This allows you to easily track trends,
    spot anomalies, and make data-driven decisions about cache optimization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting**: Set up alerts based on predefined thresholds for critical metrics.
    For example, you could receive an alert if the cache hit rate drops below a certain
    percentage or if latency exceeds a specified limit. This enables you to proactively
    address issues before they impact users.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profiling**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU profiling**: Identify CPU-intensive functions or operations within your
    cache code. This helps you pinpoint areas where optimizations can yield the most
    significant performance gains.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory profiling**: Analyze memory usage patterns to detect memory leaks
    or inefficient memory allocation. Optimizing memory usage can improve the cache’s
    overall performance and stability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With dedication and a data-driven approach, you’ll unlock the full potential
    of your distributed cache and ensure it remains an asset in your future software
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Oof! What a ride, huh? We explored a lot of design decisions and implementation
    during this chapter. Let’s wrap up what we have done.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a distributed cache from scratch. We started with
    a simple in-memory cache and gradually added features such as thread safety, HTTP
    interface, eviction policies (LRU and TTL), replication, and consistent hashing
    for sharding. Each step was a building block that contributed to the robustness,
    scalability, and performance of our cache.
  prefs: []
  type: TYPE_NORMAL
- en: While our cache is functional, it’s just the beginning. There are countless
    avenues for further exploration and optimization. The world of distributed caching
    is vast and ever-evolving, and this chapter has equipped you with the essential
    knowledge and practical skills to navigate it confidently. Remember, building
    a distributed cache is not just about the code; it’s about understanding the underlying
    principles, making informed design decisions, and continuously iterating to meet
    the evolving demands of your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve navigated the treacherous waters of design decisions and trade-offs,
    we’ve laid a solid foundation for our distributed cache. We’ve combined the right
    strategies, technologies, and a dash of cynicism to create a robust, scalable,
    and efficient system. But designing a system is only half the battle; the other
    half is writing code that doesn’t make future developers (including ourselves)
    weep tears of frustration.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Effective Code Practices*, we’ll cover essential techniques
    to elevate your Go coding game. You’ll learn how to maximize performance by efficiently
    reusing system resources, eliminate redundant task execution for streamlined processes,
    master memory management to keep your system lean and fast, and sidestep common
    issues that can degrade performance. Prepare for a deep dive into Go’s best practices,
    where precision, clarity, and a touch of sarcasm are the keys to success.
  prefs: []
  type: TYPE_NORMAL
