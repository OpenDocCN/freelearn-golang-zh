- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective Coding Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer resources are plentiful these days, but they’re far from infinite.
    Knowing how to carefully manage and use them is vital to create resilient programs.
    This chapter is crafted to explore how to use resources appropriately and avoid
    memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reusing resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing tasks once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient memory mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding common performance pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained practical experience handling
    resources using the standard library, and you will know how to avoid making common
    mistakes with it.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the code shown in this chapter can be found in the `ch14` directory of our
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reusing resources is crucial in software development because it significantly
    enhances the efficiency and performance of applications. By reusing resources,
    we can minimize the overhead associated with resource allocation and deallocation,
    reduce memory fragmentation, and decrease the latency of resource-intensive operations.
    This approach leads to more predictable and stable application behavior, particularly
    under high load. In Go, the `sync.Pool` package exemplifies this principle by
    providing a pool of reusable objects that can be dynamically allocated and freed.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, strap in kiddos – it’s time to take a wild ride through the exhilarating
    world of Go’s `sync.Pool`. You see all those folks bragging about it like it’s
    the cure for buggy code? Well, they’re not entirely wrong; it’s just not the magic
    bullet they think it is.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine `sync.Pool` as your friendly neighborhood hoarder. You know, the one
    with a garage so full of stuff that you can barely squeeze in a bicycle. Except,
    in this case, instead of old newspapers and broken furniture, we’re talking about
    goroutines and memory allocations. Yep, `sync.Pool` is like the cluttered attic
    of your program, except it’s actually organized chaos designed to optimize your
    resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: You see, `sync.Pool` comes with its own set of rules and quirks. For starters,
    *objects in the pool aren’t guaranteed to stick around forever*. They can be evicted
    at any time, leaving you high and dry when you least expect it. And then there’s
    the issue of concurrency. `sync.Pool` might be thread-safe, but that doesn’t mean
    you can just toss it into your code willy-nilly and expect everything to work.
  prefs: []
  type: TYPE_NORMAL
- en: So, what the heck is this thing good for? Well, let’s get technical. `sync.Pool`
    is a way to store and reuse objects in a way that’s safe for multiple goroutines
    to mess with simultaneously. It’s useful when you’ve got pieces of data that are
    used a lot, but temporarily, and making a new one each time is slow. Think of
    it like a temporary workspace for your goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code effectively demonstrates the use of `sync.Pool` to manage
    and reuse instances of `bytes.Buffer`, which is an efficient way to handle buffers,
    especially under high load or in highly concurrent scenarios. Here’s a breakdown
    of the code and the relevance of using `sync.Pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`BufferPool` wraps `sync.Pool`, which is used to store and manage `*``bytes.Buffer`
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This function initializes `BufferPool` with `sync.Pool`, which creates new
    `bytes.Buffer` instances when needed. The `New` function is called when `Get`
    is invoked on an empty pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Get()` retrieves `*bytes.Buffer` from the pool. If the pool is empty, it uses
    the `New` function defined in `NewBufferPool` to create a new `bytes.Buffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Put` returns `*bytes.Buffer` to the pool after resetting it, making it ready
    for reuse. Resetting the buffer is crucial to avoid data corruption between different
    uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function processes data using a buffer from `BufferPool`. It acquires a
    buffer from the pool, writes data to it, and ensures the buffer is returned to
    the pool after use with `defer bp.Put(buf)`. An example operation, `fmt.Println(buf.String())`,
    is performed to demonstrate how the buffer might be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use the code in our `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new `BufferPool`, defines some data, and processes it using `ProcessData`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few points to notice:'
  prefs: []
  type: TYPE_NORMAL
- en: By reusing `bytes.Buffer` instances, `BufferPool` reduces the need for frequent
    allocations and garbage collections, leading to better performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Pool` is suitable for managing temporary objects that are only needed
    within the scope of a single goroutine. It helps reduce contention on shared resources
    by allowing each goroutine to maintain its own set of pooled objects, minimizing
    the need for synchronization between goroutines when accessing these objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Pool` is safe for concurrent use by multiple goroutines, making `BufferPool`
    robust in concurrent environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Pool` is essentially a cache for objects. When you need a new object,
    you can request it from the pool. If the pool has an available object, it will
    return it; otherwise, it will create a new one. Once you are done with the object,
    you return it to the pool, making it available for reuse. This cycle helps manage
    memory more efficiently and reduces the computational cost of allocation.'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that we fully understand the capabilities of `sync.Pool`, let’s explore
    two more examples in different scenarios – network connections and JSON marshaling.
  prefs: []
  type: TYPE_NORMAL
- en: Using sync.Pool in a network server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this scenario, we want to use `sync.Pool` to manage buffers for handling
    network connections, since it is a typical pattern in high-performance servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, buffers are reused for each connection, significantly reducing
    the amount of garbage generated and improving the server’s performance by minimizing
    garbage collection overhead. This pattern is beneficial in scenarios with high
    concurrency and numerous short-lived connections.
  prefs: []
  type: TYPE_NORMAL
- en: Using sync.Pool for JSON marshaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this scenario, we will explore how `sync.Pool` can be used to optimize buffer
    usage during JSON marshaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use `sync.Pool` to manage the buffers into which JSON data
    is marshaled. The buffer is retrieved from the pool each time `marshalData` is
    called, and once the data is copied to a new slice to be returned, the buffer
    is put back into the pool for reuse. This approach prevents the allocation of
    a new buffer on each marshaling call.
  prefs: []
  type: TYPE_NORMAL
- en: The buffer in the marshaling process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The buffer variable in this example is `bytes.Buffer`, which acts as a reusable
    buffer for the marshaled JSON data. Here’s the process step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sync.Pool` with `bufferPool.Get()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`buffer.Reset()` before use to ensure its content is empty and ready for new
    data, ensuring data integrity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`json.NewEncoder(buffer).Encode(data)` function marshals the data directly
    into the buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Copying data**: Creating a new byte slice result and copying the marshaled
    data from the buffer is essential. This step is necessary because the buffer will
    be returned to the pool and reused, so its content must not be directly returned,
    avoiding potential data corruption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defer bufferPool.Put(buffer)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return result**: The resulting slice containing the marshaled JSON data is
    returned from the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a few considerations when using `sync.Pool`. if you want to maximize
    the benefits of `sync.Pool`, make sure that you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use it for objects that are expensive to create or set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid using it for long-lived objects, as it is optimized for objects that have
    short lifespans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be mindful that the garbage collector may automatically remove objects in the
    pool when there is high memory pressure, so always check for `nil` after fetching
    an object from the pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitfalls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While `sync.Pool` can offer substantial performance benefits, it also introduces
    complexity and potential pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data integrity**: Extra care must be taken to ensure that data does not leak
    between uses of pooled items. This often means clearing buffers or other data
    structures before reuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Pool` can lead to increased memory usage, especially if the objects held
    in the pool are large or the pool grows too large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sync.Pool` minimizes the overhead of memory allocation, it introduces synchronization
    overhead that can become a bottleneck in highly concurrent scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance is not a guessing game
  prefs: []
  type: TYPE_NORMAL
- en: When considering the use of `sync.Pool` for marshaling operations, it’s essential
    to benchmark and profile your specific application to ensure that the benefits
    outweigh the costs.
  prefs: []
  type: TYPE_NORMAL
- en: In system programming, where performance and efficiency are crucial, `sync.Pool`
    can be particularly useful. For instance, in network servers or other I/O-heavy
    applications, managing many small, short-lived objects is common. Using `sync.Pool`
    in such scenarios can minimize latency and memory usage, leading to more responsive
    and scalable systems.
  prefs: []
  type: TYPE_NORMAL
- en: There are more useful capabilities in the `sync` package. For instance, we can
    leverage this package to ensure that code segments will be called exactly once,
    with `sync.Once`. Sounds promising, right? Let’s explore this concept in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Executing tasks once
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sync.Once` – the deceptively simple tool in the `sync` package that promises
    a safe haven of “run this code only once” logic. Can this tool save the day *once*
    again (pun intended)?'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a group of hyperactive squirrels all scrambling toward the same acorn.
    That first lucky squirrel gets the prize; the rest are left staring at an empty
    spot, wondering what the heck just happened. That’s `sync.Once` for us. It’s great
    when you genuinely need that single-use, guaranteed execution – the initialization
    of a global variable, for example. But for anything more intricate, prepare for
    a headache.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a Gen-X/Millennial Java enterprise person, you might suspect that
    `sync.Once` is just a lazy initialization, singleton pattern implementation. And
    yes! It is precisely that! But if you’re a Gen-Z, let me explain in simpler, non-ancient
    words – `sync.Once` stores a boolean and a mutex (think of it like a locked door).
    The first time a goroutine calls `Do()`, that boolean flips from `false` to `true`,
    and the code inside `Do()` gets executed. All other goroutines knocking on the
    mutex door hang around waiting for their turn, which will never come.
  prefs: []
  type: TYPE_NORMAL
- en: In Go terms, it takes a function, `f`, as its argument. The first time `Do`
    is called, it executes `f`. All subsequent calls to `Do` (even from different
    goroutines) will have no effect – they will simply wait until the initial execution
    of `f` completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Too abstract? Here’s a tiny example to illustrate the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This snippet has a simple `setup` function we want to execute only once. We
    use `sync.Once`’s `Do` method to ensure that the setup function is called exactly
    once, regardless of how many times `Do` is invoked. It’s like having a bouncer
    at your function’s door, ensuring that only the first caller gets in.
  prefs: []
  type: TYPE_NORMAL
- en: I don’t know about you, but, to me, all these steps seem a bit verbose to do
    a simple thing. Coincidentally or not, the Go team feels the same, and since version
    1.21, we have had some shortcuts to do the same with three distinct functions
    – `OnceFunc`, `OnceValue`, and `OnceValues`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down their function signatures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnceFunc(f func()) func()`: This function takes a function, `f`, and returns
    a new function. The returned function, when called, will invoke `f` only once
    and return its result. This is handy when you want the result of a function that
    should only be computed once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnceValue[T any](f func() T) func() T`: This is similar to `OnceFunc`, but
    it’s specialized for functions that return a single value of type `T`. The returned
    function will return the value produced by the first (and only) call to `f`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnceValues[T1, T2 any](f func() (T1, T2)) func() (T1, T2)`: This extends the
    concept further for functions that return multiple values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These new functions eliminate some boilerplate code that you’d otherwise need
    when using `Once.Do`. They offer a concise way to capture the “initialize once
    and return value” pattern often seen in Go programs. Also, they are designed to
    capture the results of the executed function. This eliminates the need for manual
    result storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put things in perspective, let’s look at the following snippet that does
    the same task using both options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Ultimately, remember that `sync.Once` is like that overly specific kitchen tool
    you buy, thinking it’ll revolutionize your cooking, but it ends up gathering dust
    in a drawer. It has its place, but most of the time, simpler synchronization tools
    or a bit of careful refactoring will be a much less frustrating option.
  prefs: []
  type: TYPE_NORMAL
- en: We choose `sync.Once` as a synchronization tool, not a result-sharing mechanism.
    There are multiple scenarios when we want to share the result of a function with
    multiple callers but control the execution of the function itself. Even better,
    we want to be able to deduplicate concurrent function calls. In these scenarios,
    we can leverage our next tool for the job – `singleflight`!
  prefs: []
  type: TYPE_NORMAL
- en: singleflight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `singleflight` Go package is designed to prevent duplicate executions of
    a function while it is in flight. It is instrumental in system programming, where
    managing redundant operations efficiently can significantly enhance performance
    and reduce unnecessary load.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple goroutines request the same resource simultaneously, `singleflight`
    ensures that only one request proceeds to fetch or compute the resource. All other
    requests wait for the result of the initial request, receiving the same response
    once it completes. This mechanism helps avoid repetitive work, such as multiple
    database queries for the same data or redundant API calls.
  prefs: []
  type: TYPE_NORMAL
- en: This concept is essential for programmers looking to optimize their systems,
    especially in high-concurrency environments. It simplifies handling multiple requests
    by ensuring that expensive operations are not executed more than necessary. `singleflight`
    is straightforward to implement and can integrate seamlessly into existing Go
    applications, making it an attractive tool for system programmers aiming to boost
    efficiency and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how it can be used to ensure that a function
    is only executed once even *if called multiple* *times concurrently*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `fetchData` function is invoked by multiple goroutines,
    but `singleflight.Group` ensures that it is only executed once. The other goroutines
    wait and receive the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Package x/sync
  prefs: []
  type: TYPE_NORMAL
- en: '`singleflight` is part of the `golang.org/x/sync` package. In other words,
    it is not part of the standard library, yet is maintained by the Go team. Ensure
    you “go get” it before using it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore another example, but this time, we will see how to use `singleflight.Group`
    for different keys, each potentially representing different data or resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this example, different keys are handled, but the function call is *deduplicated
    per key*. For instance, multiple requests for `"alpha"` will result in only one
    execution, and all callers will receive the same `"``Alpha result"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `singleflight` package is a powerful tool for managing concurrent function
    calls in Go. Here are some of the most common scenarios where it shines:'
  prefs: []
  type: TYPE_NORMAL
- en: '`singleflight` can ensure that only one request is made to the backend or database,
    while the others wait and receive the shared result. This prevents unnecessary
    loads and improves response times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`singleflight` allows you to cache the results of the first execution. Subsequent
    calls with the same parameters will reuse the cached result, avoiding redundant
    work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`singleflight` to limit the rate at which a function is executed. For example,
    if you have a function that interacts with a rate-limited API, `singleflight`
    can prevent multiple calls from happening simultaneously, ensuring compliance
    with the API’s restrictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`singleflight` can ensure that only one instance of the task is running at
    a time, preventing resource contention and potential inconsistencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common benefit of introducing `singleflight` in these scenarios is
    preventing redundant work, especially in scenarios with high concurrency. It also
    avoids unnecessary computations or network requests.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond concurrency management, another critical aspect of system programming
    is memory management. Efficiently accessing and manipulating large datasets can
    significantly boost performance, and this is where memory mapping comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Effective memory mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`mmap` (or `mmap` comes with a side of head-scratching complexity and a few
    potential landmines. Let’s dive in, shall we?'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine `mmap` as breaking down the walls of your local library. Instead of
    laboriously checking out books (or reading from files the boring way), you gain
    direct access to the whole darn collection. You can flip through those dusty volumes
    at lightning speed, finding exactly what you need without waiting for the nice
    librarian (your operating system’s filesystem). Sounds amazing, right?
  prefs: []
  type: TYPE_NORMAL
- en: It is a system call that creates a mapping between a file on disk and a block
    of memory in your program’s address space. Suddenly, those file bytes become just
    another chunk of memory for you to play with. This is awesome for huge files,
    where traditional read/write operations would chug along like a rusty steam engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can achieve this in Go, using the cross-platform `golang.org/x/exp/mmap`
    package instead of direct syscalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the `mmap` package to manage the memory-mapped file.
    The reader object is retrieved using `mmap.Open()`, and the file is read into
    a `data` byte slice.
  prefs: []
  type: TYPE_NORMAL
- en: API usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `mmap` package provides a higher-level API for memory-mapping files, abstracting
    away the complexities of direct syscall usage. Here’s the process step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mmap.Open(filename)`, which returns a `ReaderAt` interface to read the file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`reader.ReadAt(data, 0)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Access Data**: The last byte of the file is accessed and printed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main benefits of using the `mmap` package over direct syscalls are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mmap` package abstracts away platform-specific details, allowing your code
    to run on multiple operating systems without modification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mmap` package provides a more Go-like interface, making the code easier to
    read and maintain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling**: The package handles many of the error-prone details of
    memory mapping, reducing the likelihood of bugs and increasing the robustness
    of your code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But wait a minute! Do we need to leverage the OS to synchronize the data back
    just when it wants? This seems off! There are moments when we want to ensure that
    the app writes the data. For those situations, the `msync` syscall exists.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Advanced usage with protection and mapping flags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can customize the behavior further by specifying protection and mapping
    flags. The `mmap` package doesn’t expose these directly, but understanding them
    is crucial for advanced usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`syscall.PROT_READ`: Pages may be read'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`syscall.PROT_WRITE`: Pages may be written'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`syscall.PROT_EXEC`: Pages may be executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A combination: `syscall.PROT_READ` | `syscall.PROT_WRITE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`syscall.MAP_SHARED`: Changes are shared with other processes that map the
    same file*   `syscall.MAP_PRIVATE`: Changes are private to the process and not
    written back to the file*   A combination: `syscall.MAP_SHARED` | `syscall.MAP_POPULATE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The lesson here? `mmap` is like a high-performance sports car – exhilarating
    when handled correctly, but disastrous in the hands of the inexperienced. Use
    it wisely, for scenarios such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Working with gigantic files**: Quickly search, analyze, or modify massive
    datasets that would choke traditional I/O'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared memory communication**: Create blazing-fast communication channels
    between processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, with `mmap`, you’re taking the safeties off. You need to handle synchronization,
    error checking, and potential memory corruption yourself. But when you do master
    it, the performance gains can be so satisfying that the complexity feels almost
    worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: MS_ASYNC
  prefs: []
  type: TYPE_NORMAL
- en: We can still make `Msync` async by passing the flag `MS_ASYNC`. The main difference
    is that we enqueue our request for modification, and the OS can eventually handle
    it. At this point, we can use `Munmap` or even crash. The OS will eventually handle
    writing the data unless it also crashes.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding common performance pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are performance pitfalls in Golang – you’d think that with all its built-in
    concurrency magic, we could just sprinkle some goroutines here and there and watch
    our programs fly. Unfortunately, the reality isn’t that generous, and treating
    Go like a performance panacea is like expecting a spoonful of sugar to fix a flat
    tire. It’s sweet, but oh boy – it’s not going to help when your code base starts
    to resemble a rush-hour traffic jam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive into an example that illustrates a common misstep – excessive creation
    of goroutines for tasks that aren’t CPU-bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this example, spawning a thousand goroutines to make HTTP requests is like
    sending a thousand people to fetch a single cup of coffee – inefficient and chaotic.
    Instead, using a worker pool or controlling the number of concurrent goroutines
    can significantly improve both performance and resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Even using thousands of goroutines is inefficient; the real problem is when
    we leak memory, which can literally kill our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Leaking with time.After
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `time.After` function in Go is a convenient way to create a timeout, returning
    a channel that delivers the current time after a specified duration. However,
    its simplicity can be deceptive because it can lead to memory leaks if not used
    carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s why `time.After` can lead to memory issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '`time.After` generates a new channel and starts a timer. This channel receives
    a value when the timer expires.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Garbage collection**: The channel and the timer are not eligible for garbage
    collection until the timer fires, regardless of whether you still need the timer
    or not. This means that if the duration specified is long, or if the channel is
    not read from (because the operation using the timeout finishes earlier), the
    timer and its channel continue to occupy memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time.After` before it fires. Unlike creating a timer with `time.NewTimer`,
    which provides a `Stop` method to halt the timer and release resources, `time.After`
    does not expose such a mechanism. Therefore, if the timer is no longer needed,
    it still consumes resources until it completes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example to illustrate the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this example, even though the processing might finish before the timeout
    occurs, the timer associated with `time.After` will still occupy memory until
    it sends a message to its channel, which is never read because the select block
    has already been completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For scenarios where memory efficiency is crucial and the timeouts are either
    long or not always necessary (i.e., the operation might finish before the timeout),
    it is better to use `time.NewTimer`. This way, you can stop the timer manually
    when it is no longer needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: By using `time.NewTimer` and stopping it with `timer.Stop()`, you ensure that
    resources are immediately freed once they are no longer needed, thus preventing
    a memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: Defer in for loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Go, `defer` is used to schedule a function call to be run after the function
    completes. It’s typically used to handle clean-up actions, such as closing file
    handles or database connections. However, when `defer` is used inside a loop,
    the deferred calls do not execute immediately at the end of each iteration as
    might intuitively be expected. Instead, they accumulate and execute only when
    the entire function containing the loop exits.
  prefs: []
  type: TYPE_NORMAL
- en: This behavior means that if you defer a cleanup operation inside a loop, every
    deferred call stacks up in memory until the loop exits. This can lead to high
    memory usage, especially if the loop iterates many times, which might not only
    affect performance but also lead to program crashes, due to out-of-memory errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simplified example to illustrate this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this example, if `filenames` contain hundreds or thousands of names, each
    file gets opened one by one per loop iteration, and `defer f.Close()` schedules
    the file to be closed only when the `openFiles` function exits. If the number
    of files is large, this can accumulate a substantial amount of memory reserved
    for all these open files.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid this pitfall, manage the resource within the loop itself without using
    `defer` if the resource does not need to persist beyond the scope of the loop
    iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this revised approach, each file is closed right after its related operations
    are completed within the same loop iteration. This prevents unnecessary memory
    buildup and ensures that resources are freed up as soon as they are no longer
    needed, which is much more memory efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Maps management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maps in Go are highly flexible and dynamically grow as more key-value pairs
    are added. However, one crucial aspect of maps that developers sometimes overlook
    is that maps do not automatically shrink or release memory when items are removed.
    If the keys are continuously added without management, the map will continue to
    increase in size, potentially consuming a large amount of memory – even if many
    of those keys are no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: The Go runtime optimizes map operations for speed rather than memory usage.
    When items are deleted from a map, the runtime does not immediately reclaim the
    memory associated with those entries. Instead, the memory remains part of the
    map’s underlying structure to allow for faster re-insertion of new items. The
    idea is that if space was needed once, it might be needed again, which can improve
    performance in scenarios with frequent additions and deletions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a scenario where a map is used to cache results of operations or store
    session information in a web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, even after a session is deleted using `delete(sessions,
    userID)`, the map does not release the memory where the session data was stored.
    Over time, with enough user turnover, the map can grow to consume a significant
    amount of memory, leading to a memory leak if the map continues to expand without
    bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know that the map should shrink after many deletions, consider creating
    a new map and copying over only the active items. This can release memory held
    by many deleted entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For specific use cases, such as when keys have a short lifespan or the map size
    fluctuates significantly, consider using specialized data structures or third-party
    libraries designed for more efficient memory management. Also, it’s beneficial
    to schedule regular clean-up operations where you assess the utility of data within
    the map and remove unnecessary entries. This is particularly important in caching
    scenarios where stale data can linger indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Resource management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the garbage collector effectively manages memory, it does not handle other
    types of resources, such as open files, network connections, or database connections.
    These resources must be explicitly closed to free up the system resources they
    consume. If not properly managed, these resources can remain open indefinitely,
    leading to resource leaks that can eventually exhaust the system’s available resources,
    potentially causing an application to slow down or crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common scenario where resource leaks occur is when handling files or network
    connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding function, the file is opened but never closed. This is a resource
    leak. The correct approach should include a `defer` statement to ensure that the
    file is closed after all operations on it are complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s crucial to handle resources correctly, not just when operations succeed
    but also when they fail. Consider the case of initializing a network connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, if an error occurs after the connection is established but
    before it is returned (or during any subsequent operations before the connection
    is explicitly closed), the connection might remain open. This can be mitigated
    by ensuring that connections are closed in the face of errors, possibly using
    a pattern like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Handling HTTP bodies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every `http.Response` from an HTTP client operation contains a `Body` field,
    which is `io.ReadCloser`. This `Body` field holds the response body. According
    to Go’s HTTP client documentation, the user is responsible for closing the response
    body when finished with it. Failing to close the response body can keep underlying
    sockets open longer than necessary, leading to resource leaks that can exhaust
    system resources, degrade performance, and eventually cause application instability.
  prefs: []
  type: TYPE_NORMAL
- en: 'When an `http.Response` body is not closed, the following scenarios can occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network and socket resources**: The underlying network connections can remain
    open. These are limited system resources. When they are used up, new network requests
    cannot be made, which can block or break parts of an application or even other
    applications running on the same system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory usage**: Each open connection consumes memory. If many connections
    are left open (especially in high-throughput applications), this can lead to substantial
    memory use and potential exhaustion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A typical scenario where developers might forget to close the response body
    is when handling HTTP requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the response body is never closed. Even though the function
    does not explicitly need the body, it is still fetched and must be closed to free
    up resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct way to handle this is to ensure the response body is closed as
    soon as you are done with it, using `defer` immediately after checking the error
    from the HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this corrected example, `defer resp.Body.Close()` is used immediately after
    confirming the request did not fail. This ensures that the body is always closed,
    regardless of how the rest of the function executes (whether it returns early
    due to an error or completes fully).
  prefs: []
  type: TYPE_NORMAL
- en: Channel mismanagement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using unbuffered channels, the `send` operation blocks until another goroutine
    is ready to receive the data. If the receiving goroutine has terminated or fails
    to continue execution to the point of the receive operation (due to a logic error
    or condition), the sending goroutine will be blocked indefinitely. This results
    in both the goroutine and the channel consuming resources indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: Buffered channels allow you to send multiple values without a receiver being
    ready to read immediately. However, if values remain in a channel buffer and there
    are no remaining references to this channel (e.g., all goroutines that could read
    from the channel have finished execution without draining the channel), the data
    remains in memory, leading to a memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, channels are used to control the execution flow of goroutines, such
    as signaling to stop execution. If these channels are not closed or if goroutines
    don’t have a way to exit based on channel input, it might lead to goroutines running
    indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a scenario where a goroutine sends data to a channel that is never
    read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the `produce` goroutine will block indefinitely after
    sending the first integer to the channel because there is no receiver. This causes
    the goroutine and the value in the channel to remain in memory indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 'To manage channels effectively and prevent such leaks, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`select` statements with a default case to avoid blocking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close channels when no longer needed**: This can signal to receiving goroutines
    that no more data will be sent on a channel. However, be careful to ensure that
    no goroutine attempts to send on a closed channel, as this will cause a panic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select` statement can be used with `case` for channel operations and a default
    `case` to handle the scenario where no channels are ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a refined example using a timeout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In general, to prevent resource leaks, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Always defer the closing of resources immediately after their successful creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check for errors that could occur after resource acquisition but before they
    are returned or further used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using patterns such as `defer` inside conditional blocks or immediately
    after checking for a successful resource acquisition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use tools such as static analyzers, which can help catch cases where resources
    are not closed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, learning about everyday problems and pitfalls is more than avoiding
    these features; it is about mastering the language. Think of it as tuning a guitar;
    each string must be adjusted to the right tone. Too tight, and it snaps; too loose,
    and it won’t play. Mastering Go’s and its memory management requires a similar
    touch, ensuring that each component is in harmony to produce the most efficient
    performance. Keep it simple, measure often, and adjust as necessary – your programs
    (and your sanity) will thank you.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective coding practices in Go involve efficient resource management, proper
    synchronization, and avoiding common performance pitfalls. Techniques such as
    reusing resources with `sync.Pool`, ensuring one-time task execution with `sync.Once`,
    preventing redundant operations with `singleflight`, and using memory mapping
    efficiently can significantly enhance application performance. Always be mindful
    of potential issues such as memory leaks, resource mismanagement, and improper
    use of concurrency constructs to maintain optimal performance and resource utilization.
  prefs: []
  type: TYPE_NORMAL
